<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="baidu-site-verification" content="E1Di33CelZ">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="WsojkhmMcOefku3B2Vxp02NtxlUt_JzBP1fVPrFk3Gw">


  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.0.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.4">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.4" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: 'WGLQQAQKBA',
      apiKey: '',
      indexName: 'xiemay',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  



<meta name="baidu-site-verification" content="z0tVDge8Pe">

  
  <meta name="keywords" content="Hexo, NexT">


<meta name="description" content="期待花开">
<meta property="og:type" content="website">
<meta property="og:title" content="welcome">
<meta property="og:url" content="http://xiemaycherry.github.io/page/5/index.html">
<meta property="og:site_name" content="welcome">
<meta property="og:description" content="期待花开">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="welcome">
<meta name="twitter:description" content="期待花开">






  <link rel="canonical" href="http://xiemaycherry.github.io/page/5/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>welcome</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<meta name="baidu-site-verification" content="3BmH9zSH5h"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">welcome</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>
        </li>
      
        
        <li class="menu-item menu-item-留言">
          <a href="/guestbook/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-commenting"></i> <br>留言</a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>Schedule</a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>Sitemap</a>
        </li>
      
        
        <li class="menu-item menu-item-baidusitmap">
          <a href="/baidusitmap.xml" rel="section">
            <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>baidusitmap</a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/05/28/Python-basic/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/28/Python-basic/" itemprop="url">Python Basics</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-28T10:18:04+08:00">2019-05-28</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/编程语言/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/28/Python-basic/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/05/28/Python-basic/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                3.7k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="重新学习"><a href="#重新学习" class="headerlink" title="重新学习"></a>重新学习</h1><p>开始很乱的学习Python，现在想系统学习基础，真正了解pythonic,</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/05/28/Python-basic/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/05/22/Pandas 做数据分析/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/22/Pandas 做数据分析/" itemprop="url">数据分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-22T14:54:12+08:00">2019-05-22</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/数据分析/" itemprop="url" rel="index"><span itemprop="name">数据分析</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/22/Pandas 做数据分析/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/05/22/Pandas 做数据分析/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                15k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<h1 id="Pandas-做数据分析"><a href="#Pandas-做数据分析" class="headerlink" title="Pandas 做数据分析"></a>Pandas 做数据分析</h1><p><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv" target="_blank" rel="noopener">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv</a></p>
<h3 id="Step-1-读取文件"><a href="#Step-1-读取文件" class="headerlink" title="Step 1: 读取文件"></a>Step 1: 读取文件</h3><ul>
<li><p>csv/txt</p>
<ol>
<li><strong>names</strong>: columns，当names没被赋值时，header会变成0，即选取数据文件的第一行作为列名。当 names 被赋值，header 没被赋值时，那么header会变成None。如果都赋值，就会实现两个参数的组合功能。header = 0:是第一行是名字</li>
<li><strong>sep</strong>=‘\t’</li>
<li><strong>header</strong>=None:指定列名，数据开始行数。默认0行;None = 无标题</li>
<li><strong>index_col</strong> :None； 指定列作为行索引 数值。 False表示无索引</li>
<li>usecols ; 如果列有很多，而我们不想要全部的列、而是只要指定的列就可以使用这个参数。</li>
<li><p><strong>prefix</strong> .prefix 参数，当导入的数据没有 header 时，设置此参数会自动加一个前缀。</p>
<p><a href="https://www.jianshu.com/p/42f1d2909bb6" target="_blank" rel="noopener">https://www.jianshu.com/p/42f1d2909bb6</a></p>
</li>
</ol>
</li>
</ul>
<h3 id="Step-2-缺省值处理"><a href="#Step-2-缺省值处理" class="headerlink" title="Step 2: 缺省值处理"></a>Step 2: 缺省值处理</h3><h4 id="1-drop"><a href="#1-drop" class="headerlink" title="1. drop"></a>1. drop</h4><p><code>DataFrame.drop</code><strong>(*</strong>self<strong>*,</strong> <strong>*labels</strong>=None<strong>*,</strong> <strong>*axis</strong>=0<strong>*,</strong> <em>index=None**</em>,<strong> <em>columns=None</em></strong>,<strong> <em>level=None</em></strong>,<strong> <em>inplace=False</em></strong>,<strong> <em>errors=’raise’</em></strong>)**</p>
<p>Parameters</p>
<ul>
<li><p><strong>labels</strong> single label or list-like</p>
<p>Index or column labels to drop.</p>
</li>
<li><p><strong>axis</strong> {0 or ‘index’, 1 or ‘columns’}, default 0</p>
<p>Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’).</p>
</li>
<li><p><strong>index</strong> single label or list-like</p>
<p>Alternative to specifying axis (<code>labels, axis=0</code> is equivalent to <code>index=labels</code>).<em>New in version 0.21.0.</em></p>
</li>
<li><p><strong>columns</strong> single label or list-like</p>
<p>Alternative to specifying axis (<code>labels, axis=1</code> is equivalent to <code>columns=labels</code>).<em>New in version 0.21.0.</em></p>
</li>
<li><p><strong>level </strong>int or level name, optional</p>
<p>For MultiIndex, level from which the labels will be removed.</p>
</li>
<li><p><strong>inplace</strong> bool, default False</p>
<p>If True, do operation inplace and return None.</p>
</li>
<li><p><strong>errors</strong>{‘ignore’, ‘raise’}, default ‘raise’</p>
<p>If ‘ignore’, suppress error and only existing labels are dropped.</p>
</li>
</ul>
<h4 id="2-dropna"><a href="#2-dropna" class="headerlink" title="2. dropna"></a>2. dropna</h4><p><code>DataFrame.dropna</code><strong>(*</strong>self<strong>*,</strong> <strong>*axis</strong>=0<strong>*,</strong> <strong>*how</strong>=’any’<strong>*,</strong> <em>thresh=None**</em>,<strong> *</strong>subset<strong>=None*</strong>,<strong> *</strong>inplace<strong>=False*</strong>)**4</p>
<h4 id="3-isna"><a href="#3-isna" class="headerlink" title="3. isna()"></a>3. isna()</h4><p><code>DataFrame.isna</code><strong>(*</strong>self<strong>*)</strong> </p>
<ol>
<li>填充</li>
</ol>
<h4 id="4-fillna"><a href="#4-fillna" class="headerlink" title="4. fillna()"></a>4. fillna()</h4><p>DataFrame.fillna(<strong>value</strong>=None, <strong>method</strong>=None, <strong>axis</strong>=None, <strong>inplace</strong>=False, limit=None, downcast=None, **kwargs)</p>
<p>填充空值，values可以是字典 <strong>values</strong> = {‘A’: 0, ‘B’: 1, ‘C’: 2, ‘D’: 3} </p>
<p>drop使用</p>
<ol>
<li><p><strong>dropna</strong>删除缺省值 df = df.drop(some labels) df = df.drop(df[<some boolean="" condition="">].index)</some></p>
<p>​    axis: 0(index)</p>
<p>​    how: any all</p>
<p>​    subset: label</p>
<p>​    inplace : bool</p>
</li>
<li><p><strong>drop</strong>删除 drop(labels(index, column labels), axis=0(行), level=None, inplace=False, errors=’raise’)</p>
<p>​    axis: label</p>
<p>​    index, columns </p>
</li>
<li><p><strong>fillna </strong>填充</p>
<p>​    fillna(value, method, limit)</p>
</li>
</ol>
<h3 id="Step-3-选取字段"><a href="#Step-3-选取字段" class="headerlink" title="Step 3: 选取字段"></a>Step 3: 选取字段</h3><ol>
<li>列<ol>
<li>df[labels]</li>
</ol>
</li>
<li>行<ol>
<li>df.loc[index_label,]</li>
<li>df.iloc[整数值,]</li>
</ol>
</li>
</ol>
<p>列[“”] 也可以传入条件语句</p>
<h3 id="描述性统计函数"><a href="#描述性统计函数" class="headerlink" title="描述性统计函数"></a>描述性统计函数</h3><h4 id="sum-mean-count"><a href="#sum-mean-count" class="headerlink" title="sum().mean().count()"></a>sum().mean().count()</h4><p>​        axis:1;按列求和，水平线 。往右看；所有列计算</p>
<p>​        axis:0; 按行求和, 垂直线；把字段的所有的所有行和。往下按 ；所有行计算</p>
<h4 id="describle-transpose"><a href="#describle-transpose" class="headerlink" title=".describle() .transpose()"></a>.describle() .transpose()</h4><h3 id="功能性函数"><a href="#功能性函数" class="headerlink" title="功能性函数"></a>功能性函数</h3><h4 id="groupby"><a href="#groupby" class="headerlink" title="groupby[]."></a>groupby[].</h4><h4 id="多个DataFame归并"><a href="#多个DataFame归并" class="headerlink" title="多个DataFame归并"></a>多个DataFame归并</h4><p>​    ### pd.merge(left, right, how=’inner交集/Outer并集（存在不重合的key是’, on=’key’可以是一个列表[])</p>
<h4 id="apply-方法-传入函数名：然后每一个元素都背计算"><a href="#apply-方法-传入函数名：然后每一个元素都背计算" class="headerlink" title=".apply()方法 传入函数名：然后每一个元素都背计算"></a>.apply()方法 传入函数名：然后每一个元素都背计算</h4><p>​    lambda x : x*x</p>
<h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><p> .sort_values()整个表格都这么排列</p>
<p>  DataFrame.sort_values(by=[lables],axis=[0,1], ascending:, kind:排序算法，)</p>
<h3 id="日期类型-日期等数值处理"><a href="#日期类型-日期等数值处理" class="headerlink" title="日期类型: 日期等数值处理"></a>日期类型: 日期等数值处理</h3><h4 id="str列转换成日期类型-pd-to-datetime"><a href="#str列转换成日期类型-pd-to-datetime" class="headerlink" title="str列转换成日期类型 pd.to_datetime"></a>str列转换成日期类型 pd.to_datetime</h4><p>注意非日期类型的特殊处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df[&apos;日期时间&apos;] = pd.to_datetime(df[&apos;日期时间&apos;],format=&apos;%Y/%m/%d %H:%M:%S&apos;)</span><br><span class="line"> </span><br><span class="line">#获取 日期数据 的年、月、日、时、分</span><br><span class="line">df[&apos;年&apos;] = df[&apos;日期时间&apos;].dt.year</span><br><span class="line">df[&apos;月&apos;] = df[&apos;日期时间&apos;].dt.month</span><br><span class="line">df[&apos;日&apos;] = df[&apos;日期时间&apos;].dt.day</span><br><span class="line">df[&apos;时&apos;] = df[&apos;日期时间&apos;].dt.hour</span><br><span class="line">df[&apos;分&apos;] = df[&apos;日期时间&apos;].dt.minute</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190403095153639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5MTg0Mzg4,size_16,color_FFFFFF,t_70" alt="img"></p>
<h4 id="指定类型"><a href="#指定类型" class="headerlink" title="指定类型"></a>指定类型</h4><p>​    method1 df1[‘year_month’] = df1[‘date’].apply(lambda x : x.strftime(‘%Y-%m’))</p>
<p>​    method2 df1[‘period’] = df1[‘date’].dt.<strong>to_period</strong>(‘M’) 参数 M 表示月份，Q 表示季度，A 表示年度，D 表示按天</p>
<h4 id="strp-ftime"><a href="#strp-ftime" class="headerlink" title="strp/ftime"></a>strp/ftime</h4><p>字符串和日期的转换</p>
<p>strftime: time-&gt;str</p>
<p>strptime: str-&gt;time</p>
<h3 id="datetime-timedelta"><a href="#datetime-timedelta" class="headerlink" title="datetime.timedelta"></a>datetime.timedelta</h3><p>表示时间间隔，两个时间点之间的长度，主要用于时间计算,如时间序列预测的时候，需要外推，可能涉及到时间的计算<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timedelta(weeks=<span class="number">0</span>, days=<span class="number">0</span>, hours=<span class="number">0</span>, minutes=<span class="number">0</span>,  seconds=<span class="number">0</span>, milliseconds=<span class="number">0</span>, microseconds=<span class="number">0</span>, )		 <span class="comment">#依次为 "周" "天", "时","分","秒","毫秒","微秒"</span></span><br></pre></td></tr></table></figure></p>
<h4 id="datetime模块"><a href="#datetime模块" class="headerlink" title="datetime模块"></a>datetime模块</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">类型</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">date</td>
<td style="text-align:left">以公历形式存储日历日期（年、月、日）</td>
</tr>
<tr>
<td style="text-align:left">time</td>
<td style="text-align:left">将时间存储为时、分、秒、毫秒</td>
</tr>
<tr>
<td style="text-align:left">datetime</td>
<td style="text-align:left">存储日期和时间</td>
</tr>
</tbody>
</table>
</div>
<p><strong>1）python标准库函数</strong></p>
<p>日期转换成字符串：利用str 或strftime</p>
<p>字符串转换成日期：datetime.strptime</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">stamp = datetime(<span class="number">2017</span>,<span class="number">6</span>,<span class="number">27</span>)</span><br><span class="line">str(stamp)</span><br><span class="line"> <span class="string">'2017-06-27 00:00:00'</span></span><br><span class="line">stamp.strftime(<span class="string">'%y-%m-%d'</span>)<span class="comment">#%Y是4位年，%y是2位年</span></span><br><span class="line"> <span class="string">'17-06-27'</span></span><br><span class="line"><span class="comment">#对多个时间进行解析成字符串</span></span><br><span class="line">date = [<span class="string">'2017-6-26'</span>,<span class="string">'2017-6-27'</span>]</span><br><span class="line">datetime2 = [datetime.strptime(x,<span class="string">'%Y-%m-%d'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> date]</span><br><span class="line">datetime2</span><br><span class="line">[datetime.datetime(<span class="number">2017</span>, <span class="number">6</span>, <span class="number">26</span>, <span class="number">0</span>, <span class="number">0</span>), datetime.datetime(<span class="number">2017</span>, <span class="number">6</span>, <span class="number">27</span>, <span class="number">0</span>, <span class="number">0</span>)]</span><br></pre></td></tr></table></figure>
<p><strong>3）pandas处理成组日期</strong></p>
<p>pandas通常用于处理成组日期，不管这些日期是DataFrame的轴索引还是列，to_datetime方法可以解析多种不同的日期表示形式。</p>
<p><strong>datetime 格式定义</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">代码</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">%Y</td>
<td style="text-align:left">4位数的年</td>
</tr>
<tr>
<td style="text-align:left">%y</td>
<td style="text-align:left">2位数的年</td>
</tr>
<tr>
<td style="text-align:left">%m</td>
<td style="text-align:left">2位数的月[01,12]</td>
</tr>
<tr>
<td style="text-align:left">%d</td>
<td style="text-align:left">2位数的日[01，31]</td>
</tr>
<tr>
<td style="text-align:left">%H</td>
<td style="text-align:left">时（24小时制）[00,23]</td>
</tr>
<tr>
<td style="text-align:left">%l</td>
<td style="text-align:left">时（12小时制）[01,12]</td>
</tr>
<tr>
<td style="text-align:left">%M</td>
<td style="text-align:left">2位数的分[00,59]</td>
</tr>
<tr>
<td style="text-align:left">%S</td>
<td style="text-align:left">秒[00,61]有闰秒的存在</td>
</tr>
<tr>
<td style="text-align:left">%w</td>
<td style="text-align:left">用整数表示的星期几[0（星期天），6]</td>
</tr>
<tr>
<td style="text-align:left">%F</td>
<td style="text-align:left">%Y-%m-%d简写形式例如，2017-06-27</td>
</tr>
<tr>
<td style="text-align:left">%D</td>
<td style="text-align:left">%m/%d/%y简写形式</td>
</tr>
</tbody>
</table>
</div>
<h4 id="字符串转换成datetime格式-strptime"><a href="#字符串转换成datetime格式-strptime" class="headerlink" title="字符串转换成datetime格式: strptime"></a>字符串转换成datetime格式: strptime</h4><p>datetime.strptime(str, ‘%Y/%m/%d’).date()</p>
<h4 id="datetime变回string格式-strftime"><a href="#datetime变回string格式-strftime" class="headerlink" title="datetime变回string格式: strftime"></a>datetime变回string格式: strftime</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;&quot;y&quot;: [1, 2, 3]&#125;,</span><br><span class="line">...                   index=pd.to_datetime([&quot;2000-03-31 00:00:00&quot;,</span><br><span class="line">...                                         &quot;2000-05-31 00:00:00&quot;,</span><br><span class="line">...                                         &quot;2000-08-31 00:00:00&quot;]))</span><br><span class="line">&gt;&gt;&gt; df.index.to_period(&quot;M&quot;)</span><br><span class="line">PeriodIndex([&apos;2000-03&apos;, &apos;2000-05&apos;, &apos;2000-08&apos;],</span><br><span class="line">            dtype=&apos;period[M]&apos;, freq=&apos;M&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="1—pd-Period-参数：一个时间戳生成器"><a href="#1—pd-Period-参数：一个时间戳生成器" class="headerlink" title="1—pd.Period()参数：一个时间戳生成器"></a>1—pd.Period()参数：一个时间戳生成器</h4><p><img src="https://upload-images.jianshu.io/upload_images/5798142-c03611e314662be4.png?imageMogr2/auto-orient/strip|imageView2/2/w/966/format/webp" alt="img"></p>
<h3 id="Step-4-数据透析表-pivot-table"><a href="#Step-4-数据透析表-pivot-table" class="headerlink" title="Step 4: 数据透析表 pivot_table"></a>Step 4: 数据透析表 pivot_table</h3><h4 id="pivot-table-数据透析表-分类汇总的统计数据"><a href="#pivot-table-数据透析表-分类汇总的统计数据" class="headerlink" title=".pivot_table 数据透析表 分类汇总的统计数据"></a>.pivot_table 数据透析表 分类汇总的统计数据</h4><p>​     (data,values= column to aggregate optional, index = grouper, columns=grouper, aggfunc:np.sum  )</p>
<p>​     table = pd.pivot_table(df, values=[‘D’, ‘E’], index=[‘A’, ‘C’], aggfunc={‘D’: np.mean, ‘E’: np.mean})</p>
<h3 id="groupby-1"><a href="#groupby-1" class="headerlink" title=".groupby()"></a>.groupby()</h3><p><img src="https://upload-images.jianshu.io/upload_images/2862169-51af7d4ae64c2f78.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img"></p>
<p>由于通过<code>groupby()</code>函数分组得到的是一个<code>DataFrameGroupBy</code>对象，而通过对这个对象调用<code>get_group()</code>，返回的则是一个·DataFrame·对象，所以可以将<code>DataFrameGroupBy</code>对象理解为是多个<code>DataFrame</code>组成的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grouped = df.groupby(<span class="string">'Gender'</span>)</span><br><span class="line">grouped_muti = df.groupby([<span class="string">'Gender'</span>, <span class="string">'Age'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(grouped.get_group(<span class="string">'Female'</span>))</span><br><span class="line">print(grouped_muti.get_group((<span class="string">'Female'</span>, <span class="number">17</span>)))</span><br><span class="line"></span><br><span class="line">    Name  Gender  Age  Score</span><br><span class="line"><span class="number">2</span>   Cidy  Female   <span class="number">18</span>     <span class="number">93</span></span><br><span class="line"><span class="number">4</span>  Ellen  Female   <span class="number">17</span>     <span class="number">96</span></span><br><span class="line"><span class="number">7</span>   Hebe  Female   <span class="number">22</span>     <span class="number">98</span></span><br><span class="line">    Name  Gender  Age  Score</span><br><span class="line"><span class="number">4</span>  Ellen  Female   <span class="number">17</span>     <span class="number">96</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">print(grouped.count())</span><br><span class="line">print(grouped.max()[[<span class="string">'Age'</span>, <span class="string">'Score'</span>]])</span><br><span class="line">print(grouped.mean()[[<span class="string">'Age'</span>, <span class="string">'Score'</span>]])</span><br><span class="line">        Name  Age  Score</span><br><span class="line">Gender                  </span><br><span class="line">Female     <span class="number">3</span>    <span class="number">3</span>      <span class="number">3</span></span><br><span class="line">Male       <span class="number">5</span>    <span class="number">5</span>      <span class="number">5</span></span><br><span class="line">        Age  Score</span><br><span class="line">Gender            </span><br><span class="line">Female   <span class="number">22</span>     <span class="number">98</span></span><br><span class="line">Male     <span class="number">21</span>    <span class="number">100</span></span><br><span class="line">         Age      Score</span><br><span class="line">Gender                 </span><br><span class="line">Female  <span class="number">19.0</span>  <span class="number">95.666667</span></span><br><span class="line">Male    <span class="number">19.6</span>  <span class="number">89.000000</span></span><br></pre></td></tr></table></figure>
<p>如果其中的函数无法满足你的需求，你也可以选择使用聚合函数<code>aggregate</code>，传递<code>numpy</code>或者自定义的函数，前提是返回一个聚合值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSum</span><span class="params">(data)</span>:</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        total+=d</span><br><span class="line">    <span class="keyword">return</span> total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(grouped.aggregate(np.median))</span><br><span class="line">print(grouped.aggregate(&#123;<span class="string">'Age'</span>:np.median, <span class="string">'Score'</span>:np.sum&#125;))</span><br><span class="line">print(grouped.aggregate(&#123;<span class="string">'Age'</span>:getSum&#125;))</span><br></pre></td></tr></table></figure>
<p>迭代</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">grouped = df.groupby(<span class="string">'A'</span>)</span><br><span class="line"><span class="keyword">for</span> name, group <span class="keyword">in</span> grouped:</span><br><span class="line">	    print(name)</span><br><span class="line">  		print(group)</span><br><span class="line">        </span><br><span class="line">bar</span><br><span class="line">     A      B         C         D</span><br><span class="line"><span class="number">1</span>  bar    one  <span class="number">0.254161</span>  <span class="number">1.511763</span></span><br><span class="line"><span class="number">3</span>  bar  three  <span class="number">0.215897</span> <span class="number">-0.990582</span></span><br><span class="line"><span class="number">5</span>  bar    two <span class="number">-0.077118</span>  <span class="number">1.211526</span></span><br><span class="line">foo</span><br><span class="line">     A      B         C         D</span><br><span class="line"><span class="number">0</span>  foo    one <span class="number">-0.575247</span>  <span class="number">1.346061</span></span><br><span class="line"><span class="number">2</span>  foo    two <span class="number">-1.143704</span>  <span class="number">1.627081</span></span><br><span class="line"><span class="number">4</span>  foo    two  <span class="number">1.193555</span> <span class="number">-0.441652</span></span><br><span class="line"><span class="number">6</span>  foo    one <span class="number">-0.408530</span>  <span class="number">0.268520</span></span><br><span class="line"><span class="number">7</span>  foo  three <span class="number">-0.862495</span>  <span class="number">0.024580</span></span><br></pre></td></tr></table></figure>
<p>可视化</p>
<p>对组内的数据绘制概率密度分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grouped[<span class="string">'Age'</span>].plot(kind=<span class="string">'kde'</span>, legend=<span class="keyword">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>计算不同组的某一列的值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">'race'</span>)[<span class="string">'age'</span>].mean()</span><br><span class="line">要求被不同种族内被击毙人员年龄的均值:</span><br></pre></td></tr></table></figure>
<p>对不同取值的计数: <code>.value_counts()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">'race'</span>)[<span class="string">'signs_of_mental_illness'</span>].value_counts()</span><br><span class="line">求不同种族内, 是否有精神异常迹象的分别有多少人</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">'race'</span>)[<span class="string">'signs_of_mental_illness'</span>].value_counts().unstack()</span><br><span class="line">组内操作的结果不是单个值, 是一个序列, 我们可以用.unstack()将它展开，得到DateFrame</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">'race'</span>)[<span class="string">'flee'</span>].value_counts().unstack().plot(kind=<span class="string">'bar'</span>, figsize=(<span class="number">20</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://upload-images.jianshu.io/upload_images/2862169-e149898f78d62a81.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img"></p>
<p>这里有一个之前介绍的<code>.unstack</code>操作, 这会让你得到一个<code>DateFrame</code>, 然后调用条形图, pandas就会遍历每一个组(<code>unstack</code>后为每一行), 然后作各组的条形图</p>
<h3 id="按不同逃逸类型分组-组内的年龄分布是如何的"><a href="#按不同逃逸类型分组-组内的年龄分布是如何的" class="headerlink" title="按不同逃逸类型分组, 组内的年龄分布是如何的?"></a>按不同逃逸类型分组, 组内的年龄分布是如何的?</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">'flee'</span>)[<span class="string">'age'</span>].plot(kind=<span class="string">'kde'</span>, legend=<span class="keyword">True</span>, figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://upload-images.jianshu.io/upload_images/2862169-1a6135108e73ebc2.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img"></p>
<p>这里<code>data.groupby(&#39;flee&#39;)[&#39;age&#39;]</code>是一个<code>SeriesGroupby</code>对象, 顾名思义, 就是每一个组都有一个<code>Series</code>. 因为划分了不同逃逸类型的组, 每一组包含了组内的年龄数据, 所以直接<code>plot</code>相当于遍历了每一个逃逸类型, 然后分别画分布图.</p>
<h3 id="Step-5-写入表格"><a href="#Step-5-写入表格" class="headerlink" title="Step 5: 写入表格"></a>Step 5: 写入表格</h3><p>to_csv（path_or_buf，sep，header: <strong>bool</strong> or list of str : default：true， index: <strong>bool</strong>, default true）</p>
<h2 id="字符串处理"><a href="#字符串处理" class="headerlink" title="字符串处理"></a>字符串处理</h2><h3 id="多个字符串分割"><a href="#多个字符串分割" class="headerlink" title="多个字符串分割"></a>多个字符串分割</h3><p>Python中的spilt方法只能通过指定的某个字符分割字符串，如果需要指定多个字符，需要用到re模块里的split方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import re</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a = &quot;Hello world!How are you?My friend.Tom&quot;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; re.split(&quot; |!|\?|\.&quot;, a)</span><br><span class="line"></span><br><span class="line">[&apos;Hello&apos;, &apos;world&apos;, &apos;How&apos;, &apos;are&apos;, &apos;you&apos;, &apos;My&apos;, &apos;friend&apos;, &apos;Tom&apos;]</span><br></pre></td></tr></table></figure>
<h3 id="去掉多余空格"><a href="#去掉多余空格" class="headerlink" title="去掉多余空格"></a>去掉多余空格</h3><ol>
<li>filter </li>
</ol>
<p>aStr_splited = aStr.split(‘ ‘) </p>
<p>print(filter(lambda x : x, aStr_splited))</p>
<p>list(filter(None,s.split(‘,’)))</p>
<ol>
<li><p>列表</p>
<p>[x for x in s.split(‘,’) if x]</p>
</li>
</ol>
<h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p><a href="https://docs.python.org/zh-cn/3/library/re.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/re.html</a></p>
<p>正则表达式：一种特殊的字符串，用于查找某种形式的字符串，满足某种条件的格式。</p>
<p>用于查找，匹配</p>
<p>re模块</p>
<p>re.</p>
<p>pattern</p>
<p>​    [a-z] [abc]</p>
<p>​    <sup><a href="#fn_ab" id="reffn_ab">ab</a></sup></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\d</span><br></pre></td></tr></table></figure>
<p>匹配任何十进制数字；这等价于类 <code>[0-9]</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\D</span><br></pre></td></tr></table></figure>
<p>匹配任何非数字字符；这等价于类 <code>[^0-9]</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\s</span><br></pre></td></tr></table></figure>
<p>匹配任何空白字符；这等价于类 <code>[ \t\n\r\f\v]</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\S</span><br></pre></td></tr></table></figure>
<p>匹配任何非空白字符；这相当于类 <code>[^ \t\n\r\f\v]</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\w</span><br></pre></td></tr></table></figure>
<p>匹配任何字母与数字字符；这相当于类 <code>[a-zA-Z0-9_]</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\W</span><br></pre></td></tr></table></figure>
<p>匹配任何非字母与数字字符；这相当于类 <code>[^a-zA-Z0-9_]</code>。</p>
<p><code>ca*t</code> 将匹配 <code>&#39;ct&#39;</code> (0个 <code>&#39;a&#39;</code> 字符)，<code>&#39;cat&#39;</code> (1个 <code>&#39;a&#39;</code> )， <code>&#39;caaat&#39;</code> (3个 <code>&#39;a&#39;</code> 字符)</p>
<p>另一个重复的元字符是 <code>+</code>，它匹配一次或多次。 要特别注意 <code>*</code> 和 <code>+</code> 之间的区别；<code>*</code> 匹配 <em>零次</em> 或更多次，因此重复的任何东西都可能根本不存在，而 <code>+</code> 至少需要 <em>一次</em>。 使用类似的例子，<code>ca+t</code> 将匹配 <code>&#39;cat&#39;</code> (1 个 <code>&#39;a&#39;</code>)，<code>&#39;caaat&#39;</code> (3 个 <code>&#39;a&#39;</code>)，但不会匹配 <code>&#39;ct&#39;</code>。</p>
<p>最复杂的重复限定符是 <code>{m,n}</code>，其中 <em>m</em> 和 <em>n</em> 是十进制整数。 这个限定符意味着必须至少重复 <em>m</em> 次，最多重复 <em>n</em> 次。 例如，<code>a/{1,3}b</code> 将匹配 <code>&#39;a/b&#39;</code> ，<code>&#39;a//b&#39;</code> 和 <code>&#39;a///b&#39;</code> 。 它不匹配没有斜线的 <code>&#39;ab&#39;</code>，或者有四个的 <code>&#39;a////b&#39;</code>。</p>
<p><code>^</code>表示行的开头，<code>^\d</code>表示必须以数字开头。</p>
<p><script type="math/tex">`表示行的结束，`\d</script>表示必须以数字结束。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">方法 / 属性</th>
<th style="text-align:left">目的</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>match()</code></td>
<td style="text-align:left">确定正则是否从字符串的开头匹配。</td>
</tr>
<tr>
<td style="text-align:left"><code>search()</code></td>
<td style="text-align:left">扫描字符串，查找此正则匹配的任何位置。</td>
</tr>
<tr>
<td style="text-align:left"><code>findall()</code></td>
<td style="text-align:left">找到正则匹配的所有子字符串，并将它们作为列表返回。</td>
</tr>
<tr>
<td style="text-align:left"><code>finditer()</code></td>
<td style="text-align:left">找到正则匹配的所有子字符串，并将它们返回为一个 <a href="https://docs.python.org/zh-cn/3/glossary.html#term-iterator" target="_blank" rel="noopener">iterator</a>。</td>
</tr>
</tbody>
</table>
</div>
<p>匹配对象返回值的函数</p>
<p>方法 / 属性</p>
<p>目的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">group()</span><br></pre></td></tr></table></figure>
<p>返回正则匹配的字符串</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start()</span><br></pre></td></tr></table></figure>
<p>返回匹配的开始位置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">end()</span><br></pre></td></tr></table></figure>
<p>返回匹配的结束位置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">span()</span><br></pre></td></tr></table></figure>
<p>返回包含匹配 (start, end) 位置的元组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; m.group()</span><br><span class="line">&apos;tempo&apos;</span><br><span class="line">&gt;&gt;&gt; m.start(), m.end()</span><br><span class="line">(0, 5)</span><br><span class="line">&gt;&gt;&gt; m.span()</span><br><span class="line">(0, 5)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>re.``search</code>(<em>pattern</em>, <em>string</em>, <em>flags=0</em>)<a href="https://docs.python.org/zh-cn/3/library/re.html#re.search" target="_blank" rel="noopener">¶</a></p>
<p>扫描整个 <em>字符串</em> 找到匹配样式的第一个位置，并返回一个相应的 <a href="https://docs.python.org/zh-cn/3/library/re.html#match-objects" target="_blank" rel="noopener">匹配对象</a>。如果没有匹配，就返回一个 <code>None</code> ； 注意这和找到一个零长度匹配是不同的。</p>
</li>
<li><p><code>re.``match</code>(<em>pattern</em>, <em>string</em>, <em>flags=0</em>)</p>
<p>如果 <em>string</em> 开始的0或者多个字符匹配到了正则表达式样式，就返回一个相应的 <a href="https://docs.python.org/zh-cn/3/library/re.html#match-objects" target="_blank" rel="noopener">匹配对象</a> 。 如果没有匹配，就返回 <code>None</code> ；注意它跟零长度匹配是不同的。</p>
</li>
</ul>
<h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>import matplotlib.pyplot as plt</p>
<h3 id="plt-figure-画布"><a href="#plt-figure-画布" class="headerlink" title="plt.figure() 画布"></a>plt.figure() 画布</h3><p><img src="https://pic3.zhimg.com/80/v2-307d460860c1df3e253e2ccaee95b41a_720w.jpg" alt="img"></p>
<h3 id="plt-subplot-划分子图"><a href="#plt-subplot-划分子图" class="headerlink" title="plt.subplot() 划分子图"></a>plt.subplot() 划分子图</h3><h3 id="基本属性"><a href="#基本属性" class="headerlink" title="基本属性"></a>基本属性</h3><h5 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h5><p><img src="https://pic4.zhimg.com/80/v2-2f83a0a82bffecc371460b865de580db_720w.jpg" alt="img"></p>
<p><img src="https://pic1.zhimg.com/80/v2-fc32e879695684b26c22273da2d9f904_720w.jpg" alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-1f9b00e3c04ded54f8b8197d512e65cb_720w.jpg" alt="img"></p>
<h5 id="market"><a href="#market" class="headerlink" title="market"></a>market</h5><p><img src="https://pic4.zhimg.com/80/v2-18cee5ad639c2db35ef81b3a46afc8fb_720w.jpg" alt="img"></p>
<h5 id="linstyle"><a href="#linstyle" class="headerlink" title="linstyle"></a>linstyle</h5><ul>
<li>plt.text() 设置图内文本</li>
</ul>
<h5 id="轴标签"><a href="#轴标签" class="headerlink" title="轴标签"></a>轴标签</h5><ul>
<li>plt.xlabel() 设置坐标轴标签</li>
<li>plt.ylabel()</li>
</ul>
<h5 id="范围"><a href="#范围" class="headerlink" title="范围"></a>范围</h5><ul>
<li><p>plt.xlim() 设置坐标取值范围 元组</p>
</li>
<li><p>plt.ylim()</p>
</li>
<li><p>plt.imshow()</p>
</li>
<li><p>plt.axis(“off”)</p>
</li>
</ul>
<h3 id="设置记号刻度-刻度标签"><a href="#设置记号刻度-刻度标签" class="headerlink" title="设置记号刻度 刻度标签"></a>设置记号刻度 刻度标签</h3><p>plt.xticks（[-np.pi，-np.pi / 2,0，np.pi / 2，np.pi]） </p>
<p>plt.yticks（[ -  1，0，+1]）</p>
<p>plt.xticks(new_ticks)<br>plt.yticks([-2, -1.8, -1, 1.22, 3],<br>           [r’$really\ bad$’, r’$bad$’, r’$normal\ \alpha$’, r’$good$’, r’$really\ good$’])</p>
<h3 id="设置记号标签"><a href="#设置记号标签" class="headerlink" title="设置记号标签"></a>设置记号标签</h3><p>plt.xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi],       [r’$-\pi$’, r’$-\pi/2$’, r’$0$’, r’$+\pi/2$’, r’$+\pi$’]) plt.yticks([-1, 0, +1],       [r’$-1$’, r’$0$’, r’$+1$’])</p>
<h3 id="设置坐标"><a href="#设置坐标" class="headerlink" title="设置坐标"></a>设置坐标</h3><p>plt.plot([],[],)</p>
<p>linestyle marker marketsize</p>
<p><a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html" target="_blank" rel="noopener">https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th><a href="https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_linestyle" target="_blank" rel="noopener"><code>linestyle</code></a> or ls</th>
<th>{‘-‘, ‘—‘, ‘-.’, ‘:’, ‘’, (offset, on-off-seq), …}</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_linewidth" target="_blank" rel="noopener"><code>linewidth</code></a> or lw</td>
<td>float</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_marker" target="_blank" rel="noopener"><code>marker</code></a></td>
<td>marker style</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_markeredgecolor" target="_blank" rel="noopener"><code>markeredgecolor</code></a> or mec</td>
<td>color</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_markeredgewidth" target="_blank" rel="noopener"><code>markeredgewidth</code></a> or mew</td>
<td>float</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_markerfacecolor" target="_blank" rel="noopener"><code>markerfacecolor</code></a> or mfc</td>
<td>color</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_markerfacecoloralt" target="_blank" rel="noopener"><code>markerfacecoloralt</code></a> or mfcalt</td>
<td>color</td>
</tr>
<tr>
<td><a href="https://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_markersize" target="_blank" rel="noopener"><code>markersize</code></a> or ms</td>
<td>float</td>
</tr>
</tbody>
</table>
</div>
<p>plt.axis([2011,2014,0.04,0.18])</p>
<p>plt.xticks(np.arange(2011,2015,1)) plt.yticks(np.arange(0.04,0.20,0.02))</p>
<p>plt.ylabel(“RMSE”) </p>
<p>plt.xlabel(“(a) LSTM”) </p>
<p>plt.grid(True）</p>
<p>plt.figure(2) #plt.subplot(222)</p>
<h2 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.plot(subplots=True, figsize=(6, 6)); plt.legend(loc=&apos;best&apos;)</span><br></pre></td></tr></table></figure>
<p><strong>data</strong> : DataFrame</p>
<p><strong>x</strong> : label or position, default None</p>
<p><strong>y</strong> : label or position, default None</p>
<blockquote>
<p>Allows plotting of one column versus another</p>
</blockquote>
<p><strong>kind</strong> : str</p>
<blockquote>
<ul>
<li>‘line’ : line plot (default)</li>
<li>‘bar’ : vertical bar plot</li>
<li>‘barh’ : horizontal bar plot</li>
<li>‘hist’ : histogram</li>
<li>‘box’ : boxplot</li>
<li>‘kde’ : Kernel Density Estimation plot</li>
<li>‘density’ : same as ‘kde’</li>
<li>‘area’ : area plot</li>
<li>‘pie’ : pie plot</li>
<li>‘scatter’ : scatter plot</li>
<li>‘hexbin’ : hexbin plot</li>
</ul>
</blockquote>
<p><strong>ax</strong> : matplotlib axes object, default None</p>
<h3 id="时间序列"><a href="#时间序列" class="headerlink" title="时间序列"></a>时间序列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.dates <span class="keyword">as</span> mdates</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">months = mdates.MonthLocator()</span><br><span class="line"></span><br><span class="line">dateFmt = mdates.DateFormatter(<span class="string">"%m/%d/%y"</span>)</span><br><span class="line"></span><br><span class="line">ax.xaxis.set_major_formatter(dateFmt)</span><br><span class="line">ax.xaxis.set_minor_locator(months)</span><br><span class="line">ax.tick_params(axis=<span class="string">"both"</span>, direction=<span class="string">"out"</span>, labelsize=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">date1 = datetime.date(<span class="number">2005</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">date2 = datetime.date(<span class="number">2015</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">delta = datetime.timedelta(days=<span class="number">5</span>)</span><br><span class="line">dates = mdates.drange(date1, date2, delta)</span><br><span class="line"></span><br><span class="line">y = np.random.normal(<span class="number">100</span>, <span class="number">15</span>, len(dates))</span><br><span class="line"></span><br><span class="line">ax.plot_date(dates, y, <span class="string">"#FF8800"</span>, alpha=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">fig.autofmt_xdate()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="plot"><a href="#plot" class="headerlink" title="plot()"></a>plot()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">DataFrame.plot(self, *args, **kwargs)</span><br><span class="line">kindstr</span><br><span class="line">The kind of plot to produce:</span><br><span class="line">‘line’ : line plot (default)</span><br><span class="line">‘bar’ : vertical bar plot</span><br><span class="line">‘barh’ : horizontal bar plot</span><br><span class="line">‘hist’ : histogram</span><br><span class="line">‘box’ : boxplot</span><br><span class="line">‘kde’ : Kernel Density Estimation plot</span><br><span class="line">‘density’ : same as ‘kde’</span><br><span class="line">‘area’ : area plot</span><br><span class="line">‘pie’ : pie plot</span><br><span class="line">‘scatter’ : scatter plot</span><br><span class="line">‘hexbin’ : hexbin plot.</span><br><span class="line"></span><br><span class="line">figsizea tuple (width, height) in inches</span><br><span class="line">x :label</span><br><span class="line">y:label</span><br><span class="line">xlim:</span><br><span class="line">xticks:</span><br><span class="line">title</span><br><span class="line"></span><br><span class="line">pandas.DataFrame.plot.bar</span><br><span class="line">pandas.DataFrame.plot.barh</span><br><span class="line">pandas.DataFrame.plot.box</span><br><span class="line">pandas.DataFrame.plot.density</span><br><span class="line">pandas.DataFrame.plot.hexbin</span><br><span class="line">pandas.DataFrame.plot.hist</span><br><span class="line">pandas.DataFrame.plot.kde</span><br><span class="line">pandas.DataFrame.plot.line</span><br><span class="line">pandas.DataFrame.plot.pie</span><br><span class="line">pandas.DataFrame.plot.scatter</span><br><span class="line">pandas.DataFrame.boxplot</span><br><span class="line">pandas.DataFrame.hist</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/fengbingchun/article/details/81035861?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase" target="_blank" rel="noopener">https://blog.csdn.net/fengbingchun/article/details/81035861?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase</a></p>
<p><a href="https://www.cnblogs.com/Summer-skr--blog/p/11705925.html" target="_blank" rel="noopener">https://www.cnblogs.com/Summer-skr--blog/p/11705925.html</a></p>
<h3 id="时间序列绘图"><a href="#时间序列绘图" class="headerlink" title="时间序列绘图"></a>时间序列绘图</h3><p>坐标轴xtick设置方法<br>plt.xticks(statiem,[datetime.strftime(x,’%Y-%m’) for x in statiem])</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><h3 id="list-dict-的拷贝"><a href="#list-dict-的拷贝" class="headerlink" title="list() dict()的拷贝"></a>list() dict()的拷贝</h3><p>1、<strong>b = a:</strong> 赋值引用，a 和 b 都指向同一个对象。</p>
<p><img src="https://www.runoob.com/wp-content/uploads/2017/03/1489720931-7116-4AQC6.png" alt="img"></p>
<p><strong>2、b = a.copy():</strong> 浅拷贝, a 和 b 是一个独立的对象，但他们的子对象还是指向统一对象（是引用）。</p>
<p><img src="https://www.runoob.com/wp-content/uploads/2017/03/1489720930-6827-Vtk4m.png" alt="img"></p>
<p><strong>b = copy.deepcopy(a):</strong> 深度拷贝, a 和 b 完全拷贝了父对象及其子对象，两者是完全独立的。</p>
<p><img src="https://www.runoob.com/wp-content/uploads/2017/03/1489720930-5882-BO4qO.png" alt="img"></p>
<h2 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h2><ol>
<li>df[-1:] <em>#最后一行</em></li>
<li>df[-3:-1] *#倒数第3行到倒数第1行（不包含最后1行即倒数第1行）</li>
</ol>
<h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h2 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h2><p>linux后台执行命令：&amp;和nohup</p>
<p>用途：不挂断地运行命令。</p>
<p>语法：nohup Command [ Arg … ] [　&amp; ]</p>
<p>例子： nohup sh example.sh &amp;</p>
<p><code>nohup</code> 命令可以使命令永久的执行下去，和终端没有关系，退出终端也不会影响程序的运行；<br><code>&amp;</code> 是后台运行的意思，但当用户退出的时候，命令自动也跟着退出。<br><strong>那么，把两个结合起来<code>nohup 命令 &amp;</code>这样就能使命令永久的在后台执行</strong></p>
<p><code>nohup 命令 &gt; output.log 2&gt;&amp;1 &amp;</code>让命令在后台执行。</p>
<p>其中 0、1、2分别代表如下含义：<br>0 – stdin (standard input)<br>1 – stdout (standard output)<br>2 – stderr (standard error)</p>
<p><code>nohup</code>+最后面的<code>&amp;</code>是让命令在后台执行</p>
<p><code>&gt;output.log</code> 是将信息输出到output.log日志中</p>
<p><code>2&gt;&amp;1</code>是将标准错误信息转变成标准输出，这样就可以将错误信息输出到output.log 日志里面来。</p>
<p>&amp; 后台执行<br>>  输出到<br>不过联合使用也有其他意思，比如nohup输出重定向上的应用<br>例子：nohup abc.sh &gt; nohup.log 2&gt;&amp;1 &amp;<br>其中2&gt;&amp;1  指将<a href="https://www.baidu.com/s?wd=STDERR&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">STDERR</a>重定向到前面标准输出定向到的同名文件中，即&amp;1就是nohup.log</p>
<h2 id="ps-ef-grep-python"><a href="#ps-ef-grep-python" class="headerlink" title="ps -ef|grep python"></a>ps -ef|grep python</h2><p>ps命令将某个进程显示出来</p>
<p>grep命令是查找</p>
<p>中间的|是管道命令 是指ps命令与grep同时执行</p>
<p>PS是LINUX下最常用的也是非常强大的进程查看命令</p>
<p>grep命令 是查找， 是一种强大的文本搜索工具，它能 <a href="https://www.baidu.com/s?wd=使用正则表达式&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1d9uWD3PhP9n1b4m1nduAcz0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPjfvrHnzPWT3" target="_blank" rel="noopener">使用正则表达式 </a>搜索文本，并把匹 配的行打印出来。</p>
<p>grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。</p>
<p>以下这条命令是检查 java 进程是否存在：ps -ef |grep java</p>
<p>字段含义如下：<br>UID    PID    PPID    C   STIME   TTY    TIME     CMD</p>
<p>zzw   14124  13991   0   00:38   pts/0   00:00:00  grep —color=auto dae</p>
<p><strong>UID   ：程序被该 UID 所拥有</strong></p>
<p><strong>PID   ：就是这个程序的 ID</strong> </p>
<p><strong>PPID  ：则是其上级父程序的ID</strong></p>
<p><strong>C     ：CPU使用的资源百分比</strong></p>
<p><strong>STIME ：系统启动时间</strong></p>
<p><strong>TTY   ：登入者的终端机位置</strong></p>
<p><strong>TIME  ：使用掉的CPU时间。</strong></p>
<p><strong>CMD  ：所下达的是什么指令</strong></p>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/05/12/Deel Learning ai_Convolutional Neural Networks/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/" itemprop="url">Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-12T19:31:06+08:00">2019-05-12</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                14k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="C4-Convolutional-Neural-Networks-卷积神经网络"><a href="#C4-Convolutional-Neural-Networks-卷积神经网络" class="headerlink" title="C4 : Convolutional Neural Networks(卷积神经网络)"></a>C4 : Convolutional Neural Networks(卷积神经网络)</h1><h2 id="W1-Convolutional-Neural-Networks-卷积神经网络"><a href="#W1-Convolutional-Neural-Networks-卷积神经网络" class="headerlink" title="W1 :Convolutional Neural Networks(卷积神经网络)"></a>W1 :Convolutional Neural Networks(卷积神经网络)</h2><h3 id="L1-Computer-Vision"><a href="#L1-Computer-Vision" class="headerlink" title="L1: Computer Vision"></a>L1: Computer Vision</h3><ol>
<li>Image classification</li>
<li>Object detection</li>
<li>Neural Style Transfer</li>
</ol>
<p>Problem : input big</p>
<ol>
<li>神经网络结构复杂，数据量相对较少，容易出现过拟合；</li>
<li>所需内存和计算量巨大。</li>
</ol>
<h3 id="L2-Edge-detection-example"><a href="#L2-Edge-detection-example" class="headerlink" title="L2: Edge detection example"></a>L2: Edge detection example</h3><p>我们之前提到过，神经网络由浅层到深层，分别可以检测出图片的边缘特征、局部特征（例如眼睛、鼻子等），到最后面的一层就可以根据前面检测的特征来识别整体面部轮廓。这些工作都是依托卷积神经网络来实现的。</p>
<p><strong>卷积运算（Convolutional Operation）</strong>是卷积神经网络最基本的组成部分。我们以边缘检测为例，来解释卷积是怎样运算的。</p>
<ol>
<li><p>常见的边缘检测</p>
<p>垂直边缘（Vertical Edges) 和 水平边缘（horizontal Edges)</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Different-edges.png" alt=""></p>
</li>
</ol>
<p>这张图的栏杆就对应垂直线，栏杆的水平线是水平边缘。</p>
<p>那么图片是怎么检测边缘的呢？</p>
<p>过滤器：filter</p>
<p>在数学中“”就是卷积的标准标志，但是在<strong>Python</strong>中，这个标识常常被用来表示乘法或者元素乘法。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_1.png" alt=""></p>
<p>Output; 4 by 4</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_1——2.png" alt=""></p>
<p>具体运算：</p>
<p>1）</p>
<p>为了计算第一个元素，在4×4左上角的那个元素，使用3×3的过滤器，将其覆盖在输入图像，如下图所示。然后进行元素乘法（<strong>element-wise products</strong>）运算</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_1_3png.png" alt=""></p>
<p>2）为了弄明白第二个元素是什么，你要把蓝色的方块，向右移动一步，像这样，把这些绿色的标记去掉：</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_1_4png.png" alt=""></p>
<p>6×6矩阵和3×3矩阵进行卷积运算得到4×4矩阵。这些图片和过滤器是不同维度的矩阵，但左边矩阵容易被理解为一张图片，中间的这个被理解为过滤器，右边的图片我们可以理解为另一张图片。这个就是垂直边缘检测器。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Convolutional-operation.jpg" alt=""></p>
<p>举例说明： Vertical edge detection</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_1_5.png" alt=""></p>
<p>这里在结果可能有点不对头，检测到的边缘太粗了，主要是图片太小了，</p>
<p>卷积操作API</p>
<ul>
<li>在 Python 中，卷积用<code>conv_forward()</code>表示；</li>
<li>在 Tensorflow 中，卷积用<code>tf.nn.conv2d()</code>表示；</li>
<li>在 keras 中，卷积用<code>Conv2D()</code>表示。</li>
</ul>
<h3 id="L3-Edge-Detection-Example"><a href="#L3-Edge-Detection-Example" class="headerlink" title="L3: Edge Detection Example"></a>L3: Edge Detection Example</h3><ol>
<li><p>颜色由暗到亮，还是亮到暗</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_1.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_2.png" alt=""></p>
</li>
</ol>
<p>这种滤波器可以区分明暗变化，取绝对值没有区别了</p>
<ol>
<li><p>水平边缘</p>
<p>上边相对较亮，而下方相对较暗</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_3.png" alt=""></p>
<ol>
<li>复杂栗子</li>
</ol>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_4.png" alt=""></p>
</li>
</ol>
<p>这块区域左边两列是正边，右边一列是负边，正边和负边的值加在一起得到了一个中间值。但假如这个一个非常大的1000×1000的类似这样棋盘风格的大图，就不会出现这些亮度为10的过渡带了，因为图片尺寸很大，这些中间值就会变得非常小。</p>
<ol>
<li>filter</li>
</ol>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_5.png" alt=""></p>
<p>sobel过滤器，优点在于增加了中间一行元素的权重，这使得结果的鲁棒性会更高一些。</p>
<p>charr过滤器，它有着和之前完全不同的特性，实际上也是一种垂直边缘检测，如果你将其翻转90度，你就能得到对应水平边缘检测。</p>
<p>学习的其中一件事就是当你真正想去检测出复杂图像的边缘，你不一定要去使用那些研究者们所选择的这九个数字，但你可以从中获益匪浅。把这矩阵中的9个数字当成9个参数，并且在之后你可以学习使用反向传播算法，其目标就是去理解这9个参数。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_6.png" alt=""></p>
<p>这样可能得到一个出色的边缘检测</p>
<p>相比这种单纯的垂直边缘和水平边缘，它可以检测出45°或70°或73°，甚至是任何角度的边缘。所以将矩阵的所有数字都设置为参数，通过数据反馈，让神经网络自动去学习它们，我们会发现神经网络可以学习一些低级的特征，例如这些边缘的特征。</p>
<p>不管是垂直的边缘，水平的边缘，还有其他奇怪角度的边缘，甚至是其它的连名字都没有的过滤器。</p>
<h3 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h3><p>按照我们上面讲的图片卷积，如果原始图片尺寸为$n x n$，filter尺寸为$f x f$，则卷积后的图片尺寸为$(n-f+1) x (n-f+1)$，注意f一般为奇数。这样会带来两个问题：</p>
<ul>
<li><p><strong>卷积运算后，输出图片尺寸缩小</strong></p>
</li>
<li><p><strong>原始图片边缘信息对输出贡献得少，输出图片丢失边缘信息</strong></p>
<p>边缘像素点只被一个输出所触碰或者使用，</p>
</li>
</ul>
<p>为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行<strong>填充（Padding）</strong>，以增加矩阵的大小。通常将 0 作为填充值。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Padding.jpg" alt=""></p>
<p>经过padding之后，填充p,原始图片尺寸为$(n+2p) x (n+2p)$，filter尺寸为$f x f$，则卷积后的图片尺寸为$(n+2p-f+1) x (n+2p-f+1)$。若要保证卷积前后图片尺寸不变，则p应满足：$ p=(f-1)/2$,f通常是奇数，如果是偶数，造成不对称填充，第二个原因是当你有一个奇数维过滤器，比如3×3或者5×5的，它就有一个中心点。有时在计算机视觉里，如果有一个中心像素点会更方便，便于指出过滤器的位置</p>
<ol>
<li>p=0,Valid convolution</li>
<li>p=((f-1))/2,Same convolution</li>
</ol>
<h3 id="L05-Strided-convolution（卷积步长）"><a href="#L05-Strided-convolution（卷积步长）" class="headerlink" title="L05: Strided convolution（卷积步长）"></a>L05: Strided convolution（卷积步长）</h3><p>Stride表示filter在原图片中水平方向和垂直方向每次的步进长度。之前我们默认stride=1。若stride=2，则表示filter每次步进长度为2，即隔一点移动一次。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Stride.jpg" alt=""></p>
<p>我们用s表示stride长度，p表示padding长度，如果原始图片尺寸为n x n，filter尺寸为f x f，则卷积后的图片尺寸为：</p>
<script type="math/tex; mode=display">
\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor X\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor</script><p>向下取整</p>
<p>目前为止我们学习的“卷积”实际上被称为<strong>互相关（cross-correlation）</strong>，而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）。因为这种翻转对一般为水平或垂直对称的滤波器影响不大，按照机器学习的惯例，我们通常不进行翻转操作，在简化代码的同时使神经网络能够正常工作。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_6.png" alt=""></p>
<p>互相关：过滤器沿水平和垂直轴翻转，元素相乘来计算，这些视频中定义卷积运算时，我们跳过了这个镜像操作。（不进行翻转操作）叫做卷积操作</p>
<h3 id="L06-Convolution-over-volumes-三维卷积"><a href="#L06-Convolution-over-volumes-三维卷积" class="headerlink" title="L06: Convolution over volumes(三维卷积)"></a>L06: Convolution over volumes(三维卷积)</h3><ol>
<li><p>卷积运算</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Convolutions-on-RGB-image.png" alt=""></p>
</li>
</ol>
<p>过程是将每个单通道（R，G，B）与对应的filter进行卷积运算求和，然后再将3通道的和相加，得到输出图片的一个像素值。</p>
<p>不同通道的滤波算子可以不相同。例如R通道filter实现垂直边缘检测，G和B通道不进行边缘检测，全部置零，或者将R，G，B三通道filter全部设置为水平边缘检测。</p>
<p>为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_8.png" alt=""></p>
<p>为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_9.png" alt=""></p>
<p>若输入图片的尺寸为n x n x nc，nc: 通道数目，filter尺寸为f x f x nc，则卷积后的图片尺寸为(n-f+1) x (n-f+1) x nc′。其中，nc为图片通道数目，nc′为滤波器组个数。</p>
<h3 id="L7-One-layer-of-a-convolution-network-单层神经网络"><a href="#L7-One-layer-of-a-convolution-network-单层神经网络" class="headerlink" title="L7 : One layer of a convolution network (单层神经网络)"></a>L7 : One layer of a convolution network (单层神经网络)</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_10.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_11.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_12.png" alt=""></p>
<p>CNN单层的所以标记符号，设层数$l$,</p>
<script type="math/tex; mode=display">
\begin{array}{l}{f^{[l]}=\text { filter size }} \\ {p^{[l]}=\text { padding }} \\ {g^{[l]}=\text { stride }} \\ {n_{c}^{[l]}=\text { number of filters }}\end{array}</script><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_13.png" alt=""></p>
<script type="math/tex; mode=display">
\begin{array}{c}{n_{H}^{[l]}=\left\lfloor\frac{n_{H}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right\rfloor} \\ { n_{W}^{[l]}=\left\lfloor\frac{n_{W}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right\rfloor}\end{array}</script><p>如果$m$个样本，进行向量化运算，相应的输出维度，为</p>
<script type="math/tex; mode=display">
\mathrm{m} \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}</script><h3 id="L8-A-simple-convolution-network-example（简单卷积网络示例）"><a href="#L8-A-simple-convolution-network-example（简单卷积网络示例）" class="headerlink" title="L8 : A simple convolution network example（简单卷积网络示例）"></a>L8 : A simple convolution network example（简单卷积网络示例）</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-33.jpg" alt=""></p>
<ul>
<li>一般而言，<strong>图片的height $n^{[l]}_{H}$和width $n^{[l]}_W$随着层数的增加逐渐降低，但channel $n^{[l]}_C$逐渐增加</strong>。</li>
</ul>
<p>CNN有三种类型的layer：</p>
<ul>
<li>Convolution层（CONV）</li>
<li>Pooling层（POOL）</li>
<li>Fully connected层（FC）</li>
</ul>
<h3 id="L9-Pooling-layers-池化层"><a href="#L9-Pooling-layers-池化层" class="headerlink" title="L9: Pooling layers(池化层)"></a>L9: Pooling layers(池化层)</h3><p>卷积神经网络除了卷积层，还有池化层来缩减模型的大小，提高运算速度和鲁棒性</p>
<ol>
<li>池的类型有max pooling(最大池化)</li>
</ol>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_14.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_15.png" alt=""></p>
<p>这里步幅是s=2，filter = 2*2是最大池化的超参数,如果是三维，则单独在每个通道执行最大池化操作</p>
<p>关于max pooling的直觉解释： 元素较大的值，可能是卷积过程中提取到的某些特征（比如边界），而max pooling则在压缩了矩阵大小的情况下，保留每个分区内最大的输出，即保留了提取的特征。但理论上还没有证明max pooling的原理，max pooling应用的原因是在实践中效果很好。</p>
<ol>
<li><p>Pooling layer: Average pooling</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_16.png" alt=""></p>
<p>但是最大池化更好用</p>
</li>
</ol>
<p>summary : 输入$n_H<em>n_W</em>n_C$,如果没有padding,输出$(n_h-f)/s+1<em>(n_w-f)/s+1</em>n_c$</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_17.png" alt=""></p>
<h3 id="L10-Convolutional-neural-network-example-卷积神经网络实例"><a href="#L10-Convolutional-neural-network-example-卷积神经网络实例" class="headerlink" title="L10: Convolutional neural network example (卷积神经网络实例)"></a>L10: Convolutional neural network example (卷积神经网络实例)</h3><p>做一个识别数字的CNN网络</p>
<ol>
<li><p>LeNet-5架构如下：</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\CNN.jpg" alt=""></p>
<ul>
<li>通常Conv Layer和Pooling Layer合在一起算一个layer，因为pooling layer并没有参数训练</li>
</ul>
</li>
</ol>
<ul>
<li>常见的结构：Conv ==&gt; Pool ==&gt; Conv ==&gt; Pool ==&gt; FC ==&gt; FC ==&gt; softmax</li>
<li>最终还会用FC层（全连接层），与一般NN的处理一样；并在输出层，应用softmax得到10个数字的概率。</li>
<li>在整个网络中，Height和Width是逐渐递减的，但channel和filter是递增的。</li>
<li>关于CNN如何选择超参：可以参考论文的经验。</li>
<li><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_18.png" alt=""></li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Activation shape</th>
<th style="text-align:center">Activation Size</th>
<th>#parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Input:</strong></td>
<td style="text-align:center">(32, 32, 3)</td>
<td style="text-align:center">3072</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>CONV1(f=5, s=1)</strong></td>
<td style="text-align:center">(28, 28, 6)</td>
<td style="text-align:center">4704</td>
<td>156 (=5<em>5</em>6+6)</td>
</tr>
<tr>
<td style="text-align:center"><strong>POOL1</strong></td>
<td style="text-align:center">(14, 14, 6)</td>
<td style="text-align:center">1176</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>CONV2(f=5, s=1)</strong></td>
<td style="text-align:center">(10, 10, 16)</td>
<td style="text-align:center">1600</td>
<td>416 (=5<em>5</em>16+16)</td>
</tr>
<tr>
<td style="text-align:center"><strong>POOL2</strong></td>
<td style="text-align:center">(5, 5, 16)</td>
<td style="text-align:center">400</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>FC3</strong></td>
<td style="text-align:center">(120, 1)</td>
<td style="text-align:center">120</td>
<td>48120 (=120*400+120)</td>
</tr>
<tr>
<td style="text-align:center"><strong>FC4</strong></td>
<td style="text-align:center">(84, 1)</td>
<td style="text-align:center">84</td>
<td>10164 (=84*120+84)</td>
</tr>
<tr>
<td style="text-align:center"><strong>Softmax</strong></td>
<td style="text-align:center">(10, 1)</td>
<td style="text-align:center">10</td>
<td>850 (=10*84+10)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="L11-Why-convolution"><a href="#L11-Why-convolution" class="headerlink" title="L11 Why convolution"></a>L11 Why convolution</h3><ul>
<li><p>参数共享（parameter sharing)</p>
<p> 如果用FC的话，参数爆炸啊！如果conv layer 就需要filter检测器，这个参数就少了，还参数共享</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_19.png" alt=""></p>
</li>
<li><p>稀疏连接(sparsity of connection)</p>
<p>输出中的每个单元仅和输入的一个小分区相关，比如输出的左上角的像素仅仅由输入左上角的9个像素决定（假设filter大小是3*3），而其他输入都不会影响。</p>
</li>
</ul>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_20.png" alt=""></p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><font color="read">1. 卷积神经网络的基本构造和计算过程 2. 如何整合这些模型 3.  哪些超参数 4. 为什么使用卷积 </font>

<h2 id="W2-Deep-convolutional-models-case-studies-深度卷积网络：实例探究"><a href="#W2-Deep-convolutional-models-case-studies-深度卷积网络：实例探究" class="headerlink" title="W2 : Deep convolutional models: case studies(深度卷积网络：实例探究)"></a>W2 : Deep convolutional models: case studies(深度卷积网络：实例探究)</h2><h3 id="L1-Why-look-at-case-studies-为什么要进行实例探究？"><a href="#L1-Why-look-at-case-studies-为什么要进行实例探究？" class="headerlink" title="L1 : Why look at case studies?(为什么要进行实例探究？)"></a>L1 : Why look at case studies?(为什么要进行实例探究？)</h3><p>本文将主要介绍几个典型的CNN案例。通过对具体CNN模型及案例的研究，来帮助我们理解知识并训练实际的模型。</p>
<p>典型的CNN模型包括：</p>
<ul>
<li><strong>LeNet-5</strong></li>
<li><strong>AlexNet</strong></li>
<li><strong>VGG</strong></li>
</ul>
<p>还会介绍Residual Network（ResNet）。其特点是可以构建很深很深的神经网络（目前最深的好像有152层）。还会介绍Inception Neural Network</p>
<h3 id="L2-Classic-networks-经典网络"><a href="#L2-Classic-networks-经典网络" class="headerlink" title="L2 : Classic networks(经典网络)"></a>L2 : Classic networks(经典网络)</h3><h4 id="1-LeNet-5"><a href="#1-LeNet-5" class="headerlink" title="1. LeNet-5"></a>1. LeNet-5</h4><p><strong>LeNet-5</strong>是针对灰度图片训练的，使用6个5×5的过滤器，步幅为1。由于使用了6个过滤器，步幅为1，<strong>padding</strong>为0，输出结果为28×28×6，图像尺寸从32×32缩小到28×28。然后进行池化操作，在这篇论文写成的那个年代，人们更喜欢使用平均池化，而现在我们可能用最大池化更多一些。在这个例子中，我们进行平均池化，过滤器的宽度为2，步幅为2，图像的尺寸，高度和宽度都缩小了2倍，输出结果是一个14×14×6的图像。我觉得这张图片应该不是完全按照比例绘制的，如果严格按照比例绘制，新图像的尺寸应该刚好是原图像的一半。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-34.jpg" alt=""></p>
<p>该LeNet模型总共包含了大约6万个参数。值得一提的是，当时Yann LeCun提出的LeNet-5模型池化层使用的是average pool，而且各层激活函数一般是Sigmoid和tanh。现在，我们可以根据需要，做出改进，使用max pool和激活函数ReLU。</p>
<h4 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h4><p>AlexNet模型是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同提出的，其结构如下所示：</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-35.jpg" alt=""></p>
<p><strong>AlexNet</strong>首先用一张227×227×3的图片作为输入，实际上原文中使用的图像是224×224×3，但是如果你尝试去推导一下，你会发现227×227这个尺寸更好一些。第一层我们使用96个11×11的过滤器，步幅为4，由于步幅是4，因此尺寸缩小到55×55，缩小了4倍左右。然后用一个3×3的过滤器构建最大池化层,f=3，步幅为2，卷积层尺寸缩小为27×27×96。接着再执行一个5×5的卷积，<strong>padding</strong>之后，输出是27×27×276。然后再次进行最大池化，尺寸缩小到13×13。再执行一次<strong>same</strong>卷积，相同的<strong>padding</strong>，得到的结果是13×13×384，384个过滤器。再做一次<strong>same</strong>卷积，就像这样。再做一次同样的操作，最后再进行一次最大池化，尺寸缩小到6×6×256。6×6×256等于9216，将其展开为9216个单元，然后是一些全连接层。最后使用<strong>softmax</strong>函数输出识别的结果，看它究竟是1000个可能的对象中的哪一个。</p>
<p>实际上，这种神经网络与<strong>LeNet</strong>有很多相似之处，不过<strong>AlexNet</strong>要大得多。正如前面讲到的<strong>LeNet</strong>或<strong>LeNet-5</strong>大约有6万个参数，而<strong>AlexNet</strong>包含约6000万个参数。当用于训练图像和数据集时，<strong>AlexNet</strong>能够处理非常相似的基本构造模块，这些模块往往包含着大量的隐藏单元或数据，这一点<strong>AlexNet</strong>表现出色。<strong>AlexNet</strong>比<strong>LeNet</strong>表现更为出色的另一个原因是它使用了<strong>ReLu</strong>激活函数。原作者还提到了一种优化技巧，叫做Local Response Normalization(LRN)。 而在实际应用中，LRN的效果并不突出。</p>
<h4 id="3-VGG-16"><a href="#3-VGG-16" class="headerlink" title="3. VGG-16"></a>3. VGG-16</h4><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-36.jpg" alt=""></p>
<p>首先用3×3，步幅为1的过滤器构建卷积层，<strong>padding</strong>参数为<strong>same</strong>卷积中的参数。然后用一个2×2，步幅为2的过滤器构建最大池化层。因此<strong>VGG</strong>网络的一大优点是它确实简化了神经网络结构，下面我们具体讲讲这种网络结构。</p>
<p>数字16，就是指在这个网络中包含16个卷积层和全连接层。总共包含约1.38亿个参数</p>
<h3 id="L3-Residual-Networks-ResNets-残差网络-ResNets"><a href="#L3-Residual-Networks-ResNets-残差网络-ResNets" class="headerlink" title="L3 : Residual Networks (ResNets)(残差网络(ResNets))"></a>L3 : Residual Networks (ResNets)(残差网络(ResNets))</h3><p>我们知道，如果神经网络层数越多，网络越深，源于梯度消失和梯度爆炸的影响，整个模型难以训练成功。解决的方法之一是人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为Residual Networks(ResNets)。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_21.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Residual-Network.jpg" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\ResNet-Training-Error.jpg" alt=""></p>
<h3 id="L4-Why-ResNets-work-残差网络为什么有用？"><a href="#L4-Why-ResNets-work-残差网络为什么有用？" class="headerlink" title="L4: Why ResNets work?(残差网络为什么有用？)"></a>L4: Why ResNets work?(残差网络为什么有用？)</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_22.png" alt=""></p>
<p>因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。</p>
<p>注意，如果$ a[l]$与 $a[l+2]$的维度不同，需要引入矩阵 $W_s$与 $a_{[l]}$相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a[l]$截断或者补零。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-37.jpg" alt=""></p>
<h3 id="L5-Network-in-Network-and-1×1-convolutions-网络中的网络以及-1×1-卷积"><a href="#L5-Network-in-Network-and-1×1-convolutions-网络中的网络以及-1×1-卷积" class="headerlink" title="L5 : Network in Network and 1×1 convolutions(网络中的网络以及 1×1 卷积)"></a>L5 : Network in Network and 1×1 convolutions(网络中的网络以及 1×1 卷积)</h3><ol>
<li>作用 </li>
</ol>
<p>假设这是一个28×28×192的输入层，你可以使用池化层压缩它的高度和宽度，这个过程我们很清楚。但如果通道数量很大，该如何把它压缩为28×28×32维度的层呢？你可以用32个大小为1×1的过滤器，严格来讲每个过滤器大小都是1×1×192维，因为过滤器中通道数量必须与输入层中通道的数量保持一致。但是你使用了32个过滤器，输出层为28×28×32，这就是压缩通道数（$n_c$）的方法，对于池化层我只是压缩了这些层的高度和宽度</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_23.png" alt=""></p>
<ol>
<li><strong>doing something pretty non-trivial</strong></li>
</ol>
<p>它给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数量不变，当然如果你愿意，也可以增加通道数量。</p>
<h3 id="L6-Inception-network-motivation-谷歌-Inception-网络简介"><a href="#L6-Inception-network-motivation-谷歌-Inception-网络简介" class="headerlink" title="L6 : Inception network motivation(谷歌 Inception 网络简介)"></a>L6 : Inception network motivation(谷歌 Inception 网络简介)</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\99f8fc7dbe7cd0726f5271aae11b9872.png" alt=""></p>
<p>有了这样的<strong>Inception</strong>模块，你就可以输入某个量，因为它累加了所有数字，这里的最终输出为32+32+128+64=256。有了这样的<strong>Inception</strong>模块，你就可以输入某个量，因为它累加了所有数字，这里的最终输出为32+32+128+64=256。Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。</p>
<p> 1x1 的卷积层通常被称作<strong>瓶颈层（Bottleneck layer）</strong></p>
<p>计算量为 28x28x32x5x5x192 = 1.2亿</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\The-problem-of-computational-cost.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Using-1x1-convolution.png" alt=""></p>
<p>28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。</p>
<h3 id="L7-Inception-network-Inception-网络"><a href="#L7-Inception-network-Inception-网络" class="headerlink" title="L7 : Inception network(Inception 网络)"></a>L7 : Inception network(Inception 网络)</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_24.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\example_2_25.png" alt=""></p>
<h3 id="L8-Using-open-source-implementations-使用开源的实现方案"><a href="#L8-Using-open-source-implementations-使用开源的实现方案" class="headerlink" title="L8 : Using open-source implementations( 使用开源的实现方案)"></a>L8 : Using open-source implementations( 使用开源的实现方案)</h3><p>开源项目</p>
<h3 id="L9-：-Transfer-Learning（迁移学习）"><a href="#L9-：-Transfer-Learning（迁移学习）" class="headerlink" title="L9 ： Transfer Learning（迁移学习）"></a>L9 ： Transfer Learning（迁移学习）</h3><p>如果你下载别人已经训练好网络结构的权重，你通常能够进展的相当快，用这个作为预训练，然后转换到你感兴趣的任务上。</p>
<ol>
<li>只有很小数据集： 可以你只需要训练<strong>softmax</strong>层的权重，把前面这些层的权重都冻结。</li>
<li>稍微更大的数据集： 你应该冻结更少的层，比如只把这些层冻结，然后训练后面的层。如果你的输出层的类别不同，那么你需要构建自己的输出单元；或者你可以直接去掉这几层，换成你自己的隐藏单元和你自己的<strong>softmax</strong>输出层，这些方法值得一试。</li>
<li>大量数据： 你可以用下载的权重只作为初始化，用它们来代替随机初始化，接着你可以用梯度下降训练，更新网络所有层的所有权重。</li>
</ol>
<h3 id="L10-：-Data-augmentation（数据增强）"><a href="#L10-：-Data-augmentation（数据增强）" class="headerlink" title="L10 ： Data augmentation（数据增强）"></a>L10 ： Data augmentation（数据增强）</h3><p>数据量远远不够</p>
<ol>
<li>Mirroring</li>
</ol>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Mirroring.png" alt=""></p>
<ol>
<li>Random Cropping</li>
</ol>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Mirroring_1.png" alt=""></p>
<ol>
<li><p>彩色转换color shifting</p>
<p>r,g,b数据改变</p>
</li>
</ol>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Mirroring_2.png" alt=""></p>
<p>除了随意改变RGB通道数值外，还可以更有针对性地对图片的RGB通道进行PCA color augmentation，也就是对图片颜色进行主成分分析，对主要的通道颜色进行增加或减少，可以采用高斯扰动做法。这样也能增加有效的样本数量。具体的PCA color augmentation做法可以查阅AlexNet的相关论文。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Mirroring_3.png" alt=""></p>
<p>常用的实现数据扩充的方法是使用一个线程或者是多线程，这些可以用来加载数据，实现变形失真，然后传给其他的线程或者其他进程，来训练这个（编号2）和这个（编号1），可以并行实现。</p>
<h3 id="L11：The-state-of-computer-vision-计算机视觉现状"><a href="#L11：The-state-of-computer-vision-计算机视觉现状" class="headerlink" title="L11：The state of computer vision(计算机视觉现状)"></a>L11：The state of computer vision(计算机视觉现状)</h3><ol>
<li>神经网络需要数据，不同的网络模型所需的数据量是不同的。Object dection，Image recognition，Speech recognition所需的数据量依次增加。一般来说，如果data较少，那么就需要更多的hand-engineering，对已有data进行处理。</li>
</ol>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Mirroring_4.png" alt=""></p>
<p>hand-engineering是一项非常重要也比较困难的工作。很多时候，hand-engineering对模型训练效果影响很大，特别是在数据量不多的情况下。</p>
<p>当你有少量的数据时，有一件事对你很有帮助，那就是迁移学习。在别人做好的基础上研究</p>
<ol>
<li><p>提升性能</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Mirroring_5.png" alt="">*</p>
</li>
</ol>
<p>由于计算机视觉问题建立在小数据集之上，其他人已经完成了大量的网络架构的手工工程。一个神经网络在某个计算机视觉问题上很有效，但令人惊讶的是它通常也会解决其他计算机视觉问题。</p>
<p>所以，要想建立一个实用的系统，你最好先从其他人的神经网络架构入手。如果可能的话，你可以使用开源的一些应用，因为开放的源码实现可能已经找到了所有繁琐的细节，比如学习率衰减方式或者超参数。</p>
<h2 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h2><font color="red">1. CNN的常见网络结构 重点说了一些残差网络 2.数据增加的方法 3. 多用开源框架，不用从头开始训练 </font>

<h1 id="W3-Object-detection-目标检测"><a href="#W3-Object-detection-目标检测" class="headerlink" title="W3 Object detection(目标检测)"></a>W3 Object detection(目标检测)</h1><h3 id="L1-Object-localization-目标定位"><a href="#L1-Object-localization-目标定位" class="headerlink" title="L1 :Object localization(目标定位)"></a>L1 :Object localization(目标定位)</h3><p>目标定位和目标检测</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_1.png" alt=""></p>
<p>模型</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_2.png" alt=""></p>
<p>输入还包括位置信息</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_3.png" alt=""></p>
<p>损失函数</p>
<p>情况一：检测到了</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_4.png" alt=""></p>
<p>情况二：</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_5.png" alt=""></p>
<h3 id="L2-Landmark-detection-特征点检测"><a href="#L2-Landmark-detection-特征点检测" class="headerlink" title="L2: Landmark detection(特征点检测)"></a>L2: Landmark detection(特征点检测)</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_6.png" alt=""></p>
<p>该网络模型共检测人脸上64处特征点，加上是否为face的标志位，输出label共有64x2+1=129个值。通过检测人脸特征点可以进行情绪分类与判断，或者应用于AR领域等等。</p>
<p>除了人脸特征点检测之外，还可以检测人体姿势动作，如下图所示：</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_7.png" alt=""></p>
<h3 id="L3-Object-detection-目标检测"><a href="#L3-Object-detection-目标检测" class="headerlink" title="L3 :Object detection(目标检测)"></a>L3 :Object detection(目标检测)</h3><p>学过了对象定位和特征点检测，今天我们来构建一个对象检测算法。这节课，我们将学习如何通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_8.png" alt=""></p>
<p>训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下。</p>
<p>选定特定大小的窗口，窗口圈定输入卷积神经网络，卷积神经网络开始预测。</p>
<p>重复上述操作，不过这次我们选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，你可以根据卷积网络对输入大小调整这个区域，然后输入给卷积网络，输出0或<img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_10.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_11.png" alt=""></p>
<p>如果你这样做，不论汽车在图片的什么位置，总有一个窗口可以检测到它。</p>
<p>这种算法叫作滑动窗口目标检测，因为我们以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车。</p>
<p>滑动窗算法的优点是原理简单，且不需要人为选定目标区域（检测出目标的滑动窗即为目标区域）。但是其缺点也很明显，首先滑动窗的大小和步进长度都需要人为直观设定。滑动窗过小或过大，步进长度过大均会降低目标检测正确率。而且，每次滑动窗区域都要进行一次CNN网络计算，如果滑动窗和步进长度较小，整个目标检测的算法运行时间会很长。所以，滑动窗算法虽然简单，但是性能不佳，不够快，不够灵活。</p>
<h3 id="L-4-Convolutional-implementation-of-sliding-windows-滑动窗口的卷积实现"><a href="#L-4-Convolutional-implementation-of-sliding-windows-滑动窗口的卷积实现" class="headerlink" title="L 4 : Convolutional implementation of sliding windows(滑动窗口的卷积实现)"></a>L 4 : Convolutional implementation of sliding windows(滑动窗口的卷积实现)</h3><ol>
<li><p>全连接层转化为卷积层</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_12.png" alt=""></p>
</li>
</ol>
<p>单个窗口区域卷积网络结构建立完毕之后，对于待检测图片，即可使用该网络参数和结构进行运算。例如16 x 16 x 3的图片，步进长度为2，CNN网络得到的输出层为2 x 2 x 4。其中，2 x 2表示共有4个窗口结果。对于更复杂的28 x 28 x3的图片，CNN网络得到的输出层为8 x 8 x 4，共64个窗口结果。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_13.png" alt=""></p>
<p>之前的滑动窗算法需要反复进行CNN正向计算，例如16 x 16 x 3的图片需进行4次，28 x 28 x3的图片需进行64次。而利用卷积操作代替滑动窗算法，则不管原始图片有多大，只需要进行一次CNN正向计算，因为其中共享了很多重复计算部分，这大大节约了运算成本。值得一提的是，窗口步进长度与选择的MAX POOL大小有关。如果需要步进长度为4，只需设置MAX POOL为4 x 4即可。</p>
<h3 id="L5-：-Bounding-box-predictions（Bounding-Box预测）"><a href="#L5-：-Bounding-box-predictions（Bounding-Box预测）" class="headerlink" title="L5 ： Bounding box predictions（Bounding Box预测）"></a>L5 ： Bounding box predictions（Bounding Box预测）</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_14.png" alt=""></p>
<ol>
<li><p>YOLO（You Only Look Once）算法可以解决这类问题，生成更加准确的目标区域（如上图红色窗口）。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_16.png" alt=""></p>
</li>
<li><p>如果目标中心坐标(bx,by)不在当前网格内，则当前网格Pc=0；相反，则当前网格Pc=1（即只看中心坐标是否在当前网格内）。判断有目标的网格中，bx,by,bh,bw限定了目标区域。值得注意的是，当前网格左上角坐标设定为(0, 0)，右下角坐标设定为(1, 1)，(bx,by)范围限定在[0,1]之间，但是bh,bw可以大于1。因为目标可能超出该网格，横跨多个区域，如上图所示。目标占几个网格没有关系，目标中心坐标必然在一个网格之内。</p>
</li>
</ol>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_15.png" alt=""></p>
<h3 id="L6-：Intersection-over-union（交并比"><a href="#L6-：Intersection-over-union（交并比" class="headerlink" title="L6 ：Intersection over union（交并比)"></a>L6 ：Intersection over union（交并比)</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_17.png" alt=""></p>
<p>一般约定，在计算机检测任务中，如果lou&gt;=0.5，就说检测正确，如果预测器和实际边界框完美重叠，<strong>loU</strong>就是1，因为交集就等于并集。但一般来说只要lou&gt;=0.5，那么结果是可以接受的，看起来还可以。一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这么约定，但如果你希望更严格一点，你可以将<strong>loU</strong>定得更高，比如说大于0.6或者更大的数字，但<strong>loU</strong>越高，边界框越精确。</p>
<h3 id="L7-Non-max-suppression-非极大值抑制"><a href="#L7-Non-max-suppression-非极大值抑制" class="headerlink" title="L7: Non-max suppression(非极大值抑制)"></a>L7: Non-max suppression(非极大值抑制)</h3><p>到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以确保你的算法对每个对象只检测一次，我们讲一个例子。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_18.png" alt=""></p>
<p>假设你需要在这张图片里检测行人和汽车，你可能会在上面放个19×19网格，理论上这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_19.png" alt=""></p>
<p>实际情况是格子1，2，3，4，5，6都认为里面有车。因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的pc,我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。</p>
<p>非最大值抑制（Non-max Suppression）做法很简单，图示每个网格的Pc值可以求出，Pc值反映了该网格包含目标中心坐标的可信度。首先选取Pc最大值对应的网格和区域，然后计算该区域与所有其它区域的IoU，剔除掉IoU大于阈值（例如0.5）的所有网格及区域。这样就能保证同一目标只有一个网格与之对应，且该网格Pc最大，最可信。接着，再从剩下的网格中选取Pc最大的网格，重复上一步的操作。最后，就能使得每个目标都仅由一个网格和区域对应。如下图所示：</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_20.png" alt=""></p>
<p>总结一下非最大值抑制算法的流程：</p>
<ol>
<li><strong>剔除Pc值小于某阈值（例如0.6）的所有网格；</strong></li>
<li><strong>选取Pc值最大的网格，利用IoU，摒弃与该网格交叠较大的网格；</strong></li>
<li><strong>对剩下的网格，重复步骤2。</strong></li>
</ol>
<p>到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果你想让一个格子检测出多个对象，你可以这么做，就是使用<strong>anchor box</strong>这个概念，我们从一个例子开始讲吧。方法是使用不同形状的Anchor Boxes。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_21.png" alt=""></p>
<p>这就是<strong>anchor box</strong>的概念，我们建立<strong>anchor box</strong>这个概念，是为了处理两个对象出现在同一个格子的情况，实践中这种情况很少发生</p>
<h3 id="L9-YOLO-算法（Putting-it-together-YOLO-algorithm）"><a href="#L9-YOLO-算法（Putting-it-together-YOLO-algorithm）" class="headerlink" title="L9 :  YOLO 算法（Putting it together: YOLO algorithm）"></a>L9 :  YOLO 算法（Putting it together: YOLO algorithm）</h3><p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_22.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_23.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_24.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_25.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_26.png" alt=""></p>
<p>这就是<strong>YOLO</strong>对象检测算法，这实际上是最有效的对象检测算法之一，包含了整个计算机视觉对象检测领域文献中很多最精妙的思路</p>
<h3 id="Region-proposals-Optional-（候选区域（选修））"><a href="#Region-proposals-Optional-（候选区域（选修））" class="headerlink" title="Region proposals (Optional)（候选区域（选修））"></a>Region proposals (Optional)（候选区域（选修））</h3><p>之前介绍的滑动窗算法会对原始图片的每个区域都进行扫描，即使是一些空白的或明显没有目标的区域，例如下图所示。这样会降低算法运行效率，耗费时间。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_27.png" alt=""></p>
<p>为了解决这一问题，尽量避免对无用区域的扫描，可以使用Region Proposals的方法。具体做法是先对原始图片进行分割算法处理，然后支队分割后的图片中的块进行目标检测。</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_28.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\Detering_29.png" alt=""></p>
<p>Region Proposals共有三种方法：</p>
<ul>
<li><strong>R-CNN: 滑动窗的形式，一次只对单个区域块进行目标检测，运算速度慢。</strong></li>
<li><strong>Fast R-CNN: 利用卷积实现滑动窗算法，类似第4节做法。</strong></li>
<li><strong>Faster R-CNN: 利用卷积对图片进行分割，进一步提高运行速度。</strong></li>
</ul>
<h2 id="W4：Special-applications-Face-recognition-amp-Neural-style-transfer-特殊应用：人脸识别和神经风格转换"><a href="#W4：Special-applications-Face-recognition-amp-Neural-style-transfer-特殊应用：人脸识别和神经风格转换" class="headerlink" title="W4：Special applications: Face recognition &amp;Neural style transfer( 特殊应用：人脸识别和神经风格转换)"></a>W4：Special applications: Face recognition &amp;Neural style transfer( 特殊应用：人脸识别和神经风格转换)</h2><h3 id="C1-：-What-is-face-recognition"><a href="#C1-：-What-is-face-recognition" class="headerlink" title="C1 ： What is face recognition?"></a>C1 ： What is face recognition?</h3><p>首先简单介绍一下人脸验证（face verification）和人脸识别（face recognition）的区别。</p>
<ul>
<li><strong>人脸验证：输入一张人脸图片，验证输出与模板是否为同一人，即一对一问题。</strong></li>
<li><strong>人脸识别：输入一张人脸图片，验证输出是否为K个模板中的某一个，即一对多问题。</strong></li>
</ul>
<h3 id="L2-：-One-shot-learning"><a href="#L2-：-One-shot-learning" class="headerlink" title="L2 ： One-shot learning"></a>L2 ： One-shot learning</h3><p>One-shot learning就是说数据库中每个人的训练样本只包含一张照片，然后训练一个CNN模型来进行人脸识别。若数据库有K个人，则CNN模型输出softmax层就是K维的。</p>
<p>但是One-shot learning的性能并不好，其包含了两个缺点：</p>
<ul>
<li><strong>每个人只有一张图片，训练样本少，构建的CNN网络不够健壮。</strong></li>
<li><strong>若数据库增加另一个人，输出层softmax的维度就要发生变化，相当于要重新构建CNN网络，使模型计算量大大增加，不够灵活。</strong></li>
</ul>
<p>为了解决One-shot learning的问题，我们先来介绍相似函数（similarity function）。相似函数表示两张图片的相似程度，用d(img1,img2)来表示。若d(img1,img2)较小，则表示两张图片相似；若d(img1,img2)较大，则表示两张图片不是同一个人。相似函数可以在人脸验证中使用：</p>
<ul>
<li><strong>d(img1,img2)≤τ : 一样</strong></li>
<li><strong>d(img1,img2)&gt;τ : 不一样</strong></li>
</ul>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\congtion_1.png" alt=""></p>
<p>现在你已经知道函数d是如何工作的，通过输入两张照片，它将让你能够解决一次学习问题。那么，下节视频中，我们将会学习如何训练你的神经网络学会这个函数。</p>
<h3 id="L3-Siamese-network"><a href="#L3-Siamese-network" class="headerlink" title="L3: Siamese network"></a>L3: Siamese network</h3><p>最后一层去掉softmax单元做分类</p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\congtion_2.png" alt=""></p>
<p><img src="/2019/05/12/Deel Learning ai_Convolutional Neural Networks/MyBlog\hexo\source\_posts\Deel Learning ai_Convolutional Neural Networks\congtion_3.png" alt=""></p>
<p>如果你要比较两个图片的话，例如这里的第一张（编号1）和第二张图片（编号2），你要做的就是把第二张图片喂给有同样参数的同样的神经网络，然后得到一个不同的128维的向量（编号3），这个向量代表或者编码第二个图片，我要把第二张图片的编码叫做$f(x^{(2)})$。这里我用$x^{(1)}$和$x^{(2)}$仅仅代表两个输入图片,</p>
<script type="math/tex; mode=display">
d(x^{(1)},x^{(2)})=||f(x^{(1)}-f(x^{(2)}||^2</script><p>不同的图片的CNN网络结构和参数都是一样的，目标就是利用梯度下降算法，调整网络参数</p>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/05/05/Deep Learning ai_Deep Learning Specialization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/05/Deep Learning ai_Deep Learning Specialization/" itemprop="url">The Deep Learning Specialization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-05T19:41:59+08:00">2019-05-05</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/05/Deep Learning ai_Deep Learning Specialization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/05/05/Deep Learning ai_Deep Learning Specialization/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                10k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="C3-Improving-Model-Performance"><a href="#C3-Improving-Model-Performance" class="headerlink" title="C3 Improving Model Performance"></a>C3 Improving Model Performance</h1>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/05/05/Deep Learning ai_Deep Learning Specialization/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/" itemprop="url">aiai_</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-17T09:20:51+08:00">2019-04-17</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                21k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="C2W1"><a href="#C2W1" class="headerlink" title="C2W1"></a>C2W1</h1><h2 id="L01-Train-Dev-Test-Sets"><a href="#L01-Train-Dev-Test-Sets" class="headerlink" title="L01 : Train/Dev/Test Sets"></a>L01 : Train/Dev/Test Sets</h2><h3 id="1-process"><a href="#1-process" class="headerlink" title="1. process"></a>1. process</h3><p>应用型机器学习是一个高度迭代的过程，通常在项目启动时，我们会先有一个初步想法，比如构建一个含有特定层数，隐藏单元数量或数据集个数等等的神经网络，然后编码，并尝试运行这些代码，通过运行和测试得到该神经网络或这些配置信息的运行结果，你可能会根据输出结果重新完善自己的想法，改变策略，或者为了找到更好的神经网络不断迭代更新自己的方案。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_2.png" alt=""></p>
<h3 id="2-data-split"><a href="#2-data-split" class="headerlink" title="2. data split"></a>2. data split</h3><ul>
<li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li>
<li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>或者验证不同算法的有效性。</li>
<li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li>
</ul>
<p>假设这是训练数据，我用一个长方形表示，我们通常会将这些数据划分成几部分，一部分作为训练集，一部分作为简单交叉验证集，有时也称之为验证集，方便起见，我就叫它验证集（<strong>dev set</strong>），其实都是同一个概念，最后一部分则作为测试集。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_3.png" alt=""></p>
<ol>
<li><p>在机器学习发展的小数据量时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p>
<ul>
<li>无验证集的情况：70% / 30%；</li>
<li>有验证集的情况：60% / 20% / 20%；</li>
</ul>
</li>
<li><p>在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>
<p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p>
<p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p>
</li>
</ol>
<ul>
<li>100 万数据量：98% / 1% / 1%；</li>
<li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li>
</ul>
<h3 id="3-建议"><a href="#3-建议" class="headerlink" title="3. 建议"></a>3. 建议</h3><p><strong>验证集要和训练集来自于同一个分布</strong>（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。</p>
<p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。如果只有验证集，没有测试集，我们要做的就是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估。当然，如果你不需要无偏估计，那就再好不过了。</p>
<h2 id="L02-Bias-Variance"><a href="#L02-Bias-Variance" class="headerlink" title="L02 : Bias/Variance"></a>L02 : Bias/Variance</h2><p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p>
<p>泛化误差可分解为偏差、方差与噪声之和：</p>
<ul>
<li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li>
<li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li>
<li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>
</ul>
<p>high bias ,underfitting</p>
<p>high variance, overfitting</p>
<p>just right</p>
<h3 id="1-example"><a href="#1-example" class="headerlink" title="1. example"></a>1. example</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_5.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_6.png" alt=""></p>
<p>Your algorithms ever on the training set and dev set you can try to diganose whether has problems high barriers or high variances or both or neither.</p>
<h2 id="L03-Basic-Recipe-for-Machine-learning"><a href="#L03-Basic-Recipe-for-Machine-learning" class="headerlink" title="L03 Basic Recipe for Machine learning"></a>L03 Basic Recipe for Machine learning</h2><h3 id="1-METHOD"><a href="#1-METHOD" class="headerlink" title="1. METHOD"></a>1. METHOD</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_8.png" alt=""></p>
<p>Training a bigger network almost never hurts. And the main cost of training a neural network that’s too big is just computational time, so long as you’re regularizing.</p>
<p>今天我们讲了如何通过组织机器学习来诊断偏差和方差的基本方法，然后选择解决问题的正确操作，希望大家有所了解和认识。我在课上不止一次提到了正则化，它是一种非常实用的减少方差的方法，正则化时会出现偏差方差权衡问题，偏差可能略有增加，如果网络足够大，增幅通常不会太高，我们下节课再细讲，以便大家更好理解如何实现神经网络的正则化。</p>
<p>第一点，高偏差和高方差是两种不同的情况，我们后续要尝试的方法也可能完全不同</p>
<p>只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。这两步实际要做的工作是：训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。</p>
<h2 id="L04"><a href="#L04" class="headerlink" title="L04"></a>L04</h2><h3 id="1-over-fitting"><a href="#1-over-fitting" class="headerlink" title="1. over fitting"></a>1. over fitting</h3><h3 id="regularization"><a href="#regularization" class="headerlink" title="regularization"></a>regularization</h3><p>L2 regularization</p>
<p>L1 regularizaion: w will be sparse  L1 正则化最后得到 w 向量中将存在大量的 0</p>
<p>为什么只正则化参数w？为什么不再加上参数b 呢？你可以这么做，只是我习惯省略不写，因为通常w是一个高维参数矢量，w已经可以表达高偏差问题，可能w包含有很多参数，我们不可能拟合所有参数，而只是b单个数字，所以w几乎涵盖所有参数，而不是，如果加了参数b，其实也没太大影响，因为b只是众多参数中的一个，所以我通常省略不计，如果你想加上这个参数，完全没问题。</p>
<ol>
<li><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_9.png" alt=""></li>
</ol>
<p>2.<img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_10.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_11.png" alt="">矩阵范数被称作“弗罗贝尼乌斯范数”，用下标标注F</p>
<ol>
<li><p>反向传播时，填上正则化的一项</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_12.png" alt=""></p>
<p>因此L2正则化也被称为“权重衰减”。</p>
</li>
</ol>
<p>to get more training data</p>
<h2 id="L05-Why-Regularization-Reduces-Overfitting"><a href="#L05-Why-Regularization-Reduces-Overfitting" class="headerlink" title="L05 :Why Regularization Reduces Overfitting"></a>L05 :Why Regularization Reduces Overfitting</h2><p>我们添加正则项，它可以避免数据权值矩阵过大，这就是弗罗贝尼乌斯范数，为什么压缩范数，或者弗罗贝尼乌斯范数或者参数可以减少过拟合？我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单.Regularization其实是让函数变得<strong>简化</strong>。</p>
<p>直观上理解就是如果正则化设置得足够大，权重矩阵被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。</p>
<p>总结一下，如果正则化参数变得很大，w参数很小，z也会相对变小，此时忽略的b影响，z会相对变小，实际上，z的取值范围很小，这个激活函数tanh，也就是曲线函数会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p>
<p><strong>L2 regularization的不足</strong>：要通过不断的选用不同的λ进行测试，计算量加大了。</p>
<h2 id="L06-Dropout-Regularization"><a href="#L06-Dropout-Regularization" class="headerlink" title="L06 : Dropout Regularization"></a>L06 : Dropout Regularization</h2><h3 id="1-工作原理"><a href="#1-工作原理" class="headerlink" title="1. 工作原理"></a>1. 工作原理</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_15.png" alt=""></p>
<p>如果上面这幅图存在over fitting。复制这个神经网络，dropout会遍历网络的每一层。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_13.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_14.png" alt=""></p>
<p>我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。</p>
<h3 id="2-inverted-dropout（反向随机失活）"><a href="#2-inverted-dropout（反向随机失活）" class="headerlink" title="2. inverted dropout（反向随机失活）"></a>2. <strong>inverted dropout</strong>（反向随机失活）</h3><p>对第L</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span>    <span class="comment"># 设置神经元保留概率</span></span><br><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>], al.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">al = np.multiply(al, dl)</span><br><span class="line">al /= keep_prob</span><br></pre></td></tr></table></figure>
<p>最后一步<code>al /= keep_prob</code>是因为 a[l]a[l]中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z[l+1]=W[l+1]a[l]+b[l+1]$的期望值，因此除以一个<code>keep_prob</code>。举例解释我们假设第三隐藏层上有50个单元或50个神经元，在一维上是50，我们通过因子分解将它拆分成维的，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10（50×20%=10）个，现在我们看下$z^{[4]}$，，我们的预期是$z^{[4]}=w^{[4]}a^{[3]}$，$a^{[3]}$减少20%，也就是说中有$a^{[3]}$20%的元素被归零，为了不影响的$a^{[4]}$期望值，我们需要用$w^{[4]}a^{[3]}/keep_prob$，它将会修正或弥补我们所需的那20%，$a^{[3]}$的期望值不会变，划线部分就是所谓的<strong>dropout</strong>方法。</p>
<h2 id="L07-Understanding-Dropout"><a href="#L07-Understanding-Dropout" class="headerlink" title="L07 : Understanding Dropout"></a>L07 : Understanding Dropout</h2><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，<strong>dropout</strong>将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施<strong>dropout</strong>的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_16.png" alt=""></p>
<p>计算视觉中的输入量非常大，输入太多像素，以至于没有足够的数据，所以<strong>dropout</strong>在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的选择，但要牢记一点，<strong>dropout</strong>是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然我是不会使用<strong>dropout</strong>的，所以它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合，这就是有些计算机视觉研究人员如此钟情于<strong>dropout</strong>函数的原因。直观上我认为不能概括其它学科。<strong>dropout</strong>将产生收缩权重的平方范数的效果。当然，不同的层，值可以设置成不同，如果你觉得某一层容易过拟合，把值设置小一点。</p>
<p>dropout 的一大<strong>缺点</strong>是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将<code>keep_prob</code>全部设置为 1.0 后运行代码，确保 $J(w,b)​$函数单调递减，再打开 dropout。</p>
<h2 id="L08-Other-Regularization-Methods"><a href="#L08-Other-Regularization-Methods" class="headerlink" title="L08 :  Other Regularization Methods"></a>L08 :  Other Regularization Methods</h2><ul>
<li><p>数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_17.png" alt=""></p>
</li>
<li><p>早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，当训练集误差降低但验证集误差升高，两者开始发生较大偏差时及时停止迭代，并返回具有最小验证集误差的连接权和阈值，以避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。</p>
</li>
</ul>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_18.png" alt=""></p>
<p>但对我来说<strong>early stopping</strong>的主要缺点就是你不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数，因为现在你不再尝试降低代价函数，所以代价函数的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。</p>
<p><strong>Early stopping</strong>的优点是，只运行一次梯度下降，你可以找出的w较小值，中间值和较大值，而无需尝试正则化超级参数的很多值。</p>
<h2 id="L09-：-Normalizing-inputs"><a href="#L09-：-Normalizing-inputs" class="headerlink" title="L09 ： Normalizing inputs"></a>L09 ： Normalizing inputs</h2><ol>
<li><p>零均值</p>
<p>$u=\frac{1}{m}\sum x^{(i)}$,$x-u$</p>
</li>
<li><p>归一化方差；</p>
<p>$\delta^2=\frac{1}{m}(x^{(i)})^2$,每个特征的方差，每个特征数据除以它，就归一化方差了</p>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_19.png" alt=""></p>
<h3 id="why"><a href="#why" class="headerlink" title="why"></a>why</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_20.png" alt=""></p>
<p>在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。</p>
<h2 id="L10-Vanishing-Exploding-Gradients"><a href="#L10-Vanishing-Exploding-Gradients" class="headerlink" title="L10 : Vanishing /Exploding Gradients"></a>L10 : Vanishing /Exploding Gradients</h2><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p>
<p>在深度神经网络中，激活函数将以指数级递减，虽然我只是讨论了激活函数以与相关的指数级数增长或下降，它也适用于与层数相关的导数或梯度函数，也是呈指数级增长或呈指数递减。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_21.png" alt=""></p>
<p>假定 g(z)=z,b[l]=0g(z)=z,b[l]=0，对于目标输出有：</p>
<p>$y^=W[L]W[L−1]…W[2]W[1]X$</p>
<ul>
<li>对于$ W[l]$的值大于 1 的情况，激活函数的值将以指数级递增；</li>
<li>对于 $W[l]$的值小于 1 的情况，激活函数的值将以指数级递减。</li>
</ul>
<p>对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p>
<h2 id="L11-Weight-initialization-in-a-deep-network"><a href="#L11-Weight-initialization-in-a-deep-network" class="headerlink" title="L11 : Weight initialization in a deep network"></a>L11 : Weight initialization in a deep network</h2><p>为了预防值z过大或过小，你可以看到n越大，你希望w越小，因为z是wx+b的和,最合理的方法$w_i=1/n$</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_22.png" alt=""></p>
<p>因此，实际上，你要做的就是设置某层权重矩阵</p>
<p>$w^{[l]}=n p . random. randn (shape) * np.sqrt \left(\frac{1}{n^{[l-1]}}\right)​$</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_23.png" alt=""></p>
<p>当多个节点时，也一样的看，使得这个节点$z^{<a href="i">L</a>}$不要太大，单独看每个节点既可以</p>
<p>relu : var(w(i)) = 2/n or $\frac{2}{n^{[l-1]}*n^{[l]}}$</p>
<p>tanh: var(w(i)) = 1/n</p>
<p>通过设置初始化化权重矩阵，使得不会增长太快或者太慢</p>
<h2 id="L12-：-Numerical-Approximations-of-Gradients"><a href="#L12-：-Numerical-Approximations-of-Gradients" class="headerlink" title="L12 ： Numerical Approximations of Gradients"></a>L12 ： Numerical Approximations of Gradients</h2><p>单边误差</p>
<p>$f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0} \frac{f(\theta+\varepsilon)-(\theta)}{\varepsilon}$</p>
<p>误差$O(\varepsilon)$</p>
<p>双边误差</p>
<p>$f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0}=\frac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2 \varepsilon}$</p>
<p>$O\left(\varepsilon^{2}\right)$</p>
<h2 id="L-13-Gradient-Checking"><a href="#L-13-Gradient-Checking" class="headerlink" title="L 13 Gradient Checking"></a>L 13 Gradient Checking</h2><p>梯度检验帮我们节省了很多时间，也多次帮我发现<strong>backprop</strong>实施过程中的bug，接下来，我们看看如何利用它来调试或检验<strong>backprop</strong>的实施是否正确。</p>
<p>首先要做的就是，把所有参数转换成一个巨大的向量数据，你要做的就是把矩阵w转换成一个向量，把所有矩阵w转换成向量之后，做连接运算，得到一个巨型向量$\theta$，该向量表示为参数$\theta$，代价函数J是所有W和b的函数，现在你得到了一个的代价函数（即）。接着，你得到与和顺序相同的数据，你同样可以把$dW^{[l]}$,和$db^{[l]}$ 转换成一个新的向量，用它们来初始化大向量$d\theta$，它与$\theta$具有相同维度。</p>
<p>梯度的逼近值</p>
<script type="math/tex; mode=display">
d \theta_{\text { approx }}[i]=\frac{J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}+\varepsilon, \ldots\right)-J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}-\varepsilon, \ldots\right)}{2 \varepsilon}</script><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_24.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_25.png" alt=""></p>
<p>现在你已经了解了梯度检验的工作原理，它帮助我在神经网络实施中发现了很多<strong>bug</strong>，希望它对你也有所帮助。</p>
<h1 id="L-14-Gradient-Checking-Implementation-notes"><a href="#L-14-Gradient-Checking-Implementation-notes" class="headerlink" title="L 14 : Gradient Checking Implementation notes"></a>L 14 : Gradient Checking Implementation notes</h1><ol>
<li>不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；太慢了</li>
<li>如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；</li>
<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>
<li>梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_25.png" alt=""></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><font color="red">回顾这一周，我们讲了如何配置训练集，验证集和测试集，如何分析偏差和方差，如何处理高偏差或高方差以及高偏差和高方差并存的问题，如何在神经网络中应用不同形式的正则化，如正则化和**dropout**，还有加快神经网络训练速度的技巧，最后是梯度检验。</font>



<h1 id="C2W2-Optimization-Algorithm"><a href="#C2W2-Optimization-Algorithm" class="headerlink" title="C2W2 :Optimization Algorithm"></a>C2W2 :Optimization Algorithm</h1><h2 id="L-01-Mini-Batch-Gradient-Descent"><a href="#L-01-Mini-Batch-Gradient-Descent" class="headerlink" title="L 01 : Mini Batch Gradient Descent"></a>L 01 : Mini Batch Gradient Descent</h2><ol>
<li><p>Vectorization</p>
</li>
<li><p>Mini batch</p>
<p>not entire training set </p>
<p>bady training set i，$x^{\{i\}}$</p>
<p>mini batch training set</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_1.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_2.png" alt=""></p>
</li>
</ol>
<p>mini batch gradient descent</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_3.png" alt=""></p>
<h2 id="L-02-Understanding-Mini-Batch-Gradient-Decent"><a href="#L-02-Understanding-Mini-Batch-Gradient-Decent" class="headerlink" title="L 02 : Understanding Mini-Batch Gradient Decent"></a>L 02 : Understanding Mini-Batch Gradient Decent</h2><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_4.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_6.png" alt=""></p>
<p>左图，随着iterations increased, it should decrease .if it ever goes up on iteration,something is wrong.</p>
<p>右图 : it’s as if on every iteration you’re training on a different training set or really training on a different mini batch. It should trend downwards, but it’s also going to be a little bit noisier.So if you plot J{t}, as you’re training mini batch in descent it may be over multiple epochs,you might expect to see a curve like this.</p>
<h3 id="Choosing-your-mini-batch-size"><a href="#Choosing-your-mini-batch-size" class="headerlink" title="Choosing your mini-batch size"></a>Choosing your mini-batch size</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_5.png" alt=""></p>
<h3 id="1-优缺点"><a href="#1-优缺点" class="headerlink" title="1. 优缺点"></a>1. 优缺点</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_7.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_8.png" alt=""></p>
<p>通过减小学习率，噪声会被改善或有所减小，但随机梯度下降法的一大缺点是，你会失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下，所以实践中最好选择不大不小的<strong>mini-batch</strong>尺寸，实际上学习率达到最快。你会发现两个好处，一方面，你得到了大量向量化，上个视频中我们用过的例子中，如果<strong>mini-batch</strong>大小为1000个样本，你就可以对1000个样本向量化，比你一次性处理多个样本快得多。另一方面，你不需要等待整个训练集被处理完就可以开始进行后续工作，再用一下上个视频的数字，每次训练集允许我们采取5000个梯度下降步骤，所以实际上一些位于中间的<strong>mini-batch</strong>大小效果最好。</p>
<p>使用<strong>batch</strong>梯度下降法时，每次迭代你都需要历遍整个训练集，可以预期每次迭代成本都会下降，所以如果成本函数是迭代次数的一个函数，它应该会随着每次迭代而减少，如果在某次迭代中增加了，那肯定出了问题，也许你的学习率太大。</p>
<p>在随机梯度下降法中，从某一点开始，我们重新选取一个起始点，每次迭代，你只对一个样本进行梯度下降，大部分时候你向着全局最小值靠近，有时候你会远离最小值，因为那个样本恰好给你指的方向不对，因此随机梯度下降法是有很多噪声的，平均来看，它最终会靠近最小值，不过有时候也会方向错误，因为随机梯度下降法永远不会收敛，而是会一直在最小值附近波动，但它并不会在达到最小值并停留在此。</p>
<p>用<strong>mini-batch</strong>梯度下降法，我们从这里开始，一次迭代这样做，两次，三次，四次，它不会总朝向最小值靠近，但它比随机梯度下降要更持续地靠近最小值的方向，它也不一定在很小的范围内收敛或者波动，如果出现这个问题，可以慢慢减少学习率，我们在下个视频会讲到学习率衰减，也就是如何减小学习率。</p>
<p>batch : too long,too time</p>
<p>随机： lose speeding ,噪声大</p>
<p>mini-batch和stochastic都存在噪声问题，且在local optima附近会徘徊。但设置合适大小的mini-batch size，噪声和徘徊问题可接受的范围内。</p>
<p>size=1,又叫随机梯度下降法 stochastic gradient descent </p>
<h3 id="how"><a href="#how" class="headerlink" title="how"></a>how</h3><p>如何选择mini-batch size（这是一个hyperparameter）：</p>
<ul>
<li>小数据量，比如总的样本只有几千个，完全可以直接用batch gradient descent</li>
<li>大数量，mini-batch size倾向于选择2^n个，比如64, 128, 256等</li>
<li><p>mini-batch 与CPU/GPU memory的内存容量。</p>
<p>In practice of course the mini batch size is another hyper parameter that you might do a quick search over to try to figure out which one is most sufficient of reducing the cost function j. 按照上面的方法</p>
</li>
</ul>
<h2 id="L-03-Exponentially-Weighted-Averages"><a href="#L-03-Exponentially-Weighted-Averages" class="headerlink" title="L 03: Exponentially Weighted Averages"></a>L 03: Exponentially Weighted Averages</h2><p>In order to understand those algorithms, you need to be able they use something called exponentially weighted averages. Also called exponentially weighted moving averages in statistics.</p>
<h3 id="1-指数加权平均数（Exponentially-weighted-averages）"><a href="#1-指数加权平均数（Exponentially-weighted-averages）" class="headerlink" title="1. 指数加权平均数（Exponentially weighted averages）"></a>1. 指数加权平均数（Exponentially weighted averages）</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_9.png" alt=""></p>
<p>$\theta _i$表示每一日的温度值，蓝色的点，$v_t$表示加权平均后的,红色</p>
<p>权平均方法是：每天的温度值加权值$vt$设置为前一天的温度加权值$vt−1$和当天的温度实际值$θt$做加权平均：</p>
<script type="math/tex; mode=display">
v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}</script><p>由于以后我们要考虑的原因，在计算时可视$v_T$大概是$\frac{1}{(1-\beta)}$的每日温度的加权平均，</p>
<p>如果是$\beta$=0.9，这是十天的平均值，红色</p>
<p>如果$\beta$=0.98,是50天的结果，绿色</p>
<p>如果$beta$=0.5,是2day的结果，黄色</p>
<p>由于仅平均了两天的温度，平均的数据太少，所以得到的曲线有更多的噪声，有可能出现异常值，但是这个曲线能够更快适应温度变化。</p>
<p>当 $\beta$较大时，指数加权平均值适应地更缓慢一些。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_10.png" alt=""></p>
<p>$</p>
<h2 id="L-04-Understanding-Exponentially-Weighted-Averages"><a href="#L-04-Understanding-Exponentially-Weighted-Averages" class="headerlink" title="L 04 : Understanding Exponentially Weighted Averages"></a>L 04 : Understanding Exponentially Weighted Averages</h2><p><strong>假如β=0.9，每个v的计算如下：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} v_{100} &=0.9 v_{99}+0.1 \theta_{100} \\ v_{99} &=0.9 v_{98}+0.1 \theta_{99} \\ v_{98} &=0.9 v_{97}+0.1 \theta_{98} \end{aligned}</script><p>递推可得：</p>
<script type="math/tex; mode=display">
v_{100}=0.1 \theta_{100}+0.1 * 0.9 \theta_{99}+0.1 *(0.9)^{2} \theta_{98}+\ldots</script><p>指数的衰减规律</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_11.png" alt=""></p>
<p>一般的</p>
<script type="math/tex; mode=display">
v_{t}=\sum_{i=1}^{t}(1-\beta) \beta^{t-i} \theta_{i}</script><p>无穷级数求和：</p>
<script type="math/tex; mode=display">
\sum_{t=1}^{n}(1-\beta) \beta^{t}=1</script><p>因此可以近似的认为所有项的系数之和正好为100%。</p>
<p>即，$vt$是对t日之前<strong>所有的实际温度的加权平均</strong>,权重是指数递减的。</p>
<p>十天后，曲线高度下降到了1/3,赋予权重$\beta^{t-i}$</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_12.png" alt=""></p>
<script type="math/tex; mode=display">
0.9^{10}~=0.35~=1/e</script><p>一般认为，$v_t$近似前$\frac{1}{1-\beta}$的加权平均值</p>
<h2 id="L05-Bias-correction-in-exponentially-weighted-averages"><a href="#L05-Bias-correction-in-exponentially-weighted-averages" class="headerlink" title="L05 : Bias correction in exponentially weighted averages"></a>L05 : Bias correction in exponentially weighted averages</h2><p>指数加权平均的偏差修正</p>
<p>由于计算$v1$的时候，并没有历史值做加权，这个时候令其前一个加权值$v0=0$，则会导致$v_1$远小于$\theta_1$,依次类推，<strong>在靠近前面的值会出现显著的小于实际值的情况</strong></p>
<p>因此做一个修正</p>
<script type="math/tex; mode=display">
v_{t}=\frac{\beta v_{t-1}+(1-\beta) \theta_{t}}{1-\beta^{t}}</script><p>你会发现随着$\beta^t$增加，接近于0，所以当t很大的时候，偏差修正几乎没有作用，因此当t较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_13.png" alt=""></p>
<p>因为在Machine Learning中看重的是很多次迭代后的结果，初期的偏差影响并不大。</p>
<h2 id="L-06-Gradient-Descent-With-Momentum"><a href="#L-06-Gradient-Descent-With-Momentum" class="headerlink" title="L 06 : Gradient Descent With Momentum"></a>L 06 : Gradient Descent With Momentum</h2><p>动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_14.png" alt=""></p>
<p>当慢慢下降到最小值，上下波动的梯度下降法的速度减缓，无法使用更大的学习率，</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_15.png" alt=""></p>
<p>在纵轴上，希望学校慢一点，不需要摆动，横着上，加快学校，基于此就有了Gradient descent with momentum。</p>
<script type="math/tex; mode=display">
\begin{array}{c}{v_{d W} :=\beta v_{d W}+(1-\beta) d W} \\ {v_{d b} :=\beta v_{d b}+(1-\beta) d b} \\ {w=w-\alpha v_{d W}} \\ {b=b-\alpha v_{d b}}\end{array}</script><p>这样，可以让gradient更平滑</p>
<ul>
<li>对于上图垂直方向，原来是会上下震荡，但引入了exponentially weighted average，相当于对前面的震荡进行了平均，<strong>结果就是上下震荡互相抵消了</strong>。而水平方向都是向右没有震荡，因此平均后还是向右。最终导致呈现上图红色的下降路线。</li>
<li><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_18.png" alt=""></li>
</ul>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_16.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_17.png" alt=""></p>
<h2 id="L-07-RMSprop"><a href="#L-07-RMSprop" class="headerlink" title="L 07 : RMSprop"></a>L 07 : RMSprop</h2><p>RMSprop (Root Mean Square Propagation，均方根传递)，<strong>与momentum一样，也是降低梯度的抖动</strong>。<strong>而是平抑不同大小梯度的更新速率。实际上 作用在α上的</strong></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_19.png" alt=""></p>
<p>回忆一下我们之前的例子，如果你执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅度摆动，为了分析这个例子，假设b纵轴代表参数，横轴代表参数W，可能有w1，或者w2其它重要的参数，为了便于理解，被称为b和w。</p>
<p>我们希望学习速度快，而在垂直方向，也就是例子中的方向，我们希望减缓纵轴上的摆动，所以有了$S_{d W} $和$ S_{d b}$，我们希望$S_{d W} $会相对较小，所以我们要除以一个较小的数，而希望$ S_{d b}$又较大，所以这里我们要除以较大的数字，这样就可以减缓纵轴上的变化。</p>
<p>这些微分，垂直方向的要比水平方向的大得多，所以斜率在方向特别大，所以这些微分中，db较大，dw较小，因为函数的倾斜程度，在纵轴上，也就是b方向上要大于在横轴上，也就是方向上W。db的平方较大，所以$Sdb$也会较大，而相比之下，dw会小一些，亦或dw平方会小一些，因此$Sdw$会小一些，结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。</p>
<p><strong>RMSprop</strong>的影响就是你的更新最后会变成这样（绿色线），纵轴方向上摆动较小，而横轴方向继续推进。还有个影响就是，你可以用一个更大学习率，然后加快学习，而无须在纵轴上垂直方向偏离。</p>
<p>实际中dw是一个高维度的参数向量，db也是一个高维度参数向量，但是你的直觉是，在你要消除摆动的维度中，最终你要计算一个更大的和值，这个平方和微分的加权平均值，所以你最后去掉了那些有摆动的方向。所以这就是<strong>RMSprop</strong>，全称是均方根，因为你将微分进行平方，然后最后使用平方根。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_20.png" alt=""></p>
<p>解释平方：</p>
<p>垂直方向，比较陡，梯度比较大，但我们又希望它下降的慢。因此对梯度除以一个较大的值，所以用梯度的平方的平均来表示。让不同的参数拥有不同的learning rate。</p>
<p><strong>从某种角度看，RMSprop会根据当前的梯度自动调整参数的learning rate，梯度大降低learning rate，梯度小的时候提高learning rate，从而一方面避免了震荡，另一方面避免在平坦的地方徘徊太久。</strong></p>
<p>为了避免出现分母为0</p>
<script type="math/tex; mode=display">
\begin{array}{c}{s_{d w}=\beta s_{d w}+(1-\beta)(d w)^{2}} \\ {s_{d b}=\beta s_{d b}+(1-\beta)(d b)^{2}} \\ {w :=w-\alpha \frac{d w}{\sqrt{s_{d w}+\varepsilon}}} \\ {b :=b-\alpha \frac{d b}{\sqrt{s_{d b}+\varepsilon}}}\end{array}</script><p>$\varepsilon$取$10^{-8}$不错的选择.</p>
<p>补充：</p>
<p>RMSProp算法对梯度计算了<strong>微分平方加权平均数</strong>。这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快</p>
<h2 id="L-08-Adam-optimization-algorithm"><a href="#L-08-Adam-optimization-algorithm" class="headerlink" title="L 08 Adam optimization algorithm"></a>L 08 Adam optimization algorithm</h2><p>Adam（Adaptive Moment Estimation，自适应矩估计）就是momentum和RMSprop的结合。momentum负责平滑梯度，而RMSprop负责调解learning rate。</p>
<h3 id="1-Adam"><a href="#1-Adam" class="headerlink" title="1. Adam"></a>1. Adam</h3><p>a. 引入的变量有：</p>
<ul>
<li>$v$ : 计算同momentum算法，将梯度进行指数加权平均</li>
<li>$s$: 计算同RMSprop，将梯度的平方进行指数加权平均</li>
<li>$β1$ : 计算vv的加权参数</li>
<li>$β2$ : 计算ss的加权参数</li>
</ul>
<p>b. 在迭代前，初始化参数v和s</p>
<script type="math/tex; mode=display">
v_{d W}=0, s_{d W}=0, v_{d b}=0, s_{d b}=0</script><p>c. 对第t次梯度下降的迭代 a. 首先计算dw和db的v和s</p>
<script type="math/tex; mode=display">
\begin{array}{c}{v_{d W}=\beta_{1} v_{d W}+\left(1-\beta_{1}\right) d W} \\ {s_{d W}=\beta_{2} s_{d W}+\left(1-\beta_{2}\right)(d W)^{2}} \\ {v_{d b}=\beta_{1} v_{d b}+\left(1-\beta_{1}\right) d b} \\ {s_{d b}=\beta_{2} s_{d b}+\left(1-\beta_{2}\right)(d b)^{2}}\end{array}</script><p>d. 修正</p>
<script type="math/tex; mode=display">
v_{d W}^{\text {corrected}}=\frac{v_{d W}}{1-\left(\beta_{1}\right)^{t}}\\
\begin{aligned} s_{d W}^{\text {corrected}} &=\frac{s_{d W}}{1-\left(\beta_{2}\right)^{t}} \\ v_{d b}^{\text {corrected}} &=\frac{v_{d b}}{1-\left(\beta_{1}\right)^{t}} \\ s_{d b}^{\text {corrected}} &=\frac{s_{d b}}{1-\left(\beta_{2}\right)^{t}} \end{aligned}</script><p>e. 最后更新参数W和b</p>
<script type="math/tex; mode=display">
W=W-\alpha \frac{v_{d W}^{\text {corrected}}}{\sqrt{s_{d W}^{\text { corrected }}+\varepsilon}}\\
b=b-\alpha \frac{v_{d b}^{\text {corrected}}}{\sqrt{s_{d b}^{\text { corrected }}+\varepsilon}}</script><p>超参的选择：</p>
<ul>
<li>α：需要调优</li>
<li>β1: 通常选择为0.9</li>
<li>β2: 通常选择为0.999</li>
<li>ε: 一般不需要调优，选择一个小数，比如10−8</li>
</ul>
<p>你可以尝试一系列值α，然后看哪个有效</p>
<h2 id="L09-Learning-Rate-Decay"><a href="#L09-Learning-Rate-Decay" class="headerlink" title="L09 : Learning Rate Decay"></a>L09 : Learning Rate Decay</h2><ol>
<li><p>why</p>
<p>为什么要做learning rate decay？ 较大的learning rate虽然在算法开始阶段会加快收敛速度，但在收敛接近到优化点的时候，算法会在优化点附近震荡，如下图：</p>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_22.png" alt=""></p>
<p>2.如何做learning rate decay？ <strong>思路很简单，就是引入一个函数，让α随着迭代（比如min-batch的epoch）递减</strong>。为此可以采用的decay函数有：</p>
<p>倒数：</p>
<script type="math/tex; mode=display">
\alpha :=\frac{1}{1+\text { decay rate * epoch num}} \alpha</script><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_23.png" alt=""></p>
<h2 id="L-10-The-Problem-of-local-Optima"><a href="#L-10-The-Problem-of-local-Optima" class="headerlink" title="L 10: The Problem of local Optima"></a>L 10: The Problem of local Optima</h2><p>事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p>
<p>但是一个具有高维度空间的函数，如果梯度为0，那么在每个方向，它可能是凸函数，也可能是凹函数。如果你在2万维空间中，那么想要得到局部最优，所有的2万个方向都需要是这样，但发生的机率也许很小，也许是$2^{-20000}$，因此更有可能遇到有些方向的曲线会这样向上弯曲，另一些方向曲线向下弯，而不是所有的都向上弯曲，因此在高维度空间，你更可能碰到鞍点。所有，担心收敛到local optima，真是人们想多了，实际上并没有想象的那么多local optima。在高维空间，几乎不太可能被困在一个local optima，这是一个好消息。</p>
<p>因此，在高维空间遇到的问题是高原问题（Problem of plateaus）</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_27.png" alt=""></p>
<p>Adam算法可以加速学习</p>
<h1 id="W3-Hyperparameter-tuning"><a href="#W3-Hyperparameter-tuning" class="headerlink" title="W3 Hyperparameter tuning"></a>W3 Hyperparameter tuning</h1><h2 id="L01-Tuning-process"><a href="#L01-Tuning-process" class="headerlink" title="L01 Tuning process"></a>L01 Tuning process</h2><ol>
<li>到目前为止，我们接触到的hyperparameter有：</li>
<li>learning rate: αα</li>
<li>momentum 参数: ββ</li>
<li>Adam参数: β1β1和 β2β2以及εε</li>
<li>神经网络层数: L</li>
<li>神经网络隐藏层neuron数：n[l]n[l]</li>
<li>learning rate decay参数</li>
<li>min-batch size</li>
<li>这些hyperparameter重要性排序：</li>
<li>最重要的： learning rate: αα</li>
<li>比较重要的： momentum 参数: ββ 神经网络层数: L 神经网络隐藏层neuron数：n[l]n[l]</li>
<li>次重要的： 神经网络隐藏层neuron数 learning rate decay参数</li>
<li>基本不需调整的 β1β1和 β2β2以及ε</li>
</ol>
<h4 id="1-Try-random-values-Don’t-use-a-grid"><a href="#1-Try-random-values-Don’t-use-a-grid" class="headerlink" title="1. Try random values : Don’t use a grid"></a>1. Try random values : Don’t use a grid</h4><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_28.png" alt=""></p>
<p>why:</p>
<p>举个例子，假设超参数1是（学习速率），取一个极端的例子，假设超参数2是<strong>Adam</strong>算法中，分母中的$\varepsilon$。在这种情况下，a的取值很重要，而$\varepsilon$取值则无关紧要。如果你在网格中取点，接着，你试验了a的5个取值，那你会发现，无论$\varepsilon$取何值，结果基本上都是一样的。所以，你知道共有25种模型，但进行试验的值只有5个，我认为这是很重要的。</p>
<p>对比而言，如果你随机取值，你会试验25个独立的a,$\varepsilon$，似乎你更有可能发现效果做好的那个。</p>
<h3 id="2-由粗糙到精细的策略"><a href="#2-由粗糙到精细的策略" class="headerlink" title="2. 由粗糙到精细的策略"></a>2. 由粗糙到精细的策略</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_29.png" alt=""></p>
<h2 id="L-02-Using-an-Appropriate-Scale-to-pick-hyperparameters"><a href="#L-02-Using-an-Appropriate-Scale-to-pick-hyperparameters" class="headerlink" title="L 02: Using an Appropriate Scale to pick hyperparameters"></a>L 02: Using an Appropriate Scale to pick hyperparameters</h2><p>$a$取值0.0001,1,如果你画一条从0.0001到1的数轴，沿其随机均匀取值，那90%的数值将会落在0.1到1之间，结果就是，在0.1到1之间，应用了90%的资源，而在0.0001到0.1之间，只有10%的搜索资源，这看上去不太对。</p>
<p>同时在范围内搜索，也不是均匀分布（uniformly random）的，通常有这个参数的scale，<strong>比如对数scale</strong>。</p>
<p>反而，用对数标尺搜索超参数的方式会更合理，因此这里不使用线性轴，分别依次取0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在0.0001到0.001之间，就会有更多的搜索资源可用，还有在0.001到0.01之间等等。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_32.png" alt=""></p>
<h2 id="L-03-Hyperparameter-tuning-i-practice"><a href="#L-03-Hyperparameter-tuning-i-practice" class="headerlink" title="L 03 : Hyperparameter tuning i practice"></a>L 03 : Hyperparameter tuning i practice</h2><ol>
<li>不同的算法和场景，对超参的scale敏感性可能不一样.</li>
<li>根据计算资源和数据量，可以采取两种策略来调参<ul>
<li>Panda（熊猫策略）：对一个模型先后修改参数，查看其表现，最终选择最好的参数。就像熊猫一样，一次只抚养一个后代。</li>
<li>Caviar（鱼子酱策略）：计算资源足够，可以同时运行很多模型实例，采用不同的参数，然后最终选择一个好的。类似鱼类，一次下很多卵，自动竞争成活。 </li>
</ul>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_33.png" alt=""></p>
<h3 id="L-04-Normalizing-Activations-in-a-Network"><a href="#L-04-Normalizing-Activations-in-a-Network" class="headerlink" title="L 04: Normalizing Activations in a Network"></a>L 04: Normalizing Activations in a Network</h3><h4 id="1-Implementing-Batch-Normalizing"><a href="#1-Implementing-Batch-Normalizing" class="headerlink" title="1. Implementing Batch Normalizing"></a>1. Implementing Batch Normalizing</h4><p><strong>Batch</strong>归一化,<strong>Batch</strong>归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会是你的训练更加容易，甚至是深层网络。</p>
<p>可以normalize $a^{[l]},z^{[l]}$,选择$z^{[L]}$</p>
<p><strong>设置 γ 和 β 的原因</strong>是，如果各隐藏层的输入均值在靠近 0 的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。因此，需要用 γ 和 β 对标准化后的结果做进一步处理。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_34.png" alt=""></p>
<p>需要注意的是，β和γ不是超参，而是梯度下降需学习的参数。</p>
<h2 id="L-05-Fitting-Batch-Norm-Into-Neural-Networks"><a href="#L-05-Fitting-Batch-Norm-Into-Neural-Networks" class="headerlink" title="L 05 : Fitting Batch Norm Into Neural Networks"></a>L 05 : Fitting Batch Norm Into Neural Networks</h2><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_35.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_36.png" alt=""></p>
<p>注意</p>
<p>先前我说过每层的参数是$w^{[l]}$和$b^{[l]}$，还有$\beta^{[l]}$和$b^{[l]}$，请注意计算的方式如下，$z^{[l]}=w^{[l]} a^{[l-1]}+b^{[l]}$，但<strong>Batch</strong>归一化做的是，它要看这个<strong>mini-batch</strong>，先将$z^{[l]}$归一化，结果为均值0和标准方差，再由$\beta$和b重缩放，但这意味着，无论$b^{[l]}$的值是多少，都是要被减去的，因为在<strong>Batch</strong>归一化的过程中，你要计算的$z^{[l]}$均值，再减去平均值，在此例中的<strong>mini-batch</strong>中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消.</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_37.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_38.png" alt=""></p>
<p>最后，请记住的维$z^{[l]}$，因为在这个例子中，维数会是$\left(n^{[l]}, 1\right)$，的尺寸为，如果是l层隐藏单元的数量，那$ \beta^{[l]}$和$ \gamma^{[l]}$的维度也是$\left(n^{[l]}, 1\right)$，因为这是你隐藏层的数量，你有隐藏单元，<strong>所以$\gamma^{[l]}$和</strong>$  \beta^{[l]}​$用来将每个隐藏层的均值和方差缩放为网络想要的值。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_39.png" alt=""></p>
<h3 id="L-06-Why-Doest-Batch-Norm-Work"><a href="#L-06-Why-Doest-Batch-Norm-Work" class="headerlink" title="L 06 Why Doest Batch Norm Work?"></a>L 06 Why Doest Batch Norm Work?</h3><ol>
<li>首先，起到了normalization的作用，同对输入数据X的normalization作用类似。</li>
</ol>
<ol>
<li><p>让每一层的学习，<strong>一定程度解耦了前层参数和后层参数，让各层更加独立的学习</strong>。无论前一层如何变，本层输入的数据总是保持稳定的均值和方差。（主要原因）</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_40.png" alt=""></p>
<p>所以使你数据改变分布的这个想法，有个有点怪的名字“<strong>Covariate shift</strong>”，想法是这样的，如果你已经学习了到 的映射，如果 的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由 到 映射保持不变，正如此例中，因为真实函数是此图片是否是一只猫，训练你的函数的需要变得更加迫切，如果真实函数也改变，情况就更糟了。</p>
<p>关于第二点，如果实际应用样本和训练样本的数据分布不同（例如，橘猫图片和黑猫图片），我们称发生了“<strong>Covariate Shift</strong>”。这种情况下，一般要对模型进行重新训练。Batch Normalization 的作用就是减小 Covariate Shift 所带来的影响，让模型变得更加健壮，鲁棒性（Robustness）更强。</p>
<p>即使输入的值改变了，由于 Batch Normalization 的作用，使得均值和方差保持不变（由 γ 和 β 决定），限制了在前层的参数更新对数值分布的影响程度，因此后层的学习变得更容易一些。Batch Normalization 减少了各层 W 和 b 之间的耦合性，让各层更加独立，实现自我训练学习的效果。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_41.png" alt=""></p>
<p>另外，Batch Normalization 也<strong>起到微弱的正则化</strong>（regularization）效果。因为在每个 mini-batch 而非整个数据集上计算均值和方差，只由这一小部分数据估计得出的均值和方差会有一些噪声，因此最终计算出的 z~(i)z~(i)也有一定噪声。类似于 dropout，这种噪声会使得神经元不会再特别依赖于任何一个输入特征。</p>
<p>因为 Batch Normalization 只有微弱的正则化效果，因此可以和 dropout 一起使用，以获得更强大的正则化效果。通过应用更大的 mini-batch 大小，可以减少噪声，从而减少这种正则化效果。</p>
<p>最后，不要将 Batch Normalization 作为正则化的手段，而是当作加速学习的方式。正则化只是一种非期望的副作用，Batch Normalization 解决的还是反向传播过程中的梯度问题（梯度消失和爆炸）。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_42.png" alt=""></p>
</li>
</ol>
<h3 id="L-07-Batch-Norm-At-Test-Time"><a href="#L-07-Batch-Norm-At-Test-Time" class="headerlink" title="L 07 : Batch Norm At Test Time"></a>L 07 : Batch Norm At Test Time</h3><p>问题：BN算法在训练时是一个批次的数据训练，能算出每一层Z的均值和方差；而在测试时，输入的则是单个数据，<strong>单条数据没法做均值和方差的计算</strong>，怎么在测试期输入均值和方差呢?</p>
<p>实际应用中一般不使用这种方法，而是使用之前学习过的指数加权平均的方法来预测测试过程单个样本的 μ 和 $σ^2$</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_43.png" alt=""></p>
<p>计算$z_{\text { norm }}^{(\hat{2})}$，用$\mu$ 和$ \sigma^{2}$的指数加权平均，用你手头的最新数值来做调整，然后你可以用左边我们刚算出来的和你在神经网络训练过程中得到的$\beta$和$\sigma$参数来计算你那个测试样本的z值。</p>
<h3 id="L-08-Softmax-Regression"><a href="#L-08-Softmax-Regression" class="headerlink" title="L 08 : Softmax Regression"></a>L 08 : Softmax Regression</h3><h4 id="1-Multi-class-classification"><a href="#1-Multi-class-classification" class="headerlink" title="1. [Multi-class classification]"></a>1. [Multi-class classification]</h4><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_44.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_45.png" alt=""></p>
<p>最后一层是概率，之和为1，要用到<strong>Softmax</strong>层，<strong>Softmax</strong>激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量，最后输出一个向量。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_46.png" alt=""></p>
<h4 id="2-Softmax-example"><a href="#2-Softmax-example" class="headerlink" title="2. Softmax example"></a>2. Softmax example</h4><p>没有隐藏层的softmax,代表一些决策边界</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_47.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_48.png" alt=""></p>
<h3 id="L-09-Training-SoftMax-classifier"><a href="#L-09-Training-SoftMax-classifier" class="headerlink" title="L 09 Training SoftMax classifier"></a>L 09 Training SoftMax classifier</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_49.png" alt=""></p>
<p><strong>Softmax</strong>这个名称的来源是与所谓<strong>hardmax</strong>对比,<strong>Softmax</strong>回归或<strong>Softmax</strong>激活函数将<strong>logistic</strong>激活函数推广到类，而不仅仅是两类，结果就是如果C=2，那么C=2的<strong>Softmax</strong>实际上变回了<strong>logistic</strong>回归，</p>
<h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_50.png" alt=""></p>
<script type="math/tex; mode=display">
J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_51.png" alt=""></p>
<h4 id="3-Gradient-descent-with-softmax"><a href="#3-Gradient-descent-with-softmax" class="headerlink" title="3. Gradient descent with softmax"></a>3. Gradient descent with softmax</h4><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_52.png" alt=""></p>
<p>最后一层求导，softmax激活函数</p>
<script type="math/tex; mode=display">
J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><h2 id="L11-TensorFlow"><a href="#L11-TensorFlow" class="headerlink" title="L11 TensorFlow"></a>L11 TensorFlow</h2><h4 id="1-基本流程"><a href="#1-基本流程" class="headerlink" title="1. 基本流程"></a>1. 基本流程</h4><p>使用tensorflow，只要告诉tensorflow forward prop，它自己就会做backprop，因此不用自己实现backprop</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_53.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#导入TensorFlow</span></span><br><span class="line">​</span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype = tf.float32)</span><br><span class="line"><span class="comment">#接下来，让我们定义参数w，在TensorFlow中，你要用tf.Variable()来定义参数</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#然后我们定义损失函数：</span></span><br><span class="line">​</span><br><span class="line">cost = tf.add(tf.add(w**<span class="number">2</span>,tf.multiply(- <span class="number">10.</span>,w)),<span class="number">25</span>)</span><br><span class="line"><span class="comment">#然后我们定义损失函数J</span></span><br><span class="line">然后我们再写：</span><br><span class="line">​</span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line"><span class="comment">#(让我们用0.01的学习率，目标是最小化损失)。</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#最后下面的几行是惯用表达式:</span></span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line">session = tf.Session()<span class="comment">#这样就开启了一个TensorFlow session。</span></span><br><span class="line">​</span><br><span class="line">session.run(init)<span class="comment">#来初始化全局变量。</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#然后让TensorFlow评估一个变量，我们要用到:</span></span><br><span class="line">​</span><br><span class="line">session.run(w)</span><br><span class="line">​</span><br><span class="line"><span class="comment">#上面的这一行将w初始化为0，并定义损失函数，我们定义train为学习算法，它用梯度下降法优化器使损失函数最小化，但实际上我们还没有运行学习算法，所以#上面的这一行将w初始化为0，并定义损失函数，我们定义train为学习算法，它用梯度下降法优化器使损失函数最小化，但实际上我们还没有运行学习算法，所以session.run(w)评估了w，让我：：</span></span><br><span class="line">​</span><br><span class="line">print(session.run(w))</span><br><span class="line">​</span><br><span class="line">所以如果我们运行这个，它评估等于<span class="number">0</span>，因为我们什么都还没运行。</span><br><span class="line"></span><br><span class="line"><span class="comment">#现在让我们输入：</span></span><br><span class="line">​</span><br><span class="line">$session.run(train)，它所做的就是运行一步梯度下降法。</span><br><span class="line"><span class="comment">#接下来在运行了一步梯度下降法后，让我们评估一下w的值，再print：</span></span><br><span class="line">​</span><br><span class="line">print(session.run(w))</span><br><span class="line"><span class="comment">#在一步梯度下降法之后，w现在是0.1。</span></span><br></pre></td></tr></table></figure>
<h4 id="2-如何用训练数据"><a href="#2-如何用训练数据" class="headerlink" title="2. 如何用训练数据"></a>2. 如何用训练数据</h4><p>placeholder 在实际的训练过程中，要用不同的样本反复放到一个待优化函数中的，这个时候就可以用tensorflow的placeholder实现,在run的时候，对应给出<code>feed_dict</code>，表名占位符x的实际值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 导入Tensorflow</span></span><br><span class="line"></span><br><span class="line">coefficient = np.array([[<span class="number">2.</span>],[<span class="number">-10.</span>],[<span class="number">25.</span>]])</span><br><span class="line"></span><br><span class="line">w = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="number">3</span>,<span class="number">1</span>]) <span class="comment"># 3x1大小的placeholder</span></span><br><span class="line">cost = w**x[<span class="number">0</span>][<span class="number">0</span>] - x[<span class="number">1</span>][<span class="number">0</span>]*w + x[<span class="number">2</span>][<span class="number">0</span>] <span class="comment"># 要优化的cost function（即forward prop的形式）</span></span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost) </span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line">print(session.run(w))</span><br><span class="line"></span><br><span class="line">session.run(train, feed_dict=&#123;x:coefficient&#125;) <span class="comment"># x占位符替换为coefficient</span></span><br><span class="line">print(session.run(w))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    session.run(train, feed_dict=&#123;x:coefficient&#125;) <span class="comment"># # x占位符替换为coefficient</span></span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure>
<h4 id="3-计算流"><a href="#3-计算流" class="headerlink" title="3. 计算流"></a>3. 计算流</h4><p><strong>TensorFlow</strong>程序的核心是计算损失函数，然后<strong>TensorFlow</strong>自动计算出导数，以及如何最小化损失，因此这个等式或者这行代码所做的就是让<strong>TensorFlow</strong>建立计算图，</p>
<p>with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。建立计算流的过程，前向传播的过程，operation</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_56.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_54.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_55.png" alt=""></p>
<h1 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h1><font color="read">how to systematically organize the hyper parameter search process and  batch normalization and framework </font>

<p><a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2" target="_blank" rel="noopener">http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2</a></p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3" target="_blank" rel="noopener">http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3</a></p>
<p><a href="http://dl-notes.imshuai.com/#/c2w1?id=_4-heros-of-deep-learning-yoshua-bengio-interview" target="_blank" rel="noopener">http://dl-notes.imshuai.com/#/c2w1?id=_4-heros-of-deep-learning-yoshua-bengio-interview</a></p>
<p><a href="https://www.youtube.com/watch?v=4Ct3Yujl1dk&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=14" target="_blank" rel="noopener">https://www.youtube.com/watch?v=4Ct3Yujl1dk&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=14</a></p>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">XieMay</p>
              <p class="site-description motion-element" itemprop="description">期待花开</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">49</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">39</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="http://www.cnblogs.com/shiyiandchuixue/" target="_blank" title="BLOGS"><i class="fa fa-fw fa-globe"></i>BLOGS</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:2323020965@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://github.com/shiyichuixue" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="powered-by">
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XieMay</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Symbols count total&#58;</span>
    
    <span title="Symbols count total">277k</span>
  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Pisces</a> v6.0.4</div>



</div>
        








        
      </div>
    </footer>

    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


















  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.4"></script>



  



	





  









  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
    
  
  <script src="[object Object]"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '1Dfsb4DwGQPCIlbyCKt9egUR-gzGzoHsz',
        appKey: '8OKqJPeMRRQnxx2vaAwIkM8y',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    
      
    
      
    
      
        
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"model":"wanko","bottom":-30,"mobileShow":false,"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
