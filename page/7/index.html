<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="baidu-site-verification" content="E1Di33CelZ">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="WsojkhmMcOefku3B2Vxp02NtxlUt_JzBP1fVPrFk3Gw">


  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.0.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.4">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.4" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: 'WGLQQAQKBA',
      apiKey: '',
      indexName: 'xiemay',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  



<meta name="baidu-site-verification" content="z0tVDge8Pe">

  
  <meta name="keywords" content="Hexo, NexT">


<meta name="description" content="期待花开">
<meta property="og:type" content="website">
<meta property="og:title" content="welcome">
<meta property="og:url" content="http://xiemaycherry.github.io/page/7/index.html">
<meta property="og:site_name" content="welcome">
<meta property="og:description" content="期待花开">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="welcome">
<meta name="twitter:description" content="期待花开">






  <link rel="canonical" href="http://xiemaycherry.github.io/page/7/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>welcome</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<meta name="baidu-site-verification" content="3BmH9zSH5h"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">welcome</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>
        </li>
      
        
        <li class="menu-item menu-item-留言">
          <a href="/guestbook/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-commenting"></i> <br>留言</a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>Schedule</a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>Sitemap</a>
        </li>
      
        
        <li class="menu-item menu-item-baidusitmap">
          <a href="/baidusitmap.xml" rel="section">
            <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>baidusitmap</a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/13/machine learning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/13/machine learning/" itemprop="url">test</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-13T19:59:23+08:00">2019-04-13</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/13/machine learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/13/machine learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                109字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/2019/04/13/machine learning/%E5%90%B4%E6%81%A9%E8%BE%BE%5Cimage-20200624204341481.png" alt="image-20200624204341481"></p>
<p>英伟达:芯片，GPU</p>
<p>开发框架：tensorflow，pytorch caffe</p>
<h2 id="监督学习"><a class="header-anchor" href="#监督学习">¶</a>监督学习</h2>
<p>学习目的是学习一个输入到输出的映射，称为模型。模型的集合就是假设空间。</p>
<p>模型：概率模型；非概率模型；</p>
<p>学习过程：搜索过程</p>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/11/tensorflow/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/11/tensorflow/" itemprop="url">tensorflow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-11T15:04:23+08:00">2019-04-11</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/编程语言/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/11/tensorflow/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/11/tensorflow/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                11k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1>official definition</h1>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2019/04/11/tensorflow/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/" itemprop="url">Deep Learning Neural Network and Deep Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-11T09:25:45+08:00">2019-04-11</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习-Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">深度学习(Deep Learning)</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                28k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1>Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)</h1>
<h1>C1W1</h1>
<h2 id="c1w1l01-welcome"><a class="header-anchor" href="#c1w1l01-welcome">¶</a>C1W1L01: Welcome</h2>
<p>AI is the new Electricity!</p>
<p>Course 1: Neural Networks and Deep Learning</p>
<p>Course 2: Improving Deep Neural Networks: Hyperparameter tuning,Regularization and Optimization</p>
<p>Course 3: Structuring your Machine Learning project</p>
<p>Course 4: Convolutional Neural Networks</p>
<p>Course 5: Natural Langurge Processing: Building sequence models</p>
<h2 id="c1w1l02-what-is-neural-network"><a class="header-anchor" href="#c1w1l02-what-is-neural-network">¶</a>C1W1L02 : What is Neural Network</h2>
<p>Deep Learning = training (very large) neural network</p>
<h3 id="for-example-of-house-prize-prediction-the-simplest-neural-network"><a class="header-anchor" href="#for-example-of-house-prize-prediction-the-simplest-neural-network">¶</a>For example of house prize prediction : the simplest neural network</h3>
<p>如果现在有六栋房子的信息，分别是房子的大小(size of house)和对应的价格(prize),绘制出如下的。自然的想法：线性回归，得到拟合的直线。值得注意的是，房价不可能是负数吧！因此下图中蓝色的线，大致就是我们所需要的函数。这个对应一个最简单神经网络（neural network）</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\housr_prize_1.png)</p>
<p>上述是一个tiny little neural network，更大的，更复杂的神经网络是</p>
<p>把很多最简单的single neural堆积(stacking)到一起。</p>
<h3 id="for-example-of-house-prize-prediction-stacking-the-neural"><a class="header-anchor" href="#for-example-of-house-prize-prediction-stacking-the-neural">¶</a>For example of house prize prediction : stacking the  neural</h3>
<p>上面这个例子，仅仅考虑特征是size,实际情况上，与房屋相关的特征还有number of bedrooms、zip code、wealth, number of bedrooms and size affect family size. The zip code is a feature that tells you you know walkability. The wealth tells you how good is the school quality</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\house_prize_2.png)</p>
<p>hidden layer 用输入层计算得到，因此说输入层与中间层紧密连接起来了</p>
<h3 id="the-actual-application-of-neural-networks"><a class="header-anchor" href="#the-actual-application-of-neural-networks">¶</a>The actual application of neural networks</h3>
<p>hidden layer 与上一层的连接情况并不是手工确定，每一层都是上一层所有的输入函数，所以建立的神经网络如下：</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\house prize 3.png)</p>
<p>The remarkable thing about neural network</p>
<ol>
<li>Given enough data about X&amp;Y (x,y) which good at freaking out functions :map x to y</li>
<li>Most powerful in supervised learning</li>
</ol>
<h3 id="c2w1cl03-supervised-learning-with-neural-network"><a class="header-anchor" href="#c2w1cl03-supervised-learning-with-neural-network">¶</a>C2W1CL03 : Supervised Learning with Neural Network</h3>
<h3 id="常见的监督学习"><a class="header-anchor" href="#常见的监督学习">¶</a>常见的监督学习</h3>
<p>截止到目前，<strong>Neural Network的成功应用基本都在Supervised Learning</strong>。比如：Ad，Images vision, Audio to Text, Machine translation, Autonomous Driving</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\supervised-learning-exmples.png)</p>
<h3 id="常见的神经网络的设计"><a class="header-anchor" href="#常见的神经网络的设计">¶</a>常见的神经网络的设计</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\NeuralNetworkExamples.png)</p>
<p>卷积神经网络：<strong>Convolutional Neural Network</strong> (CNN) 通常有用图像数据</p>
<p>递归神经网络： <strong>Recurrent Neural Network</strong> (RNN) 通常用于time series</p>
<p>对应复杂的应用中，定制一些复杂的混合的神经网络结构</p>
<h3 id="结构化和非结构化数据"><a class="header-anchor" href="#结构化和非结构化数据">¶</a>结构化和非结构化数据</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\datastructure.png)</p>
<p>处理非结构化数据是很难的，与结构化数据比较，让计算机理解非结构化数据很难</p>
<h3 id="c1w1l04-why-is-deep-learning-taking-off"><a class="header-anchor" href="#c1w1l04-why-is-deep-learning-taking-off">¶</a>C1W1L04: Why is deep learning taking off</h3>
<p>Answer: scale</p>
<p>If you want to hit this very high level of performance ,firstly, you need to be able train a big enough neural network in order to take advantage of the huge amount of data and second you need to be out here on the x axes you do need a lot of data.</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\scale.jpg)</p>
<ol>
<li>If you do not have a lot training data is often up to your skill at hand engineering features that determines the foreman.在这个小的训练集中，各种算法的优先级事实上定义的也不是很明确，所以如果你没有大量的训练集，那效果会取决于你的特征工程能力，那将决定最终的性能。</li>
<li>这个图形区域的左边，各种算法之间的优先级并不是定义的很明确，最终的性能更多的是取决于你在用<strong>工程选择特征</strong>方面的能力以及<strong>算法处理方面</strong>的一些细节.</li>
<li>只是在某些大数据规模非常庞大的训练集，也就是在右边这个会非常的大时，我们能更加持续地看到更大的由神经网络控制其它方法.</li>
</ol>
<h3 id="the-reason"><a class="header-anchor" href="#the-reason">¶</a>The Reason</h3>
<ol>
<li>
<p>the scale of data</p>
</li>
<li>
<p>the speed of computation  such as GPUS</p>
</li>
<li>
<p>innovation of algorithm</p>
<p>许多算法方面的创新，一直是在尝试着使得神经网络运行的更快</p>
<p>switch sigmoid function to relu function</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\algorithm——rul.jpg)</p>
</li>
</ol>
<p>在这个区域，也就是这个<strong>sigmoid</strong>函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做<strong>ReLU</strong>的函数（修正线性单元），<strong>ReLU</strong>它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。</p>
<p>训练你的神经网络的过程，很多时候是凭借直觉的，往往你对神经网络架构有了一个想法，于是你尝试写代码实现你的想法，然后让你运行一个试验环境来告诉你，你的神经网络效果有多好，通过参考这个结果再返回去修改你的神经网络里面的一些细节，然后你不断的重复上面的操作，当你的神经网络需要很长时间去训练，需要很长时间重复这一循环，在这里就有很大的区别，根据你的生产效率去构建更高效的神经网络。当你能够有一个想法，试一试，看效果如何。在10分钟内，或者也许要花上一整天，如果你训练你的神经网络用了一个月的时间，有时候发生这样的事情，也是值得的，因为你很快得到了一个结果。在10分钟内或者一天内，你应该尝试更多的想法，那极有可能使得你的神经网络在你的应用方面工作的更好、更快的计算，在提高速度方面真的有帮助，那样你就能更快地得到你的实验结果。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\faster.jpg)</p>
<h2 id="font-color-red-summary-font"><a class="header-anchor" href="#font-color-red-summary-font">¶</a><font color="red">Summary</font></h2>
<p><font color="green">早上花了2h小时学习第一周的视频，先看一遍视频的字幕，逐字逐句的理解，虽然很多时候都是自己乱猜的，大概清楚讲的什么！然后再看大牛的笔记，然后再看一篇结合PPT。下午也看了半个多小时。问题：1. 自己的英文水平不够，这个需要大大的提高讷。2. 其实只要看别人的笔记就可以知道内容，但是还是想听andow ng的讲解。3. 视频都比较短，每个视频设计的知识点或者内容不多，1到3个，分成知识点做笔记还是不错的</font></p>
<p><font color="blue">这一周的内容，也就是今天我学习的知识简单和容易理解。学习了神经网络的大致结构，神经网络的应用领域，深度学习为什么取得快速的发展的三点原因，尤其是数据scale与其他方法和神经网络规模的大致性能关系</font></p>
<h1>C1W2</h1>
<h3 id="c1w2l01-binary-classification"><a class="header-anchor" href="#c1w2l01-binary-classification">¶</a>C1W2L01: Binary Classification</h3>
<p>In this week, we’re going to go over the basics of neural network programming. We are going to study handle data without for loop.</p>
<p>forward password for propagation</p>
<p>backward pass or what’s called a backward propagation step</p>
<p>Why the computations in learning in a neural network can be organized in this board propagation and a separate backward propagation by using logistic regression to convey(传达) theses ideas.</p>
<h3 id="binary-classification"><a class="header-anchor" href="#binary-classification">¶</a>Binary Classification</h3>
<p>Input； an image . three separate matrices corresponding red green and blue color channels of this image. 如果你的图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值</p>
<p>unroll all of these pixel intensity values into  a feature vector</p>
<p>pixel intensity values of this image</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\w_piexl.jpg)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\2_blue_green_read.jpg)</p>
<p>notation</p>
<p>(x,y)： a pair X comma Y</p>
<p>$M_{train}$: M subscript train</p>
<p>每条测试集在矩阵中都是以列向量的形式存在</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\2_noation.png)</p>
<p>Matrix capital</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\2_nation_2.png)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\2_all_nation.jpg)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\2_all_nation_1.jpg)</p>
<h3 id="model-hypothesis-function-logistic-regression"><a class="header-anchor" href="#model-hypothesis-function-logistic-regression">¶</a>Model : hypothesis Function :Logistic Regression</h3>
<p>So given an input X and the parameters W and b, how do we generate the output Y hat? Well, one thing you could try, that doesn’t work, would be to have Y hat be w transpose X plus B, kind of a linear function of the input X. And in fact, this is what you use if you were doing linear regression. But this isn’t a very good algorithm for binary classification because you want Y hat to be the chance that Y is equal to one.</p>
<p>So,Y hat should really be between zero and one. This is what the sigmoid function looks like.</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\log_1.png)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\log_3.jpg)</p>
<h3 id="sigmoid-function"><a class="header-anchor" href="#sigmoid-function">¶</a>sigmoid function</h3>
<p>$$<br>
\sigma(z) = \frac{1}{1+e^{-z}}<br>
$$</p>
<p>因为你想让$\hat{y}$表示实际值$y$等于1的机率的话， 应该在0到1之间。这是一个需要解决的问题，因为可能比1要大得多，或者甚至为一个负值。对于你想要的在0和1之间的概率来说它是没有意义的，因此在逻辑回归中，我们的输出应该是等于由上面得到的线性函数式子作为自变量的<strong>sigmoid</strong>函数中，公式如上图最下面所示，将线性函数转换为非线性函数。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\log_2.jpg)</p>
<p>注意：原来$w,b$是分开在，这里就合并，引入变量$x_0=1$,对应偏置$b$,</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\log_3.png)</p>
<h3 id="strategy：cost-function"><a class="header-anchor" href="#strategy：cost-function">¶</a>Strategy：Cost function</h3>
<p>Firstly : Loss function<br>
$$<br>
L(\hat{y},y)=\frac{1}{2}\sum{(y_i-\hat{y_i})^2}<br>
$$<br>
这个优化问题不是凸优化问题(non-convex)，因此不选用这个</p>
<p>Secondly，<br>
$$<br>
L(y,\hat{y})=-(ylog<sup>{\hat{y}}+(1-y)log</sup>{1-\hat{y}})<br>
$$<br>
![](Deep Learning.ai_Neural Networks and Deep Learning\log_cost_1.jpg)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\log__cost_2.jpg)</p>
<h3 id="algorithm-gradient-descent"><a class="header-anchor" href="#algorithm-gradient-descent">¶</a>Algorithm: Gradient Descent</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\GD1.jpg)</p>
<p>Gradient Descent算法步骤：</p>
<ol>
<li>Initialize $w$, $b$ to zero</li>
<li>repeat：</li>
</ol>
<p>$w :=w−\alpha \frac{∂J(w,b)}{∂w}$</p>
<p>$b :=b-\alpha \frac{∂J(w,b)}{∂b}$</p>
<h3 id="c1w2l05-c1w2l06-derivatives"><a class="header-anchor" href="#c1w2l05-c1w2l06-derivatives">¶</a>C1W2L05 &amp; C1W2L06 Derivatives</h3>
<p>求导，这个是微积分的内容，不用写了！</p>
<h3 id="c1w2l07：-computation-graph"><a class="header-anchor" href="#c1w2l07：-computation-graph">¶</a>C1W2L07： Computation Graph</h3>
<h3 id="c1w2l08-derivatives-with-compution-graphs"><a class="header-anchor" href="#c1w2l08-derivatives-with-compution-graphs">¶</a>C1W2L08 : Derivatives with compution graphs</h3>
<p>链式法则<br>
$$<br>
\frac{\partial L}{\partial v}=\frac{\partial L}{\partial u}\frac{\partial u}{\partial v}<br>
$$</p>
<h3 id="c1w2l09-logistic-regression-gradient-descent"><a class="header-anchor" href="#c1w2l09-logistic-regression-gradient-descent">¶</a>C1W2L09 : Logistic Regression Gradient Descent</h3>
<h4 id="single-training-example"><a class="header-anchor" href="#single-training-example">¶</a>single training example</h4>
<p>You’ve seen the loss function that measures how well you’re doing on the single training example. You’ve also seen the cost function that measures how well your parameters w and b are doing on your entire training set.</p>
<p>You’ve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives.</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\gd_1.jpg)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\gd_2.png)<br>
$$<br>
\frac{\partial L}{\partial w}=\frac{\partial L}{\partial \alpha }\frac{\partial \alpha }{\partial z}\frac{\partial z}{\partial w}<br>
\=-(\frac{y}{a}+\frac{1-y}{1-a})a(1-a)x=(a-y)x<br>
$$</p>
<h3 id="c1w2l10-gradient-descent-on-m-example"><a class="header-anchor" href="#c1w2l10-gradient-descent-on-m-example">¶</a>C1W2L10 Gradient Descent on m example</h3>
<p>$$<br>
\min L(w,b)=\sum_{i=1}^{m}L(\alpha_i,y_i)/m\<br>
\frac{\partial L }{\partial w}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial w})/m=(\sum_{i=1}^{m}(a-y_i)x_i)/m\<br>
\frac{\partial L }{\partial b}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial b})/m=(\sum_{i=1}^{m}(a-1)x_i)/m<br>
$$</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\gd_3jpg.jpg)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\cost_3.png)</p>
<p>上面的伪代码告诉我们，需要多次for loop完成代码，但是这会造成运算速度下降！因为我们越来越多地训练非常大的数据集，因此你真的需要你的代码变得非常高效。所以在接下来的几个视频中，我们会谈到向量化，以及如何应用向量化而连一个<strong>for</strong>循环都不使用。所以学习了这些，我希望你有关于如何应用逻辑回归，或是用于逻辑回归的梯度下降，事情会变得更加清晰</p>
<h3 id="summary"><a class="header-anchor" href="#summary">¶</a>summary</h3>
<p><font color="red">今天主要学习了以logistics regression 为例，如何通过链式求导的过程，简单的练习一下，以及再次了解什么是梯度下降法，以及训练学习算法的需要一个损失函数，训练的过程就是求损失函数最优值的过程</font></p>
<h3 id="c1w2l11-vectorization"><a class="header-anchor" href="#c1w2l11-vectorization">¶</a>C1W2L11: Vectorization</h3>
<h4 id="1-什么是vectorization：将-for-loop-尽可能转换为矩阵运算"><a class="header-anchor" href="#1-什么是vectorization：将-for-loop-尽可能转换为矩阵运算">¶</a>1. 什么是Vectorization：将 for loop 尽可能转换为矩阵运算</h4>
<p>通过<strong>numpy</strong>内置函数和避开显式的循环(<strong>loop</strong>)的方式进行向量化，从而有效提高代码速度。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\v_1.jpg)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.dot(a,b)</span><br><span class="line">如果a,b是一维数组，则计算点积</span><br><span class="line">如果a,b是多维数据，则矩阵乘法</span><br></pre></td></tr></table></figure>
<h4 id="2-an-example-of-vectorization"><a class="header-anchor" href="#2-an-example-of-vectorization">¶</a>2. An example of vectorization</h4>
<p>vectorization的好处：conciser code, but faster execution 一个简单的对比实验：1,000,000大小的两个向量内积计算，for loop要比Vectorization快300倍。 在Deep Learning时代，vectorization是一项重要的技能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入numpy库</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment">#创建一个数据a</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># [1 2 3 4]</span></span><br><span class="line"><span class="keyword">import</span> time <span class="comment">#导入时间库</span></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>) <span class="comment">#通过round随机得到两个一百万维度的数组</span></span><br><span class="line">tic = time.time() <span class="comment">#现在测量一下当前时间</span></span><br><span class="line"><span class="comment">#向量化的版本</span></span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line">print(“Vectorized version:” + str(<span class="number">1000</span>*(toc-tic)) +”ms”) <span class="comment">#打印一下向量化的版本的时间</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#继续增加非向量化的版本</span></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(“For loop:” + str(<span class="number">1000</span>*(toc-tic)) + “ms”)<span class="comment">#打印for循环的版本的时间</span></span><br></pre></td></tr></table></figure>
<h4 id="3-gpu-or-cpu"><a class="header-anchor" href="#3-gpu-or-cpu">¶</a>3. GPU or CPU</h4>
<ol>
<li>
<p>大规模的深度学习再<strong>GPU</strong>或者图像处理单元运行”，<strong>CPU</strong>和<strong>GPU</strong>都有并行化的指令，他们有时候会叫做<strong>SIMD</strong>指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了<strong>built-in</strong>函数,像<code>np.function</code>或者并不要求你实现循环的函数，它可以让<strong>python</strong>的充分利用并行化计算。</p>
</li>
<li>
<p>只是在<strong>GPU</strong>和<strong>CPU</strong>上面计算，<strong>GPU</strong>更加擅长<strong>SIMD</strong>计算，但是<strong>CPU</strong>事实上也不是太差，可能没有<strong>GPU</strong>那么擅长吧。SIMD Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data)</p>
</li>
</ol>
<h3 id="c12l12-：-more-vectorization-example"><a class="header-anchor" href="#c12l12-：-more-vectorization-example">¶</a>C12L12 ： More Vectorization Example</h3>
<h3 id="矩阵和向量乘法"><a class="header-anchor" href="#矩阵和向量乘法">¶</a>矩阵和向量乘法</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\v_2.png)</p>
<h3 id="向量函数"><a class="header-anchor" href="#向量函数">¶</a>向量函数</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\v_3.png)</p>
<ol>
<li>原则：whenever possible, avoid explict for-loops</li>
<li>使用Element wised的矩阵运算，将函数作用在每个矩阵元素上，比如：
<ul>
<li>np.exp()</li>
<li>np.log()</li>
<li>np.abs()</li>
<li>np.maxium()</li>
<li>1/v</li>
<li>v**2</li>
</ul>
</li>
</ol>
<h3 id="c1w2l13-vectorizing-logistic-regression"><a class="header-anchor" href="#c1w2l13-vectorizing-logistic-regression">¶</a>C1W2L13: Vectorizing Logistic Regression</h3>
<h3 id="1-前向传播"><a class="header-anchor" href="#1-前向传播">¶</a>1. 前向传播</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\v_3.jpg)<br>
$$<br>
\hat{y}=σ(w^TX+b)=(a(1),a(2),…,a(m−1),a(m))=\<br>
(\alpha(z_1),\alpha(z_m),…,\alpha(z_m))=\<br>
(\alpha(w<sup>Tx_1+b),\alpha(w</sup>Tx_2+b),…,\alpha(w^Tx_m+b))=<br>
$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">z=np.dot(W^T,X)+b</span><br><span class="line"><span class="comment"># z这里就是python 巧妙的地方，b是实数，但是向量加上实数后，b扩展成向量，被称为广播（brosdcasting）</span></span><br></pre></td></tr></table></figure>
<p>个人经验：</p>
<ol>
<li>首先，熟悉每个变量的记号和维度，必要的话，可以画出来，更直观。</li>
<li>先从一个样本做向量化，再把m个样本的操作向量化。</li>
<li>for-loop里面是循环乘法，则向量化一定是一个乘法形式，若对于不确定乘法的左右关系，是否需转置，可以根据目标变量的维度推测。或者先乘起来，再根据目标变量看是否要转置。</li>
</ol>
<h3 id="c1w2l14-vectorzing-logistic-regression-s-gradient-compution"><a class="header-anchor" href="#c1w2l14-vectorzing-logistic-regression-s-gradient-compution">¶</a>C1W2L14 : Vectorzing Logistic Regression’s Gradient Compution</h3>
<ol>
<li>
<p>backforwd</p>
</li>
<li>
<p>$$<br>
\frac{∂J}{∂w}=\frac{1}{m}X(A−Y)T\<br>
\frac{∂J}{∂b}=\frac{1}{m}(a(i)−y(i))<br>
$$</p>
</li>
</ol>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\v-4.jpg)</p>
<p>重要的是弄清楚，里面的行列关系，代表的意思，运算时候，先自己理清楚。还有点积、等等运算性质对应的操作，或者对应的内置函数</p>
<h3 id="c1w2l15-broadcasting-in-python"><a class="header-anchor" href="#c1w2l15-broadcasting-in-python">¶</a>C1W2L15: Broadcasting in Python</h3>
<h3 id="one-example"><a class="header-anchor" href="#one-example">¶</a>One Example</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\bordacasing_4.png)</p>
<p><code>A.sum(axis = 0)</code>中的参数<code>axis</code>。<strong>axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行。</strong></p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\boradcasing_5.png)</p>
<p>第二个<code>A/cal.reshape(1,4)</code>指令则调用了<strong>numpy</strong>中的广播机制。这里使用 3 by 4的矩阵除以1 by 4 的矩阵。技术上来讲，其实并不需要再将矩阵 <code>reshape</code>(重塑)成 ，因为矩阵本身已经是 了。但是当我们写代码时不确定矩阵维度的时候，通常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作<code>reshape</code>是一个常量时间的操作，时间复杂度是，它的调用代价极低。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\boradcasing_6.png)</p>
<h3 id="secondly-example"><a class="header-anchor" href="#secondly-example">¶</a>Secondly Example</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\bordacasing_2.png)</p>
<p>python的广播机制会将常数扩展成4by 1的列向量</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\boradcasing_3.png)</p>
<p>其实是将1by*n 的矩阵复制成为mbyn的矩阵</p>
<h3 id="广播机制的举例"><a class="header-anchor" href="#广播机制的举例">¶</a>广播机制的举例</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\bordcasing_1.png)</p>
<h3 id="axis"><a class="header-anchor" href="#axis">¶</a>axis</h3>
<p>补充：numpy中，类似sum的函数，经常涉及axis参数，可以取值为0或1，甚至其他。经常记不住，这里我查了了一下，是这样的（<a href="https://stackoverflow.com/questions/17079279/how-is-axis-indexed-in-numpys-array" target="_blank" rel="noopener">原文</a>）：</p>
<ol>
<li><strong>axis的数字，和数组的shape参数的索引是对应的</strong>。比如一个数组的shape是(5,6)，则代表5个row，6个column。即在shape中，row和column的个数的索引是0和1。也就第1个坐标，在shape中的第一个元素，<strong>索引是0，代表row的方向</strong>；第2个坐标，在shape中的第2个元素，<strong>索引是1，代表row的方向</strong>。</li>
<li>对于sum函数，axis指的是sum“<strong>沿着</strong>”的方向，经过计算，这个方向的维度因为求和后就消失了，比如sum(axis=0)代表是沿着“row”方向进行求和，</li>
<li>当然axis可以是一个tupe，那就相当于沿着多个多个方向求和。</li>
<li>sum如果不传入axis参数，默认是对所有维度求和。</li>
</ol>
<h3 id="broadcasting"><a class="header-anchor" href="#broadcasting">¶</a>broadcasting</h3>
<p>当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播（broadcasting）。</p>
<p>三种广播情况</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\bordacasing.png)</p>
<h3 id="c1w2l16-a-note-on-python-numpy-vectors"><a class="header-anchor" href="#c1w2l16-a-note-on-python-numpy-vectors">¶</a>C1W2L16 A Note on Python/numpy vectors</h3>
<p>本节主要讲<strong>Python</strong>中的<strong>numpy</strong>一维数组的特性，以及与行向量或列向量的区别</p>
<h3 id="1-一维数组的特性"><a class="header-anchor" href="#1-一维数组的特性">¶</a>1. 一维数组的特性</h3>
<p>首先设置a = np.array.random.randn(5)，这样会生成存储在数组a中的5个高斯随机数变量。之后输出 ，从屏幕上可以得知，此时a 的<strong>shape</strong>（形状）是一个的结构。这在<strong>Python</strong>中被称作<strong>一个一维数组</strong>。它既不是一个行向量也不是一个列向量，这也导致它有一些不是很直观的效果。举个例子，如果我输出一个转置阵，最终结果它会和看起来一样，所以和的转置阵最终结果看起来一样。而如果我输出和的转置阵的内积，你可能会想：乘以的转置返回给你的可能会是一个矩阵。但是如果我这样做，你只会得到一个数。</p>
<p>所以我建议当你编写神经网络时，不要在使用的<strong>shape</strong>(5,1)是还是(n,)或者一维数组。相反，如果你设置(5,1)，那么这就是5行1列向量。在先前的操作里a和a的转置看起来一样，而现在这样的 a变成一个新的a 的转置，并且它是一个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出a 的转置时有两对方括号，而之前只有一对方括号，所以这就是1行5列的矩阵和一维数组的差别。</p>
<h3 id="2-行向量和列向量"><a class="header-anchor" href="#2-行向量和列向量">¶</a>2. 行向量和列向量</h3>
<p><strong>rank 1 array问题</strong>：shape是(x,)的数组，既不是行向量，也不是列向量，没法参与正常的矩阵运算，应该总是使用(x,1)或(1,x)的shape来表示向量。但可以通过reshape方法将rank 1 array转换为行向量或列向量。（什么是rank，就是一个数组的维度）一维的数组既不是行向量也不是列向量，转置后，依然是本身。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\note_1.jpg)</p>
<h3 id="3-解决方法"><a class="header-anchor" href="#3-解决方法">¶</a>3. 解决方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape=（<span class="number">5</span>，<span class="number">1</span>)）</span><br><span class="line"><span class="comment"># 为了确保你的矩阵或向量所需要的维数时，不要羞于 reshape 操作</span></span><br></pre></td></tr></table></figure>
<h3 id="c1w2l18-：quick-tour-of-jupyter-ipython-notebooks"><a class="header-anchor" href="#c1w2l18-：quick-tour-of-jupyter-ipython-notebooks">¶</a>C1W2L18 ：Quick Tour of Jupyter/iPython Notebooks</h3>
<h3 id="c1w2l18-explanation-of-logistic-regression-cost-function"><a class="header-anchor" href="#c1w2l18-explanation-of-logistic-regression-cost-function">¶</a>C1W2L18: Explanation of Logistic Regression Cost Function</h3>
<p>对应logistic regression，输出$\hat{y}=p(y=1|x)$,那么$p(y=0|x)=1-\hat{y}$</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\cost_2png.png)</p>
<p>综合上面<br>
$$<br>
p(y|x)= \hat{y}<sup>y*(1-\hat{y})</sup>{1-y}<br>
$$<br>
对于整个训练集，</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\cost_1png.png)</p>
<p>假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积:<br>
$$<br>
p(labels \ in\  training\  set)=\Pi_{i=1}^mp(y_i|x_i)<br>
$$<br>
如果利用极大似然法做，找到一组参数，使得样本观测值概率最大<br>
$$<br>
\max log p(label \ in \ training \ set)=log \Pi_{i=1}^mp(y_i|x_i)=\sum -L(\hat{y<sup>i},y</sup>i)<br>
$$</p>
<p>$$<br>
\min cost J(w,b)=\frac{1}{m}L(\hat{y<sup>i},y</sup>i)<br>
$$</p>
<p>总结一下，为了最小化成本函数，我们从<strong>logistic</strong>回归模型的最大似然估计的角度出发，假设训练集中的样本都是独立同分布的条件下</p>
<h3 id="day3-summary"><a class="header-anchor" href="#day3-summary">¶</a>Day3 : summary</h3>
<p><font color="red">主要学习了python编程的如何才能高效率，内置函数的具有并行性，simd指令，以及一维数组的使用注意事项，logistic regression的lost function的原理证明</font></p>
<h1>C1W3</h1>
<h2 id="c1w3l01-neural-network-overview"><a class="header-anchor" href="#c1w3l01-neural-network-overview">¶</a>C1W3L01 : Neural Network Overview</h2>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_2.png)</p>
<p>许多<strong>sigmoid</strong>单元堆叠起来形成一个神经网络。</p>
<p>正向传播：输入层到layer one<br>
$$<br>
\left.\begin{array}{c}{x} \ {W^{[1]}} \ {b^{[1]}}\end{array}\right} \Longrightarrow z<sup>{[1]}=W</sup>{[1]} x+b^{[1]} \Longrightarrow a<sup>{[1]}=\sigma\left(z</sup>{[1)}\right)<br>
$$<br>
layer one 到layer two<br>
$$<br>
\left.\begin{array}{r}{a<sup>{(1]}=\sigma\left(z</sup>{[1]}\right)} \ {W^{[2]}} \ {b^{[2]}}\end{array}\right}\begin{array}{l}{\Longrightarrow z<sup>{[2]}=W</sup>{[2]} a<sup>{[1]}+b</sup>{[2]} \Longrightarrow a<sup>{[2]}=\sigma\left(z</sup>{[2]}\right)} \ {\Longrightarrow L\left(a^{[2]}, y\right)}\end{array}<br>
$$<br>
反向传播<br>
$$<br>
\left.\begin{array}{r}{d a^{[1]}=d \sigma\left(z^{[1]}\right)} \ {d W^{[2]}} \ {d b^{[2]}}\end{array}\right}\begin{array}{l}{\Longleftarrow d z<sup>{[2]}=d\left(W</sup>{[2]} \alpha<sup>{[1]}+b</sup>{[2]}\right) \Longleftarrow d a^{[2]}=d \sigma\left(z^{[2]}\right)} \ {\Longleftarrow d L\left(a^{[2]}, y\right)}\end{array}<br>
$$<br>
![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_3.png)</p>
<p>$W$的行数是本次结点个数，列数是上层节点个数</p>
<h2 id="c1w3l02-nerual-network-representations"><a class="header-anchor" href="#c1w3l02-nerual-network-representations">¶</a>C1W3L02 : Nerual Network Representations</h2>
<p>符号说明</p>
<h2 id="c1w3l03：-computation-neural-network-output"><a class="header-anchor" href="#c1w3l03：-computation-neural-network-output">¶</a>C1W3L03： Computation Neural Network Output</h2>
<h3 id="a-simple-training-examples"><a class="header-anchor" href="#a-simple-training-examples">¶</a>A simple training examples</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_5.png)</p>
<p>其中，x表示输入特征，a表示每个神经元的输出，W表示特征的权重，上标表示神经网络的层数（隐藏层为1），下标表示该层的第几个神经元。这是神经网络的<strong>符号惯例</strong>，下同。</p>
<p><strong>神经网络的计算</strong></p>
<p>关于神经网络是怎么计算的，从我们之前提及的逻辑回归开始，如下图所示。用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先你按步骤计算出，然后在第二步中你以<strong>sigmoid</strong>函数为激活函数计算（得出），一个神经网络只是这样子做了好多次重复计算。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_6.png)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_4.png)</p>
<p>说明：$w_i<sup>{[1]}$和$W</sup>{[1]}$的关系，一个按照logistic regression ，一个是矩阵表示。</p>
<p><strong>向量化计算</strong> 如果你执行神经网络的程序，用for循环来做这些看起来真的很低效。所以接下来我们要做的就是把这四个等式向量化。向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的纵向堆积起来变成一个(4,3)的矩阵，用符号$W^{[1]}$表示。另一个看待这个的方法是我们有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的参数——向量，把这四个向量堆积在一起，你会得出这4×3的矩阵。<br>
$$<br>
z<sup>{[n]}=W</sup>{[n]}X+b^{[n]}<br>
$$</p>
<p>$$<br>
a^{[1]}=\left[ \begin{array}{c}{a_{1}^{[1]}} \ {a_{2}^{[1]}} \ {a_{3}^{[1]}} \ {a_{4}<sup>{[1]}}\end{array}\right]=\sigma\left(z</sup>{[1]}\right)<br>
$$</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_7.png)</p>
<p>Given input X（a single training set)<br>
$$<br>
\begin{array}{c}{z<sup>{[1]}=W</sup>{[1]} a<sup>{[0]}+b</sup>{[1]}} \ {a<sup>{[1]}=\sigma\left(z</sup>{[1]}\right)} \ {z<sup>{[2]}=W</sup>{[2]} a<sup>{[1]}+b</sup>{[2]}} \ {a<sup>{[2]}=\sigma\left(z</sup>{[2]}\right)}\end{array}<br>
$$<br>
说明：</p>
<p>$W$的第$i$行表示，当前层到上一层的权重行向量，再计算单个的时候，由于是按照logristics regression的方式，所以认为$w_i$是列向量，所以转置成行向量。上面的图也说明了：如何从单个操作到矩阵操作，权重矩阵是怎么构造，怎么表示的。</p>
<p>b是列向量。</p>
<h2 id="c1w3l04-vectorizing-across-mutilple-example"><a class="header-anchor" href="#c1w3l04-vectorizing-across-mutilple-example">¶</a>C1W3L04: Vectorizing Across Mutilple Example</h2>
<p>Different training examples in different columns of the matrix</p>
<ol>
<li>
<p>for loop</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_8.png)</p>
</li>
<li>
<p>vectorizing : stacking training set in columns<br>
$$<br>
x=\left[ \begin{array}{cccc}{\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \ {x^{(1)}} &amp; {x^{(2)}} &amp; {\dots} &amp; {x} \ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots}\end{array}\right]<br>
$$</p>
</li>
</ol>
<p>就有<br>
$$<br>
\left{\begin{array}{l}{A<sup>{[1]}=\sigma\left(z</sup>{[1]}\right)} \ {z<sup>{[2]}=W</sup>{[2]} A<sup>{[1]}+b</sup>{[2]}} \ {A<sup>{[2]}=\sigma\left(z</sup>{[2]}\right)}\end{array}\right.<br>
$$<br>
![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_9.png)</p>
<p>当垂直扫描，是索引到隐藏单位的数字。当水平扫描，将从第一个训练示例中从第一个隐藏的单元到第二个训练样本，第三个训练样本……直到节点对应于第一个隐藏单元的激活值，且这个隐藏单元是位于这个训练样本中的最终训练样本。</p>
<p>从水平上看，矩阵代表了各个训练样本。从竖直上看，矩阵的不同的索引对应于不同的隐藏单元。</p>
<h2 id="c1w3l05-explanation-for-vectorized-implement"><a class="header-anchor" href="#c1w3l05-explanation-for-vectorized-implement">¶</a>C1W3L05 : Explanation for vectorized implement</h2>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_10.png)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_11.png)</p>
<h2 id="c1w3l06-activation-function"><a class="header-anchor" href="#c1w3l06-activation-function">¶</a>C1W3L06 : Activation Function</h2>
<p>在讨论优化算法时，有一点要说明：基本已经不用<strong>sigmoid</strong>激活函数了，<strong>tanh</strong>函数在所有场合都优于<strong>sigmoid</strong>函数。</p>
<p><strong>sigmoid</strong>函数和<strong>tanh</strong>函数两者共同的缺点是，在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。</p>
<p>在机器学习另一个很流行的函数是：修正线性单元的函数（<strong>ReLu</strong>），<strong>ReLu</strong>函数图像是如下图。$ a = max(0,z)$：  所以，只要是正值的情况下，导数恒等于1，当是负值的时候，导数恒等于0。从实际上来说，当使用的导数时，=0的导数是没有定义的。但是当编程实现的时候，的取值刚好等于0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，是等于0的时候，假设一个导数是1或者0效果都可以。</p>
<p>如果输出是0、1值（二分类问题），则输出层选择<strong>sigmoid</strong>函数，然后其它的所有单元都选择<strong>Relu</strong>函数。</p>
<p>这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用<strong>Relu</strong>激活函数。有时，也会使用<strong>tanh</strong>激活函数，但<strong>Relu</strong>的一个缺点是：当z是负值的时候，导数等于0。</p>
<p>这里也有另一个版本的<strong>Relu</strong>被称为<strong>Leaky Relu</strong>。</p>
<p>当是负值时，这个函数的值不是等于0，而是轻微的倾斜。</p>
<p>两者的优点是：</p>
<p>第一，在的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个<strong>if-else</strong>语句，而<strong>sigmoid</strong>函数需要进行浮点四则运算，在实践中，使用<strong>ReLu</strong>激活函数神经网络通常会比使用<strong>sigmoid</strong>或者<strong>tanh</strong>激活函数学习的更快。</p>
<p>第二，<strong>sigmoid</strong>和<strong>tanh</strong>函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而<strong>Relu</strong>和<strong>Leaky ReLu</strong>函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，<strong>Relu</strong>进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而<strong>Leaky ReLu</strong>不会有这问题)</p>
<p>在<strong>ReLu</strong>的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_12.png)</p>
<p><strong>sigmoid</strong>激活函数：除了输出层是一个二分类问题基本不会用它。</p>
<p><strong>tanh</strong>激活函数：<strong>tanh</strong>是非常优秀的，几乎适合所有场合。</p>
<p><strong>ReLu</strong>激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用<strong>ReLu</strong>或者<strong>Leaky ReLu</strong>。</p>
<p>通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<h2 id="c1w3l07-why-non-linear-activation-functions"><a class="header-anchor" href="#c1w3l07-why-non-linear-activation-functions">¶</a>C1W3L07 : Why non-linear activation Functions</h2>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_13.png)</p>
<p>通过推导可以得出，如果使用线性激活函数，相当于没有隐藏层。无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。当当然，在output layer是可以不用activation function，或者用linear activation function；这种情况一般是要求输出实数集结果（比如预测房价）。即便如此，在hidden layer还是要用non-linear activation function。</p>
<h3 id="sigmoid-activation-function"><a class="header-anchor" href="#sigmoid-activation-function">¶</a>sigmoid activation function</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_14.png)<br>
$$<br>
\frac{d}{d z} g(z)=\frac{1}{1+e<sup>{-z}}\left(1-\frac{1}{1+e</sup>{-z}}\right)=g(z)(1-g(z))<br>
$$</p>
<h3 id="tanh-activation-function"><a class="header-anchor" href="#tanh-activation-function">¶</a>tanh activation function</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_15.png)<br>
$$<br>
g(z)=\tanh (z)=\frac{e<sup>{z}-e</sup>{-z}}{e<sup>{x}+e</sup>{-z}}<br>
$$</p>
<p>$$<br>
\frac{d}{d z} g(z)=1-(\tanh (z))^{2}<br>
$$</p>
<h3 id="rectified-linear-unit-relu"><a class="header-anchor" href="#rectified-linear-unit-relu">¶</a>Rectified linear unit(RelU)</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_16.png)<br>
$$<br>
g(z)^{\prime}=\left{\begin{array}{ll}{0} &amp; {\text { if } z&lt;0} \ {1} &amp; {\text { if } z&gt;0} \ {\text {undefined}} &amp; {\text { if } z=0}\end{array}\right.<br>
$$<br>
注：通常在z= 0的时候给定其导数1,0；当然=0的情况很少</p>
<h3 id="leaky-linear-unit-leaky-relu"><a class="header-anchor" href="#leaky-linear-unit-leaky-relu">¶</a><strong>Leaky linear unit (Leaky ReLU)</strong></h3>
<p>$$<br>
g(z)=\max (0.01 z, z)<br>
$$</p>
<p>$$<br>
g(z)^{\prime}=\left{\begin{array}{ll}{0.01} &amp; {\text { if } z&lt;0} \ {1} &amp; {\text { if } z&gt;0} \ {\text {undefined}} &amp; {\text { if } z=0}\end{array}\right.<br>
$$</p>
<p>注：通常在的z=0时候给定其导数1,0.01；当然的情况很少。</p>
<h2 id="c1w3l09-gradient-descent-for-neural-networks"><a class="header-anchor" href="#c1w3l09-gradient-descent-for-neural-networks">¶</a>C1W3L09 : Gradient Descent For Neural Networks</h2>
<ol>
<li>
<p>gradient descent的关键是求cost function对参数的偏导数</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_17.png)</p>
</li>
<li>
<p>求导过程使用的是Backpropagation</p>
<ol>
<li>
<p>首先做forward propagation，求解出每一层的输出A<br>
$$<br>
(1) z<sup>{[1]}=W</sup>{[1]} x+b^{[1]}\<br>
(2) a<sup>{[1]}=\sigma\left(z</sup>{[1]}\right)\(3) z<sup>{[2]}=W</sup>{[2]}=W^{[2]} a<sup>{[1]}+b</sup>{[2]}\(4) a<sup>{[2]}=g</sup>{[2]}\left(z<sup>{[z]}\right)=\sigma\left(z</sup>{[2]}\right)<br>
$$</p>
</li>
<li>
<p>然后向后，逐层求解对每一层参数的偏导数</p>
</li>
</ol>
</li>
</ol>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_18.png)</p>
<p>sum，<code>keepdims</code>是防止<strong>python</strong>输出那些古怪的秩数(n,)，加上这个确保阵矩阵这个向量输出的维度为(n,1）这样标准的形式。</p>
<h2 id="c1wl10-backpropagation-intuition-optional"><a class="header-anchor" href="#c1wl10-backpropagation-intuition-optional">¶</a>C1WL10: Backpropagation intuition (optional)</h2>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_19.png)</p>
<p>实现后向传播有个技巧，就是要保证矩阵的维度相互匹配</p>
<p>其实，对于一个神经元，输入部分：是权重和上一层输出的线性组合；输出：激活函数作用于输入，因此对$W$求偏导时，对激活函数求一次，再对线性组合求一次。对$b$求偏导是，对线性部分求偏导是1,这里用求和。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_20.png)</p>
<h2 id="c1w3l11-random-initialization"><a class="header-anchor" href="#c1w3l11-random-initialization">¶</a>C1W3L11: Random Initialization</h2>
<p>`</p>
<p>与logistic regression不同，初始化参数不可固定为0，而是每个参数都要随机初始化。</p>
<p>主要原因是：<strong>如果每个参数w和b都是0，则同一层的每个neuron计算结果完全一样</strong>（输入一样a，参数一样w，则z一样,<strong>symmetry breaking problem</strong>）；接下来反向传播时的偏导数也一样，下一轮迭代同一层的每个neuron的w又是一样的。这样整个neural Network上每一层的neuron是同质的，自然不会有好的performance。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_13 (1).png)</p>
<p>不过，对b参数，可以都初始化为0。</p>
<p>另外需要注意，虽然w是随机初始化，但最好使用较小的随机数。主要是避免让z的计算值过大，导致activation function对z的偏导数趋于0，导致Gradient descent下降较慢。 通常的做法是对random的值乘以一个比率，比如0.01（但具体怎么选这个比率，也要根据情况而定，这应该又是一个超参了）：</p>
<p>$W[1]=np.random.randn((2,2))∗0.01$</p>
<p>因为如果你用<strong>tanh</strong>或者<strong>sigmoid</strong>激活函数，或者说只在输出层有一个<strong>Sigmoid</strong>，如果（数值）波动太大，当你计算激活值时如果很大，就会很大或者很小，因此这种情况下你很可能停在<strong>tanh</strong>/<strong>sigmoid</strong>函数的平坦的地方，这些地方梯度很小也就意味着梯度下降会很慢，因此学习也就很慢。</p>
<p>事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能要试试0.01以外的常数。</p>
<h2 id="summary-v2"><a class="header-anchor" href="#summary-v2">¶</a>summary</h2>
<p><font color="red">如何建立一个一层的神经网络了，初始化参数，用前向传播预测，还有计算导数，结合反向传播用在梯度下降中。</font></p>
<font color="blue">
<ol>
<li>
<p>Define the neural network structure ( # of input units, # of hidden units, etc).</p>
</li>
<li>
<p>Initialize the model’s parameters</p>
</li>
<li>
<p>Loop:</p>
<ul>
<li>Implement forward propagation</li>
<li>Compute loss</li>
<li>Implement backward propagation to get the gradients</li>
<li>Update parameters (gradient descent)</li>
</ul>
</li>
</ol>
</font>
<h1>C1W4</h1>
<h2 id="c1w4l01-deep-layer-neural-network"><a class="header-anchor" href="#c1w4l01-deep-layer-neural-network">¶</a>C1W4L01 Deep-layer neural network</h2>
<h3 id="1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network"><a class="header-anchor" href="#1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network">¶</a>1. logistics regression and shallow neural network and deep-layer neural network</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_1.png)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_2.png)</p>
<h3 id="2-notation"><a class="header-anchor" href="#2-notation">¶</a>2. notation</h3>
<p>神经网络模型<br>
$$<br>
\begin{array}{l}{X \in \mathbb{R}^{n_{x} \times m}} 代表输入的矩阵\{x^{(i)} \in \mathbb{R}^{n_{x}}} 代表第 i 个样本的列向量\<br>
{Y \in \mathbb{R}^{n_{y} \times n}} 标记矩阵\ {y^{(i)} \in \mathbb{R}^{n_{v}}}是第i样本的输出标签\<br>
W^{[l]} \in \mathbb{R}^{l \times(l-1)}代表第[l]层的权重矩阵\ b^{[l]} \in \mathbb{R}^{l}代表第[l]层的偏差矩阵\<br>
{\hat{y}^{(i)} \in \mathbb{R}^{n_{v}}}是预测输出向量\end{array}<br>
$$</p>
<p>$$<br>
通用激活公式：<br>
a_{j}<sup>{[l]}=g</sup>{[l]}\left(z_{j}<sup>{[l]}\right)=g</sup>{ | l ]}\left(\sum_{k} w_{j k}^{[l]} a_{k}<sup>{[l-1]}+b_{j}</sup>{[l]}\right)<br>
$$</p>
<h2 id="c1w4l02：-forward-and-backward-propagation"><a class="header-anchor" href="#c1w4l02：-forward-and-backward-propagation">¶</a>C1W4L02： Forward and Backward propagation</h2>
<h3 id="1-forward-propagation"><a class="header-anchor" href="#1-forward-propagation">¶</a>1. forward propagation</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_3.png)</p>
<h3 id="2-backward-propagation"><a class="header-anchor" href="#2-backward-propagation">¶</a>2. backward propagation</h3>
<p>$$<br>
\begin{array}{l}{d z^{[l]}=d a^{[l]} * g<sup>{[l]}\left(z</sup>{l l}\right)} \ {d w^{[l]}=d z^{[l]} \cdot a^{[l-1]}}\d b^{[l]}=d z^{[l]}\<br>
d a<sup>{[l-1]}=w</sup>{[l]} \cdot d z^{[l]}\<br>
d z<sup>{[l]}=w</sup>{[l+1] T} d z^{[l+1]} \cdot g<sup>{[l]</sup>{\prime}}\left(z^{[l]}\right)\end{array}<br>
$$</p>
<p>向量化<br>
$$<br>
\begin{array}{l}{d Z^{[l]}=d A^{[l]} * g<sup>{[l]}\left(Z</sup>{[l]}\right)} \ {d W^{[l]}=\frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T}}\<br>
\begin{array}{l}{d b^{[l]}=\frac{1}{m} n p \cdot \operatorname{sum}\left(d z^{[l]}, \text { axis }=1, \text {keepdims}=\text {True}\right)} \ {d A<sup>{[l-1]}=W</sup>{[l] T} \cdot d Z^{[l]}}\end{array}\end{array}<br>
$$<br>
summary</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_5.png)</p>
<h2 id="c1w4l03-forward-propagation-in-d-deep-network"><a class="header-anchor" href="#c1w4l03-forward-propagation-in-d-deep-network">¶</a>C1W4L03 : Forward Propagation in d deep network</h2>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_6.png)</p>
<p>这里只能用一个显式<strong>for</strong>循环，从1到，然后一层接着一层去计算。</p>
<h2 id="c1w4l04-getting-matrix-dimension-right"><a class="header-anchor" href="#c1w4l04-getting-matrix-dimension-right">¶</a>C1W4L04 Getting matrix dimension right</h2>
<p>当实现深度神经网络的时候，其中一个常用的检查代码是否有错的方法就是拿出一张纸过一遍算法中矩阵的维数。</p>
<p>$d_w<sup>{[l]}$和$w</sup>{[l]}$维度相同，$db<sup>{[l]}$和$b</sup>{[l]}$维度相同，且w和b向量化维度不变，但z,a以及x的维度会向量化后发生变化。</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_8.png)</p>
<p>反向传播的维数检查</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_7.png)</p>
<p>在你做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大大提高代码通过率。</p>
<h2 id="c1w4l05-why-deep-representations"><a class="header-anchor" href="#c1w4l05-why-deep-representations">¶</a>C1W4L05 Why deep representations?</h2>
<p>神经网络不需要很大，但是得有深度，也就是隐藏层需要很多，</p>
<h3 id="1-for-example-of-face-detector"><a class="header-anchor" href="#1-for-example-of-face-detector">¶</a>1. for example of face detector</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_9.png)</p>
<h2 id="c1w4l06-building-blocks-of-a-deep-neural-network"><a class="header-anchor" href="#c1w4l06-building-blocks-of-a-deep-neural-network">¶</a>C1W4L06 :Building blocks of a deep neural network</h2>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_10.png)</p>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_11.png)</p>
<p>可以看得出，再反向传播的时候，需要用到$Z<sup>{[L]},W</sup>{[L]},b^{[L]}$,因此cash them</p>
<p>正向传播：$Z<sup>{[1]},A</sup>{[1]}…$,反向传播：$dA<sup>{[L]},dZ{[L]},dW</sup>{[L]}dB<sup>{[L]},dA</sup>{[L-1]}$</p>
<h2 id="c1w4l07：parameters-vs-hyperparameters"><a class="header-anchor" href="#c1w4l07：parameters-vs-hyperparameters">¶</a>C1W4L07：Parameters vs Hyperparameters</h2>
<h3 id="1-what"><a class="header-anchor" href="#1-what">¶</a>1 What</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_12.png)</p>
<h3 id="2-how"><a class="header-anchor" href="#2-how">¶</a>2 How</h3>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_13.png)</p>
<p><strong>Idea—Code—Experiment—Idea</strong>这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代</p>
<p>今天的深度学习应用领域，还是很经验性的过程，通常你有个想法，比如你可能大致知道一个最好的学习率值，可能说最好，我会想先试试看，然后你可以实际试一下，训练一下看看效果如何。然后基于尝试的结果你会发现，你觉得学习率设定再提高到0.05会比较好。如果你不确定什么值是最好的，你大可以先试试一个学习率，再看看损失函数J的值有没有下降。然后你可以试一试大一些的值，然后发现损失函数的值增加并发散了。然后可能试试其他数，看结果是否下降的很快或者收敛到在更高的位置。你可能尝试不同的并观察损失函数这么变了，试试一组值，然后可能损失函数变成这样，这个值会加快学习过程，并且收敛在更低的损失函数值上（箭头标识），我就用这个值了。</p>
<p>在前面几页中，还有很多不同的超参数。然而，当你开始开发新应用时，预先很难确切知道，究竟超参数的最优值应该是什么。所以通常，你必须尝试很多不同的值，并走这个循环，试试各种参数。试试看5个隐藏层，这个数目的隐藏单元，实现模型并观察是否成功，然后再迭代。这页的标题是，应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。</p>
<p>所以我经常建议人们，特别是刚开始应用于新问题的人们，去试一定范围的值看看结果如何。然后下一门课程，我们会用更系统的方法，用系统性的尝试各种超参数取值。然后其次，甚至是你已经用了很久的模型，可能你在做网络广告应用，在你开发途中，很有可能学习率的最优数值或是其他超参数的最优值是会变的，所以即使你每天都在用当前最优的参数调试你的系统，你还是会发现，最优值过一年就会变化，因为电脑的基础设施，<strong>CPU</strong>或是<strong>GPU</strong>可能会变化很大。所以有一条经验规律可能每几个月就会变。如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。</p>
<p>有一条经验规律：经常试试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的直觉。</p>
<p>总结：超参数的设定，靠经验，尝试，并调，根据结果调，</p>
<h2 id="c1w4l08-what-does-this-have-to-do-with-the-brain"><a class="header-anchor" href="#c1w4l08-what-does-this-have-to-do-with-the-brain">¶</a>C1W4L08 : What does this have to do with the brain?</h2>
<h1># summary : forward prop and back prop</h1>
<h2 id="1-logistics-regression-shallow-neural-network-and-deep-neural-network"><a class="header-anchor" href="#1-logistics-regression-shallow-neural-network-and-deep-neural-network">¶</a>1. logistics regression,shallow neural network and deep neural network</h2>
<p>logistics regression<br>
$$<br>
Z = W^TX+B\<br>
A = \frac{1}{1+e^{-Z}}\<br>
L(A,Y)=-\frac{1}{m}(Ylog<sup>A+(1-Y)log</sup>{1-A}\<br>
\frac{\partial L}{\partial Z}=(A-Y)\<br>
\frac{\partial L}{\partial W}=X(A-Y)\<br>
$$<br>
说明：X是样本按列堆积，W是列向量</p>
<p>shallow neural network 以二分问题为例<br>
$$<br>
Z<sup>{[1]}=W</sup>{[1]}A<sup>{[0]}+b</sup>{[1]}\<br>
A<sup>{[1]}=g</sup>{[1]}(Z^{[1]})\<br>
\ \<br>
Z<sup>{[2]}=W</sup>{[2]}A<sup>{[1]}+b</sup>{[2]}\<br>
A<sup>{[2]}=g</sup>{[2]}(Z^{[2]})\<br>
\ \ \<br>
\ \<br>
L(A<sup>{[2]},Y)=-\frac{1}{m}(Ylog</sup>{A}+(1-Y)log^{1-A})\<br>
\frac{\partial L}{\partial Z<sup>{[2]}}=(A</sup>{[2]}-Y)\<br>
\frac{\partial L}{\partial W<sup>{[2]}}=(A</sup>{[2]}-Y)A<sup>{[1]</sup>T}\<br>
\frac{\partial L}{\partial b<sup>{[2]}}=(A</sup>{[2]}-Y)1_{1<em>m}^T\<br>
\frac{\partial L}{\partial a<sup>{[1]}}=W</sup>{[2]<sup>T}(A</sup>{[2]}-Y)\<br>
\ \<br>
\frac{\partial L}{\partial Z<sup>{[1]}}=W</sup>{[2]<sup>T}(A</sup>{[2]}-Y)</em> g<sup>{’[1]}(Z</sup>{[1]})\<br>
$$<br>
说明：W是按列排$W<sup>{[L]}$是$n</sup>{[L]}*n^{[L-1]}$矩阵，A,Z是按列堆积，记得检查矩阵维数就好了</p>
<h2 id="deep-neural-network"><a class="header-anchor" href="#deep-neural-network">¶</a>deep neural network</h2>
<p>$$<br>
Z<sup>{[l]}=W</sup>{[l]}A<sup>{[l-1]}+b</sup>{[l]}\<br>
A<sup>{[l]}=g</sup>{[l]}(Z^{[l]})\<br>
\ \ \<br>
\ \<br>
\frac{\partial L}{\partial Z^{[l]}}=\partial A<sup>&lt;!–￼0–&gt;*g</sup>{’[l]}(Z^{l})\<br>
\frac{\partial L}{\partial W^{[l]}}=\partial Z^{[l]} A<sup>{[1-1]</sup>T}\<br>
\frac{\partial L}{\partial b^{[l]}}=\partial Z^{[l]}\<br>
\frac{\partial L}{\partial a<sup>{[l-1]}}=W</sup>{[l]^T}\partial Z^{[l]}\<br>
\ \<br>
\frac{\partial L}{\partial Z<sup>{[l-1]}}=W</sup>{[l]^T}\partial Z^{[l]}* g<sup>{’[l-1]}(Z</sup>{[l-1]})\<br>
$$</p>
<h2 id="2-vectorization"><a class="header-anchor" href="#2-vectorization">¶</a>2. vectorization</h2>
<ol>
<li>推导的时候要向量化，注意矩阵维数表示，可以从单个推导到mutli</li>
<li>充分利用python的广播属性，和内置函数的并行化</li>
<li>python一维，二维数组的特性</li>
</ol>
<h2 id="3-知识结构"><a class="header-anchor" href="#3-知识结构">¶</a>3. 知识结构</h2>
<p>![](Deep Learning.ai_Neural Networks and Deep Learning\C1.png)</p>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/11/deep-learning-ai深度学习笔记/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/11/deep-learning-ai深度学习笔记/" itemprop="url">deep_learning.ai深度学习笔记<andrew ng=""></andrew></a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-11T09:15:23+08:00">2019-04-11</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习-Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">深度学习(Deep Learning)</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/11/deep-learning-ai深度学习笔记/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/11/deep-learning-ai深度学习笔记/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                1.8k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1>C5: Sequence Models</h1>
<h2 id="w1-recurrent-neural-networks-循环序列模型"><a class="header-anchor" href="#w1-recurrent-neural-networks-循环序列模型">¶</a>W1 : Recurrent Neural Networks (循环序列模型)</h2>
<h3 id="l1-：-why-sequence-models"><a class="header-anchor" href="#l1-：-why-sequence-models">¶</a>L1 ： Why Sequence Models?</h3>
<p>循环神经网络（<strong>RNN</strong>）之类的模型在语音识别、自然语言处理和其他领域中引起变革。</p>
<p>序列模型的列子</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc11.png" alt=""></p>
<h3 id="l2-notation-数学符号"><a class="header-anchor" href="#l2-notation-数学符号">¶</a>L2 : Notation 数学符号</h3>
<p>NLP</p>
<p>我们用$X<sup>{(i)}$来表示第个i训练样本，所以为了指代第个t元素，或者说是训练样本i的序列中第t个元素用$X</sup>{(i)}<t>$这个符号来表示。如果是序列长度$T_x$，那么你的训练集里不同的训练样本就会有不同的长度，所以$T_x<sup>{(i)}$就代表第个训练样本的输入序列长度。同样$y</sup>{(i)}<t>$代表第i个训练样本中第t个元素，$T_y^{(i)}$就是第i个训练样本的输出序列的长度。</t></t></p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc12.png" alt=""></p>
<p>预先有一个词典</p>
<h3 id="l3-recurrent-neural-network-model-循环神经网络模型"><a class="header-anchor" href="#l3-recurrent-neural-network-model-循环神经网络模型">¶</a>L3 : Recurrent Neural Network Model (循环神经网络模型)</h3>
<p>现在我们讨论一下怎样才能建立一个模型，建立一个神经网络来学习X到Y的映射</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc13.png" alt=""></p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc14.png" alt=""></p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc15.png" alt=""></p>
<p>$a^{&lt;0&gt;}$通常 是零向量</p>
<p>N模型包含三类权重系数，分别是Wax，Waa，Wya。且不同元素之间同一位置共享同一权重系数。</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc16.png" alt=""></p>
<p>RNN的正向传播（Forward Propagation）过程为：</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc17.png" alt=""></p>
<p>循环神经网络用的激活函数经常是<strong>tanh</strong>，不过有时候也会用<strong>ReLU</strong>，但是<strong>tanh</strong>是更通常的选择，我们有其他方法来避免梯度消失问题，我们将在之后进行讲述。选用哪个激活函数是取决于你的输出y，如果它是一个二分问题，那么我猜你会用<strong>sigmoid</strong>函数作为激活函数，如果是k类别分类问题的话，那么可以选用<strong>softmax</strong>作为激活函数。不过这里激活函数的类型取决于你有什么样类型的输出y，对于命名实体识别来说y只可能是0或者1，那我猜这里第二个激活函数g可以是<strong>sigmoid</strong>激活函数。</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc18.png" alt=""></p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc19.png" alt=""></p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc110.png" alt=""></p>
<h3 id="c4-backpropagation-through-time-通过时间的反向传播"><a class="header-anchor" href="#c4-backpropagation-through-time-通过时间的反向传播">¶</a>c4: Backpropagation through time ( 通过时间的反向传播)</h3>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc111.png" alt=""></p>
<p>参数的关系<img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc112.png" alt="">*</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc113.png" alt=""></p>
<p>单个元素的Loss function:</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc114.png" alt=""></p>
<p>该样本所有元素的Loss function为：</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc115.png" alt=""></p>
<p>然后，反向传播（Backpropagation）过程就是从右到左分别计算L(y^,y)对参数Wa，Wy，ba，by的偏导数。思路与做法与标准的神经网络是一样的。一般可以通过成熟的深度学习框架自动求导，例如PyTorch、Tensorflow等。这种从右到左的求导过程被称为Backpropagation through time</p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc116.png" alt=""></p>
<h3 id="l5-different-types-of-rnns-不同类型的循环神经网络"><a class="header-anchor" href="#l5-different-types-of-rnns-不同类型的循环神经网络">¶</a>L5: Different types of <strong>RNN</strong>s (不同类型的循环神经网络)</h3>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc117.png" alt=""></p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc118.png" alt=""></p>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc119.png" alt=""></p>
<h3 id="l6-language-model-and-sequence-generation-语言模型和序列生成"><a class="header-anchor" href="#l6-language-model-and-sequence-generation-语言模型和序列生成">¶</a>L6 : Language model and sequence generation (语言模型和序列生成)</h3>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc120.png" alt=""></p>
<h3 id="l7-sampling-novel-sequences-对新序列采样"><a class="header-anchor" href="#l7-sampling-novel-sequences-对新序列采样">¶</a>L7 : Sampling novel sequences (对新序列采样)</h3>
<p><img src="/2019/04/11/deep-learning-ai深度学习笔记/K:%5CMyBlog%5Chexo%5Csource_posts%5Cdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Cc121.png" alt=""></p>
<h3 id="vanishing-gradients-with-rnns-循环神经网络的梯度消失"><a class="header-anchor" href="#vanishing-gradients-with-rnns-循环神经网络的梯度消失">¶</a>Vanishing gradients with <strong>RNN</strong>s (循环神经网络的梯度消失)</h3>
<p>首先从左到右前向传播，然后反向传播。但是反向传播会很困难，因为同样的梯度消失的问题，后面层的输出误差（上图编号6所示）很难影响前面层（上图编号7所示的层）的计算。这就意味着，实际上很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的<strong>was</strong>或者<strong>were</strong>。而且在英语里面，这中间的内容（上图编号8所示）可以任意长，对吧？所以你需要长时间记住单词是单数还是复数，这样后面的句子才能用到这些信息。也正是这个原因，所以基本的<strong>RNN</strong>模型会有很多局部影响</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson5-week1.html" target="_blank" rel="noopener">http://www.ai-start.com/dl2017/html/lesson5-week1.html</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect</a></p>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/07/英语学习方法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/07/英语学习方法/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-07T13:43:18+08:00">2019-04-07</time>
            

            
            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/07/英语学习方法/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/07/英语学习方法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                784字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[^]:</p>
<p>[TOC]</p>
<h1>利用听力材料学英语最高效</h1>
<p><strong>有效输出应该是稍高于现有水平的、更准确、更得体的表达输出</strong></p>
<p>先记住以下两点，下面我们再一一解析应该要怎么做到。</p>
<p><strong>1.</strong> <strong>学习需要反馈，来告诉我们这输出是正确的，可以继续；或者是错误的，需要修正。</strong></p>
<p><strong>2.</strong> <strong>将输入内化，变成自己的知识。</strong></p>
<p>我一般会听<strong>四遍</strong>，但不是全听原文。</p>
<p>**第一遍：泛听原文，不要看字幕或脚本，清楚录音的内容。**听第一遍的时候我通常连笔记都不做，目的是为了让自己流畅的听完，对听力的内容有一个整体的掌握。</p>
<p><strong>第二遍</strong>：<strong>先听原文，根据内容段落，开始复述内容，并录音</strong>。这一步的目的是强迫自己调用已经学过的知识，组织语言和进行练习。</p>
<p>**第三遍，听自己的录音，然后做出修正。**这一步很重要，可以让你了解自己的发音问题和语法问题，并把可听出来的语法问题进行修改。通过这一步，我们就可以把简单重复输入的语言材料，转化为有效输出。</p>
<p><strong>第四遍，听原文看字幕和脚本，看把听不懂的地方标注</strong>，说明为什么听不懂（比如是因为自己发音不准导致的听不出，或者就是因为这个词没背过、不熟悉）。不熟悉的用法和自己用错的地方总结，背下来，下次试着用。</p>
<p>Tips：</p>
<ol>
<li>
<p>不要选择太难的材料，太难的材料容易使自己丧失学习兴趣。</p>
</li>
<li>
<p>一开始，不要选择太长的听力材料。10分钟左右最佳。在这里推荐ted，可以选择有字幕或关闭字幕。</p>
</li>
<li>
<p>在一个相近的时间段内，选择相近题材的材料。比如我会在两个星期内选择“心理”题材的录音。这样我就会有更大的几率用上刚学过的结构和词汇。</p>
</li>
<li>
<p>及时总结，及时复习已背过的材料，复习的重要性大家都懂，这里就不多说了。</p>
</li>
</ol>
<h1>真题听写</h1>
<ol>
<li>
<p>材料选择</p>
<p>能够听得懂 70%的材料</p>
</li>
</ol>
<p>2 具体执行方法</p>
<ol>
<li>
<p>先泛听一篇</p>
</li>
<li>
<p>再循环听几遍</p>
</li>
<li>
<p>再逐句逐句的听</p>
</li>
</ol>
<h2 id="美剧精听"><a class="header-anchor" href="#美剧精听">¶</a>美剧精听</h2>
<ol>
<li>先看中文听</li>
<li>英文，查</li>
<li>听找</li>
<li>台词，跟读</li>
<li>重复三四<br>
至少10</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">XieMay</p>
              <p class="site-description motion-element" itemprop="description">期待花开</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">51</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">36</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="http://www.cnblogs.com/shiyiandchuixue/" target="_blank" title="BLOGS"><i class="fa fa-fw fa-globe"></i>BLOGS</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:2323020965@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://github.com/shiyichuixue" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="powered-by">
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XieMay</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Symbols count total&#58;</span>
    
    <span title="Symbols count total">325k</span>
  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Pisces</a> v6.0.4</div>



</div>
        








        
      </div>
    </footer>

    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


















  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.4"></script>



  



	





  









  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
    
  
  <script src="[object Object]"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '1Dfsb4DwGQPCIlbyCKt9egUR-gzGzoHsz',
        appKey: '8OKqJPeMRRQnxx2vaAwIkM8y',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"model":"wanko","bottom":-30,"mobileShow":false,"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body>
</html>
