<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"xiemayshiyi.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"WGLQQAQKBA","indexName":"xiemay","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="wise">
<meta property="og:type" content="website">
<meta property="og:title" content="Welcome to shiyi&#39;s world">
<meta property="og:url" content="http://xiemayshiyi.github.io/page/15/index.html">
<meta property="og:site_name" content="Welcome to shiyi&#39;s world">
<meta property="og:description" content="wise">
<meta property="og:locale">
<meta property="article:author" content="XieMay">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://xiemayshiyi.github.io/page/15/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>Welcome to shiyi's world</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Welcome to shiyi's world</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://xiemayshiyi.github.io/2019/04/13/machine%20learning%20test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="wise">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to shiyi's world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/13/machine%20learning%20test/" class="post-title-link" itemprop="url">test</a>
        </h2>

        <div class="post-meta">
			
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-13 19:59:23" itemprop="dateCreated datePublished" datetime="2019-04-13T19:59:23+08:00">2019-04-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-02 14:29:40" itemprop="dateModified" datetime="2021-05-02T14:29:40+08:00">2021-05-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Logistics-Regression"><a href="#Logistics-Regression" class="headerlink" title="Logistics Regression"></a>Logistics Regression</h1><p>   如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！<strong>逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</strong></p>
<p>​     这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2019/04/13/machine%20learning%20test/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://xiemayshiyi.github.io/2019/04/11/tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="wise">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to shiyi's world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/11/tensorflow/" class="post-title-link" itemprop="url">tensorflow</a>
        </h2>

        <div class="post-meta">
			
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-11 15:04:23" itemprop="dateCreated datePublished" datetime="2019-04-11T15:04:23+08:00">2019-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-29 16:25:30" itemprop="dateModified" datetime="2020-06-29T16:25:30+08:00">2020-06-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>10k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="official-definition"><a href="#official-definition" class="headerlink" title="official definition"></a>official definition</h1>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2019/04/11/tensorflow/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://xiemayshiyi.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="wise">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to shiyi's world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/" class="post-title-link" itemprop="url">Deep Learning Neural Network and Deep Learning</a>
        </h2>

        <div class="post-meta">
			
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-11 09:25:45" itemprop="dateCreated datePublished" datetime="2019-04-11T09:25:45+08:00">2019-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-05 21:47:41" itemprop="dateModified" datetime="2020-10-05T21:47:41+08:00">2020-10-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>20k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>19 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1><span id="course-one-neural-networks-and-deep-learning-course-1-of-the-deep-learning-specialization">Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)</span><a href="#course-one-neural-networks-and-deep-learning-course-1-of-the-deep-learning-specialization" class="header-anchor">#</a></h1><h1><span id="c1w1">C1W1</span><a href="#c1w1" class="header-anchor">#</a></h1><h2><span id="c1w1l01-welcome">C1W1L01: Welcome</span><a href="#c1w1l01-welcome" class="header-anchor">#</a></h2><p>AI is the new Electricity!</p>
<p>Course 1: Neural Networks and Deep Learning</p>
<p>Course 2: Improving Deep Neural Networks: Hyperparameter tuning,Regularization and Optimization</p>
<p>Course 3: Structuring your Machine Learning project</p>
<p>Course 4: Convolutional Neural Networks</p>
<p>Course 5: Natural Langurge Processing: Building sequence models</p>
<h2><span id="c1w1l02-what-is-neural-network">C1W1L02 : What is Neural Network</span><a href="#c1w1l02-what-is-neural-network" class="header-anchor">#</a></h2><p>Deep Learning = training (very large) neural network</p>
<h3><span id="for-example-of-house-prize-prediction-the-simplest-neural-network">For example of house prize prediction : the simplest neural network</span><a href="#for-example-of-house-prize-prediction-the-simplest-neural-network" class="header-anchor">#</a></h3><p>如果现在有六栋房子的信息，分别是房子的大小(size of house)和对应的价格(prize),绘制出如下的。自然的想法：线性回归，得到拟合的直线。值得注意的是，房价不可能是负数吧！因此下图中蓝色的线，大致就是我们所需要的函数。这个对应一个最简单神经网络（neural network）</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/housr_prize_1.png" alt></p>
<p>上述是一个tiny little neural network，更大的，更复杂的神经网络是</p>
<p>把很多最简单的single neural堆积(stacking)到一起。</p>
<h3><span id="for-example-of-house-prize-prediction-stacking-the-neural">For example of house prize prediction : stacking the  neural</span><a href="#for-example-of-house-prize-prediction-stacking-the-neural" class="header-anchor">#</a></h3><p>上面这个例子，仅仅考虑特征是size,实际情况上，与房屋相关的特征还有number of bedrooms、zip code、wealth, number of bedrooms and size affect family size. The zip code is a feature that tells you you know walkability. The wealth tells you how good is the school quality</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/house_prize_2.png" alt></p>
<p>hidden layer 用输入层计算得到，因此说输入层与中间层紧密连接起来了</p>
<h3><span id="the-actual-application-of-neural-networks">The actual application of neural networks</span><a href="#the-actual-application-of-neural-networks" class="header-anchor">#</a></h3><p>hidden layer 与上一层的连接情况并不是手工确定，每一层都是上一层所有的输入函数，所以建立的神经网络如下：</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/house prize 3.png" alt></p>
<p>The remarkable thing about neural network</p>
<ol>
<li>Given enough data about X&amp;Y (x,y) which good at freaking out functions :map x to y</li>
<li>Most powerful in supervised learning</li>
</ol>
<h3><span id="c2w1cl03-supervised-learning-with-neural-network">C2W1CL03 : Supervised Learning with Neural Network</span><a href="#c2w1cl03-supervised-learning-with-neural-network" class="header-anchor">#</a></h3><h3><span id="chang-jian-de-jian-du-xue-xi">常见的监督学习</span><a href="#chang-jian-de-jian-du-xue-xi" class="header-anchor">#</a></h3><p>截止到目前，<strong>Neural Network的成功应用基本都在Supervised Learning</strong>。比如：Ad，Images vision, Audio to Text, Machine translation, Autonomous Driving</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/supervised-learning-exmples.png" alt></p>
<h3><span id="chang-jian-de-shen-jing-wang-luo-de-she-ji">常见的神经网络的设计</span><a href="#chang-jian-de-shen-jing-wang-luo-de-she-ji" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/NeuralNetworkExamples.png" alt></p>
<p>卷积神经网络：<strong>Convolutional Neural Network</strong> (CNN) 通常有用图像数据</p>
<p>递归神经网络： <strong>Recurrent Neural Network</strong> (RNN) 通常用于time series</p>
<p>对应复杂的应用中，定制一些复杂的混合的神经网络结构</p>
<h3><span id="jie-gou-hua-he-fei-jie-gou-hua-shu-ju">结构化和非结构化数据</span><a href="#jie-gou-hua-he-fei-jie-gou-hua-shu-ju" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/datastructure.png" alt></p>
<p>处理非结构化数据是很难的，与结构化数据比较，让计算机理解非结构化数据很难</p>
<h3><span id="c1w1l04-why-is-deep-learning-taking-off">C1W1L04: Why is deep learning taking off</span><a href="#c1w1l04-why-is-deep-learning-taking-off" class="header-anchor">#</a></h3><p>Answer: scale</p>
<p>If you want to hit this very high level of performance ,firstly, you need to be able train a big enough neural network in order to take advantage of the huge amount of data and second you need to be out here on the x axes you do need a lot of data.</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/scale.jpg" alt></p>
<ol>
<li>If you do not have a lot training data is often up to your skill at hand engineering features that determines the foreman.在这个小的训练集中，各种算法的优先级事实上定义的也不是很明确，所以如果你没有大量的训练集，那效果会取决于你的特征工程能力，那将决定最终的性能。</li>
<li>这个图形区域的左边，各种算法之间的优先级并不是定义的很明确，最终的性能更多的是取决于你在用<strong>工程选择特征</strong>方面的能力以及<strong>算法处理方面</strong>的一些细节.</li>
<li>只是在某些大数据规模非常庞大的训练集，也就是在右边这个会非常的大时，我们能更加持续地看到更大的由神经网络控制其它方法.</li>
</ol>
<h3><span id="the-reason">The Reason</span><a href="#the-reason" class="header-anchor">#</a></h3><ol>
<li><p>the scale of data</p>
</li>
<li><p>the speed of computation  such as GPUS</p>
</li>
<li><p>innovation of algorithm </p>
<p>许多算法方面的创新，一直是在尝试着使得神经网络运行的更快</p>
<p>switch sigmoid function to relu function</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/algorithm——rul.jpg" alt></p>
</li>
</ol>
<p>在这个区域，也就是这个<strong>sigmoid</strong>函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做<strong>ReLU</strong>的函数（修正线性单元），<strong>ReLU</strong>它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。</p>
<p>训练你的神经网络的过程，很多时候是凭借直觉的，往往你对神经网络架构有了一个想法，于是你尝试写代码实现你的想法，然后让你运行一个试验环境来告诉你，你的神经网络效果有多好，通过参考这个结果再返回去修改你的神经网络里面的一些细节，然后你不断的重复上面的操作，当你的神经网络需要很长时间去训练，需要很长时间重复这一循环，在这里就有很大的区别，根据你的生产效率去构建更高效的神经网络。当你能够有一个想法，试一试，看效果如何。在10分钟内，或者也许要花上一整天，如果你训练你的神经网络用了一个月的时间，有时候发生这样的事情，也是值得的，因为你很快得到了一个结果。在10分钟内或者一天内，你应该尝试更多的想法，那极有可能使得你的神经网络在你的应用方面工作的更好、更快的计算，在提高速度方面真的有帮助，那样你就能更快地得到你的实验结果。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/faster.jpg" alt></p>
<h2><span id="summary"><font color="red">Summary</font></span><a href="#summary" class="header-anchor">#</a></h2><font color="green">早上花了2h小时学习第一周的视频，先看一遍视频的字幕，逐字逐句的理解，虽然很多时候都是自己乱猜的，大概清楚讲的什么！然后再看大牛的笔记，然后再看一篇结合PPT。下午也看了半个多小时。问题：1. 自己的英文水平不够，这个需要大大的提高讷。2. 其实只要看别人的笔记就可以知道内容，但是还是想听andow ng的讲解。3. 视频都比较短，每个视频设计的知识点或者内容不多，1到3个，分成知识点做笔记还是不错的</font>

<font color="blue">这一周的内容，也就是今天我学习的知识简单和容易理解。学习了神经网络的大致结构，神经网络的应用领域，深度学习为什么取得快速的发展的三点原因，尤其是数据scale与其他方法和神经网络规模的大致性能关系</font>

<h1><span id="c1w2">C1W2</span><a href="#c1w2" class="header-anchor">#</a></h1><h3><span id="c1w2l01-binary-classification">C1W2L01: Binary Classification</span><a href="#c1w2l01-binary-classification" class="header-anchor">#</a></h3><p>In this week, we’re going to go over the basics of neural network programming. We are going to study handle data without for loop.</p>
<p>forward password for propagation </p>
<p> backward pass or what’s called a backward propagation step</p>
<p>Why the computations in learning in a neural network can be organized in this board propagation and a separate backward propagation by using logistic regression to convey(传达) theses ideas.</p>
<h3><span id="binary-classification">Binary Classification</span><a href="#binary-classification" class="header-anchor">#</a></h3><p>Input； an image . three separate matrices corresponding red green and blue color channels of this image. 如果你的图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值</p>
<p>unroll all of these pixel intensity values into  a feature vector </p>
<p>pixel intensity values of this image </p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/w_piexl.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_blue_green_read.jpg" alt></p>
<p>notation</p>
<p>(x,y)： a pair X comma Y</p>
<p>$M_{train}$: M subscript train</p>
<p>每条测试集在矩阵中都是以列向量的形式存在</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_noation.png" alt></p>
<p>Matrix capital</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_nation_2.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_all_nation.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_all_nation_1.jpg" alt></p>
<h3><span id="model-hypothesis-function-logistic-regression">Model : hypothesis Function :Logistic Regression</span><a href="#model-hypothesis-function-logistic-regression" class="header-anchor">#</a></h3><p>So given an input X and the parameters W and b, how do we generate the output Y hat? Well, one thing you could try, that doesn’t work, would be to have Y hat be w transpose X plus B, kind of a linear function of the input X. And in fact, this is what you use if you were doing linear regression. But this isn’t a very good algorithm for binary classification because you want Y hat to be the chance that Y is equal to one. </p>
<p>So,Y hat should really be between zero and one. This is what the sigmoid function looks like. </p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_1.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_3.jpg" alt></p>
<h3><span id="sigmoid-function">sigmoid function</span><a href="#sigmoid-function" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\sigma(z) = \frac{1}{1+e^{-z}}</script><p>因为你想让$\hat{y}$表示实际值$y$等于1的机率的话， 应该在0到1之间。这是一个需要解决的问题，因为可能比1要大得多，或者甚至为一个负值。对于你想要的在0和1之间的概率来说它是没有意义的，因此在逻辑回归中，我们的输出应该是等于由上面得到的线性函数式子作为自变量的<strong>sigmoid</strong>函数中，公式如上图最下面所示，将线性函数转换为非线性函数。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_2.jpg" alt></p>
<p>注意：原来$w,b$是分开在，这里就合并，引入变量$x_0=1$,对应偏置$b$,</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_3.png" alt></p>
<h3><span id="strategy-cost-function">Strategy：Cost function</span><a href="#strategy-cost-function" class="header-anchor">#</a></h3><p>Firstly : Loss function</p>
<script type="math/tex; mode=display">
L(\hat{y},y)=\frac{1}{2}\sum{(y_i-\hat{y_i})^2}</script><p>这个优化问题不是凸优化问题(non-convex)，因此不选用这个</p>
<p>Secondly，</p>
<script type="math/tex; mode=display">
L(y,\hat{y})=-(ylog^{\hat{y}}+(1-y)log^{1-\hat{y}})</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_cost_1.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log__cost_2.jpg" alt></p>
<h3><span id="algorithm-gradient-descent">Algorithm: Gradient Descent</span><a href="#algorithm-gradient-descent" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/GD1.jpg" alt></p>
<p>Gradient Descent算法步骤：</p>
<ol>
<li>Initialize $w$, $b$ to zero</li>
<li>repeat：</li>
</ol>
<p>$w :=w−\alpha \frac{∂J(w,b)}{∂w}$</p>
<p>$b :=b-\alpha \frac{∂J(w,b)}{∂b}$</p>
<h3><span id="c1w2l05-amp-c1w2l06-derivatives">C1W2L05 &amp; C1W2L06 Derivatives</span><a href="#c1w2l05-amp-c1w2l06-derivatives" class="header-anchor">#</a></h3><p>求导，这个是微积分的内容，不用写了！</p>
<h3><span id="c1w2l07-computation-graph">C1W2L07： Computation Graph</span><a href="#c1w2l07-computation-graph" class="header-anchor">#</a></h3><h3><span id="c1w2l08-derivatives-with-compution-graphs">C1W2L08 : Derivatives with compution graphs</span><a href="#c1w2l08-derivatives-with-compution-graphs" class="header-anchor">#</a></h3><p>链式法则</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial v}=\frac{\partial L}{\partial u}\frac{\partial u}{\partial v}</script><h3><span id="c1w2l09-logistic-regression-gradient-descent">C1W2L09 : Logistic Regression Gradient Descent</span><a href="#c1w2l09-logistic-regression-gradient-descent" class="header-anchor">#</a></h3><h4><span id="single-training-example">single training example</span><a href="#single-training-example" class="header-anchor">#</a></h4><p>You’ve seen the loss function that measures how well you’re doing on the single training example. You’ve also seen the cost function that measures how well your parameters w and b are doing on your entire training set.</p>
<p>You’ve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives.</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_1.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_2.png" alt></p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w}=\frac{\partial L}{\partial \alpha }\frac{\partial \alpha }{\partial z}\frac{\partial z}{\partial w}
\\=-(\frac{y}{a}+\frac{1-y}{1-a})a(1-a)x=(a-y)x</script><h3><span id="c1w2l10-gradient-descent-on-m-example">C1W2L10 Gradient Descent on m example</span><a href="#c1w2l10-gradient-descent-on-m-example" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\min L(w,b)=\sum_{i=1}^{m}L(\alpha_i,y_i)/m\\
\frac{\partial L }{\partial w}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial w})/m=(\sum_{i=1}^{m}(a-y_i)x_i)/m\\
\frac{\partial L }{\partial b}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial b})/m=(\sum_{i=1}^{m}(a-1)x_i)/m</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_3jpg.jpg" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_3.png" alt></p>
<p>上面的伪代码告诉我们，需要多次for loop完成代码，但是这会造成运算速度下降！因为我们越来越多地训练非常大的数据集，因此你真的需要你的代码变得非常高效。所以在接下来的几个视频中，我们会谈到向量化，以及如何应用向量化而连一个<strong>for</strong>循环都不使用。所以学习了这些，我希望你有关于如何应用逻辑回归，或是用于逻辑回归的梯度下降，事情会变得更加清晰</p>
<h3><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h3><font color="red">今天主要学习了以logistics regression 为例，如何通过链式求导的过程，简单的练习一下，以及再次了解什么是梯度下降法，以及训练学习算法的需要一个损失函数，训练的过程就是求损失函数最优值的过程</font>

<h3><span id="c1w2l11-vectorization">C1W2L11: Vectorization</span><a href="#c1w2l11-vectorization" class="header-anchor">#</a></h3><h4><span id="1-shi-me-shi-vectorization-jiang-for-loop-jin-ke-neng-zhuan-huan-wei-ju-zhen-yun-suan">1. 什么是Vectorization：将 for loop 尽可能转换为矩阵运算</span><a href="#1-shi-me-shi-vectorization-jiang-for-loop-jin-ke-neng-zhuan-huan-wei-ju-zhen-yun-suan" class="header-anchor">#</a></h4><p>通过<strong>numpy</strong>内置函数和避开显式的循环(<strong>loop</strong>)的方式进行向量化，从而有效提高代码速度。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_1.jpg" alt></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.dot(a,b)</span><br><span class="line">如果a,b是一维数组，则计算点积</span><br><span class="line">如果a,b是多维数据，则矩阵乘法</span><br></pre></td></tr></tbody></table></figure>
<h4><span id="2-an-example-of-vectorization">2. An example of vectorization</span><a href="#2-an-example-of-vectorization" class="header-anchor">#</a></h4><p>vectorization的好处：conciser code, but faster execution 一个简单的对比实验：1,000,000大小的两个向量内积计算，for loop要比Vectorization快300倍。 在Deep Learning时代，vectorization是一项重要的技能。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入numpy库</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment">#创建一个数据a</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># [1 2 3 4]</span></span><br><span class="line"><span class="keyword">import</span> time <span class="comment">#导入时间库</span></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>) <span class="comment">#通过round随机得到两个一百万维度的数组</span></span><br><span class="line">tic = time.time() <span class="comment">#现在测量一下当前时间</span></span><br><span class="line"><span class="comment">#向量化的版本</span></span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line">print(“Vectorized version:” + str(<span class="number">1000</span>*(toc-tic)) +”ms”) <span class="comment">#打印一下向量化的版本的时间</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#继续增加非向量化的版本</span></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(“For loop:” + str(<span class="number">1000</span>*(toc-tic)) + “ms”)<span class="comment">#打印for循环的版本的时间</span></span><br></pre></td></tr></tbody></table></figure>
<h4><span id="3-gpu-or-cpu">3. GPU or CPU</span><a href="#3-gpu-or-cpu" class="header-anchor">#</a></h4><ol>
<li><p>大规模的深度学习再<strong>GPU</strong>或者图像处理单元运行”，<strong>CPU</strong>和<strong>GPU</strong>都有并行化的指令，他们有时候会叫做<strong>SIMD</strong>指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了<strong>built-in</strong>函数,像<code>np.function</code>或者并不要求你实现循环的函数，它可以让<strong>python</strong>的充分利用并行化计算。</p>
</li>
<li><p>只是在<strong>GPU</strong>和<strong>CPU</strong>上面计算，<strong>GPU</strong>更加擅长<strong>SIMD</strong>计算，但是<strong>CPU</strong>事实上也不是太差，可能没有<strong>GPU</strong>那么擅长吧。SIMD Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data)</p>
</li>
</ol>
<h3><span id="c12l12-more-vectorization-example">C12L12 ： More Vectorization Example</span><a href="#c12l12-more-vectorization-example" class="header-anchor">#</a></h3><h3><span id="ju-zhen-he-xiang-liang-cheng-fa">矩阵和向量乘法</span><a href="#ju-zhen-he-xiang-liang-cheng-fa" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_2.png" alt></p>
<h3><span id="xiang-liang-han-shu">向量函数</span><a href="#xiang-liang-han-shu" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_3.png" alt></p>
<ol>
<li>原则：whenever possible, avoid explict for-loops</li>
<li>使用Element wised的矩阵运算，将函数作用在每个矩阵元素上，比如：<ul>
<li>np.exp()</li>
<li>np.log()</li>
<li>np.abs()</li>
<li>np.maxium()</li>
<li>1/v</li>
<li>v**2</li>
</ul>
</li>
</ol>
<h3><span id="c1w2l13-vectorizing-logistic-regression">C1W2L13: Vectorizing Logistic Regression</span><a href="#c1w2l13-vectorizing-logistic-regression" class="header-anchor">#</a></h3><h3><span id="1-qian-xiang-chuan-bo">1. 前向传播</span><a href="#1-qian-xiang-chuan-bo" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_3.jpg" alt></p>
<script type="math/tex; mode=display">
\hat{y}=σ(w^TX+b)=(a(1),a(2),...,a(m−1),a(m))=\\
(\alpha(z_1),\alpha(z_m),...,\alpha(z_m))=\\
(\alpha(w^Tx_1+b),\alpha(w^Tx_2+b),...,\alpha(w^Tx_m+b))=</script><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">z=np.dot(W^T,X)+b</span><br><span class="line"><span class="comment"># z这里就是python 巧妙的地方，b是实数，但是向量加上实数后，b扩展成向量，被称为广播（brosdcasting）</span></span><br></pre></td></tr></tbody></table></figure>
<p>个人经验：</p>
<ol>
<li>首先，熟悉每个变量的记号和维度，必要的话，可以画出来，更直观。</li>
<li>先从一个样本做向量化，再把m个样本的操作向量化。</li>
<li>for-loop里面是循环乘法，则向量化一定是一个乘法形式，若对于不确定乘法的左右关系，是否需转置，可以根据目标变量的维度推测。或者先乘起来，再根据目标变量看是否要转置。</li>
</ol>
<h3><span id="c1w2l14-vectorzing-logistic-regression-s-gradient-compution">C1W2L14 : Vectorzing Logistic Regression’s Gradient Compution</span><a href="#c1w2l14-vectorzing-logistic-regression-s-gradient-compution" class="header-anchor">#</a></h3><ol>
<li><p>backforwd</p>
</li>
<li><script type="math/tex; mode=display">
\frac{∂J}{∂w}=\frac{1}{m}X(A−Y)T\\
\frac{∂J}{∂b}=\frac{1}{m}(a(i)−y(i))</script></li>
</ol>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v-4.jpg" alt></p>
<p>重要的是弄清楚，里面的行列关系，代表的意思，运算时候，先自己理清楚。还有点积、等等运算性质对应的操作，或者对应的内置函数</p>
<h3><span id="c1w2l15-broadcasting-in-python">C1W2L15: Broadcasting in Python</span><a href="#c1w2l15-broadcasting-in-python" class="header-anchor">#</a></h3><h3><span id="one-example">One Example</span><a href="#one-example" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing_4.png" alt></p>
<p><code>A.sum(axis = 0)</code>中的参数<code>axis</code>。<strong>axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行。</strong></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_5.png" alt></p>
<p>第二个<code>A/cal.reshape(1,4)</code>指令则调用了<strong>numpy</strong>中的广播机制。这里使用 3 by 4的矩阵除以1 by 4 的矩阵。技术上来讲，其实并不需要再将矩阵 <code>reshape</code>(重塑)成 ，因为矩阵本身已经是 了。但是当我们写代码时不确定矩阵维度的时候，通常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作<code>reshape</code>是一个常量时间的操作，时间复杂度是，它的调用代价极低。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_6.png" alt></p>
<h3><span id="secondly-example">Secondly Example</span><a href="#secondly-example" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing_2.png" alt></p>
<p>python的广播机制会将常数扩展成4by 1的列向量</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_3.png" alt></p>
<p>其实是将1by*n 的矩阵复制成为mbyn的矩阵</p>
<h3><span id="guang-bo-ji-zhi-de-ju-li">广播机制的举例</span><a href="#guang-bo-ji-zhi-de-ju-li" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordcasing_1.png" alt></p>
<h3><span id="axis">axis</span><a href="#axis" class="header-anchor">#</a></h3><p>补充：numpy中，类似sum的函数，经常涉及axis参数，可以取值为0或1，甚至其他。经常记不住，这里我查了了一下，是这样的（<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/17079279/how-is-axis-indexed-in-numpys-array">原文</a>）：</p>
<ol>
<li><strong>axis的数字，和数组的shape参数的索引是对应的</strong>。比如一个数组的shape是(5,6)，则代表5个row，6个column。即在shape中，row和column的个数的索引是0和1。也就第1个坐标，在shape中的第一个元素，<strong>索引是0，代表row的方向</strong>；第2个坐标，在shape中的第2个元素，<strong>索引是1，代表row的方向</strong>。</li>
<li>对于sum函数，axis指的是sum“<strong>沿着</strong>”的方向，经过计算，这个方向的维度因为求和后就消失了，比如sum(axis=0)代表是沿着“row”方向进行求和，</li>
<li>当然axis可以是一个tupe，那就相当于沿着多个多个方向求和。</li>
<li>sum如果不传入axis参数，默认是对所有维度求和。</li>
</ol>
<h3><span id="broadcasting">broadcasting</span><a href="#broadcasting" class="header-anchor">#</a></h3><p>  当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播（broadcasting）。</p>
<p>三种广播情况</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing.png" alt></p>
<h3><span id="c1w2l16-a-note-on-python-numpy-vectors">C1W2L16 A Note on Python/numpy vectors</span><a href="#c1w2l16-a-note-on-python-numpy-vectors" class="header-anchor">#</a></h3><p>本节主要讲<strong>Python</strong>中的<strong>numpy</strong>一维数组的特性，以及与行向量或列向量的区别</p>
<h3><span id="1-yi-wei-shu-zu-de-te-xing">1. 一维数组的特性</span><a href="#1-yi-wei-shu-zu-de-te-xing" class="header-anchor">#</a></h3><p>首先设置a = np.array.random.randn(5)，这样会生成存储在数组a中的5个高斯随机数变量。之后输出 ，从屏幕上可以得知，此时a 的<strong>shape</strong>（形状）是一个的结构。这在<strong>Python</strong>中被称作<strong>一个一维数组</strong>。它既不是一个行向量也不是一个列向量，这也导致它有一些不是很直观的效果。举个例子，如果我输出一个转置阵，最终结果它会和看起来一样，所以和的转置阵最终结果看起来一样。而如果我输出和的转置阵的内积，你可能会想：乘以的转置返回给你的可能会是一个矩阵。但是如果我这样做，你只会得到一个数。</p>
<p>所以我建议当你编写神经网络时，不要在使用的<strong>shape</strong>(5,1)是还是(n,)或者一维数组。相反，如果你设置(5,1)，那么这就是5行1列向量。在先前的操作里a和a的转置看起来一样，而现在这样的 a变成一个新的a 的转置，并且它是一个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出a 的转置时有两对方括号，而之前只有一对方括号，所以这就是1行5列的矩阵和一维数组的差别。</p>
<h3><span id="2-xing-xiang-liang-he-lie-xiang-liang">2. 行向量和列向量</span><a href="#2-xing-xiang-liang-he-lie-xiang-liang" class="header-anchor">#</a></h3><p><strong>rank 1 array问题</strong>：shape是(x,)的数组，既不是行向量，也不是列向量，没法参与正常的矩阵运算，应该总是使用(x,1)或(1,x)的shape来表示向量。但可以通过reshape方法将rank 1 array转换为行向量或列向量。（什么是rank，就是一个数组的维度）一维的数组既不是行向量也不是列向量，转置后，依然是本身。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/note_1.jpg" alt></p>
<h3><span id="3-jie-jue-fang-fa">3. 解决方法</span><a href="#3-jie-jue-fang-fa" class="header-anchor">#</a></h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape=（<span class="number">5</span>，<span class="number">1</span>)）</span><br><span class="line"><span class="comment"># 为了确保你的矩阵或向量所需要的维数时，不要羞于 reshape 操作</span></span><br></pre></td></tr></tbody></table></figure>
<h3><span id="c1w2l18-quick-tour-of-jupyter-ipython-notebooks">C1W2L18 ：Quick Tour of Jupyter/iPython Notebooks</span><a href="#c1w2l18-quick-tour-of-jupyter-ipython-notebooks" class="header-anchor">#</a></h3><h3><span id="c1w2l18-explanation-of-logistic-regression-cost-function">C1W2L18: Explanation of Logistic Regression Cost Function</span><a href="#c1w2l18-explanation-of-logistic-regression-cost-function" class="header-anchor">#</a></h3><p>对应logistic regression，输出$\hat{y}=p(y=1|x)$,那么$p(y=0|x)=1-\hat{y}$</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_2png.png" alt></p>
<p>综合上面</p>
<script type="math/tex; mode=display">
p(y|x)= \hat{y}^y*(1-\hat{y})^{1-y}</script><p>对于整个训练集，</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_1png.png" alt></p>
<p>假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积:</p>
<script type="math/tex; mode=display">
p(labels \ in\  training\  set)=\Pi_{i=1}^mp(y_i|x_i)</script><p>如果利用极大似然法做，找到一组参数，使得样本观测值概率最大</p>
<script type="math/tex; mode=display">
\max log p(label \ in \ training \ set)=log \Pi_{i=1}^mp(y_i|x_i)=\sum -L(\hat{y^i},y^i)</script><script type="math/tex; mode=display">
\min cost J(w,b)=\frac{1}{m}L(\hat{y^i},y^i)</script><p>总结一下，为了最小化成本函数，我们从<strong>logistic</strong>回归模型的最大似然估计的角度出发，假设训练集中的样本都是独立同分布的条件下</p>
<h3><span id="day3-summary">Day3 : summary</span><a href="#day3-summary" class="header-anchor">#</a></h3><font color="red">主要学习了python编程的如何才能高效率，内置函数的具有并行性，simd指令，以及一维数组的使用注意事项，logistic regression的lost function的原理证明</font>

<h1><span id="c1w3">C1W3</span><a href="#c1w3" class="header-anchor">#</a></h1><h2><span id="c1w3l01-neural-network-overview">C1W3L01 : Neural Network Overview</span><a href="#c1w3l01-neural-network-overview" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_2.png" alt></p>
<p>许多<strong>sigmoid</strong>单元堆叠起来形成一个神经网络。</p>
<p>正向传播：输入层到layer one</p>
<script type="math/tex; mode=display">
\left.\begin{array}{c}{x} \\ {W^{[1]}} \\ {b^{[1]}}\end{array}\right\} \Longrightarrow z^{[1]}=W^{[1]} x+b^{[1]} \Longrightarrow a^{[1]}=\sigma\left(z^{[1)}\right)</script><p>layer one 到layer two</p>
<script type="math/tex; mode=display">
\left.\begin{array}{r}{a^{(1]}=\sigma\left(z^{[1]}\right)} \\ {W^{[2]}} \\ {b^{[2]}}\end{array}\right\}\begin{array}{l}{\Longrightarrow z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \Longrightarrow a^{[2]}=\sigma\left(z^{[2]}\right)} \\ {\Longrightarrow L\left(a^{[2]}, y\right)}\end{array}</script><p>反向传播</p>
<script type="math/tex; mode=display">
\left.\begin{array}{r}{d a^{[1]}=d \sigma\left(z^{[1]}\right)} \\ {d W^{[2]}} \\ {d b^{[2]}}\end{array}\right\}\begin{array}{l}{\Longleftarrow d z^{[2]}=d\left(W^{[2]} \alpha^{[1]}+b^{[2]}\right) \Longleftarrow d a^{[2]}=d \sigma\left(z^{[2]}\right)} \\ {\Longleftarrow d L\left(a^{[2]}, y\right)}\end{array}</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_3.png" alt></p>
<p>$W$的行数是本次结点个数，列数是上层节点个数</p>
<h2><span id="c1w3l02-nerual-network-representations">C1W3L02 : Nerual Network Representations</span><a href="#c1w3l02-nerual-network-representations" class="header-anchor">#</a></h2><p>符号说明</p>
<h2><span id="c1w3l03-computation-neural-network-output">C1W3L03： Computation Neural Network Output</span><a href="#c1w3l03-computation-neural-network-output" class="header-anchor">#</a></h2><h3><span id="a-simple-training-examples">A simple training examples</span><a href="#a-simple-training-examples" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_5.png" alt></p>
<p>其中，x表示输入特征，a表示每个神经元的输出，W表示特征的权重，上标表示神经网络的层数（隐藏层为1），下标表示该层的第几个神经元。这是神经网络的<strong>符号惯例</strong>，下同。</p>
<p><strong>神经网络的计算</strong></p>
<p>关于神经网络是怎么计算的，从我们之前提及的逻辑回归开始，如下图所示。用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先你按步骤计算出，然后在第二步中你以<strong>sigmoid</strong>函数为激活函数计算（得出），一个神经网络只是这样子做了好多次重复计算。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_6.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_4.png" alt></p>
<p>说明：$w_i^{[1]}$和$W^{[1]}$的关系，一个按照logistic regression ，一个是矩阵表示。</p>
<p><strong>向量化计算</strong> 如果你执行神经网络的程序，用for循环来做这些看起来真的很低效。所以接下来我们要做的就是把这四个等式向量化。向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的纵向堆积起来变成一个(4,3)的矩阵，用符号$W^{[1]}$表示。另一个看待这个的方法是我们有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的参数——向量，把这四个向量堆积在一起，你会得出这4×3的矩阵。</p>
<script type="math/tex; mode=display">
z^{[n]}=W^{[n]}X+b^{[n]}</script><script type="math/tex; mode=display">
a^{[1]}=\left[ \begin{array}{c}{a_{1}^{[1]}} \\ {a_{2}^{[1]}} \\ {a_{3}^{[1]}} \\ {a_{4}^{[1]}}\end{array}\right]=\sigma\left(z^{[1]}\right)</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_7.png" alt></p>
<p>Given input X（a single training set)</p>
<script type="math/tex; mode=display">
\begin{array}{c}{z^{[1]}=W^{[1]} a^{[0]}+b^{[1]}} \\ {a^{[1]}=\sigma\left(z^{[1]}\right)} \\ {z^{[2]}=W^{[2]} a^{[1]}+b^{[2]}} \\ {a^{[2]}=\sigma\left(z^{[2]}\right)}\end{array}</script><p>说明：</p>
<p>$W$的第$i$行表示，当前层到上一层的权重行向量，再计算单个的时候，由于是按照logristics regression的方式，所以认为$w_i$是列向量，所以转置成行向量。上面的图也说明了：如何从单个操作到矩阵操作，权重矩阵是怎么构造，怎么表示的。</p>
<p>b是列向量。</p>
<h2><span id="c1w3l04-vectorizing-across-mutilple-example">C1W3L04: Vectorizing Across Mutilple Example</span><a href="#c1w3l04-vectorizing-across-mutilple-example" class="header-anchor">#</a></h2><p>Different training examples in different columns of the matrix </p>
<ol>
<li><p>for loop</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_8.png" alt></p>
</li>
<li><p>vectorizing : stacking training set in columns</p>
<script type="math/tex; mode=display">
x=\left[ \begin{array}{cccc}{\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {x^{(1)}} & {x^{(2)}} & {\dots} & {x} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots}\end{array}\right]</script></li>
</ol>
<p>就有</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}{A^{[1]}=\sigma\left(z^{[1]}\right)} \\ {z^{[2]}=W^{[2]} A^{[1]}+b^{[2]}} \\ {A^{[2]}=\sigma\left(z^{[2]}\right)}\end{array}\right.</script><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_9.png" alt></p>
<p>当垂直扫描，是索引到隐藏单位的数字。当水平扫描，将从第一个训练示例中从第一个隐藏的单元到第二个训练样本，第三个训练样本……直到节点对应于第一个隐藏单元的激活值，且这个隐藏单元是位于这个训练样本中的最终训练样本。</p>
<p>从水平上看，矩阵代表了各个训练样本。从竖直上看，矩阵的不同的索引对应于不同的隐藏单元。</p>
<h2><span id="c1w3l05-explanation-for-vectorized-implement">C1W3L05 : Explanation for vectorized implement</span><a href="#c1w3l05-explanation-for-vectorized-implement" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_10.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_11.png" alt></p>
<h2><span id="c1w3l06-activation-function">C1W3L06 : Activation Function</span><a href="#c1w3l06-activation-function" class="header-anchor">#</a></h2><p>在讨论优化算法时，有一点要说明：基本已经不用<strong>sigmoid</strong>激活函数了，<strong>tanh</strong>函数在所有场合都优于<strong>sigmoid</strong>函数。</p>
<p><strong>sigmoid</strong>函数和<strong>tanh</strong>函数两者共同的缺点是，在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。</p>
<p>在机器学习另一个很流行的函数是：修正线性单元的函数（<strong>ReLu</strong>），<strong>ReLu</strong>函数图像是如下图。$ a = max(0,z)$：  所以，只要是正值的情况下，导数恒等于1，当是负值的时候，导数恒等于0。从实际上来说，当使用的导数时，=0的导数是没有定义的。但是当编程实现的时候，的取值刚好等于0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，是等于0的时候，假设一个导数是1或者0效果都可以。</p>
<p>如果输出是0、1值（二分类问题），则输出层选择<strong>sigmoid</strong>函数，然后其它的所有单元都选择<strong>Relu</strong>函数。</p>
<p>这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用<strong>Relu</strong>激活函数。有时，也会使用<strong>tanh</strong>激活函数，但<strong>Relu</strong>的一个缺点是：当z是负值的时候，导数等于0。</p>
<p>这里也有另一个版本的<strong>Relu</strong>被称为<strong>Leaky Relu</strong>。</p>
<p>当是负值时，这个函数的值不是等于0，而是轻微的倾斜。</p>
<p>两者的优点是：</p>
<p>第一，在的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个<strong>if-else</strong>语句，而<strong>sigmoid</strong>函数需要进行浮点四则运算，在实践中，使用<strong>ReLu</strong>激活函数神经网络通常会比使用<strong>sigmoid</strong>或者<strong>tanh</strong>激活函数学习的更快。</p>
<p>第二，<strong>sigmoid</strong>和<strong>tanh</strong>函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而<strong>Relu</strong>和<strong>Leaky ReLu</strong>函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，<strong>Relu</strong>进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而<strong>Leaky ReLu</strong>不会有这问题)</p>
<p>在<strong>ReLu</strong>的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_12.png" alt></p>
<p><strong>sigmoid</strong>激活函数：除了输出层是一个二分类问题基本不会用它。</p>
<p><strong>tanh</strong>激活函数：<strong>tanh</strong>是非常优秀的，几乎适合所有场合。</p>
<p><strong>ReLu</strong>激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用<strong>ReLu</strong>或者<strong>Leaky ReLu</strong>。</p>
<p>通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<h2><span id="c1w3l07-why-non-linear-activation-functions">C1W3L07 : Why non-linear activation Functions</span><a href="#c1w3l07-why-non-linear-activation-functions" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_13.png" alt></p>
<p>通过推导可以得出，如果使用线性激活函数，相当于没有隐藏层。无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。当当然，在output layer是可以不用activation function，或者用linear activation function；这种情况一般是要求输出实数集结果（比如预测房价）。即便如此，在hidden layer还是要用non-linear activation function。</p>
<h3><span id="sigmoid-activation-function">sigmoid activation function</span><a href="#sigmoid-activation-function" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_14.png" alt></p>
<script type="math/tex; mode=display">
\frac{d}{d z} g(z)=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)=g(z)(1-g(z))</script><h3><span id="tanh-activation-function">tanh activation function</span><a href="#tanh-activation-function" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_15.png" alt></p>
<script type="math/tex; mode=display">
g(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{x}+e^{-z}}</script><script type="math/tex; mode=display">
\frac{d}{d z} g(z)=1-(\tanh (z))^{2}</script><h3><span id="rectified-linear-unit-relu">Rectified linear unit(RelU)</span><a href="#rectified-linear-unit-relu" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_16.png" alt></p>
<script type="math/tex; mode=display">
g(z)^{\prime}=\left\{\begin{array}{ll}{0} & {\text { if } z<0} \\ {1} & {\text { if } z>0} \\ {\text {undefined}} & {\text { if } z=0}\end{array}\right.</script><p>注：通常在z= 0的时候给定其导数1,0；当然=0的情况很少</p>
<h3><span id="leaky-linear-unit-leaky-relu"><strong>Leaky linear unit (Leaky ReLU)</strong></span><a href="#leaky-linear-unit-leaky-relu" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
g(z)=\max (0.01 z, z)</script><script type="math/tex; mode=display">
g(z)^{\prime}=\left\{\begin{array}{ll}{0.01} & {\text { if } z<0} \\ {1} & {\text { if } z>0} \\ {\text {undefined}} & {\text { if } z=0}\end{array}\right.</script><p>注：通常在的z=0时候给定其导数1,0.01；当然的情况很少。</p>
<h2><span id="c1w3l09-gradient-descent-for-neural-networks">C1W3L09 : Gradient Descent For Neural Networks</span><a href="#c1w3l09-gradient-descent-for-neural-networks" class="header-anchor">#</a></h2><ol>
<li><p>gradient descent的关键是求cost function对参数的偏导数</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_17.png" alt></p>
</li>
<li><p>求导过程使用的是Backpropagation</p>
<ol>
<li><p>首先做forward propagation，求解出每一层的输出A</p>
<script type="math/tex; mode=display">
(1) z^{[1]}=W^{[1]} x+b^{[1]}\\
(2) a^{[1]}=\sigma\left(z^{[1]}\right)\\(3) z^{[2]}=W^{[2]}=W^{[2]} a^{[1]}+b^{[2]}\\(4) a^{[2]}=g^{[2]}\left(z^{[z]}\right)=\sigma\left(z^{[2]}\right)</script></li>
<li><p>然后向后，逐层求解对每一层参数的偏导数</p>
</li>
</ol>
</li>
</ol>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_18.png" alt></p>
<p>sum，<code>keepdims</code>是防止<strong>python</strong>输出那些古怪的秩数(n,)，加上这个确保阵矩阵这个向量输出的维度为(n,1）这样标准的形式。</p>
<h2><span id="c1wl10-backpropagation-intuition-optional">C1WL10: Backpropagation intuition (optional)</span><a href="#c1wl10-backpropagation-intuition-optional" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_19.png" alt></p>
<p>实现后向传播有个技巧，就是要保证矩阵的维度相互匹配</p>
<p>其实，对于一个神经元，输入部分：是权重和上一层输出的线性组合；输出：激活函数作用于输入，因此对$W$求偏导时，对激活函数求一次，再对线性组合求一次。对$b$求偏导是，对线性部分求偏导是1,这里用求和。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_20.png" alt></p>
<h2><span id="c1w3l11-random-initialization">C1W3L11: Random Initialization</span><a href="#c1w3l11-random-initialization" class="header-anchor">#</a></h2><p>`</p>
<p>与logistic regression不同，初始化参数不可固定为0，而是每个参数都要随机初始化。</p>
<p>主要原因是：<strong>如果每个参数w和b都是0，则同一层的每个neuron计算结果完全一样</strong>（输入一样a，参数一样w，则z一样,<strong>symmetry breaking problem</strong>）；接下来反向传播时的偏导数也一样，下一轮迭代同一层的每个neuron的w又是一样的。这样整个neural Network上每一层的neuron是同质的，自然不会有好的performance。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_13 (1" alt>.png)</p>
<p>不过，对b参数，可以都初始化为0。</p>
<p>另外需要注意，虽然w是随机初始化，但最好使用较小的随机数。主要是避免让z的计算值过大，导致activation function对z的偏导数趋于0，导致Gradient descent下降较慢。 通常的做法是对random的值乘以一个比率，比如0.01（但具体怎么选这个比率，也要根据情况而定，这应该又是一个超参了）：</p>
<p>$W[1]=np.random.randn((2,2))∗0.01$</p>
<p>因为如果你用<strong>tanh</strong>或者<strong>sigmoid</strong>激活函数，或者说只在输出层有一个<strong>Sigmoid</strong>，如果（数值）波动太大，当你计算激活值时如果很大，就会很大或者很小，因此这种情况下你很可能停在<strong>tanh</strong>/<strong>sigmoid</strong>函数的平坦的地方，这些地方梯度很小也就意味着梯度下降会很慢，因此学习也就很慢。</p>
<p>事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能要试试0.01以外的常数。</p>
<h2><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h2><font color="red">如何建立一个一层的神经网络了，初始化参数，用前向传播预测，还有计算导数，结合反向传播用在梯度下降中。</font>

<font color="blue">

1. Define the neural network structure ( # of input units, # of hidden units, etc).
2. Initialize the model's parameters

1. Loop:
   - Implement forward propagation
   - Compute loss
   - Implement backward propagation to get the gradients
   - Update parameters (gradient descent)

</font>

<h1><span id="c1w4">C1W4</span><a href="#c1w4" class="header-anchor">#</a></h1><h2><span id="c1w4l01-deep-layer-neural-network">C1W4L01 Deep-layer neural network</span><a href="#c1w4l01-deep-layer-neural-network" class="header-anchor">#</a></h2><h3><span id="1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network">1. logistics regression and shallow neural network and deep-layer neural network</span><a href="#1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_1.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_2.png" alt></p>
<h3><span id="2-notation">2. notation</span><a href="#2-notation" class="header-anchor">#</a></h3><p>神经网络模型</p>
<script type="math/tex; mode=display">
\begin{array}{l}{X \in \mathbb{R}^{n_{x} \times m}} 代表输入的矩阵\\{x^{(i)} \in \mathbb{R}^{n_{x}}} 代表第 i 个样本的列向量\\
{Y \in \mathbb{R}^{n_{y} \times n}} 标记矩阵\\ {y^{(i)} \in \mathbb{R}^{n_{v}}}是第i样本的输出标签\\
W^{[l]} \in \mathbb{R}^{l \times(l-1)}代表第[l]层的权重矩阵\\ b^{[l]} \in \mathbb{R}^{l}代表第[l]层的偏差矩阵\\
 {\hat{y}^{(i)} \in \mathbb{R}^{n_{v}}}是预测输出向量\end{array}</script><script type="math/tex; mode=display">
通用激活公式：
a_{j}^{[l]}=g^{[l]}\left(z_{j}^{[l]}\right)=g^{ | l ]}\left(\sum_{k} w_{j k}^{[l]} a_{k}^{[l-1]}+b_{j}^{[l]}\right)</script><h2><span id="c1w4l02-forward-and-backward-propagation">C1W4L02： Forward and Backward propagation</span><a href="#c1w4l02-forward-and-backward-propagation" class="header-anchor">#</a></h2><h3><span id="1-forward-propagation">1. forward propagation</span><a href="#1-forward-propagation" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_3.png" alt></p>
<h3><span id="2-backward-propagation">2. backward propagation</span><a href="#2-backward-propagation" class="header-anchor">#</a></h3><script type="math/tex; mode=display">
\begin{array}{l}{d z^{[l]}=d a^{[l]} * g^{[l]}\left(z^{l l}\right)} \\ {d w^{[l]}=d z^{[l]} \cdot a^{[l-1]}}\\d b^{[l]}=d z^{[l]}\\
d a^{[l-1]}=w^{[l]} \cdot d z^{[l]}\\
d z^{[l]}=w^{[l+1] T} d z^{[l+1]} \cdot g^{[l]^{\prime}}\left(z^{[l]}\right)\end{array}</script><p>向量化</p>
<script type="math/tex; mode=display">
\begin{array}{l}{d Z^{[l]}=d A^{[l]} * g^{[l]}\left(Z^{[l]}\right)} \\ {d W^{[l]}=\frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T}}\\
\begin{array}{l}{d b^{[l]}=\frac{1}{m} n p \cdot \operatorname{sum}\left(d z^{[l]}, \text { axis }=1, \text {keepdims}=\text {True}\right)} \\ {d A^{[l-1]}=W^{[l] T} \cdot d Z^{[l]}}\end{array}\end{array}</script><p>summary</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_5.png" alt></p>
<h2><span id="c1w4l03-forward-propagation-in-d-deep-network">C1W4L03 : Forward Propagation in d deep network</span><a href="#c1w4l03-forward-propagation-in-d-deep-network" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_6.png" alt></p>
<p>这里只能用一个显式<strong>for</strong>循环，从1到，然后一层接着一层去计算。</p>
<h2><span id="c1w4l04-getting-matrix-dimension-right">C1W4L04 Getting matrix dimension right</span><a href="#c1w4l04-getting-matrix-dimension-right" class="header-anchor">#</a></h2><p>当实现深度神经网络的时候，其中一个常用的检查代码是否有错的方法就是拿出一张纸过一遍算法中矩阵的维数。</p>
<p>$d_w^{[l]}$和$w^{[l]}$维度相同，$db^{[l]}$和$b^{[l]}$维度相同，且w和b向量化维度不变，但z,a以及x的维度会向量化后发生变化。</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_8.png" alt></p>
<p>反向传播的维数检查</p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_7.png" alt></p>
<p>在你做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大大提高代码通过率。</p>
<h2><span id="c1w4l05-why-deep-representations">C1W4L05 Why deep representations?</span><a href="#c1w4l05-why-deep-representations" class="header-anchor">#</a></h2><p>神经网络不需要很大，但是得有深度，也就是隐藏层需要很多，</p>
<h3><span id="1-for-example-of-face-detector">1. for example of face detector</span><a href="#1-for-example-of-face-detector" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_9.png" alt></p>
<h2><span id="c1w4l06-building-blocks-of-a-deep-neural-network">C1W4L06 :Building blocks of a deep neural network</span><a href="#c1w4l06-building-blocks-of-a-deep-neural-network" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_10.png" alt></p>
<p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_11.png" alt></p>
<p>可以看得出，再反向传播的时候，需要用到$Z^{[L]},W^{[L]},b^{[L]}$,因此cash them</p>
<p>正向传播：$Z^{[1]},A^{[1]}…………$,反向传播：$dA^{[L]},dZ{[L]},dW^{[L]}dB^{[L]},dA^{[L-1]}$</p>
<h2><span id="c1w4l07-parameters-vs-hyperparameters">C1W4L07：Parameters vs Hyperparameters</span><a href="#c1w4l07-parameters-vs-hyperparameters" class="header-anchor">#</a></h2><h3><span id="1-what">1 What</span><a href="#1-what" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_12.png" alt></p>
<h3><span id="2-how">2 How</span><a href="#2-how" class="header-anchor">#</a></h3><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_13.png" alt></p>
<p><strong>Idea—Code—Experiment—Idea</strong>这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代</p>
<p>今天的深度学习应用领域，还是很经验性的过程，通常你有个想法，比如你可能大致知道一个最好的学习率值，可能说最好，我会想先试试看，然后你可以实际试一下，训练一下看看效果如何。然后基于尝试的结果你会发现，你觉得学习率设定再提高到0.05会比较好。如果你不确定什么值是最好的，你大可以先试试一个学习率，再看看损失函数J的值有没有下降。然后你可以试一试大一些的值，然后发现损失函数的值增加并发散了。然后可能试试其他数，看结果是否下降的很快或者收敛到在更高的位置。你可能尝试不同的并观察损失函数这么变了，试试一组值，然后可能损失函数变成这样，这个值会加快学习过程，并且收敛在更低的损失函数值上（箭头标识），我就用这个值了。</p>
<p>在前面几页中，还有很多不同的超参数。然而，当你开始开发新应用时，预先很难确切知道，究竟超参数的最优值应该是什么。所以通常，你必须尝试很多不同的值，并走这个循环，试试各种参数。试试看5个隐藏层，这个数目的隐藏单元，实现模型并观察是否成功，然后再迭代。这页的标题是，应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。</p>
<p>所以我经常建议人们，特别是刚开始应用于新问题的人们，去试一定范围的值看看结果如何。然后下一门课程，我们会用更系统的方法，用系统性的尝试各种超参数取值。然后其次，甚至是你已经用了很久的模型，可能你在做网络广告应用，在你开发途中，很有可能学习率的最优数值或是其他超参数的最优值是会变的，所以即使你每天都在用当前最优的参数调试你的系统，你还是会发现，最优值过一年就会变化，因为电脑的基础设施，<strong>CPU</strong>或是<strong>GPU</strong>可能会变化很大。所以有一条经验规律可能每几个月就会变。如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。</p>
<p>有一条经验规律：经常试试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的直觉。</p>
<p>总结：超参数的设定，靠经验，尝试，并调，根据结果调，</p>
<h2><span id="c1w4l08-what-does-this-have-to-do-with-the-brain">C1W4L08 : What does this have to do with the brain?</span><a href="#c1w4l08-what-does-this-have-to-do-with-the-brain" class="header-anchor">#</a></h2><h1><span id="summary-forward-prop-and-back-prop"># summary : forward prop and back prop</span><a href="#summary-forward-prop-and-back-prop" class="header-anchor">#</a></h1><h2><span id="1-logistics-regression-shallow-neural-network-and-deep-neural-network">1. logistics regression,shallow neural network and deep neural network</span><a href="#1-logistics-regression-shallow-neural-network-and-deep-neural-network" class="header-anchor">#</a></h2><p>logistics regression</p>
<script type="math/tex; mode=display">
Z = W^TX+B\\
A = \frac{1}{1+e^{-Z}}\\
L(A,Y)=-\frac{1}{m}(Ylog^A+(1-Y)log^{1-A}\\
\frac{\partial L}{\partial Z}=(A-Y)\\
\frac{\partial L}{\partial W}=X(A-Y)\\</script><p>说明：X是样本按列堆积，W是列向量</p>
<p>shallow neural network 以二分问题为例</p>
<script type="math/tex; mode=display">
Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}\\
A^{[1]}=g^{[1]}(Z^{[1]})\\
\ \\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=g^{[2]}(Z^{[2]})\\
\ \ \\
\ \\
L(A^{[2]},Y)=-\frac{1}{m}(Ylog^{A}+(1-Y)log^{1-A})\\
\frac{\partial L}{\partial Z^{[2]}}=(A^{[2]}-Y)\\
\frac{\partial L}{\partial W^{[2]}}=(A^{[2]}-Y)A^{[1]^T}\\
\frac{\partial L}{\partial b^{[2]}}=(A^{[2]}-Y)1_{1*m}^T\\
\frac{\partial L}{\partial a^{[1]}}=W^{[2]^T}(A^{[2]}-Y)\\
\ \\
\frac{\partial L}{\partial Z^{[1]}}=W^{[2]^T}(A^{[2]}-Y)* g^{'[1]}(Z^{[1]})\\</script><p>说明：W是按列排$W^{[L]}$是$n^{[L]}*n^{[L-1]}$矩阵，A,Z是按列堆积，记得检查矩阵维数就好了</p>
<h2><span id="deep-neural-network">deep neural network</span><a href="#deep-neural-network" class="header-anchor">#</a></h2><script type="math/tex; mode=display">
Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\
A^{[l]}=g^{[l]}(Z^{[l]})\\
\ \ \\
\ \\
\frac{\partial L}{\partial Z^{[l]}}=\partial A^*g^{'[l]}(Z^{l})\\
\frac{\partial L}{\partial W^{[l]}}=\partial Z^{[l]} A^{[1-1]^T}\\
\frac{\partial L}{\partial b^{[l]}}=\partial Z^{[l]}\\
\frac{\partial L}{\partial a^{[l-1]}}=W^{[l]^T}\partial Z^{[l]}\\
\ \\
\frac{\partial L}{\partial Z^{[l-1]}}=W^{[l]^T}\partial Z^{[l]}* g^{'[l-1]}(Z^{[l-1]})\\</script><h2><span id="2-vectorization">2. vectorization</span><a href="#2-vectorization" class="header-anchor">#</a></h2><ol>
<li>推导的时候要向量化，注意矩阵维数表示，可以从单个推导到mutli</li>
<li>充分利用python的广播属性，和内置函数的并行化</li>
<li>python一维，二维数组的特性</li>
</ol>
<h2><span id="3-zhi-shi-jie-gou">3. 知识结构</span><a href="#3-zhi-shi-jie-gou" class="header-anchor">#</a></h2><p><img src="/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/C1.png" alt></p>

      
    </div>

    
    
    

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://xiemayshiyi.github.io/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="wise">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to shiyi's world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">deep_learning.ai深度学习笔记<Andrew Ng></a>
        </h2>

        <div class="post-meta">
			
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-11 09:15:23" itemprop="dateCreated datePublished" datetime="2019-04-11T09:15:23+08:00">2019-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-05 21:47:45" itemprop="dateModified" datetime="2020-10-05T21:47:45+08:00">2020-10-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-Journal-of-Studying/" itemprop="url" rel="index"><span itemprop="name">学习の历程(Journal of Studying)</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1><span id="c5-sequence-models">C5: Sequence Models</span><a href="#c5-sequence-models" class="header-anchor">#</a></h1><h2><span id="w1-recurrent-neural-networks-xun-huan-xu-lie-mo-xing">W1 : Recurrent Neural Networks (循环序列模型)</span><a href="#w1-recurrent-neural-networks-xun-huan-xu-lie-mo-xing" class="header-anchor">#</a></h2><h3><span id="l1-why-sequence-models">L1 ： Why Sequence Models?</span><a href="#l1-why-sequence-models" class="header-anchor">#</a></h3><p>循环神经网络（<strong>RNN</strong>）之类的模型在语音识别、自然语言处理和其他领域中引起变革。</p>
<p>序列模型的列子</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c11.png" alt></p>
<h3><span id="l2-notation-shu-xue-fu-hao">L2 : Notation 数学符号</span><a href="#l2-notation-shu-xue-fu-hao" class="header-anchor">#</a></h3><p>NLP</p>
<p>我们用$X^{(i)}$来表示第个i训练样本，所以为了指代第个t元素，或者说是训练样本i的序列中第t个元素用$X^{(i)}<t>$这个符号来表示。如果是序列长度$T_x$，那么你的训练集里不同的训练样本就会有不同的长度，所以$T_x^{(i)}$就代表第个训练样本的输入序列长度。同样$y^{(i)}<t>$代表第i个训练样本中第t个元素，$T_y^{(i)}$就是第i个训练样本的输出序列的长度。</t></t></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c12.png" alt></p>
<p>预先有一个词典</p>
<h3><span id="l3-recurrent-neural-network-model-xun-huan-shen-jing-wang-luo-mo-xing">L3 : Recurrent Neural Network Model (循环神经网络模型)</span><a href="#l3-recurrent-neural-network-model-xun-huan-shen-jing-wang-luo-mo-xing" class="header-anchor">#</a></h3><p>现在我们讨论一下怎样才能建立一个模型，建立一个神经网络来学习X到Y的映射</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c13.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c14.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c15.png" alt></p>
<p>$a^{&lt;0&gt;}$通常 是零向量<!--0--></p>
<p>N模型包含三类权重系数，分别是Wax，Waa，Wya。且不同元素之间同一位置共享同一权重系数。</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c16.png" alt></p>
<p>RNN的正向传播（Forward Propagation）过程为：</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c17.png" alt></p>
<p>循环神经网络用的激活函数经常是<strong>tanh</strong>，不过有时候也会用<strong>ReLU</strong>，但是<strong>tanh</strong>是更通常的选择，我们有其他方法来避免梯度消失问题，我们将在之后进行讲述。选用哪个激活函数是取决于你的输出y，如果它是一个二分问题，那么我猜你会用<strong>sigmoid</strong>函数作为激活函数，如果是k类别分类问题的话，那么可以选用<strong>softmax</strong>作为激活函数。不过这里激活函数的类型取决于你有什么样类型的输出y，对于命名实体识别来说y只可能是0或者1，那我猜这里第二个激活函数g可以是<strong>sigmoid</strong>激活函数。</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c18.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c19.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c110.png" alt></p>
<h3><span id="c4-backpropagation-through-time-tong-guo-shi-jian-de-fan-xiang-chuan-bo">c4: Backpropagation through time ( 通过时间的反向传播)</span><a href="#c4-backpropagation-through-time-tong-guo-shi-jian-de-fan-xiang-chuan-bo" class="header-anchor">#</a></h3><p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c111.png" alt></p>
<p>参数的关系<img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c112.png" alt>*</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c113.png" alt></p>
<p>单个元素的Loss function:</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c114.png" alt></p>
<p>该样本所有元素的Loss function为：</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c115.png" alt></p>
<p>然后，反向传播（Backpropagation）过程就是从右到左分别计算L(y^,y)对参数Wa，Wy，ba，by的偏导数。思路与做法与标准的神经网络是一样的。一般可以通过成熟的深度学习框架自动求导，例如PyTorch、Tensorflow等。这种从右到左的求导过程被称为Backpropagation through time</p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c116.png" alt></p>
<h3><span id="l5-different-types-of-rnns-bu-tong-lei-xing-de-xun-huan-shen-jing-wang-luo">L5: Different types of <strong>RNN</strong>s (不同类型的循环神经网络)</span><a href="#l5-different-types-of-rnns-bu-tong-lei-xing-de-xun-huan-shen-jing-wang-luo" class="header-anchor">#</a></h3><p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c117.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c118.png" alt></p>
<p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c119.png" alt></p>
<h3><span id="l6-language-model-and-sequence-generation-yu-yan-mo-xing-he-xu-lie-sheng-cheng">L6 : Language model and sequence generation (语言模型和序列生成)</span><a href="#l6-language-model-and-sequence-generation-yu-yan-mo-xing-he-xu-lie-sheng-cheng" class="header-anchor">#</a></h3><p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c120.png" alt></p>
<h3><span id="l7-sampling-novel-sequences-dui-xin-xu-lie-cai-yang">L7 : Sampling novel sequences (对新序列采样)</span><a href="#l7-sampling-novel-sequences-dui-xin-xu-lie-cai-yang" class="header-anchor">#</a></h3><p><img src="/2019/04/11/deep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/c121.png" alt></p>
<h3><span id="vanishing-gradients-with-rnns-xun-huan-shen-jing-wang-luo-de-ti-du-xiao-shi">Vanishing gradients with <strong>RNN</strong>s (循环神经网络的梯度消失)</span><a href="#vanishing-gradients-with-rnns-xun-huan-shen-jing-wang-luo-de-ti-du-xiao-shi" class="header-anchor">#</a></h3><p>首先从左到右前向传播，然后反向传播。但是反向传播会很困难，因为同样的梯度消失的问题，后面层的输出误差（上图编号6所示）很难影响前面层（上图编号7所示的层）的计算。这就意味着，实际上很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的<strong>was</strong>或者<strong>were</strong>。而且在英语里面，这中间的内容（上图编号8所示）可以任意长，对吧？所以你需要长时间记住单词是单数还是复数，这样后面的句子才能用到这些信息。也正是这个原因，所以基本的<strong>RNN</strong>模型会有很多局部影响</p>
<p><a target="_blank" rel="noopener" href="http://www.ai-start.com/dl2017/html/lesson5-week1.html">http://www.ai-start.com/dl2017/html/lesson5-week1.html</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect</a></p>

      
    </div>

    
    
    

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://xiemayshiyi.github.io/2019/04/03/deeplearningvideo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="wise">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to shiyi's world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/03/deeplearningvideo/" class="post-title-link" itemprop="url">deeplearningvideo</a>
        </h2>

        <div class="post-meta">
			
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-03 10:19:55" itemprop="dateCreated datePublished" datetime="2019-04-03T10:19:55+08:00">2019-04-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-15 11:07:24" itemprop="dateModified" datetime="2019-11-15T11:07:24+08:00">2019-11-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%A7%86%E9%A2%91%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">视频学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>Coursera深度学习教程中文笔记</strong></p>
<!-- more  -->
<p>课程概述</p>
<p><a target="_blank" rel="noopener" href="https://mooc.study.163.com/university/deeplearning_ai#/c">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p>
<p>这些课程专为已有一定基础（基本的编程知识，熟悉<strong>Python</strong>、对机器学习有基本了解），想要尝试进入人工智能领域的计算机专业人士准备。介绍显示：“深度学习是科技业最热门的技能之一，本课程将帮你掌握深度学习。”</p>
<p>在这5堂课中，学生将可以学习到深度学习的基础，学会构建神经网络，并用在包括吴恩达本人在内的多位业界顶尖专家指导下创建自己的机器学习项目。<strong>Deep Learning Specialization</strong>对卷积神经网络 (<strong>CNN</strong>)、递归神经网络 (<strong>RNN</strong>)、长短期记忆 (<strong>LSTM</strong>) 等深度学习常用的网络结构、工具和知识都有涉及。</p>
<p><strong>笔记是根据视频和字幕写的，没有技术含量，只需要专注和严谨。</strong></p>
<p>2018-04-14</p>
<p><strong>本课程视频教程地址：</strong><a target="_blank" rel="noopener" href="https://mooc.study.163.com/university/deeplearning_ai#/c">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p>
<p>（该视频从www.deeplearning.ai 网站下载，因众所周知的原因，国内用户观看某些在线视频非常不容易，故一些学者一起制作了离线视频，旨在方便国内用户个人学习使用，请勿用于商业用途。视频内嵌中英文字幕，推荐使用<strong>potplayer</strong>播放。版权属于吴恩达老师所有，若在线视频流畅，请到官方网站观看。）</p>
<p><a target="_blank" rel="noopener" href="http://www.ai-start.com/">笔记网站(适合手机阅读)</a></p>
<p>吴恩达老师的机器学习课程笔记和视频：<a target="_blank" rel="noopener" href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p>
<h1><span id="shen-du-xue-xi-bi-ji-mu-lu">深度学习笔记目录</span><a href="#shen-du-xue-xi-bi-ji-mu-lu" class="header-anchor">#</a></h1><h2><span id="di-yi-men-ke-shen-jing-wang-luo-he-shen-du-xue-xi-neural-networks-and-deep-learning">第一门课 神经网络和深度学习(Neural Networks and Deep Learning)</span><a href="#di-yi-men-ke-shen-jing-wang-luo-he-shen-du-xue-xi-neural-networks-and-deep-learning" class="header-anchor">#</a></h2><p>第一周：深度学习引言(Introduction to Deep Learning)</p>
<p>1.1 欢迎(Welcome)</p>
<p>1.2 什么是神经网络？(What is a Neural Network)</p>
<p>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</p>
<p>1.4 为什么神经网络会流行？(Why is Deep Learning taking off?)</p>
<p>1.5 关于本课程(About this Course)</p>
<p>1.6 课程资源(Course Resources)</p>
<p>1.7 Geoffery Hinton 专访(Geoffery Hinton interview)</p>
<p>第二周：神经网络的编程基础(Basics of Neural Network programming)</p>
<p>2.1 二分类(Binary Classification)</p>
<p>2.2 逻辑回归(Logistic Regression)</p>
<p>2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</p>
<p>2.4 梯度下降（Gradient Descent）</p>
<p>2.5 导数（Derivatives）</p>
<p>2.6 更多的导数例子（More Derivative Examples）</p>
<p>2.7 计算图（Computation Graph）</p>
<p>2.8 计算图导数（Derivatives with a Computation Graph）</p>
<p>2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）</p>
<p>2.10 梯度下降的例子(Gradient Descent on m Examples)</p>
<p>2.11 向量化(Vectorization)</p>
<p>2.12 更多的向量化例子（More Examples of Vectorization）</p>
<p>2.13 向量化逻辑回归(Vectorizing Logistic Regression)</p>
<p>2.14 向量化逻辑回归的梯度计算（Vectorizing Logistic Regression’s Gradient）</p>
<p>2.15 Python中的广播机制（Broadcasting in Python）</p>
<p>2.16 关于 Python与numpy向量的使用（A note on python or numpy vectors）</p>
<p>2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</p>
<p>2.18 逻辑回归损失函数详解（Explanation of logistic regression cost function）</p>
<p>第三周：浅层神经网络(Shallow neural networks)</p>
<p>3.1 神经网络概述（Neural Network Overview）</p>
<p>3.2 神经网络的表示（Neural Network Representation）</p>
<p>3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</p>
<p>3.4 多样本向量化（Vectorizing across multiple examples）</p>
<p>3.5 向量化实现的解释（Justification for vectorized implementation）</p>
<p>3.6 激活函数（Activation functions）</p>
<p>3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</p>
<p>3.8 激活函数的导数（Derivatives of activation functions）</p>
<p>3.9 神经网络的梯度下降（Gradient descent for neural networks）</p>
<p>3.10（选修）直观理解反向传播（Backpropagation intuition）</p>
<p>3.11 随机初始化（Random+Initialization）</p>
<p>第四周：深层神经网络(Deep Neural Networks)</p>
<p>4.1 深层神经网络（Deep L-layer neural network）</p>
<p>4.2 前向传播和反向传播（Forward and backward propagation）</p>
<p>4.3 深层网络中的前向和反向传播（Forward propagation in a Deep Network）</p>
<p>4.4 核对矩阵的维数（Getting your matrix dimensions right）</p>
<p>4.5 为什么使用深层表示？（Why deep representations?）</p>
<p>4.6 搭建神经网络块（Building blocks of deep neural networks）</p>
<p>4.7 参数VS超参数（Parameters vs Hyperparameters）</p>
<p>4.8 深度学习和大脑的关联性（What does this have to do with the brain?）</p>
<h2><span id="di-er-men-ke-gai-shan-shen-ceng-shen-jing-wang-luo-chao-can-shu-diao-shi-zheng-ze-hua-yi-ji-you-hua-improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization">第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)</span><a href="#di-er-men-ke-gai-shan-shen-ceng-shen-jing-wang-luo-chao-can-shu-diao-shi-zheng-ze-hua-yi-ji-you-hua-improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization" class="header-anchor">#</a></h2><p>第一周：深度学习的实用层面(Practical aspects of Deep Learning)</p>
<p>1.1 训练，验证，测试集（Train / Dev / Test sets）</p>
<p>1.2 偏差，方差（Bias /Variance）</p>
<p>1.3 机器学习基础（Basic Recipe for Machine Learning）</p>
<p>1.4 正则化（Regularization）</p>
<p>1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</p>
<p>1.6 dropout 正则化（Dropout Regularization）</p>
<p>1.7 理解 dropout（Understanding Dropout）</p>
<p>1.8 其他正则化方法（Other regularization methods）</p>
<p>1.9 标准化输入（Normalizing inputs）</p>
<p>1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</p>
<p>1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing /Exploding gradients）</p>
<p>1.12 梯度的数值逼近（Numerical approximation of gradients）</p>
<p>1.13 梯度检验（Gradient checking）</p>
<p>1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）</p>
<p>第二周：优化算法 (Optimization algorithms)</p>
<p>2.1 Mini-batch 梯度下降（Mini-batch gradient descent）</p>
<p>2.2 理解Mini-batch 梯度下降（Understanding Mini-batch gradient descent）</p>
<p>2.3 指数加权平均（Exponentially weighted averages）</p>
<p>2.4 理解指数加权平均（Understanding Exponentially weighted averages）</p>
<p>2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted averages）</p>
<p>2.6 momentum梯度下降（Gradient descent with momentum）</p>
<p>2.7 RMSprop——root mean square prop（RMSprop）</p>
<p>2.8 Adam优化算法（Adam optimization algorithm）</p>
<p>2.9 学习率衰减（Learning rate decay）</p>
<p>2.10 局部最优问题（The problem of local optima）</p>
<p>第三周超参数调试，batch正则化和程序框架（Hyperparameter tuning, Batch Normalization and Programming Frameworks)</p>
<p>3.1 调试处理（Tuning process）</p>
<p>3.2 为超参数选择和适合范围（Using an appropriate scale to pick hyperparameters）</p>
<p>3.3 超参数训练的实践：Pandas vs. Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）</p>
<p>3.4 网络中的正则化激活函数（Normalizing activations in a network）</p>
<p>3.5 将 Batch Norm拟合进神经网络（Fitting Batch Norm into a neural network）</p>
<p>3.6 为什么Batch Norm奏效？（Why does Batch Norm work?）</p>
<p>3.7 测试时的Batch Norm（Batch Norm at test time）</p>
<p>3.8 Softmax 回归（Softmax Regression）</p>
<p>3.9 训练一个Softmax 分类器（Training a softmax classifier）</p>
<p>3.10 深度学习框架（Deep learning frameworks）</p>
<p>3.11 TensorFlow（TensorFlow）</p>
<h2><span id="di-san-men-ke-jie-gou-hua-ji-qi-xue-xi-xiang-mu-structuring-machine-learning-projects">第三门课 结构化机器学习项目 (Structuring Machine Learning Projects)</span><a href="#di-san-men-ke-jie-gou-hua-ji-qi-xue-xi-xiang-mu-structuring-machine-learning-projects" class="header-anchor">#</a></h2><p>第一周：机器学习策略（1）(ML Strategy (1))</p>
<p>1.1 为什么是ML策略？ (Why ML Strategy)</p>
<p>1.2 正交化(Orthogonalization)</p>
<p>1.3 单一数字评估指标(Single number evaluation metric)</p>
<p>1.4 满足和优化指标 (Satisficing and Optimizing metric)</p>
<p>1.5 训练集、开发集、测试集的划分(Train/dev/test distributions)</p>
<p>1.6 开发集和测试集的大小 (Size of the dev and test sets)</p>
<p>1.7 什么时候改变开发集/测试集和评估指标(When to change dev/test sets and metrics)</p>
<p>1.8 为什么是人的表现 (Why human-level performance?)</p>
<p>1.9 可避免偏差(Avoidable bias)</p>
<p>1.10 理解人类的表现 (Understanding human-level performance)</p>
<p>1.11 超过人类的表现(Surpassing human-level performance)</p>
<p>1.12 改善你的模型表现 (Improving your model performance)</p>
<p>第二周：机器学习策略（2）(ML Strategy (2))</p>
<p>2.1 误差分析 (Carrying out error analysis)</p>
<p>2.2 清除标注错误的数据(Cleaning up incorrectly labeled data)</p>
<p>2.3 快速搭建你的第一个系统，并进行迭代(Build your first system quickly, then iterate)</p>
<p>2.4 在不同的分布上的训练集和测试集 (Training and testing on different distributions)</p>
<p>2.5 数据分布不匹配的偏差与方差分析 (Bias and Variance with mismatched data distributions)</p>
<p>2.6 处理数据不匹配问题(Addressing data mismatch)</p>
<p>2.7 迁移学习 (Transfer learning)</p>
<p>2.8 多任务学习(Multi-task learning)</p>
<p>2.9 什么是端到端的深度学习？ (What is end-to-end deep learning?)</p>
<p>2.10 是否使用端到端的深度学习方法 (Whether to use end-to-end deep learning)</p>
<h2><span id="di-si-men-ke-juan-ji-shen-jing-wang-luo-convolutional-neural-networks">第四门课 卷积神经网络（Convolutional Neural Networks）</span><a href="#di-si-men-ke-juan-ji-shen-jing-wang-luo-convolutional-neural-networks" class="header-anchor">#</a></h2><p>第一周 卷积神经网络(Foundations of Convolutional Neural Networks)</p>
<p>1.1    计算机视觉（Computer vision）</p>
<p>1.2    边缘检测示例（Edge detection example）</p>
<p>1.3    更多边缘检测内容（More edge detection）</p>
<p>1.4    Padding</p>
<p>1.5    卷积步长（Strided convolutions）</p>
<p>1.6    三维卷积（Convolutions over volumes）</p>
<p>1.7    单层卷积网络（One layer of a convolutional network）</p>
<p>1.8    简单卷积网络示例（A simple convolution network example）</p>
<p>1.9    池化层（Pooling layers）</p>
<p>1.10 卷积神经网络示例（Convolutional neural network example）</p>
<p>1.11 为什么使用卷积？（Why convolutions?）</p>
<p>第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)</p>
<p>2.1 为什么要进行实例探究？（Why look at case studies?）</p>
<p>2.2 经典网络（Classic networks）</p>
<p>2.3 残差网络（Residual Networks (ResNets)）</p>
<p>2.4 残差网络为什么有用？（Why ResNets work?）</p>
<p>2.5 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）</p>
<p>2.6 谷歌 Inception 网络简介（Inception network motivation）</p>
<p>2.7 Inception 网络（Inception network）</p>
<p>2.8 使用开源的实现方案（Using open-source implementations）</p>
<p>2.9 迁移学习（Transfer Learning）</p>
<p>2.10 数据扩充（Data augmentation）</p>
<p>2.11 计算机视觉现状（The state of computer vision）</p>
<p>第三周 目标检测（Object detection）</p>
<p>3.1 目标定位（Object localization）</p>
<p>3.2 特征点检测（Landmark detection）</p>
<p>3.3 目标检测（Object detection）</p>
<p>3.4 卷积的滑动窗口实现（Convolutional implementation of sliding windows）</p>
<p>3.5 Bounding Box预测（Bounding box predictions）</p>
<p>3.6 交并比（Intersection over union）</p>
<p>3.7 非极大值抑制（Non-max suppression）</p>
<p>3.8 Anchor Boxes</p>
<p>3.9 YOLO 算法（Putting it together: YOLO algorithm）</p>
<p>3.10 候选区域（选修）（Region proposals (Optional)）</p>
<p>第四周 特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;Neural style transfer）</p>
<p>4.1 什么是人脸识别？(What is face recognition?)</p>
<p>4.2 One-Shot学习（One-shot learning）</p>
<p>4.3 Siamese 网络（Siamese network）</p>
<p>4.4 Triplet 损失（Triplet 损失）</p>
<p>4.5 面部验证与二分类（Face verification and binary classification）</p>
<p>4.6 什么是神经风格转换？（What is neural style transfer?）</p>
<p>4.7 什么是深度卷积网络？（What are deep ConvNets learning?）</p>
<p>4.8 代价函数（Cost function）</p>
<p>4.9 内容代价函数（Content cost function）</p>
<p>4.10 风格代价函数（Style cost function）</p>
<p>4.11 一维到三维推广（1D and 3D generalizations of models）</p>
<h1><span id="di-wu-men-ke-xu-lie-mo-xing-sequence-models">第五门课 序列模型(Sequence Models)</span><a href="#di-wu-men-ke-xu-lie-mo-xing-sequence-models" class="header-anchor">#</a></h1><p>第一周 循环序列模型（Recurrent Neural Networks） 1.1 为什么选择序列模型？（Why Sequence Models?）</p>
<p>1.2 数学符号（Notation）</p>
<p>1.3 循环神经网络模型（Recurrent Neural Network Model）</p>
<p>1.4 通过时间的反向传播（Backpropagation through time）</p>
<p>1.5 不同类型的循环神经网络（Different types of RNNs）</p>
<p>1.6 语言模型和序列生成（Language model and sequence generation）</p>
<p>1.7 对新序列采样（Sampling novel sequences）</p>
<p>1.8 循环神经网络的梯度消失（Vanishing gradients with RNNs）</p>
<p>1.9 GRU单元（Gated Recurrent Unit（GRU））</p>
<p>1.10 长短期记忆（LSTM（long short term memory）unit）</p>
<p>1.11 双向循环神经网络（Bidirectional RNN）</p>
<p>1.12 深层循环神经网络（Deep RNNs）</p>
<p>第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）</p>
<p>2.1 词汇表征（Word Representation）</p>
<p>2.2 使用词嵌入（Using Word Embeddings）</p>
<p>2.3 词嵌入的特性（Properties of Word Embeddings）</p>
<p>2.4 嵌入矩阵（Embedding Matrix）</p>
<p>2.5 学习词嵌入（Learning Word Embeddings）</p>
<p>2.6 Word2Vec</p>
<p>2.7 负采样（Negative Sampling）</p>
<p>2.8 GloVe 词向量（GloVe Word Vectors）</p>
<p>2.9 情绪分类（Sentiment Classification）</p>
<p>2.10 词嵌入除偏（Debiasing Word Embeddings）</p>
<p>第三周 序列模型和注意力机制（Sequence models &amp; Attention mechanism）</p>
<p>3.1 基础模型（Basic Models）</p>
<p>3.2 选择最可能的句子（Picking the most likely sentence）</p>
<p>3.3 集束搜索（Beam Search）</p>
<p>3.4 改进集束搜索（Refinements to Beam Search）</p>
<p>3.5 集束搜索的误差分析（Error analysis in beam search）</p>
<p>3.6 Bleu 得分（选修）（Bleu Score (optional)）</p>
<p>3.7 注意力模型直观理解（Attention Model Intuition）</p>
<p>3.8注意力模型（Attention Model）</p>
<p>3.9语音识别（Speech recognition）</p>
<p>3.10触发字检测（Trigger Word Detection）</p>
<p>3.11结论和致谢（Conclusion and thank you）</p>
<p>人工智能大师访谈</p>
<p>吴恩达采访 Geoffery Hinton</p>
<p>吴恩达采访 Ian Goodfellow</p>
<p>吴恩达采访 Ruslan Salakhutdinov</p>
<p>吴恩达采访 Yoshua Bengio</p>
<p>吴恩达采访 林元庆</p>
<p>吴恩达采访 Pieter Abbeel</p>
<p>吴恩达采访 Andrej Karpathy</p>
<p>附件</p>
<p>深度学习符号指南（原课程翻译）</p>

      
    </div>

    
    
    

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/14/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/16/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">XieMay</p>
  <div class="site-description" itemprop="description">wise</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">88</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/shiyichuixue" title="GitHub → https://github.com/shiyichuixue" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:2323020965@qq.com" title="E-Mail → mailto:2323020965@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XieMay</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">493k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">7:28</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共176.2k字</span>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '[object Object]';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/hibiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"tagMode":false});</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body>
</html>
