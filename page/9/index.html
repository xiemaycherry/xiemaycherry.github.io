<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="baidu-site-verification" content="E1Di33CelZ">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="WsojkhmMcOefku3B2Vxp02NtxlUt_JzBP1fVPrFk3Gw">


  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.0.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.4">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.4" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: 'WGLQQAQKBA',
      apiKey: '',
      indexName: 'xiemay',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  



<meta name="baidu-site-verification" content="z0tVDge8Pe">

  
  <meta name="keywords" content="Hexo, NexT">


<meta name="description" content="CAT_cat">
<meta property="og:type" content="website">
<meta property="og:title" content="welcome">
<meta property="og:url" content="http://xiemaycherry.github.io/page/9/index.html">
<meta property="og:site_name" content="welcome">
<meta property="og:description" content="CAT_cat">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="welcome">
<meta name="twitter:description" content="CAT_cat">






  <link rel="canonical" href="http://xiemaycherry.github.io/page/9/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>welcome</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<meta name="baidu-site-verification" content="3BmH9zSH5h"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">welcome</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>
        </li>
      
        
        <li class="menu-item menu-item-留言">
          <a href="/guestbook/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-commenting"></i> <br>留言</a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>Schedule</a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>Sitemap</a>
        </li>
      
        
        <li class="menu-item menu-item-baidusitmap">
          <a href="/baidusitmap.xml" rel="section">
            <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>baidusitmap</a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/05/05/Deep Learning ai_Deep Learning Specialization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/05/Deep Learning ai_Deep Learning Specialization/" itemprop="url">The Deep Learning Specialization</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-05T19:41:59+08:00">2019-05-05</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/05/Deep Learning ai_Deep Learning Specialization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/05/05/Deep Learning ai_Deep Learning Specialization/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                10k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="C3-Improving-Model-Performance"><a href="#C3-Improving-Model-Performance" class="headerlink" title="C3 Improving Model Performance"></a>C3 Improving Model Performance</h1><h2 id="W1-ML-Strategy-1"><a href="#W1-ML-Strategy-1" class="headerlink" title="W1 ML Strategy(1)"></a>W1 ML Strategy(1)</h2><h3 id="L01-Improving-Model-Performance"><a href="#L01-Improving-Model-Performance" class="headerlink" title="L01 Improving Model Performance"></a>L01 Improving Model Performance</h3><p>需要提高训练结果的表现，表现得更好的措施 Machine Learning Strategy</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-24_21-03-13.jpg" alt=""></p>
<h3 id="L2-Orthogonalization-正交化"><a href="#L2-Orthogonalization-正交化" class="headerlink" title="L2 : Orthogonalization(正交化)"></a>L2 : Orthogonalization(正交化)</h3><p>所谓正交，<strong>就是你的操控效果尽量只影响一个方面</strong>。比如以老式电视机为例，调节图像的大小、左右偏移、上下偏移。而不是一个按钮可以同时调节图像大小和左右偏移，那样会很难操作。</p>
<p>具体到supervised learning，有以下4个假设是正交的？</p>
<ol>
<li>Fit <strong>training set</strong> well in cost function If it doesn’t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.</li>
<li>Fit <strong>development set</strong> well on cost function If it doesn’t fit well, regularization or using bigger training set might help.</li>
<li>Fit <strong>test set</strong> well on cost function If it doesn’t fit well, the use of a bigger development set might help</li>
<li>Performs well in <strong>real world</strong> If it doesn’t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing.</li>
</ol>
<p>在训练集上表现欠佳，需要切换到好的优化算法</p>
<p>在验证集上表现不好，一组正则化按钮</p>
<p>在测试集表现不好，需要更好的验证集</p>
<p>在用户体验不好，需要改变测试集大小或者成本函数</p>
<h3 id="L3-Single-number-evaluation-metric-单一数字评估指标"><a href="#L3-Single-number-evaluation-metric-单一数字评估指标" class="headerlink" title="L3 Single number evaluation metric(单一数字评估指标)"></a>L3 Single number evaluation metric(单一数字评估指标)</h3><h4 id="classification"><a href="#classification" class="headerlink" title="classification"></a>classification</h4><p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-25_20-42-10.jpg" alt=""></p>
<h4 id="Precesion-（查准率）"><a href="#Precesion-（查准率）" class="headerlink" title="Precesion （查准率）"></a>Precesion （查准率）</h4><p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-25_20-47-46.jpg" alt=""></p>
<h4 id="recall（查全率）"><a href="#recall（查全率）" class="headerlink" title="recall（查全率）"></a>recall（查全率）</h4><p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-25_20-48-11.jpg" alt=""></p>
<script type="math/tex; mode=display">
F 1=\frac{2}{\frac{1}{P}+\frac{1}{R}}=\frac{2 P R}{P+R}</script><h3 id="L4-Satisficing-and-optimizing-metrics-满足和优化指标"><a href="#L4-Satisficing-and-optimizing-metrics-满足和优化指标" class="headerlink" title="L4  Satisficing and optimizing metrics(满足和优化指标)"></a>L4  Satisficing and optimizing metrics(满足和优化指标)</h3><p>如果我们还想要将分类器的运行时间也纳入考虑范围，将其和精确率、召回率组合成一个单值评价指标显然不那么合适。这时，我们可以将某些指标作为<strong>优化指标（Optimizing Matric）</strong>，寻求它们的最优值；而将某些指标作为<strong>满足指标（Satisficing Matric）</strong>，只要在一定阈值以内即可。</p>
<p>在这个例子中，准确率就是一个优化指标，因为我们想要分类器尽可能做到正确分类；而运行时间就是一个满足指标，如果你想要分类器的运行时间不多于某个阈值，那最终选择的分类器就应该是以这个阈值为界里面准确率最高的那个。</p>
<p>如此，accuracy就变成了<strong>optimizing metric</strong>，而running time则是<strong>satisfying metric</strong>，statisfying metric只要达到标准即可，而optimizing metric则追求更好。一般的，选择一项metric作为optimizing metric，其他的则设置为satisfying metric： </p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-09-36.jpg" alt=""></p>
<h3 id="L-5-Train-dev-test-distributions-训练-开发-测试集划分"><a href="#L-5-Train-dev-test-distributions-训练-开发-测试集划分" class="headerlink" title="L 5: Train/dev/test distributions(训练/开发/测试集划分)"></a>L 5: Train/dev/test distributions(训练/开发/测试集划分)</h3><p>开发（<strong>dev</strong>）集也叫做开发集（<strong>development set</strong>），有时称为保留交叉验证集（<strong>hold out cross validation set</strong>）。</p>
<p>如何设置Train/dev/test集，很大程度上影响了机器学习的速度。</p>
<p>Train/dev/test的区别 Workflow in machine learning is that you try a lot of ideas, train up different models on the training set, and then use the dev set to evaluate the different ideas and pick one. And, keep innovating to improve dev set performance until, finally, you have one class that you’re happy with that you then evaluate on your test set.</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-09-37.jpg" alt=""></p>
<p>开发集合和开发集合来自同一分布，如果是不同分布，相当于靶心移动了</p>
<h3 id="L-6-Size-of-dev-and-test-sets-开发集和测试集的大小"><a href="#L-6-Size-of-dev-and-test-sets-开发集和测试集的大小" class="headerlink" title="L 6: Size of dev and test sets(开发集和测试集的大小)"></a>L 6: Size of dev and test sets(开发集和测试集的大小)</h3><p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-29-51.jpg" alt=""></p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-25.jpg" alt=""></p>
<h3 id="L7-When-to-change-dev-test-sets-and-metrics-什么时候该改变开发-测试集和指标"><a href="#L7-When-to-change-dev-test-sets-and-metrics-什么时候该改变开发-测试集和指标" class="headerlink" title="L7 : When to change dev/test sets and metrics(什么时候该改变开发/测试集和指标)"></a>L7 : When to change dev/test sets and metrics(什么时候该改变开发/测试集和指标)</h3><p>如果发现设定目标和实际期望不符，那就调整目标。</p>
<ol>
<li><p>举个例子</p>
<p>A可能把一些色情照片也分类成猫了，因此改变优化指标</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-26.jpg" alt=""></p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-27.jpg" alt=""></p>
</li>
</ol>
<p>我想你处理机器学习问题时，应该把它切分成独立的步骤。一步是弄清楚如何定义一个指标来衡量你想做的事情的表现，然后我们可以分开考虑如何改善系统在这个指标上的表现。你们要把机器学习任务看成两个独立的步骤，用目标这个比喻，第一步就是设定目标。所以要定义你要瞄准的目标，这是完全独立的一步，这是你可以调节的一个旋钮。如何设立目标是一个完全独立的问题，把它看成是一个单独的旋钮，可以调试算法表现的旋钮，如何精确瞄准，如何命中目标，定义指标是第一步。</p>
<p>后第二步要做别的事情，在逼近目标的时候，也许你的学习算法针对某个长这样的成本函数优化，$J=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)$你要最小化训练集上的损失。你可以做的其中一件事是，修改这个，为了引入这些权重，也许最后需要修改这个归一化常数，$J=\frac{1}{\sum w^{(i)}} \sum_{i=1}^{m} w^{(i)} L\left(\hat{y}^{(i)}, y^{(i)}\right)$</p>
<p>再次，如何定义J并不重要，关键在于正交化的思路，把设立目标定为第一步，然后瞄准和射击目标是独立的第二步。换种说法，我鼓励你们将定义指标看成一步，然后在定义了指标之后，你才能想如何优化系统来提高这个指标评分。比如改变你神经网络要优化的成本函数J。</p>
<h3 id="L8-Why-human-level-performance-为什么是人的表现？"><a href="#L8-Why-human-level-performance-为什么是人的表现？" class="headerlink" title="L8 : Why human-level performance?(为什么是人的表现？)"></a>L8 : Why human-level performance?(为什么是人的表现？)</h3><p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Bayes-Optimal-Error.png" alt=""></p>
<p>上图展示了随着时间的推进，机器学习系统和人的表现水平的变化。一般的，当机器学习超过人的表现水平后，它的进步速度逐渐变得缓慢，最终性能无法超过某个理论上限，这个上限被称为<strong>贝叶斯最优误差（Bayes Optimal Error）</strong>。</p>
<p>也因此，只要建立的机器学习模型的表现还没达到人类的表现水平时，就可以通过各种手段来提升它。例如采用人工标记过的数据进行训练，通过人工误差分析了解为什么人能够正确识别，或者是进行偏差、方差分析。</p>
<p>当模型的表现超过人类后，这些手段起的作用就微乎其微了。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/e1ef954731399bb4fbf18f2fb99b863a.png" alt=""></p>
<h3 id="L9-Avoidable-bias-可避免偏差"><a href="#L9-Avoidable-bias-可避免偏差" class="headerlink" title="L9 : Avoidable bias(可避免偏差)"></a>L9 : Avoidable bias(可避免偏差)</h3><ol>
<li><p>training error</p>
<p>我们经常使用猫分类器来做例子，比如人类具有近乎完美的准确度，所以人类水平的错误是1%。在这种情况下，如果您的学习算法达到8%的训练错误率和10%的开发错误率，那么你也许想在训练集上得到更好的结果。所以事实上，你的算法在训练集上的表现和人类水平的表现有很大差距的话，说明你的算法对训练集的拟合并不好。所以从减少偏差和方差的工具这个角度看，在这种情况下，我会把重点放在减少偏差上。你需要做的是，比如说训练更大的神经网络，或者跑久一点梯度下降，就试试能不能在训练集上做得更好。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-28.jpg" alt=""></p>
</li>
</ol>
<ol>
<li><p>dev error</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-29.jpg" alt=""></p>
</li>
</ol>
<p><strong>贝叶斯错误率或者对贝叶斯错误率的估计和训练错误率之间的差值称为可避免偏差</strong></p>
<h3 id="L-10-Understanding-human-level-performance-理解人的表现"><a href="#L-10-Understanding-human-level-performance-理解人的表现" class="headerlink" title="L 10: Understanding human-level performance(理解人的表现)"></a>L 10: Understanding human-level performance(理解人的表现)</h3><p>还记得上个视频中，我们用过这个词“人类水平错误率”用来估计贝叶斯误差，那就是理论最低的错误率，任何函数不管是现在还是将来，能够到达的最低值</p>
<h3 id="L11-Surpassing-human-level-performance-超过人的表现"><a href="#L11-Surpassing-human-level-performance-超过人的表现" class="headerlink" title="L11 : Surpassing human- level performance(超过人的表现)"></a>L11 : Surpassing human- level performance(超过人的表现)</h3><p>现在，机器学习有很多问题已经可以大大超越人类水平了。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/de2eb0ddc7918f6e9213871e07b8fa56.png" alt=""></p>
<h3 id="L12-Improving-your-model-performance-改善你的模型的表现"><a href="#L12-Improving-your-model-performance-改善你的模型的表现" class="headerlink" title="L12 : Improving your model performance(改善你的模型的表现)"></a>L12 : Improving your model performance(改善你的模型的表现)</h3><p>你们学过正交化，如何设立开发集和测试集，用人类水平错误率来估计贝叶斯错误率以及如何估计可避免偏差和方差。我们现在把它们全部组合起来写成一套指导方针，如何提高学习算法性能的指导方针。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-39.jpg" alt=""></p>
<p>method</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-49.jpg" alt=""></p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><font color="red">这一周的内容主要是改善模型的表现，主要是按照正交化，使得更好的满足 1. 评价指标 2. 数据集的划分 3. 人的表现的重要性 4. 当出现表现不好的时候，如何改善呢，有哪些方法呢？ </font>

<h2 id="W2-ML-Strategy-2"><a href="#W2-ML-Strategy-2" class="headerlink" title="W2 ML Strategy(2)"></a>W2 ML Strategy(2)</h2><h3 id="C-1-Carrying-out-error-analysis-进行误差分析"><a href="#C-1-Carrying-out-error-analysis-进行误差分析" class="headerlink" title="C 1: Carrying out error analysis(进行误差分析)"></a>C 1: Carrying out error analysis(进行误差分析)</h3><h4 id="1-simple-analysis"><a href="#1-simple-analysis" class="headerlink" title="1. simple analysis"></a>1. simple analysis</h4><p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/e1ef954731399bb4fbf18f2fb99b863.png" alt=""></p>
<p>通过观察发现算法分类出错的例子，是把狗分成猫，提高准确率的方法就是如何针对狗的图片优化算法。你可以针对狗，收集更多的狗图，或者设计一些只处理狗的算法功能之类的，为了让你的猫分类器在狗图上做的更好，让算法不再将狗分类成猫。现在考虑的是应该不应该这么去做呢？统计一下dev set里面多少是错误标记是狗的个数，分析出可以改善的算法的上限。</p>
<h4 id="mutiply-analysis"><a href="#mutiply-analysis" class="headerlink" title="mutiply analysis"></a>mutiply analysis</h4><p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-51.jpg" alt=""></p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-50.jpg" alt=""></p>
<h3 id="C2-Cleaning-up-Incorrectly-labeled-data-清除标注错误的数据"><a href="#C2-Cleaning-up-Incorrectly-labeled-data-清除标注错误的数据" class="headerlink" title="C2 : Cleaning up Incorrectly labeled data(清除标注错误的数据)"></a>C2 : Cleaning up Incorrectly labeled data(清除标注错误的数据)</h3><h4 id="incorrct-label"><a href="#incorrct-label" class="headerlink" title="incorrct label"></a>incorrct label</h4><h4 id="traning-set"><a href="#traning-set" class="headerlink" title="traning set"></a>traning set</h4><p>DL algorithms are quite robust to random errors in the traning set so long as your errors or your labeled example to once those errors are not too far from random .</p>
<h4 id="distribution"><a href="#distribution" class="headerlink" title="distribution"></a>distribution</h4><p>首先，我鼓励你不管用什么修正手段，都要同时作用到开发集和测试集上，我们之前讨论过为什么，开发和测试集必须来自相同的分布。开发集确定了你的目标，当你击中目标后，你希望算法能够推广到测试集上，这样你的团队能够更高效的在来自同一分布的开发集和测试集上迭代。如果你打算修正开发集上的部分数据，那么最好也对测试集做同样的修正以确保它们继续来自相同的分布。所以我们雇佣了一个人来仔细检查这些标签，但必须同时检查开发集和测试集。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-52.jpg" alt=""></p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-53.jpg" alt=""></p>
<h4 id="suggestion"><a href="#suggestion" class="headerlink" title="suggestion"></a>suggestion</h4><p>最后我讲几个建议：</p>
<p>首先，深度学习研究人员有时会喜欢这样说：“我只是把数据提供给算法，我训练过了，效果拔群”。这话说出了很多深度学习错误的真相，更多时候，我们把数据喂给算法，然后训练它，并减少人工干预，减少使用人类的见解。但我认为，在构造实际系统时，通常需要更多的人工错误分析，更多的人类见解来架构这些系统，尽管深度学习的研究人员不愿意承认这点。</p>
<p>其次，不知道为什么，我看一些工程师和研究人员不愿意亲自去看这些样本，也许做这些事情很无聊，坐下来看100或几百个样本来统计错误数量，但我经常亲自这么做。当我带领一个机器学习团队时，我想知道它所犯的错误，我会亲自去看看这些数据，尝试和一部分错误作斗争。我想就因为花了这几分钟，或者几个小时去亲自统计数据，真的可以帮你找到需要优先处理的任务，我发现花时间亲自检查数据非常值得，所以我强烈建议你们这样做，如果你在搭建你的机器学习系统的话，然后你想确定应该优先尝试哪些想法，或者哪些方向。</p>
<p>这就是错误分析过程，在下一个视频中，我想分享一下错误分析是如何在启动新的机器学习项目中发挥作用的。</p>
<h3 id="C3-Build-your-first-system-quickly-then-iterate-快速搭建你的第一个系统，并进行迭代"><a href="#C3-Build-your-first-system-quickly-then-iterate-快速搭建你的第一个系统，并进行迭代" class="headerlink" title="C3: Build your first system quickly, then iterate(快速搭建你的第一个系统，并进行迭代)"></a>C3: Build your first system quickly, then iterate(快速搭建你的第一个系统，并进行迭代)</h3><h4 id="1-iteration"><a href="#1-iteration" class="headerlink" title="1. iteration"></a>1. iteration</h4><p>I recommend that you first quickly set up a definition and metrics so this is really you know  deciding where to place your target and you get it wrong you can always move it later we just set up a target somewhere and then I recommend you build an inital machine learning system quickly find the traning set train it and see start to see and understand how well your are doing against your Devon chess setting evaluation metric when you build your initial system you then be able to use bias variance analysis we should talk about earlier as well as error analysis whick we talked about just in last several videos to prioritize the next step in particular if error analysis causes you to realize that a lot of the errors are from the spearker being very far from the mirophone which causes special challenges speech recognitin then that would give you a good reason to focus on techniques to address this it called fast used speech recognition which basically means handling when the speaker is very far from microphone along the value of building this inital  system  it can be a quick  and diry implementation you know do not overthink it but all the value of the inital system is having some learning system having some tranin system allows you lok at bias and variance to do error analysis look at some mistakes to figure out all the different directins you could go in.</p>
<p>我鼓励你们搭建快速而粗糙的实现，然后用它做偏差/方差分析，用它做错误分析，然后用分析结果确定下一步优先要做的方向。</p>
<h3 id="C4-Training-and-testing-on-different-distributions-使用来自不同分布的数据，进行训练和测试"><a href="#C4-Training-and-testing-on-different-distributions-使用来自不同分布的数据，进行训练和测试" class="headerlink" title="C4 : Training and testing on different distributions(使用来自不同分布的数据，进行训练和测试)"></a>C4 : Training and testing on different distributions(使用来自不同分布的数据，进行训练和测试)</h3><p>this is resulted in many teams sometimes taking one of the days you can find and just shoving it into the training set .</p>
<ol>
<li><p>Cat app example </p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/Xnip2018-06-26_08-32-54.jpg" alt=""></p>
<p>假设你在开发一个手机应用，用户会上传他们用手机拍摄的照片，你想识别用户从应用中上传的图片是不是猫。现在你有两个数据来源，一个是你真正关心的数据分布，来自应用上传的数据，比如右边的应用，这些照片一般更业余，取景不太好，有些甚至很模糊，因为它们都是业余用户拍的。另一个数据来源就是你可以用爬虫程序挖掘网页直接下载，就这个样本而言，可以下载很多取景专业、高分辨率、拍摄专业的猫图片。如果你的应用用户数还不多，也许你只收集到10,000张用户上传的照片，但通过爬虫挖掘网页，你可以下载到海量猫图，也许你从互联网上下载了超过20万张猫图。而你真正关心的算法表现是你的最终系统处理来自应用程序的这个图片分布时效果好不好，因为最后你的用户会上传类似右边这些图片，你的分类器必须在这个任务中表现良好。现在你就陷入困境了，因为你有一个相对小的数据集，只有10,000个样本来自那个分布，而你还有一个大得多的数据集来自另一个分布，图片的外观和你真正想要处理的并不一样。但你又不想直接用这10,000张图片，因为这样你的训练集就太小了，使用这20万张图片似乎有帮助。但是，困境在于，这20万张图片并不完全来自你想要的分布，那么你可以怎么做呢？</p>
<p>我们真正关心的是来自手机手机收集的数据，而不是来自网页。方法一，随机分配训练集、验证集、测试集，这样的后果就是花了大量时间在实际不关心的数据分布去优化。</p>
<p>训练集20万张网络，5000手机，验证集和测试集各2500，这样可以保证验证集和测试集更接近实际应用场景，我们试试搭建一个学习系统，让系统在处理手机上传图片分布时效果良好。缺点在于，当然了，现在你的训练集分布和你的开发集、测试集分布并不一样。但事实证明，这样把数据分成训练、开发和测试集，在长期能给你带来更好的系统性能。我们以后会讨论一些特殊的技巧，可以处理 训练集的分布和开发集和测试集分布不一样的情况。</p>
</li>
</ol>
<h2 id="C5-Bias-and-Variance-with-mismatched-data-distributions（数据分布不匹配时，偏差与方差的分析）"><a href="#C5-Bias-and-Variance-with-mismatched-data-distributions（数据分布不匹配时，偏差与方差的分析）" class="headerlink" title="C5: Bias and Variance with mismatched data distributions（数据分布不匹配时，偏差与方差的分析）"></a>C5: Bias and Variance with mismatched data distributions（数据分布不匹配时，偏差与方差的分析）</h2><p>首先算法只看过训练集数据，没看过开发集数据。第二，开发集数据来自不同的分布。很难确认这增加的9%误差率有多少是因为算法没看到开发集中的数据导致的，这么评估呢？到底哪个影响元素更大，</p>
<p>评估方法，训练集的分布挖出，traning-dev set : Same distributation as traning set ,but not used for training.</p>
<p>现在，我们有了<em>训练集</em>错误率、<em>训练-验证集</em>错误率，以及<em>验证集</em>错误率。其中，<em>训练集</em>错误率和<em>训练-验证集</em>错误率的差值反映了方差；而<em>训练-验证集</em>错误率和<em>验证集</em>错误率的差值反映了样本分布不一致的问题，从而说明<strong>模型擅长处理的数据和我们关心的数据来自不同的分布</strong>，我们称之为<strong>数据不匹配（Data Mismatch）</strong>问题。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Analysis-With-Data-Mismatch.png" alt=""></p>
<h3 id="C6-Addressing-data-mismatch（处理数据不匹配问题）"><a href="#C6-Addressing-data-mismatch（处理数据不匹配问题）" class="headerlink" title="C6: Addressing data mismatch（处理数据不匹配问题）"></a>C6: Addressing data mismatch（处理数据不匹配问题）</h3><p>I<img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-55.jpg" alt=""></p>
<p>Data: Artifical data synthesis</p>
<p>所以，总而言之，如果你认为存在数据不匹配问题，我建议你做错误分析，或者看看训练集，或者看看开发集，试图找出，试图了解这两个数据分布到底有什么不同，然后看看是否有办法收集更多看起来像开发集的数据作训练。</p>
<h3 id="C7-Transfer-learning（迁移学习）"><a href="#C7-Transfer-learning（迁移学习）" class="headerlink" title="C7: Transfer learning（迁移学习）"></a>C7: Transfer learning（迁移学习）</h3><p><strong>迁移学习（Tranfer Learning）</strong>是通过将已训练好的神经网络模型的一部分网络结构应用到另一模型，将一个神经网络从某个任务中学到的知识和经验运用到另一个任务中，以显著提高学习任务的性能。</p>
<p>例如，我们将为猫识别器构建的神经网络迁移应用到放射科诊断中。因为猫识别器的神经网络已经学习到了有关图像的结构和性质等方面的知识，所以只要先删除神经网络中原有的输出层，加入新的输出层并随机初始化权重系数（$W[L]$、$b[L]​$），随后用新的训练集进行训练，就完成了以上的迁移学习。</p>
<p>如果新的数据集很小，可能只需要重新训练输出层前的最后一层的权重，即$W[L]$$、b[L]$，并保持其他参数不变；而如果有足够多的数据，可以只保留网络结构，重新训练神经网络中所有层的系数。这时初始权重由之前的模型训练得到，这个过程称为<strong>预训练（Pre-Training）</strong>，之后的权重更新过程称为<strong>微调（Fine-Tuning）</strong>。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-56.jpg" alt=""></p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-57.jpg" alt=""></p>
<p>在下述场合进行迁移学习是有意义的：</p>
<p>两个任务有同样的输入（比如都是图像或者都是音频）；<br>拥有更多数据的任务迁移到数据较少的任务；<br>某一任务的低层次特征（底层神经网络的某些功能）对另一个任务的学习有帮助。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-58.jpg" alt=""></p>
<h3 id="C8-Multi-task-learning-（多任务学习）"><a href="#C8-Multi-task-learning-（多任务学习）" class="headerlink" title="C8; Multi-task learning （多任务学习）"></a>C8; Multi-task learning （多任务学习）</h3><p>For example, autonomous driving example,check cars,stop signs,trfffic lights ,输出也是一个向量，</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-59.jpg" alt=""></p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-60.jpg" alt=""></p>
<h3 id="C9-What-is-end-to-end-deep-learning-什么是端到端的深度学习"><a href="#C9-What-is-end-to-end-deep-learning-什么是端到端的深度学习" class="headerlink" title="C9 :  What is end-to-end deep learning?(什么是端到端的深度学习)"></a>C9 :  What is end-to-end deep learning?(什么是端到端的深度学习)</h3><p>在传统的机器学习分块模型中，每一个模块处理一种输入，然后其输出作为下一个模块的输入，构成一条流水线。而<strong>端到端深度学习（End-to-end Deep Learning）</strong>只用一个单一的神经网络模型来实现所有的功能。它将所有模块混合在一起，只关心输入和输出。</p>
<p><img src="/2019/05/05/Deep Learning ai_Deep Learning Specialization/MyBlog\hexo\source\_posts\Deep Learning ai_Deep Learning Specialization\End-to-end-Deep-Learning.png" alt=""></p>
<h3 id="优点与缺点"><a href="#优点与缺点" class="headerlink" title="优点与缺点"></a>优点与缺点</h3><p>应用端到端学习的优点：</p>
<ul>
<li>只要有足够多的数据，剩下的全部交给一个足够大的神经网络。比起传统的机器学习分块模型，可能更能捕获数据中的任何统计信息，而不需要用人类固有的认知（或者说，成见）来进行分析；</li>
<li>所需手工设计的组件更少，简化设计工作流程；</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要大量的数据；</li>
<li>排除了可能有用的人工设计组件；</li>
</ul>
<p>根据以上分析，决定一个问题是否应用端到端学习的<strong>关键点</strong>是：是否有足够的数据，支持能够直接学习从 x 映射到 y 并且足够复杂的函数？</p>
<h3 id="Whether-to-use-end-to-end-learning-是否要使用端到端的深度学习"><a href="#Whether-to-use-end-to-end-learning-是否要使用端到端的深度学习" class="headerlink" title="Whether to use end-to-end learning?(是否要使用端到端的深度学习?)"></a>Whether to use end-to-end learning?(是否要使用端到端的深度学习?)</h3><p>Pros:</p>
<p>​    let the data speak : x-&gt;y</p>
<p>​    less hand-designing of components needed</p>
<p>Cons:</p>
<p>​    May need large amount of data</p>
<p>​    excludes potentially useful hand-designed components</p>
<p>Key question: Do you hava sufficient data to learn a function of the complexity needed to map x to y?</p>
<p>如果你想使用机器学习或者深度学习来学习某些单独的组件，那么当你应用监督学习时，你应该仔细选择要学习的x到y映射类型，这取决于那些任务你可以收集数据。相比之下，谈论纯端到端深度学习方法是很激动人心的，你输入图像，直接得出方向盘转角，但是就目前能收集到的数据而言，还有我们今天能够用神经网络学习的数据类型而言，这实际上不是最有希望的方法，或者说这个方法并不是团队想出的最好用的方法。而我认为这种纯粹的端到端深度学习方法，其实前景不如这样更复杂的多步方法。因为目前能收集到的数据，还有我们现在训练神经网络的能力是有局限的。</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><font color="green">Summary</font></h1><font color="red">学习如何通过一些手段提高模型的表现，首先了解模型的性能的体现，bias、variance、贝叶斯误差。以及如何一步步的改善性能。具体解决了如下问题，1. 数据的划分 2. 人的表现与机器性能的关系、偏差、方差 3. 训练集和验证集的分布问题，当数据样本对于解决问题不足的时候的解决办法，4. 迁移学习 5. 端到端的学习 6. 多任务学习。6. 在性能不好的情况下，可能需要手动的分析误差，对测试集错误样例做统计等等， </font>
          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/" itemprop="url">aiai_</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-17T09:20:51+08:00">2019-04-17</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                21k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="C2W1"><a href="#C2W1" class="headerlink" title="C2W1"></a>C2W1</h1><h2 id="L01-Train-Dev-Test-Sets"><a href="#L01-Train-Dev-Test-Sets" class="headerlink" title="L01 : Train/Dev/Test Sets"></a>L01 : Train/Dev/Test Sets</h2><h3 id="1-process"><a href="#1-process" class="headerlink" title="1. process"></a>1. process</h3><p>应用型机器学习是一个高度迭代的过程，通常在项目启动时，我们会先有一个初步想法，比如构建一个含有特定层数，隐藏单元数量或数据集个数等等的神经网络，然后编码，并尝试运行这些代码，通过运行和测试得到该神经网络或这些配置信息的运行结果，你可能会根据输出结果重新完善自己的想法，改变策略，或者为了找到更好的神经网络不断迭代更新自己的方案。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_2.png" alt=""></p>
<h3 id="2-data-split"><a href="#2-data-split" class="headerlink" title="2. data split"></a>2. data split</h3><ul>
<li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li>
<li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>或者验证不同算法的有效性。</li>
<li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li>
</ul>
<p>假设这是训练数据，我用一个长方形表示，我们通常会将这些数据划分成几部分，一部分作为训练集，一部分作为简单交叉验证集，有时也称之为验证集，方便起见，我就叫它验证集（<strong>dev set</strong>），其实都是同一个概念，最后一部分则作为测试集。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_3.png" alt=""></p>
<ol>
<li><p>在机器学习发展的小数据量时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p>
<ul>
<li>无验证集的情况：70% / 30%；</li>
<li>有验证集的情况：60% / 20% / 20%；</li>
</ul>
</li>
<li><p>在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>
<p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p>
<p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p>
</li>
</ol>
<ul>
<li>100 万数据量：98% / 1% / 1%；</li>
<li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li>
</ul>
<h3 id="3-建议"><a href="#3-建议" class="headerlink" title="3. 建议"></a>3. 建议</h3><p><strong>验证集要和训练集来自于同一个分布</strong>（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。</p>
<p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。如果只有验证集，没有测试集，我们要做的就是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估。当然，如果你不需要无偏估计，那就再好不过了。</p>
<h2 id="L02-Bias-Variance"><a href="#L02-Bias-Variance" class="headerlink" title="L02 : Bias/Variance"></a>L02 : Bias/Variance</h2><p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p>
<p>泛化误差可分解为偏差、方差与噪声之和：</p>
<ul>
<li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li>
<li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li>
<li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>
</ul>
<p>high bias ,underfitting</p>
<p>high variance, overfitting</p>
<p>just right</p>
<h3 id="1-example"><a href="#1-example" class="headerlink" title="1. example"></a>1. example</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_5.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_6.png" alt=""></p>
<p>Your algorithms ever on the training set and dev set you can try to diganose whether has problems high barriers or high variances or both or neither.</p>
<h2 id="L03-Basic-Recipe-for-Machine-learning"><a href="#L03-Basic-Recipe-for-Machine-learning" class="headerlink" title="L03 Basic Recipe for Machine learning"></a>L03 Basic Recipe for Machine learning</h2><h3 id="1-METHOD"><a href="#1-METHOD" class="headerlink" title="1. METHOD"></a>1. METHOD</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_8.png" alt=""></p>
<p>Training a bigger network almost never hurts. And the main cost of training a neural network that’s too big is just computational time, so long as you’re regularizing.</p>
<p>今天我们讲了如何通过组织机器学习来诊断偏差和方差的基本方法，然后选择解决问题的正确操作，希望大家有所了解和认识。我在课上不止一次提到了正则化，它是一种非常实用的减少方差的方法，正则化时会出现偏差方差权衡问题，偏差可能略有增加，如果网络足够大，增幅通常不会太高，我们下节课再细讲，以便大家更好理解如何实现神经网络的正则化。</p>
<p>第一点，高偏差和高方差是两种不同的情况，我们后续要尝试的方法也可能完全不同</p>
<p>只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。这两步实际要做的工作是：训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。</p>
<h2 id="L04"><a href="#L04" class="headerlink" title="L04"></a>L04</h2><h3 id="1-over-fitting"><a href="#1-over-fitting" class="headerlink" title="1. over fitting"></a>1. over fitting</h3><h3 id="regularization"><a href="#regularization" class="headerlink" title="regularization"></a>regularization</h3><p>L2 regularization</p>
<p>L1 regularizaion: w will be sparse  L1 正则化最后得到 w 向量中将存在大量的 0</p>
<p>为什么只正则化参数w？为什么不再加上参数b 呢？你可以这么做，只是我习惯省略不写，因为通常w是一个高维参数矢量，w已经可以表达高偏差问题，可能w包含有很多参数，我们不可能拟合所有参数，而只是b单个数字，所以w几乎涵盖所有参数，而不是，如果加了参数b，其实也没太大影响，因为b只是众多参数中的一个，所以我通常省略不计，如果你想加上这个参数，完全没问题。</p>
<ol>
<li><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_9.png" alt=""></li>
</ol>
<p>2.<img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_10.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_11.png" alt="">矩阵范数被称作“弗罗贝尼乌斯范数”，用下标标注F</p>
<ol>
<li><p>反向传播时，填上正则化的一项</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_12.png" alt=""></p>
<p>因此L2正则化也被称为“权重衰减”。</p>
</li>
</ol>
<p>to get more training data</p>
<h2 id="L05-Why-Regularization-Reduces-Overfitting"><a href="#L05-Why-Regularization-Reduces-Overfitting" class="headerlink" title="L05 :Why Regularization Reduces Overfitting"></a>L05 :Why Regularization Reduces Overfitting</h2><p>我们添加正则项，它可以避免数据权值矩阵过大，这就是弗罗贝尼乌斯范数，为什么压缩范数，或者弗罗贝尼乌斯范数或者参数可以减少过拟合？我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单.Regularization其实是让函数变得<strong>简化</strong>。</p>
<p>直观上理解就是如果正则化设置得足够大，权重矩阵被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。</p>
<p>总结一下，如果正则化参数变得很大，w参数很小，z也会相对变小，此时忽略的b影响，z会相对变小，实际上，z的取值范围很小，这个激活函数tanh，也就是曲线函数会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p>
<p><strong>L2 regularization的不足</strong>：要通过不断的选用不同的λ进行测试，计算量加大了。</p>
<h2 id="L06-Dropout-Regularization"><a href="#L06-Dropout-Regularization" class="headerlink" title="L06 : Dropout Regularization"></a>L06 : Dropout Regularization</h2><h3 id="1-工作原理"><a href="#1-工作原理" class="headerlink" title="1. 工作原理"></a>1. 工作原理</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_15.png" alt=""></p>
<p>如果上面这幅图存在over fitting。复制这个神经网络，dropout会遍历网络的每一层。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_13.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_14.png" alt=""></p>
<p>我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。</p>
<h3 id="2-inverted-dropout（反向随机失活）"><a href="#2-inverted-dropout（反向随机失活）" class="headerlink" title="2. inverted dropout（反向随机失活）"></a>2. <strong>inverted dropout</strong>（反向随机失活）</h3><p>对第L</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span>    <span class="comment"># 设置神经元保留概率</span></span><br><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>], al.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">al = np.multiply(al, dl)</span><br><span class="line">al /= keep_prob</span><br></pre></td></tr></table></figure>
<p>最后一步<code>al /= keep_prob</code>是因为 a[l]a[l]中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z[l+1]=W[l+1]a[l]+b[l+1]$的期望值，因此除以一个<code>keep_prob</code>。举例解释我们假设第三隐藏层上有50个单元或50个神经元，在一维上是50，我们通过因子分解将它拆分成维的，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10（50×20%=10）个，现在我们看下$z^{[4]}$，，我们的预期是$z^{[4]}=w^{[4]}a^{[3]}$，$a^{[3]}$减少20%，也就是说中有$a^{[3]}$20%的元素被归零，为了不影响的$a^{[4]}$期望值，我们需要用$w^{[4]}a^{[3]}/keep_prob$，它将会修正或弥补我们所需的那20%，$a^{[3]}$的期望值不会变，划线部分就是所谓的<strong>dropout</strong>方法。</p>
<h2 id="L07-Understanding-Dropout"><a href="#L07-Understanding-Dropout" class="headerlink" title="L07 : Understanding Dropout"></a>L07 : Understanding Dropout</h2><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，<strong>dropout</strong>将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施<strong>dropout</strong>的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_16.png" alt=""></p>
<p>计算视觉中的输入量非常大，输入太多像素，以至于没有足够的数据，所以<strong>dropout</strong>在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的选择，但要牢记一点，<strong>dropout</strong>是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然我是不会使用<strong>dropout</strong>的，所以它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合，这就是有些计算机视觉研究人员如此钟情于<strong>dropout</strong>函数的原因。直观上我认为不能概括其它学科。<strong>dropout</strong>将产生收缩权重的平方范数的效果。当然，不同的层，值可以设置成不同，如果你觉得某一层容易过拟合，把值设置小一点。</p>
<p>dropout 的一大<strong>缺点</strong>是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将<code>keep_prob</code>全部设置为 1.0 后运行代码，确保 $J(w,b)​$函数单调递减，再打开 dropout。</p>
<h2 id="L08-Other-Regularization-Methods"><a href="#L08-Other-Regularization-Methods" class="headerlink" title="L08 :  Other Regularization Methods"></a>L08 :  Other Regularization Methods</h2><ul>
<li><p>数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_17.png" alt=""></p>
</li>
<li><p>早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，当训练集误差降低但验证集误差升高，两者开始发生较大偏差时及时停止迭代，并返回具有最小验证集误差的连接权和阈值，以避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。</p>
</li>
</ul>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_18.png" alt=""></p>
<p>但对我来说<strong>early stopping</strong>的主要缺点就是你不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数，因为现在你不再尝试降低代价函数，所以代价函数的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。</p>
<p><strong>Early stopping</strong>的优点是，只运行一次梯度下降，你可以找出的w较小值，中间值和较大值，而无需尝试正则化超级参数的很多值。</p>
<h2 id="L09-：-Normalizing-inputs"><a href="#L09-：-Normalizing-inputs" class="headerlink" title="L09 ： Normalizing inputs"></a>L09 ： Normalizing inputs</h2><ol>
<li><p>零均值</p>
<p>$u=\frac{1}{m}\sum x^{(i)}$,$x-u$</p>
</li>
<li><p>归一化方差；</p>
<p>$\delta^2=\frac{1}{m}(x^{(i)})^2$,每个特征的方差，每个特征数据除以它，就归一化方差了</p>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_19.png" alt=""></p>
<h3 id="why"><a href="#why" class="headerlink" title="why"></a>why</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_20.png" alt=""></p>
<p>在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。</p>
<h2 id="L10-Vanishing-Exploding-Gradients"><a href="#L10-Vanishing-Exploding-Gradients" class="headerlink" title="L10 : Vanishing /Exploding Gradients"></a>L10 : Vanishing /Exploding Gradients</h2><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p>
<p>在深度神经网络中，激活函数将以指数级递减，虽然我只是讨论了激活函数以与相关的指数级数增长或下降，它也适用于与层数相关的导数或梯度函数，也是呈指数级增长或呈指数递减。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_21.png" alt=""></p>
<p>假定 g(z)=z,b[l]=0g(z)=z,b[l]=0，对于目标输出有：</p>
<p>$y^=W[L]W[L−1]…W[2]W[1]X$</p>
<ul>
<li>对于$ W[l]$的值大于 1 的情况，激活函数的值将以指数级递增；</li>
<li>对于 $W[l]$的值小于 1 的情况，激活函数的值将以指数级递减。</li>
</ul>
<p>对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p>
<h2 id="L11-Weight-initialization-in-a-deep-network"><a href="#L11-Weight-initialization-in-a-deep-network" class="headerlink" title="L11 : Weight initialization in a deep network"></a>L11 : Weight initialization in a deep network</h2><p>为了预防值z过大或过小，你可以看到n越大，你希望w越小，因为z是wx+b的和,最合理的方法$w_i=1/n$</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_22.png" alt=""></p>
<p>因此，实际上，你要做的就是设置某层权重矩阵</p>
<p>$w^{[l]}=n p . random. randn (shape) * np.sqrt \left(\frac{1}{n^{[l-1]}}\right)​$</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_23.png" alt=""></p>
<p>当多个节点时，也一样的看，使得这个节点$z^{<a href="i">L</a>}$不要太大，单独看每个节点既可以</p>
<p>relu : var(w(i)) = 2/n or $\frac{2}{n^{[l-1]}*n^{[l]}}$</p>
<p>tanh: var(w(i)) = 1/n</p>
<p>通过设置初始化化权重矩阵，使得不会增长太快或者太慢</p>
<h2 id="L12-：-Numerical-Approximations-of-Gradients"><a href="#L12-：-Numerical-Approximations-of-Gradients" class="headerlink" title="L12 ： Numerical Approximations of Gradients"></a>L12 ： Numerical Approximations of Gradients</h2><p>单边误差</p>
<p>$f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0} \frac{f(\theta+\varepsilon)-(\theta)}{\varepsilon}$</p>
<p>误差$O(\varepsilon)$</p>
<p>双边误差</p>
<p>$f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0}=\frac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2 \varepsilon}$</p>
<p>$O\left(\varepsilon^{2}\right)$</p>
<h2 id="L-13-Gradient-Checking"><a href="#L-13-Gradient-Checking" class="headerlink" title="L 13 Gradient Checking"></a>L 13 Gradient Checking</h2><p>梯度检验帮我们节省了很多时间，也多次帮我发现<strong>backprop</strong>实施过程中的bug，接下来，我们看看如何利用它来调试或检验<strong>backprop</strong>的实施是否正确。</p>
<p>首先要做的就是，把所有参数转换成一个巨大的向量数据，你要做的就是把矩阵w转换成一个向量，把所有矩阵w转换成向量之后，做连接运算，得到一个巨型向量$\theta$，该向量表示为参数$\theta$，代价函数J是所有W和b的函数，现在你得到了一个的代价函数（即）。接着，你得到与和顺序相同的数据，你同样可以把$dW^{[l]}$,和$db^{[l]}$ 转换成一个新的向量，用它们来初始化大向量$d\theta$，它与$\theta$具有相同维度。</p>
<p>梯度的逼近值</p>
<script type="math/tex; mode=display">
d \theta_{\text { approx }}[i]=\frac{J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}+\varepsilon, \ldots\right)-J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}-\varepsilon, \ldots\right)}{2 \varepsilon}</script><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_24.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_25.png" alt=""></p>
<p>现在你已经了解了梯度检验的工作原理，它帮助我在神经网络实施中发现了很多<strong>bug</strong>，希望它对你也有所帮助。</p>
<h1 id="L-14-Gradient-Checking-Implementation-notes"><a href="#L-14-Gradient-Checking-Implementation-notes" class="headerlink" title="L 14 : Gradient Checking Implementation notes"></a>L 14 : Gradient Checking Implementation notes</h1><ol>
<li>不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；太慢了</li>
<li>如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；</li>
<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>
<li>梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week1_25.png" alt=""></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><font color="red">回顾这一周，我们讲了如何配置训练集，验证集和测试集，如何分析偏差和方差，如何处理高偏差或高方差以及高偏差和高方差并存的问题，如何在神经网络中应用不同形式的正则化，如正则化和**dropout**，还有加快神经网络训练速度的技巧，最后是梯度检验。</font>



<h1 id="C2W2-Optimization-Algorithm"><a href="#C2W2-Optimization-Algorithm" class="headerlink" title="C2W2 :Optimization Algorithm"></a>C2W2 :Optimization Algorithm</h1><h2 id="L-01-Mini-Batch-Gradient-Descent"><a href="#L-01-Mini-Batch-Gradient-Descent" class="headerlink" title="L 01 : Mini Batch Gradient Descent"></a>L 01 : Mini Batch Gradient Descent</h2><ol>
<li><p>Vectorization</p>
</li>
<li><p>Mini batch</p>
<p>not entire training set </p>
<p>bady training set i，$x^{\{i\}}$</p>
<p>mini batch training set</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_1.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_2.png" alt=""></p>
</li>
</ol>
<p>mini batch gradient descent</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_3.png" alt=""></p>
<h2 id="L-02-Understanding-Mini-Batch-Gradient-Decent"><a href="#L-02-Understanding-Mini-Batch-Gradient-Decent" class="headerlink" title="L 02 : Understanding Mini-Batch Gradient Decent"></a>L 02 : Understanding Mini-Batch Gradient Decent</h2><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_4.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_6.png" alt=""></p>
<p>左图，随着iterations increased, it should decrease .if it ever goes up on iteration,something is wrong.</p>
<p>右图 : it’s as if on every iteration you’re training on a different training set or really training on a different mini batch. It should trend downwards, but it’s also going to be a little bit noisier.So if you plot J{t}, as you’re training mini batch in descent it may be over multiple epochs,you might expect to see a curve like this.</p>
<h3 id="Choosing-your-mini-batch-size"><a href="#Choosing-your-mini-batch-size" class="headerlink" title="Choosing your mini-batch size"></a>Choosing your mini-batch size</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_5.png" alt=""></p>
<h3 id="1-优缺点"><a href="#1-优缺点" class="headerlink" title="1. 优缺点"></a>1. 优缺点</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_7.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_8.png" alt=""></p>
<p>通过减小学习率，噪声会被改善或有所减小，但随机梯度下降法的一大缺点是，你会失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下，所以实践中最好选择不大不小的<strong>mini-batch</strong>尺寸，实际上学习率达到最快。你会发现两个好处，一方面，你得到了大量向量化，上个视频中我们用过的例子中，如果<strong>mini-batch</strong>大小为1000个样本，你就可以对1000个样本向量化，比你一次性处理多个样本快得多。另一方面，你不需要等待整个训练集被处理完就可以开始进行后续工作，再用一下上个视频的数字，每次训练集允许我们采取5000个梯度下降步骤，所以实际上一些位于中间的<strong>mini-batch</strong>大小效果最好。</p>
<p>使用<strong>batch</strong>梯度下降法时，每次迭代你都需要历遍整个训练集，可以预期每次迭代成本都会下降，所以如果成本函数是迭代次数的一个函数，它应该会随着每次迭代而减少，如果在某次迭代中增加了，那肯定出了问题，也许你的学习率太大。</p>
<p>在随机梯度下降法中，从某一点开始，我们重新选取一个起始点，每次迭代，你只对一个样本进行梯度下降，大部分时候你向着全局最小值靠近，有时候你会远离最小值，因为那个样本恰好给你指的方向不对，因此随机梯度下降法是有很多噪声的，平均来看，它最终会靠近最小值，不过有时候也会方向错误，因为随机梯度下降法永远不会收敛，而是会一直在最小值附近波动，但它并不会在达到最小值并停留在此。</p>
<p>用<strong>mini-batch</strong>梯度下降法，我们从这里开始，一次迭代这样做，两次，三次，四次，它不会总朝向最小值靠近，但它比随机梯度下降要更持续地靠近最小值的方向，它也不一定在很小的范围内收敛或者波动，如果出现这个问题，可以慢慢减少学习率，我们在下个视频会讲到学习率衰减，也就是如何减小学习率。</p>
<p>batch : too long,too time</p>
<p>随机： lose speeding ,噪声大</p>
<p>mini-batch和stochastic都存在噪声问题，且在local optima附近会徘徊。但设置合适大小的mini-batch size，噪声和徘徊问题可接受的范围内。</p>
<p>size=1,又叫随机梯度下降法 stochastic gradient descent </p>
<h3 id="how"><a href="#how" class="headerlink" title="how"></a>how</h3><p>如何选择mini-batch size（这是一个hyperparameter）：</p>
<ul>
<li>小数据量，比如总的样本只有几千个，完全可以直接用batch gradient descent</li>
<li>大数量，mini-batch size倾向于选择2^n个，比如64, 128, 256等</li>
<li><p>mini-batch 与CPU/GPU memory的内存容量。</p>
<p>In practice of course the mini batch size is another hyper parameter that you might do a quick search over to try to figure out which one is most sufficient of reducing the cost function j. 按照上面的方法</p>
</li>
</ul>
<h2 id="L-03-Exponentially-Weighted-Averages"><a href="#L-03-Exponentially-Weighted-Averages" class="headerlink" title="L 03: Exponentially Weighted Averages"></a>L 03: Exponentially Weighted Averages</h2><p>In order to understand those algorithms, you need to be able they use something called exponentially weighted averages. Also called exponentially weighted moving averages in statistics.</p>
<h3 id="1-指数加权平均数（Exponentially-weighted-averages）"><a href="#1-指数加权平均数（Exponentially-weighted-averages）" class="headerlink" title="1. 指数加权平均数（Exponentially weighted averages）"></a>1. 指数加权平均数（Exponentially weighted averages）</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_9.png" alt=""></p>
<p>$\theta _i$表示每一日的温度值，蓝色的点，$v_t$表示加权平均后的,红色</p>
<p>权平均方法是：每天的温度值加权值$vt$设置为前一天的温度加权值$vt−1$和当天的温度实际值$θt$做加权平均：</p>
<script type="math/tex; mode=display">
v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}</script><p>由于以后我们要考虑的原因，在计算时可视$v_T$大概是$\frac{1}{(1-\beta)}$的每日温度的加权平均，</p>
<p>如果是$\beta$=0.9，这是十天的平均值，红色</p>
<p>如果$\beta$=0.98,是50天的结果，绿色</p>
<p>如果$beta$=0.5,是2day的结果，黄色</p>
<p>由于仅平均了两天的温度，平均的数据太少，所以得到的曲线有更多的噪声，有可能出现异常值，但是这个曲线能够更快适应温度变化。</p>
<p>当 $\beta$较大时，指数加权平均值适应地更缓慢一些。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_10.png" alt=""></p>
<p>$</p>
<h2 id="L-04-Understanding-Exponentially-Weighted-Averages"><a href="#L-04-Understanding-Exponentially-Weighted-Averages" class="headerlink" title="L 04 : Understanding Exponentially Weighted Averages"></a>L 04 : Understanding Exponentially Weighted Averages</h2><p><strong>假如β=0.9，每个v的计算如下：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} v_{100} &=0.9 v_{99}+0.1 \theta_{100} \\ v_{99} &=0.9 v_{98}+0.1 \theta_{99} \\ v_{98} &=0.9 v_{97}+0.1 \theta_{98} \end{aligned}</script><p>递推可得：</p>
<script type="math/tex; mode=display">
v_{100}=0.1 \theta_{100}+0.1 * 0.9 \theta_{99}+0.1 *(0.9)^{2} \theta_{98}+\ldots</script><p>指数的衰减规律</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_11.png" alt=""></p>
<p>一般的</p>
<script type="math/tex; mode=display">
v_{t}=\sum_{i=1}^{t}(1-\beta) \beta^{t-i} \theta_{i}</script><p>无穷级数求和：</p>
<script type="math/tex; mode=display">
\sum_{t=1}^{n}(1-\beta) \beta^{t}=1</script><p>因此可以近似的认为所有项的系数之和正好为100%。</p>
<p>即，$vt$是对t日之前<strong>所有的实际温度的加权平均</strong>,权重是指数递减的。</p>
<p>十天后，曲线高度下降到了1/3,赋予权重$\beta^{t-i}$</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_12.png" alt=""></p>
<script type="math/tex; mode=display">
0.9^{10}~=0.35~=1/e</script><p>一般认为，$v_t$近似前$\frac{1}{1-\beta}$的加权平均值</p>
<h2 id="L05-Bias-correction-in-exponentially-weighted-averages"><a href="#L05-Bias-correction-in-exponentially-weighted-averages" class="headerlink" title="L05 : Bias correction in exponentially weighted averages"></a>L05 : Bias correction in exponentially weighted averages</h2><p>指数加权平均的偏差修正</p>
<p>由于计算$v1$的时候，并没有历史值做加权，这个时候令其前一个加权值$v0=0$，则会导致$v_1$远小于$\theta_1$,依次类推，<strong>在靠近前面的值会出现显著的小于实际值的情况</strong></p>
<p>因此做一个修正</p>
<script type="math/tex; mode=display">
v_{t}=\frac{\beta v_{t-1}+(1-\beta) \theta_{t}}{1-\beta^{t}}</script><p>你会发现随着$\beta^t$增加，接近于0，所以当t很大的时候，偏差修正几乎没有作用，因此当t较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_13.png" alt=""></p>
<p>因为在Machine Learning中看重的是很多次迭代后的结果，初期的偏差影响并不大。</p>
<h2 id="L-06-Gradient-Descent-With-Momentum"><a href="#L-06-Gradient-Descent-With-Momentum" class="headerlink" title="L 06 : Gradient Descent With Momentum"></a>L 06 : Gradient Descent With Momentum</h2><p>动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_14.png" alt=""></p>
<p>当慢慢下降到最小值，上下波动的梯度下降法的速度减缓，无法使用更大的学习率，</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_15.png" alt=""></p>
<p>在纵轴上，希望学校慢一点，不需要摆动，横着上，加快学校，基于此就有了Gradient descent with momentum。</p>
<script type="math/tex; mode=display">
\begin{array}{c}{v_{d W} :=\beta v_{d W}+(1-\beta) d W} \\ {v_{d b} :=\beta v_{d b}+(1-\beta) d b} \\ {w=w-\alpha v_{d W}} \\ {b=b-\alpha v_{d b}}\end{array}</script><p>这样，可以让gradient更平滑</p>
<ul>
<li>对于上图垂直方向，原来是会上下震荡，但引入了exponentially weighted average，相当于对前面的震荡进行了平均，<strong>结果就是上下震荡互相抵消了</strong>。而水平方向都是向右没有震荡，因此平均后还是向右。最终导致呈现上图红色的下降路线。</li>
<li><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_18.png" alt=""></li>
</ul>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_16.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_17.png" alt=""></p>
<h2 id="L-07-RMSprop"><a href="#L-07-RMSprop" class="headerlink" title="L 07 : RMSprop"></a>L 07 : RMSprop</h2><p>RMSprop (Root Mean Square Propagation，均方根传递)，<strong>与momentum一样，也是降低梯度的抖动</strong>。<strong>而是平抑不同大小梯度的更新速率。实际上 作用在α上的</strong></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_19.png" alt=""></p>
<p>回忆一下我们之前的例子，如果你执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅度摆动，为了分析这个例子，假设b纵轴代表参数，横轴代表参数W，可能有w1，或者w2其它重要的参数，为了便于理解，被称为b和w。</p>
<p>我们希望学习速度快，而在垂直方向，也就是例子中的方向，我们希望减缓纵轴上的摆动，所以有了$S_{d W} $和$ S_{d b}$，我们希望$S_{d W} $会相对较小，所以我们要除以一个较小的数，而希望$ S_{d b}$又较大，所以这里我们要除以较大的数字，这样就可以减缓纵轴上的变化。</p>
<p>这些微分，垂直方向的要比水平方向的大得多，所以斜率在方向特别大，所以这些微分中，db较大，dw较小，因为函数的倾斜程度，在纵轴上，也就是b方向上要大于在横轴上，也就是方向上W。db的平方较大，所以$Sdb$也会较大，而相比之下，dw会小一些，亦或dw平方会小一些，因此$Sdw$会小一些，结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。</p>
<p><strong>RMSprop</strong>的影响就是你的更新最后会变成这样（绿色线），纵轴方向上摆动较小，而横轴方向继续推进。还有个影响就是，你可以用一个更大学习率，然后加快学习，而无须在纵轴上垂直方向偏离。</p>
<p>实际中dw是一个高维度的参数向量，db也是一个高维度参数向量，但是你的直觉是，在你要消除摆动的维度中，最终你要计算一个更大的和值，这个平方和微分的加权平均值，所以你最后去掉了那些有摆动的方向。所以这就是<strong>RMSprop</strong>，全称是均方根，因为你将微分进行平方，然后最后使用平方根。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_20.png" alt=""></p>
<p>解释平方：</p>
<p>垂直方向，比较陡，梯度比较大，但我们又希望它下降的慢。因此对梯度除以一个较大的值，所以用梯度的平方的平均来表示。让不同的参数拥有不同的learning rate。</p>
<p><strong>从某种角度看，RMSprop会根据当前的梯度自动调整参数的learning rate，梯度大降低learning rate，梯度小的时候提高learning rate，从而一方面避免了震荡，另一方面避免在平坦的地方徘徊太久。</strong></p>
<p>为了避免出现分母为0</p>
<script type="math/tex; mode=display">
\begin{array}{c}{s_{d w}=\beta s_{d w}+(1-\beta)(d w)^{2}} \\ {s_{d b}=\beta s_{d b}+(1-\beta)(d b)^{2}} \\ {w :=w-\alpha \frac{d w}{\sqrt{s_{d w}+\varepsilon}}} \\ {b :=b-\alpha \frac{d b}{\sqrt{s_{d b}+\varepsilon}}}\end{array}</script><p>$\varepsilon$取$10^{-8}$不错的选择.</p>
<p>补充：</p>
<p>RMSProp算法对梯度计算了<strong>微分平方加权平均数</strong>。这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快</p>
<h2 id="L-08-Adam-optimization-algorithm"><a href="#L-08-Adam-optimization-algorithm" class="headerlink" title="L 08 Adam optimization algorithm"></a>L 08 Adam optimization algorithm</h2><p>Adam（Adaptive Moment Estimation，自适应矩估计）就是momentum和RMSprop的结合。momentum负责平滑梯度，而RMSprop负责调解learning rate。</p>
<h3 id="1-Adam"><a href="#1-Adam" class="headerlink" title="1. Adam"></a>1. Adam</h3><p>a. 引入的变量有：</p>
<ul>
<li>$v$ : 计算同momentum算法，将梯度进行指数加权平均</li>
<li>$s$: 计算同RMSprop，将梯度的平方进行指数加权平均</li>
<li>$β1$ : 计算vv的加权参数</li>
<li>$β2$ : 计算ss的加权参数</li>
</ul>
<p>b. 在迭代前，初始化参数v和s</p>
<script type="math/tex; mode=display">
v_{d W}=0, s_{d W}=0, v_{d b}=0, s_{d b}=0</script><p>c. 对第t次梯度下降的迭代 a. 首先计算dw和db的v和s</p>
<script type="math/tex; mode=display">
\begin{array}{c}{v_{d W}=\beta_{1} v_{d W}+\left(1-\beta_{1}\right) d W} \\ {s_{d W}=\beta_{2} s_{d W}+\left(1-\beta_{2}\right)(d W)^{2}} \\ {v_{d b}=\beta_{1} v_{d b}+\left(1-\beta_{1}\right) d b} \\ {s_{d b}=\beta_{2} s_{d b}+\left(1-\beta_{2}\right)(d b)^{2}}\end{array}</script><p>d. 修正</p>
<script type="math/tex; mode=display">
v_{d W}^{\text {corrected}}=\frac{v_{d W}}{1-\left(\beta_{1}\right)^{t}}\\
\begin{aligned} s_{d W}^{\text {corrected}} &=\frac{s_{d W}}{1-\left(\beta_{2}\right)^{t}} \\ v_{d b}^{\text {corrected}} &=\frac{v_{d b}}{1-\left(\beta_{1}\right)^{t}} \\ s_{d b}^{\text {corrected}} &=\frac{s_{d b}}{1-\left(\beta_{2}\right)^{t}} \end{aligned}</script><p>e. 最后更新参数W和b</p>
<script type="math/tex; mode=display">
W=W-\alpha \frac{v_{d W}^{\text {corrected}}}{\sqrt{s_{d W}^{\text { corrected }}+\varepsilon}}\\
b=b-\alpha \frac{v_{d b}^{\text {corrected}}}{\sqrt{s_{d b}^{\text { corrected }}+\varepsilon}}</script><p>超参的选择：</p>
<ul>
<li>α：需要调优</li>
<li>β1: 通常选择为0.9</li>
<li>β2: 通常选择为0.999</li>
<li>ε: 一般不需要调优，选择一个小数，比如10−8</li>
</ul>
<p>你可以尝试一系列值α，然后看哪个有效</p>
<h2 id="L09-Learning-Rate-Decay"><a href="#L09-Learning-Rate-Decay" class="headerlink" title="L09 : Learning Rate Decay"></a>L09 : Learning Rate Decay</h2><ol>
<li><p>why</p>
<p>为什么要做learning rate decay？ 较大的learning rate虽然在算法开始阶段会加快收敛速度，但在收敛接近到优化点的时候，算法会在优化点附近震荡，如下图：</p>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_22.png" alt=""></p>
<p>2.如何做learning rate decay？ <strong>思路很简单，就是引入一个函数，让α随着迭代（比如min-batch的epoch）递减</strong>。为此可以采用的decay函数有：</p>
<p>倒数：</p>
<script type="math/tex; mode=display">
\alpha :=\frac{1}{1+\text { decay rate * epoch num}} \alpha</script><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_23.png" alt=""></p>
<h2 id="L-10-The-Problem-of-local-Optima"><a href="#L-10-The-Problem-of-local-Optima" class="headerlink" title="L 10: The Problem of local Optima"></a>L 10: The Problem of local Optima</h2><p>事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p>
<p>但是一个具有高维度空间的函数，如果梯度为0，那么在每个方向，它可能是凸函数，也可能是凹函数。如果你在2万维空间中，那么想要得到局部最优，所有的2万个方向都需要是这样，但发生的机率也许很小，也许是$2^{-20000}$，因此更有可能遇到有些方向的曲线会这样向上弯曲，另一些方向曲线向下弯，而不是所有的都向上弯曲，因此在高维度空间，你更可能碰到鞍点。所有，担心收敛到local optima，真是人们想多了，实际上并没有想象的那么多local optima。在高维空间，几乎不太可能被困在一个local optima，这是一个好消息。</p>
<p>因此，在高维空间遇到的问题是高原问题（Problem of plateaus）</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_27.png" alt=""></p>
<p>Adam算法可以加速学习</p>
<h1 id="W3-Hyperparameter-tuning"><a href="#W3-Hyperparameter-tuning" class="headerlink" title="W3 Hyperparameter tuning"></a>W3 Hyperparameter tuning</h1><h2 id="L01-Tuning-process"><a href="#L01-Tuning-process" class="headerlink" title="L01 Tuning process"></a>L01 Tuning process</h2><ol>
<li>到目前为止，我们接触到的hyperparameter有：</li>
<li>learning rate: αα</li>
<li>momentum 参数: ββ</li>
<li>Adam参数: β1β1和 β2β2以及εε</li>
<li>神经网络层数: L</li>
<li>神经网络隐藏层neuron数：n[l]n[l]</li>
<li>learning rate decay参数</li>
<li>min-batch size</li>
<li>这些hyperparameter重要性排序：</li>
<li>最重要的： learning rate: αα</li>
<li>比较重要的： momentum 参数: ββ 神经网络层数: L 神经网络隐藏层neuron数：n[l]n[l]</li>
<li>次重要的： 神经网络隐藏层neuron数 learning rate decay参数</li>
<li>基本不需调整的 β1β1和 β2β2以及ε</li>
</ol>
<h4 id="1-Try-random-values-Don’t-use-a-grid"><a href="#1-Try-random-values-Don’t-use-a-grid" class="headerlink" title="1. Try random values : Don’t use a grid"></a>1. Try random values : Don’t use a grid</h4><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_28.png" alt=""></p>
<p>why:</p>
<p>举个例子，假设超参数1是（学习速率），取一个极端的例子，假设超参数2是<strong>Adam</strong>算法中，分母中的$\varepsilon$。在这种情况下，a的取值很重要，而$\varepsilon$取值则无关紧要。如果你在网格中取点，接着，你试验了a的5个取值，那你会发现，无论$\varepsilon$取何值，结果基本上都是一样的。所以，你知道共有25种模型，但进行试验的值只有5个，我认为这是很重要的。</p>
<p>对比而言，如果你随机取值，你会试验25个独立的a,$\varepsilon$，似乎你更有可能发现效果做好的那个。</p>
<h3 id="2-由粗糙到精细的策略"><a href="#2-由粗糙到精细的策略" class="headerlink" title="2. 由粗糙到精细的策略"></a>2. 由粗糙到精细的策略</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_29.png" alt=""></p>
<h2 id="L-02-Using-an-Appropriate-Scale-to-pick-hyperparameters"><a href="#L-02-Using-an-Appropriate-Scale-to-pick-hyperparameters" class="headerlink" title="L 02: Using an Appropriate Scale to pick hyperparameters"></a>L 02: Using an Appropriate Scale to pick hyperparameters</h2><p>$a$取值0.0001,1,如果你画一条从0.0001到1的数轴，沿其随机均匀取值，那90%的数值将会落在0.1到1之间，结果就是，在0.1到1之间，应用了90%的资源，而在0.0001到0.1之间，只有10%的搜索资源，这看上去不太对。</p>
<p>同时在范围内搜索，也不是均匀分布（uniformly random）的，通常有这个参数的scale，<strong>比如对数scale</strong>。</p>
<p>反而，用对数标尺搜索超参数的方式会更合理，因此这里不使用线性轴，分别依次取0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在0.0001到0.001之间，就会有更多的搜索资源可用，还有在0.001到0.01之间等等。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_32.png" alt=""></p>
<h2 id="L-03-Hyperparameter-tuning-i-practice"><a href="#L-03-Hyperparameter-tuning-i-practice" class="headerlink" title="L 03 : Hyperparameter tuning i practice"></a>L 03 : Hyperparameter tuning i practice</h2><ol>
<li>不同的算法和场景，对超参的scale敏感性可能不一样.</li>
<li>根据计算资源和数据量，可以采取两种策略来调参<ul>
<li>Panda（熊猫策略）：对一个模型先后修改参数，查看其表现，最终选择最好的参数。就像熊猫一样，一次只抚养一个后代。</li>
<li>Caviar（鱼子酱策略）：计算资源足够，可以同时运行很多模型实例，采用不同的参数，然后最终选择一个好的。类似鱼类，一次下很多卵，自动竞争成活。 </li>
</ul>
</li>
</ol>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_33.png" alt=""></p>
<h3 id="L-04-Normalizing-Activations-in-a-Network"><a href="#L-04-Normalizing-Activations-in-a-Network" class="headerlink" title="L 04: Normalizing Activations in a Network"></a>L 04: Normalizing Activations in a Network</h3><h4 id="1-Implementing-Batch-Normalizing"><a href="#1-Implementing-Batch-Normalizing" class="headerlink" title="1. Implementing Batch Normalizing"></a>1. Implementing Batch Normalizing</h4><p><strong>Batch</strong>归一化,<strong>Batch</strong>归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会是你的训练更加容易，甚至是深层网络。</p>
<p>可以normalize $a^{[l]},z^{[l]}$,选择$z^{[L]}$</p>
<p><strong>设置 γ 和 β 的原因</strong>是，如果各隐藏层的输入均值在靠近 0 的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。因此，需要用 γ 和 β 对标准化后的结果做进一步处理。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_34.png" alt=""></p>
<p>需要注意的是，β和γ不是超参，而是梯度下降需学习的参数。</p>
<h2 id="L-05-Fitting-Batch-Norm-Into-Neural-Networks"><a href="#L-05-Fitting-Batch-Norm-Into-Neural-Networks" class="headerlink" title="L 05 : Fitting Batch Norm Into Neural Networks"></a>L 05 : Fitting Batch Norm Into Neural Networks</h2><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_35.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_36.png" alt=""></p>
<p>注意</p>
<p>先前我说过每层的参数是$w^{[l]}$和$b^{[l]}$，还有$\beta^{[l]}$和$b^{[l]}$，请注意计算的方式如下，$z^{[l]}=w^{[l]} a^{[l-1]}+b^{[l]}$，但<strong>Batch</strong>归一化做的是，它要看这个<strong>mini-batch</strong>，先将$z^{[l]}$归一化，结果为均值0和标准方差，再由$\beta$和b重缩放，但这意味着，无论$b^{[l]}$的值是多少，都是要被减去的，因为在<strong>Batch</strong>归一化的过程中，你要计算的$z^{[l]}$均值，再减去平均值，在此例中的<strong>mini-batch</strong>中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消.</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_37.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_38.png" alt=""></p>
<p>最后，请记住的维$z^{[l]}$，因为在这个例子中，维数会是$\left(n^{[l]}, 1\right)$，的尺寸为，如果是l层隐藏单元的数量，那$ \beta^{[l]}$和$ \gamma^{[l]}$的维度也是$\left(n^{[l]}, 1\right)$，因为这是你隐藏层的数量，你有隐藏单元，<strong>所以$\gamma^{[l]}$和</strong>$  \beta^{[l]}​$用来将每个隐藏层的均值和方差缩放为网络想要的值。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_39.png" alt=""></p>
<h3 id="L-06-Why-Doest-Batch-Norm-Work"><a href="#L-06-Why-Doest-Batch-Norm-Work" class="headerlink" title="L 06 Why Doest Batch Norm Work?"></a>L 06 Why Doest Batch Norm Work?</h3><ol>
<li>首先，起到了normalization的作用，同对输入数据X的normalization作用类似。</li>
</ol>
<ol>
<li><p>让每一层的学习，<strong>一定程度解耦了前层参数和后层参数，让各层更加独立的学习</strong>。无论前一层如何变，本层输入的数据总是保持稳定的均值和方差。（主要原因）</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_40.png" alt=""></p>
<p>所以使你数据改变分布的这个想法，有个有点怪的名字“<strong>Covariate shift</strong>”，想法是这样的，如果你已经学习了到 的映射，如果 的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由 到 映射保持不变，正如此例中，因为真实函数是此图片是否是一只猫，训练你的函数的需要变得更加迫切，如果真实函数也改变，情况就更糟了。</p>
<p>关于第二点，如果实际应用样本和训练样本的数据分布不同（例如，橘猫图片和黑猫图片），我们称发生了“<strong>Covariate Shift</strong>”。这种情况下，一般要对模型进行重新训练。Batch Normalization 的作用就是减小 Covariate Shift 所带来的影响，让模型变得更加健壮，鲁棒性（Robustness）更强。</p>
<p>即使输入的值改变了，由于 Batch Normalization 的作用，使得均值和方差保持不变（由 γ 和 β 决定），限制了在前层的参数更新对数值分布的影响程度，因此后层的学习变得更容易一些。Batch Normalization 减少了各层 W 和 b 之间的耦合性，让各层更加独立，实现自我训练学习的效果。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_41.png" alt=""></p>
<p>另外，Batch Normalization 也<strong>起到微弱的正则化</strong>（regularization）效果。因为在每个 mini-batch 而非整个数据集上计算均值和方差，只由这一小部分数据估计得出的均值和方差会有一些噪声，因此最终计算出的 z~(i)z~(i)也有一定噪声。类似于 dropout，这种噪声会使得神经元不会再特别依赖于任何一个输入特征。</p>
<p>因为 Batch Normalization 只有微弱的正则化效果，因此可以和 dropout 一起使用，以获得更强大的正则化效果。通过应用更大的 mini-batch 大小，可以减少噪声，从而减少这种正则化效果。</p>
<p>最后，不要将 Batch Normalization 作为正则化的手段，而是当作加速学习的方式。正则化只是一种非期望的副作用，Batch Normalization 解决的还是反向传播过程中的梯度问题（梯度消失和爆炸）。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_42.png" alt=""></p>
</li>
</ol>
<h3 id="L-07-Batch-Norm-At-Test-Time"><a href="#L-07-Batch-Norm-At-Test-Time" class="headerlink" title="L 07 : Batch Norm At Test Time"></a>L 07 : Batch Norm At Test Time</h3><p>问题：BN算法在训练时是一个批次的数据训练，能算出每一层Z的均值和方差；而在测试时，输入的则是单个数据，<strong>单条数据没法做均值和方差的计算</strong>，怎么在测试期输入均值和方差呢?</p>
<p>实际应用中一般不使用这种方法，而是使用之前学习过的指数加权平均的方法来预测测试过程单个样本的 μ 和 $σ^2$</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_43.png" alt=""></p>
<p>计算$z_{\text { norm }}^{(\hat{2})}$，用$\mu$ 和$ \sigma^{2}$的指数加权平均，用你手头的最新数值来做调整，然后你可以用左边我们刚算出来的和你在神经网络训练过程中得到的$\beta$和$\sigma$参数来计算你那个测试样本的z值。</p>
<h3 id="L-08-Softmax-Regression"><a href="#L-08-Softmax-Regression" class="headerlink" title="L 08 : Softmax Regression"></a>L 08 : Softmax Regression</h3><h4 id="1-Multi-class-classification"><a href="#1-Multi-class-classification" class="headerlink" title="1. [Multi-class classification]"></a>1. [Multi-class classification]</h4><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_44.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_45.png" alt=""></p>
<p>最后一层是概率，之和为1，要用到<strong>Softmax</strong>层，<strong>Softmax</strong>激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量，最后输出一个向量。</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_46.png" alt=""></p>
<h4 id="2-Softmax-example"><a href="#2-Softmax-example" class="headerlink" title="2. Softmax example"></a>2. Softmax example</h4><p>没有隐藏层的softmax,代表一些决策边界</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_47.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_48.png" alt=""></p>
<h3 id="L-09-Training-SoftMax-classifier"><a href="#L-09-Training-SoftMax-classifier" class="headerlink" title="L 09 Training SoftMax classifier"></a>L 09 Training SoftMax classifier</h3><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_49.png" alt=""></p>
<p><strong>Softmax</strong>这个名称的来源是与所谓<strong>hardmax</strong>对比,<strong>Softmax</strong>回归或<strong>Softmax</strong>激活函数将<strong>logistic</strong>激活函数推广到类，而不仅仅是两类，结果就是如果C=2，那么C=2的<strong>Softmax</strong>实际上变回了<strong>logistic</strong>回归，</p>
<h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_50.png" alt=""></p>
<script type="math/tex; mode=display">
J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_51.png" alt=""></p>
<h4 id="3-Gradient-descent-with-softmax"><a href="#3-Gradient-descent-with-softmax" class="headerlink" title="3. Gradient descent with softmax"></a>3. Gradient descent with softmax</h4><p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_52.png" alt=""></p>
<p>最后一层求导，softmax激活函数</p>
<script type="math/tex; mode=display">
J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><h2 id="L11-TensorFlow"><a href="#L11-TensorFlow" class="headerlink" title="L11 TensorFlow"></a>L11 TensorFlow</h2><h4 id="1-基本流程"><a href="#1-基本流程" class="headerlink" title="1. 基本流程"></a>1. 基本流程</h4><p>使用tensorflow，只要告诉tensorflow forward prop，它自己就会做backprop，因此不用自己实现backprop</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_53.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#导入TensorFlow</span></span><br><span class="line">​</span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype = tf.float32)</span><br><span class="line"><span class="comment">#接下来，让我们定义参数w，在TensorFlow中，你要用tf.Variable()来定义参数</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#然后我们定义损失函数：</span></span><br><span class="line">​</span><br><span class="line">cost = tf.add(tf.add(w**<span class="number">2</span>,tf.multiply(- <span class="number">10.</span>,w)),<span class="number">25</span>)</span><br><span class="line"><span class="comment">#然后我们定义损失函数J</span></span><br><span class="line">然后我们再写：</span><br><span class="line">​</span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line"><span class="comment">#(让我们用0.01的学习率，目标是最小化损失)。</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#最后下面的几行是惯用表达式:</span></span><br><span class="line">​</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">​</span><br><span class="line">session = tf.Session()<span class="comment">#这样就开启了一个TensorFlow session。</span></span><br><span class="line">​</span><br><span class="line">session.run(init)<span class="comment">#来初始化全局变量。</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#然后让TensorFlow评估一个变量，我们要用到:</span></span><br><span class="line">​</span><br><span class="line">session.run(w)</span><br><span class="line">​</span><br><span class="line"><span class="comment">#上面的这一行将w初始化为0，并定义损失函数，我们定义train为学习算法，它用梯度下降法优化器使损失函数最小化，但实际上我们还没有运行学习算法，所以#上面的这一行将w初始化为0，并定义损失函数，我们定义train为学习算法，它用梯度下降法优化器使损失函数最小化，但实际上我们还没有运行学习算法，所以session.run(w)评估了w，让我：：</span></span><br><span class="line">​</span><br><span class="line">print(session.run(w))</span><br><span class="line">​</span><br><span class="line">所以如果我们运行这个，它评估等于<span class="number">0</span>，因为我们什么都还没运行。</span><br><span class="line"></span><br><span class="line"><span class="comment">#现在让我们输入：</span></span><br><span class="line">​</span><br><span class="line">$session.run(train)，它所做的就是运行一步梯度下降法。</span><br><span class="line"><span class="comment">#接下来在运行了一步梯度下降法后，让我们评估一下w的值，再print：</span></span><br><span class="line">​</span><br><span class="line">print(session.run(w))</span><br><span class="line"><span class="comment">#在一步梯度下降法之后，w现在是0.1。</span></span><br></pre></td></tr></table></figure>
<h4 id="2-如何用训练数据"><a href="#2-如何用训练数据" class="headerlink" title="2. 如何用训练数据"></a>2. 如何用训练数据</h4><p>placeholder 在实际的训练过程中，要用不同的样本反复放到一个待优化函数中的，这个时候就可以用tensorflow的placeholder实现,在run的时候，对应给出<code>feed_dict</code>，表名占位符x的实际值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># 导入Tensorflow</span></span><br><span class="line"></span><br><span class="line">coefficient = np.array([[<span class="number">2.</span>],[<span class="number">-10.</span>],[<span class="number">25.</span>]])</span><br><span class="line"></span><br><span class="line">w = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="number">3</span>,<span class="number">1</span>]) <span class="comment"># 3x1大小的placeholder</span></span><br><span class="line">cost = w**x[<span class="number">0</span>][<span class="number">0</span>] - x[<span class="number">1</span>][<span class="number">0</span>]*w + x[<span class="number">2</span>][<span class="number">0</span>] <span class="comment"># 要优化的cost function（即forward prop的形式）</span></span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost) </span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line">print(session.run(w))</span><br><span class="line"></span><br><span class="line">session.run(train, feed_dict=&#123;x:coefficient&#125;) <span class="comment"># x占位符替换为coefficient</span></span><br><span class="line">print(session.run(w))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    session.run(train, feed_dict=&#123;x:coefficient&#125;) <span class="comment"># # x占位符替换为coefficient</span></span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure>
<h4 id="3-计算流"><a href="#3-计算流" class="headerlink" title="3. 计算流"></a>3. 计算流</h4><p><strong>TensorFlow</strong>程序的核心是计算损失函数，然后<strong>TensorFlow</strong>自动计算出导数，以及如何最小化损失，因此这个等式或者这行代码所做的就是让<strong>TensorFlow</strong>建立计算图，</p>
<p>with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。建立计算流的过程，前向传播的过程，operation</p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_56.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_54.png" alt=""></p>
<p><img src="/2019/04/17/ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization/L2_week2_55.png" alt=""></p>
<h1 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h1><font color="read">how to systematically organize the hyper parameter search process and  batch normalization and framework </font>

<p><a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2" target="_blank" rel="noopener">http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2</a></p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3" target="_blank" rel="noopener">http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3</a></p>
<p><a href="http://dl-notes.imshuai.com/#/c2w1?id=_4-heros-of-deep-learning-yoshua-bengio-interview" target="_blank" rel="noopener">http://dl-notes.imshuai.com/#/c2w1?id=_4-heros-of-deep-learning-yoshua-bengio-interview</a></p>
<p><a href="https://www.youtube.com/watch?v=4Ct3Yujl1dk&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=14" target="_blank" rel="noopener">https://www.youtube.com/watch?v=4Ct3Yujl1dk&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=14</a></p>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/16/彩铅DailyLifeStyle/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/16/彩铅DailyLifeStyle/" itemprop="url">彩铅DailyLifeStyle</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-16T20:20:46+08:00">2019-04-16</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/娱乐生活/" itemprop="url" rel="index"><span itemprop="name">娱乐生活</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/16/彩铅DailyLifeStyle/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/16/彩铅DailyLifeStyle/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                287字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Day-one"><a href="#Day-one" class="headerlink" title="Day one"></a>Day one</h1><h2 id="1-工具简单介绍"><a href="#1-工具简单介绍" class="headerlink" title="1. 工具简单介绍"></a>1. 工具简单介绍</h2><ol>
<li>彩铅<br>水溶性，油性</li>
<li>彩铅纸</li>
<li>铅笔<ol>
<li>B<br>2B&lt;4B黑的程度</li>
<li>H<br>4H&lt;8H软度</li>
</ol>
</li>
<li>橡皮<ol>
<li>软橡皮</li>
<li>硬橡皮</li>
<li>电动橡皮擦</li>
</ol>
</li>
<li>铅笔刀<ol>
<li>可跳档类型</li>
</ol>
</li>
<li>勾线笔<ol>
<li>针管笔 （樱花）</li>
</ol>
</li>
<li>笔套</li>
<li>高光笔<ol>
<li>可以用修正液替换（三棱)</li>
</ol>
</li>
<li>纸擦笔<ol>
<li>玛丽</li>
</ol>
</li>
<li>刷子</li>
<li>画板<ol>
<li>速写板<h2 id="2-颜色"><a href="#2-颜色" class="headerlink" title="2. 颜色"></a>2. 颜色</h2>三原色： 红 黄 蓝</li>
</ol>
</li>
<li>色相<br>颜色</li>
<li>饱和度<ol>
<li>鲜艳程度</li>
</ol>
</li>
<li>明度<ol>
<li>明暗程度</li>
</ol>
</li>
<li>邻近色</li>
<li>对比色<ol>
<li>红-绿</li>
<li>蓝-橙</li>
<li>紫-黄</li>
</ol>
</li>
<li>暖色和冷色</li>
</ol>
<h2 id="3-排线"><a href="#3-排线" class="headerlink" title="3. 排线"></a>3. 排线</h2><ol>
<li>一个方向<br>往同一个方向排，无连接</li>
<li>来回<br>相连接，一条线</li>
<li>不同方向排列<br>注意：力度和间距<h2 id="4-平涂"><a href="#4-平涂" class="headerlink" title="4. 平涂"></a>4. 平涂</h2>力度一致<h2 id="5-渐变"><a href="#5-渐变" class="headerlink" title="5. 渐变"></a>5. 渐变</h2>力度不一致</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/13/machine learning test/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/13/machine learning test/" itemprop="url">test</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-13T19:59:23+08:00">2019-04-13</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/13/machine learning test/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/13/machine learning test/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                3.1k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Logistics-Regression"><a href="#Logistics-Regression" class="headerlink" title="Logistics Regression"></a>Logistics Regression</h1><p>   如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！<strong>逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</strong></p>
<p>​     这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。</p>
<h2 id="逻辑回归的基本假设"><a href="#逻辑回归的基本假设" class="headerlink" title="逻辑回归的基本假设"></a><strong>逻辑回归的基本假设</strong></h2><ul>
<li><ul>
<li><p>任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的</p>
<p>第一个</p>
<p>基本假设是</p>
<p>假设数据服从伯努利分布。</p>
<p>伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是p,抛中为负面的概率是 1-p,在逻辑回归这个模型里面是假设 $h_θ(x)$为样本为正的概率，</p>
<p>$1−h_θ(x)$为样本为负的概率。那么整个模型可以描述为$h_θ(x;θ)=p$</p>
</li>
<li><p>逻辑回归的第二个假设是假设样本为正的概率是 $p=\frac{1}{1+e^{w^Tx}}$</p>
</li>
</ul>
</li>
<li><ul>
<li><p>所以逻辑回归的最终形式 </p>
<p>$h_θ(x;θ)=\frac{1}{1+e^{w^Tx}}$</p>
</li>
</ul>
</li>
</ul>
<h2 id="逻辑回归的损失函数"><a href="#逻辑回归的损失函数" class="headerlink" title="逻辑回归的损失函数"></a><strong>逻辑回归的损失函数</strong></h2><ul>
<li><p>逻辑回归的损失函数是它的极大似然函数</p>
<p>$Lθ(x)=\pi_{i=1}^{m}h_θ(xi;θ)^y_i∗(1−h_θ(xi;θ))^{1−y_i}$</p>
</li>
</ul>
<h2 id="逻辑回归的求解方法"><a href="#逻辑回归的求解方法" class="headerlink" title="逻辑回归的求解方法"></a><strong>逻辑回归的求解方法</strong></h2><ul>
<li><p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。</p>
<ul>
<li>简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</li>
<li>随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。</li>
<li>小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</li>
</ul>
</li>
<li><ul>
<li>其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。<ul>
<li>第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。</li>
<li>第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。这里我们不展开，有空我会专门出一个专题介绍。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="逻辑回归的目的"><a href="#逻辑回归的目的" class="headerlink" title="逻辑回归的目的"></a><strong>逻辑回归的目的</strong></h2><ul>
<li><p>该函数的目的便是将数据二分类，提高准确率。</p>
</li>
<li><p>逻辑回归如何分类</p>
<ul>
<li>逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。</li>
</ul>
</li>
</ul>
<h2 id="3-对逻辑回归的进一步提问"><a href="#3-对逻辑回归的进一步提问" class="headerlink" title="3.对逻辑回归的进一步提问"></a><strong>3.对逻辑回归的进一步提问</strong></h2><p>​    逻辑回归虽然从形式上非常的简单，但是其内涵是非常的丰富。有很多问题是可以进行思考的</p>
<ul>
<li><p><strong>逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？</strong></p>
<ul>
<li><p>损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新</p>
<p>$w_j=w_j−(y^i−h_w(x^i;w))∗x_j^i\theta​$</p>
<p>这个式子的更新速度只和$x_j,y_j</p>
<p>相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。</p>
</li>
<li><p>为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p>
</li>
</ul>
</li>
<li><p><strong>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</strong></p>
</li>
<li><p>先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。</p>
</li>
<li>但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。</li>
<li><p>如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。</p>
</li>
<li><p><strong>为什么我们还是会在训练的过程当中将高度相关的特征去掉？</strong></p>
<ul>
<li>去掉高度相关的特征会让模型的可解释性更好</li>
<li>可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。</li>
</ul>
</li>
</ul>
<h2 id="4-逻辑回归的优缺点总结"><a href="#4-逻辑回归的优缺点总结" class="headerlink" title="4.逻辑回归的优缺点总结"></a><strong>4.逻辑回归的优缺点总结</strong></h2><p>​    面试的时候，别人也经常会问到，你在使用逻辑回归的时候有哪些感受。觉得它有哪些优缺点。</p>
<p>​     <strong>在这里我们总结了逻辑回归应用到工业界当中一些优点：</strong></p>
<ul>
<li><p>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</p>
</li>
<li><p>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</p>
</li>
<li><p>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</p>
</li>
<li><p>资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。</p>
</li>
<li><p>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</p>
</li>
</ul>
<p>​      <strong>但是逻辑回归本身也有许多的缺点:</strong></p>
<ul>
<li><p>准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</p>
</li>
<li><p>很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</p>
</li>
<li><p>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。</p>
</li>
<li><p>逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</p>
</li>
</ul>
<h2 id="模型、策略、算法"><a href="#模型、策略、算法" class="headerlink" title="模型、策略、算法"></a>模型、策略、算法</h2><h2 id="Codings"><a href="#Codings" class="headerlink" title="Codings"></a>Codings</h2>
          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/13/machine learning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/13/machine learning/" itemprop="url">test</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-13T19:59:23+08:00">2019-04-13</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/13/machine learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/13/machine learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                109字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/2019/04/13/machine learning/%E5%90%B4%E6%81%A9%E8%BE%BE%5Cimage-20200624204341481.png" alt="image-20200624204341481"></p>
<p>英伟达:芯片，GPU</p>
<p>开发框架：tensorflow，pytorch caffe</p>
<h2 id="监督学习"><a class="header-anchor" href="#监督学习">¶</a>监督学习</h2>
<p>学习目的是学习一个输入到输出的映射，称为模型。模型的集合就是假设空间。</p>
<p>模型：概率模型；非概率模型；</p>
<p>学习过程：搜索过程</p>

          
        
      
    </div>

    

    
    
    

    

    

    
<div>
  
</div>
    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">XieMay</p>
              <p class="site-description motion-element" itemprop="description">CAT_cat</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">66</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">46</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="http://www.cnblogs.com/shiyiandchuixue/" target="_blank" title="BLOGS"><i class="fa fa-fw fa-globe"></i>BLOGS</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:2323020965@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://github.com/shiyichuixue" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="powered-by">
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XieMay</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Symbols count total&#58;</span>
    
    <span title="Symbols count total">260k</span>
  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Pisces</a> v6.0.4</div>



</div>
        








        
      </div>
    </footer>

    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


















  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.4"></script>



  



	





  









  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
    
  
  <script src="[object Object]"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '1Dfsb4DwGQPCIlbyCKt9egUR-gzGzoHsz',
        appKey: '8OKqJPeMRRQnxx2vaAwIkM8y',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"model":"wanko","bottom":-30,"mobileShow":false,"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body>
</html>
