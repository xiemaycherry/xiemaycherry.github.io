<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"mycherrymay.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"WGLQQAQKBA","indexName":"xiemay","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="C4 : Convolutional Neural Networks(卷积神经网络)#W1 :Convolutional Neural Networks(卷积神经网络)#L1: Computer Vision# Image classification Object detection Neural Style Transfer  Problem : input big  神经网络结构复杂，数据量">
<meta property="og:type" content="article">
<meta property="og:title" content="Networks">
<meta property="og:url" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/index.html">
<meta property="og:site_name" content="Welcome to shiyi&#39;s world">
<meta property="og:description" content="C4 : Convolutional Neural Networks(卷积神经网络)#W1 :Convolutional Neural Networks(卷积神经网络)#L1: Computer Vision# Image classification Object detection Neural Style Transfer  Problem : input big  神经网络结构复杂，数据量">
<meta property="og:locale">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Different-edges.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1——2.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_3png.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_4png.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Convolutional-operation.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_5.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_1.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_2.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_3.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_4.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_5.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_6.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Padding.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Stride.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_6.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Convolutions-on-RGB-image.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_8.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_9.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_10.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_11.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_12.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_13.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-33.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_14.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_15.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_16.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_17.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/CNN.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_18.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_19.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_20.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-34.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-35.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-36.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_21.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Residual-Network.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/ResNet-Training-Error.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_22.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-37.jpg">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_23.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/99f8fc7dbe7cd0726f5271aae11b9872.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/The-problem-of-computational-cost.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Using-1x1-convolution.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_24.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_25.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_1.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_2.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_3.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_4.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_5.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_1.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_2.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_3.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_4.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_5.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_6.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_7.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_8.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_10.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_11.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_12.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_13.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_14.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_16.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_15.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_17.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_18.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_19.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_20.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_21.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_22.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_23.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_24.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_25.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_26.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_27.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_28.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_29.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_1.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_2.png">
<meta property="og:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_3.png">
<meta property="article:published_time" content="2019-05-12T11:31:06.000Z">
<meta property="article:modified_time" content="2020-10-08T07:46:14.581Z">
<meta property="article:author" content="XieMay">
<meta property="article:tag" content="Deep learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Different-edges.png">

<link rel="canonical" href="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>Networks | Welcome to shiyi's world</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Welcome to shiyi's world</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://mycherrymay.github.io/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="wise">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Welcome to shiyi's world">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Networks
        </h1>

        <div class="post-meta">
			
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-05-12 19:31:06" itemprop="dateCreated datePublished" datetime="2019-05-12T19:31:06+08:00">2019-05-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-08 15:46:14" itemprop="dateModified" datetime="2020-10-08T15:46:14+08:00">2020-10-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1><span id="c4-convolutional-neural-networks-juan-ji-shen-jing-wang-luo">C4 : Convolutional Neural Networks(卷积神经网络)</span><a href="#c4-convolutional-neural-networks-juan-ji-shen-jing-wang-luo" class="header-anchor">#</a></h1><h2><span id="w1-convolutional-neural-networks-juan-ji-shen-jing-wang-luo">W1 :Convolutional Neural Networks(卷积神经网络)</span><a href="#w1-convolutional-neural-networks-juan-ji-shen-jing-wang-luo" class="header-anchor">#</a></h2><h3><span id="l1-computer-vision">L1: Computer Vision</span><a href="#l1-computer-vision" class="header-anchor">#</a></h3><ol>
<li>Image classification</li>
<li>Object detection</li>
<li>Neural Style Transfer</li>
</ol>
<p>Problem : input big</p>
<ol>
<li>神经网络结构复杂，数据量相对较少，容易出现过拟合；</li>
<li>所需内存和计算量巨大。</li>
</ol>
<h3><span id="l2-edge-detection-example">L2: Edge detection example</span><a href="#l2-edge-detection-example" class="header-anchor">#</a></h3><p>我们之前提到过，神经网络由浅层到深层，分别可以检测出图片的边缘特征、局部特征（例如眼睛、鼻子等），到最后面的一层就可以根据前面检测的特征来识别整体面部轮廓。这些工作都是依托卷积神经网络来实现的。</p>
<p><strong>卷积运算（Convolutional Operation）</strong>是卷积神经网络最基本的组成部分。我们以边缘检测为例，来解释卷积是怎样运算的。</p>
<ol>
<li><p>常见的边缘检测</p>
<p>垂直边缘（Vertical Edges) 和 水平边缘（horizontal Edges)</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Different-edges.png" alt></p>
</li>
</ol>
<p>这张图的栏杆就对应垂直线，栏杆的水平线是水平边缘。</p>
<p>那么图片是怎么检测边缘的呢？</p>
<p>过滤器：filter</p>
<p>在数学中“”就是卷积的标准标志，但是在<strong>Python</strong>中，这个标识常常被用来表示乘法或者元素乘法。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1.png" alt></p>
<p>Output; 4 by 4</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1——2.png" alt></p>
<p>具体运算：</p>
<p>1）</p>
<p>为了计算第一个元素，在4×4左上角的那个元素，使用3×3的过滤器，将其覆盖在输入图像，如下图所示。然后进行元素乘法（<strong>element-wise products</strong>）运算</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_3png.png" alt></p>
<p>2）为了弄明白第二个元素是什么，你要把蓝色的方块，向右移动一步，像这样，把这些绿色的标记去掉：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_4png.png" alt></p>
<p>6×6矩阵和3×3矩阵进行卷积运算得到4×4矩阵。这些图片和过滤器是不同维度的矩阵，但左边矩阵容易被理解为一张图片，中间的这个被理解为过滤器，右边的图片我们可以理解为另一张图片。这个就是垂直边缘检测器。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Convolutional-operation.jpg" alt></p>
<p>举例说明： Vertical edge detection</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_1_5.png" alt></p>
<p>这里在结果可能有点不对头，检测到的边缘太粗了，主要是图片太小了，</p>
<p>卷积操作API</p>
<ul>
<li>在 Python 中，卷积用<code>conv_forward()</code>表示；</li>
<li>在 Tensorflow 中，卷积用<code>tf.nn.conv2d()</code>表示；</li>
<li>在 keras 中，卷积用<code>Conv2D()</code>表示。</li>
</ul>
<h3><span id="l3-edge-detection-example">L3: Edge Detection Example</span><a href="#l3-edge-detection-example" class="header-anchor">#</a></h3><ol>
<li><p>颜色由暗到亮，还是亮到暗</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_1.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_2.png" alt></p>
</li>
</ol>
<p>这种滤波器可以区分明暗变化，取绝对值没有区别了</p>
<ol>
<li><p>水平边缘</p>
<p>上边相对较亮，而下方相对较暗</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_3.png" alt></p>
<ol>
<li>复杂栗子</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_4.png" alt></p>
</li>
</ol>
<p>这块区域左边两列是正边，右边一列是负边，正边和负边的值加在一起得到了一个中间值。但假如这个一个非常大的1000×1000的类似这样棋盘风格的大图，就不会出现这些亮度为10的过渡带了，因为图片尺寸很大，这些中间值就会变得非常小。</p>
<ol>
<li>filter</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_5.png" alt></p>
<p>sobel过滤器，优点在于增加了中间一行元素的权重，这使得结果的鲁棒性会更高一些。</p>
<p>charr过滤器，它有着和之前完全不同的特性，实际上也是一种垂直边缘检测，如果你将其翻转90度，你就能得到对应水平边缘检测。</p>
<p>学习的其中一件事就是当你真正想去检测出复杂图像的边缘，你不一定要去使用那些研究者们所选择的这九个数字，但你可以从中获益匪浅。把这矩阵中的9个数字当成9个参数，并且在之后你可以学习使用反向传播算法，其目标就是去理解这9个参数。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_6.png" alt></p>
<p>这样可能得到一个出色的边缘检测</p>
<p>相比这种单纯的垂直边缘和水平边缘，它可以检测出45°或70°或73°，甚至是任何角度的边缘。所以将矩阵的所有数字都设置为参数，通过数据反馈，让神经网络自动去学习它们，我们会发现神经网络可以学习一些低级的特征，例如这些边缘的特征。</p>
<p>不管是垂直的边缘，水平的边缘，还有其他奇怪角度的边缘，甚至是其它的连名字都没有的过滤器。</p>
<h3><span id="padding">Padding</span><a href="#padding" class="header-anchor">#</a></h3><p>按照我们上面讲的图片卷积，如果原始图片尺寸为$n x n$，filter尺寸为$f x f$，则卷积后的图片尺寸为$(n-f+1) x (n-f+1)$，注意f一般为奇数。这样会带来两个问题：</p>
<ul>
<li><p><strong>卷积运算后，输出图片尺寸缩小</strong></p>
</li>
<li><p><strong>原始图片边缘信息对输出贡献得少，输出图片丢失边缘信息</strong></p>
<p>边缘像素点只被一个输出所触碰或者使用，</p>
</li>
</ul>
<p>为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行<strong>填充（Padding）</strong>，以增加矩阵的大小。通常将 0 作为填充值。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Padding.jpg" alt></p>
<p>经过padding之后，填充p,原始图片尺寸为$(n+2p) x (n+2p)$，filter尺寸为$f x f$，则卷积后的图片尺寸为$(n+2p-f+1) x (n+2p-f+1)$。若要保证卷积前后图片尺寸不变，则p应满足：$ p=(f-1)/2$,f通常是奇数，如果是偶数，造成不对称填充，第二个原因是当你有一个奇数维过滤器，比如3×3或者5×5的，它就有一个中心点。有时在计算机视觉里，如果有一个中心像素点会更方便，便于指出过滤器的位置</p>
<ol>
<li>p=0,Valid convolution</li>
<li>p=((f-1))/2,Same convolution</li>
</ol>
<h3><span id="l05-strided-convolution-juan-ji-bu-chang">L05: Strided convolution（卷积步长）</span><a href="#l05-strided-convolution-juan-ji-bu-chang" class="header-anchor">#</a></h3><p>Stride表示filter在原图片中水平方向和垂直方向每次的步进长度。之前我们默认stride=1。若stride=2，则表示filter每次步进长度为2，即隔一点移动一次。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Stride.jpg" alt></p>
<p>我们用s表示stride长度，p表示padding长度，如果原始图片尺寸为n x n，filter尺寸为f x f，则卷积后的图片尺寸为：</p>
<script type="math/tex; mode=display">
\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor X\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor</script><p>向下取整</p>
<p>目前为止我们学习的“卷积”实际上被称为<strong>互相关（cross-correlation）</strong>，而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）。因为这种翻转对一般为水平或垂直对称的滤波器影响不大，按照机器学习的惯例，我们通常不进行翻转操作，在简化代码的同时使神经网络能够正常工作。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_6.png" alt></p>
<p>互相关：过滤器沿水平和垂直轴翻转，元素相乘来计算，这些视频中定义卷积运算时，我们跳过了这个镜像操作。（不进行翻转操作）叫做卷积操作</p>
<h3><span id="l06-convolution-over-volumes-san-wei-juan-ji">L06: Convolution over volumes(三维卷积)</span><a href="#l06-convolution-over-volumes-san-wei-juan-ji" class="header-anchor">#</a></h3><ol>
<li><p>卷积运算</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Convolutions-on-RGB-image.png" alt></p>
</li>
</ol>
<p>过程是将每个单通道（R，G，B）与对应的filter进行卷积运算求和，然后再将3通道的和相加，得到输出图片的一个像素值。</p>
<p>不同通道的滤波算子可以不相同。例如R通道filter实现垂直边缘检测，G和B通道不进行边缘检测，全部置零，或者将R，G，B三通道filter全部设置为水平边缘检测。</p>
<p>为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_8.png" alt></p>
<p>为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_9.png" alt></p>
<p>若输入图片的尺寸为n x n x nc，nc: 通道数目，filter尺寸为f x f x nc，则卷积后的图片尺寸为(n-f+1) x (n-f+1) x nc′。其中，nc为图片通道数目，nc′为滤波器组个数。</p>
<h3><span id="l7-one-layer-of-a-convolution-network-dan-ceng-shen-jing-wang-luo">L7 : One layer of a convolution network (单层神经网络)</span><a href="#l7-one-layer-of-a-convolution-network-dan-ceng-shen-jing-wang-luo" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_10.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_11.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_12.png" alt></p>
<p>CNN单层的所以标记符号，设层数$l$,</p>
<script type="math/tex; mode=display">
\begin{array}{l}{f^{[l]}=\text { filter size }} \\ {p^{[l]}=\text { padding }} \\ {g^{[l]}=\text { stride }} \\ {n_{c}^{[l]}=\text { number of filters }}\end{array}</script><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_13.png" alt></p>
<script type="math/tex; mode=display">
\begin{array}{c}{n_{H}^{[l]}=\left\lfloor\frac{n_{H}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right\rfloor} \\ { n_{W}^{[l]}=\left\lfloor\frac{n_{W}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right\rfloor}\end{array}</script><p>如果$m$个样本，进行向量化运算，相应的输出维度，为</p>
<script type="math/tex; mode=display">
\mathrm{m} \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}</script><h3><span id="l8-a-simple-convolution-network-example-jian-dan-juan-ji-wang-luo-shi-li">L8 : A simple convolution network example（简单卷积网络示例）</span><a href="#l8-a-simple-convolution-network-example-jian-dan-juan-ji-wang-luo-shi-li" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-33.jpg" alt></p>
<ul>
<li>一般而言，<strong>图片的height $n^{[l]}_{H}$和width $n^{[l]}_W$随着层数的增加逐渐降低，但channel $n^{[l]}_C$逐渐增加</strong>。</li>
</ul>
<p>CNN有三种类型的layer：</p>
<ul>
<li>Convolution层（CONV）</li>
<li>Pooling层（POOL）</li>
<li>Fully connected层（FC）</li>
</ul>
<h3><span id="l9-pooling-layers-chi-hua-ceng">L9: Pooling layers(池化层)</span><a href="#l9-pooling-layers-chi-hua-ceng" class="header-anchor">#</a></h3><p>卷积神经网络除了卷积层，还有池化层来缩减模型的大小，提高运算速度和鲁棒性</p>
<ol>
<li>池的类型有max pooling(最大池化)</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_14.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_15.png" alt></p>
<p>这里步幅是s=2，filter = 2*2是最大池化的超参数,如果是三维，则单独在每个通道执行最大池化操作</p>
<p>关于max pooling的直觉解释： 元素较大的值，可能是卷积过程中提取到的某些特征（比如边界），而max pooling则在压缩了矩阵大小的情况下，保留每个分区内最大的输出，即保留了提取的特征。但理论上还没有证明max pooling的原理，max pooling应用的原因是在实践中效果很好。</p>
<ol>
<li><p>Pooling layer: Average pooling</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_16.png" alt></p>
<p>但是最大池化更好用</p>
</li>
</ol>
<p>summary : 输入$n_H<em>n_W</em>n_C$,如果没有padding,输出$(n_h-f)/s+1<em>(n_w-f)/s+1</em>n_c$</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_17.png" alt></p>
<h3><span id="l10-convolutional-neural-network-example-juan-ji-shen-jing-wang-luo-shi-li">L10: Convolutional neural network example (卷积神经网络实例)</span><a href="#l10-convolutional-neural-network-example-juan-ji-shen-jing-wang-luo-shi-li" class="header-anchor">#</a></h3><p>做一个识别数字的CNN网络</p>
<ol>
<li><p>LeNet-5架构如下：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/CNN.jpg" alt></p>
<ul>
<li>通常Conv Layer和Pooling Layer合在一起算一个layer，因为pooling layer并没有参数训练</li>
</ul>
</li>
</ol>
<ul>
<li>常见的结构：Conv ==&gt; Pool ==&gt; Conv ==&gt; Pool ==&gt; FC ==&gt; FC ==&gt; softmax</li>
<li>最终还会用FC层（全连接层），与一般NN的处理一样；并在输出层，应用softmax得到10个数字的概率。</li>
<li>在整个网络中，Height和Width是逐渐递减的，但channel和filter是递增的。</li>
<li>关于CNN如何选择超参：可以参考论文的经验。</li>
<li><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_18.png" alt></li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Activation shape</th>
<th style="text-align:center">Activation Size</th>
<th>#parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Input:</strong></td>
<td style="text-align:center">(32, 32, 3)</td>
<td style="text-align:center">3072</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>CONV1(f=5, s=1)</strong></td>
<td style="text-align:center">(28, 28, 6)</td>
<td style="text-align:center">4704</td>
<td>156 (=5<em>5</em>6+6)</td>
</tr>
<tr>
<td style="text-align:center"><strong>POOL1</strong></td>
<td style="text-align:center">(14, 14, 6)</td>
<td style="text-align:center">1176</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>CONV2(f=5, s=1)</strong></td>
<td style="text-align:center">(10, 10, 16)</td>
<td style="text-align:center">1600</td>
<td>416 (=5<em>5</em>16+16)</td>
</tr>
<tr>
<td style="text-align:center"><strong>POOL2</strong></td>
<td style="text-align:center">(5, 5, 16)</td>
<td style="text-align:center">400</td>
<td>0</td>
</tr>
<tr>
<td style="text-align:center"><strong>FC3</strong></td>
<td style="text-align:center">(120, 1)</td>
<td style="text-align:center">120</td>
<td>48120 (=120*400+120)</td>
</tr>
<tr>
<td style="text-align:center"><strong>FC4</strong></td>
<td style="text-align:center">(84, 1)</td>
<td style="text-align:center">84</td>
<td>10164 (=84*120+84)</td>
</tr>
<tr>
<td style="text-align:center"><strong>Softmax</strong></td>
<td style="text-align:center">(10, 1)</td>
<td style="text-align:center">10</td>
<td>850 (=10*84+10)</td>
</tr>
</tbody>
</table>
</div>
<h3><span id="l11-why-convolution">L11 Why convolution</span><a href="#l11-why-convolution" class="header-anchor">#</a></h3><ul>
<li><p>参数共享（parameter sharing)</p>
<p> 如果用FC的话，参数爆炸啊！如果conv layer 就需要filter检测器，这个参数就少了，还参数共享</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_19.png" alt></p>
</li>
<li><p>稀疏连接(sparsity of connection)</p>
<p>输出中的每个单元仅和输入的一个小分区相关，比如输出的左上角的像素仅仅由输入左上角的9个像素决定（假设filter大小是3*3），而其他输入都不会影响。</p>
</li>
</ul>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_20.png" alt></p>
<h2><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h2><font color="read">1. 卷积神经网络的基本构造和计算过程 2. 如何整合这些模型 3.  哪些超参数 4. 为什么使用卷积 </font>

<h2><span id="w2-deep-convolutional-models-case-studies-shen-du-juan-ji-wang-luo-shi-li-tan-jiu">W2 : Deep convolutional models: case studies(深度卷积网络：实例探究)</span><a href="#w2-deep-convolutional-models-case-studies-shen-du-juan-ji-wang-luo-shi-li-tan-jiu" class="header-anchor">#</a></h2><h3><span id="l1-why-look-at-case-studies-wei-shi-me-yao-jin-xing-shi-li-tan-jiu">L1 : Why look at case studies?(为什么要进行实例探究？)</span><a href="#l1-why-look-at-case-studies-wei-shi-me-yao-jin-xing-shi-li-tan-jiu" class="header-anchor">#</a></h3><p>本文将主要介绍几个典型的CNN案例。通过对具体CNN模型及案例的研究，来帮助我们理解知识并训练实际的模型。</p>
<p>典型的CNN模型包括：</p>
<ul>
<li><strong>LeNet-5</strong></li>
<li><strong>AlexNet</strong></li>
<li><strong>VGG</strong></li>
</ul>
<p>还会介绍Residual Network（ResNet）。其特点是可以构建很深很深的神经网络（目前最深的好像有152层）。还会介绍Inception Neural Network</p>
<h3><span id="l2-classic-networks-jing-dian-wang-luo">L2 : Classic networks(经典网络)</span><a href="#l2-classic-networks-jing-dian-wang-luo" class="header-anchor">#</a></h3><h4><span id="1-lenet-5">1. LeNet-5</span><a href="#1-lenet-5" class="header-anchor">#</a></h4><p><strong>LeNet-5</strong>是针对灰度图片训练的，使用6个5×5的过滤器，步幅为1。由于使用了6个过滤器，步幅为1，<strong>padding</strong>为0，输出结果为28×28×6，图像尺寸从32×32缩小到28×28。然后进行池化操作，在这篇论文写成的那个年代，人们更喜欢使用平均池化，而现在我们可能用最大池化更多一些。在这个例子中，我们进行平均池化，过滤器的宽度为2，步幅为2，图像的尺寸，高度和宽度都缩小了2倍，输出结果是一个14×14×6的图像。我觉得这张图片应该不是完全按照比例绘制的，如果严格按照比例绘制，新图像的尺寸应该刚好是原图像的一半。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-34.jpg" alt></p>
<p>该LeNet模型总共包含了大约6万个参数。值得一提的是，当时Yann LeCun提出的LeNet-5模型池化层使用的是average pool，而且各层激活函数一般是Sigmoid和tanh。现在，我们可以根据需要，做出改进，使用max pool和激活函数ReLU。</p>
<h4><span id="1-alexnet">1. AlexNet</span><a href="#1-alexnet" class="header-anchor">#</a></h4><p>AlexNet模型是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同提出的，其结构如下所示：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-35.jpg" alt></p>
<p><strong>AlexNet</strong>首先用一张227×227×3的图片作为输入，实际上原文中使用的图像是224×224×3，但是如果你尝试去推导一下，你会发现227×227这个尺寸更好一些。第一层我们使用96个11×11的过滤器，步幅为4，由于步幅是4，因此尺寸缩小到55×55，缩小了4倍左右。然后用一个3×3的过滤器构建最大池化层,f=3，步幅为2，卷积层尺寸缩小为27×27×96。接着再执行一个5×5的卷积，<strong>padding</strong>之后，输出是27×27×276。然后再次进行最大池化，尺寸缩小到13×13。再执行一次<strong>same</strong>卷积，相同的<strong>padding</strong>，得到的结果是13×13×384，384个过滤器。再做一次<strong>same</strong>卷积，就像这样。再做一次同样的操作，最后再进行一次最大池化，尺寸缩小到6×6×256。6×6×256等于9216，将其展开为9216个单元，然后是一些全连接层。最后使用<strong>softmax</strong>函数输出识别的结果，看它究竟是1000个可能的对象中的哪一个。</p>
<p>实际上，这种神经网络与<strong>LeNet</strong>有很多相似之处，不过<strong>AlexNet</strong>要大得多。正如前面讲到的<strong>LeNet</strong>或<strong>LeNet-5</strong>大约有6万个参数，而<strong>AlexNet</strong>包含约6000万个参数。当用于训练图像和数据集时，<strong>AlexNet</strong>能够处理非常相似的基本构造模块，这些模块往往包含着大量的隐藏单元或数据，这一点<strong>AlexNet</strong>表现出色。<strong>AlexNet</strong>比<strong>LeNet</strong>表现更为出色的另一个原因是它使用了<strong>ReLu</strong>激活函数。原作者还提到了一种优化技巧，叫做Local Response Normalization(LRN)。 而在实际应用中，LRN的效果并不突出。</p>
<h4><span id="3-vgg-16">3. VGG-16</span><a href="#3-vgg-16" class="header-anchor">#</a></h4><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-36.jpg" alt></p>
<p>首先用3×3，步幅为1的过滤器构建卷积层，<strong>padding</strong>参数为<strong>same</strong>卷积中的参数。然后用一个2×2，步幅为2的过滤器构建最大池化层。因此<strong>VGG</strong>网络的一大优点是它确实简化了神经网络结构，下面我们具体讲讲这种网络结构。</p>
<p>数字16，就是指在这个网络中包含16个卷积层和全连接层。总共包含约1.38亿个参数</p>
<h3><span id="l3-residual-networks-resnets-can-chai-wang-luo-resnets">L3 : Residual Networks (ResNets)(残差网络(ResNets))</span><a href="#l3-residual-networks-resnets-can-chai-wang-luo-resnets" class="header-anchor">#</a></h3><p>我们知道，如果神经网络层数越多，网络越深，源于梯度消失和梯度爆炸的影响，整个模型难以训练成功。解决的方法之一是人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为Residual Networks(ResNets)。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_21.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Residual-Network.jpg" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/ResNet-Training-Error.jpg" alt></p>
<h3><span id="l4-why-resnets-work-can-chai-wang-luo-wei-shi-me-you-yong">L4: Why ResNets work?(残差网络为什么有用？)</span><a href="#l4-why-resnets-work-can-chai-wang-luo-wei-shi-me-you-yong" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_22.png" alt></p>
<p>因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。</p>
<p>注意，如果$ a[l]$与 $a[l+2]$的维度不同，需要引入矩阵 $W_s$与 $a_{[l]}$相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a[l]$截断或者补零。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Xnip2018-07-04_08-28-37.jpg" alt></p>
<h3><span id="l5-network-in-network-and-1x1-convolutions-wang-luo-zhong-de-wang-luo-yi-ji-1x1-juan-ji">L5 : Network in Network and 1×1 convolutions(网络中的网络以及 1×1 卷积)</span><a href="#l5-network-in-network-and-1x1-convolutions-wang-luo-zhong-de-wang-luo-yi-ji-1x1-juan-ji" class="header-anchor">#</a></h3><ol>
<li>作用 </li>
</ol>
<p>假设这是一个28×28×192的输入层，你可以使用池化层压缩它的高度和宽度，这个过程我们很清楚。但如果通道数量很大，该如何把它压缩为28×28×32维度的层呢？你可以用32个大小为1×1的过滤器，严格来讲每个过滤器大小都是1×1×192维，因为过滤器中通道数量必须与输入层中通道的数量保持一致。但是你使用了32个过滤器，输出层为28×28×32，这就是压缩通道数（$n_c$）的方法，对于池化层我只是压缩了这些层的高度和宽度</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_23.png" alt></p>
<ol>
<li><strong>doing something pretty non-trivial</strong></li>
</ol>
<p>它给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数量不变，当然如果你愿意，也可以增加通道数量。</p>
<h3><span id="l6-inception-network-motivation-gu-ge-inception-wang-luo-jian-jie">L6 : Inception network motivation(谷歌 Inception 网络简介)</span><a href="#l6-inception-network-motivation-gu-ge-inception-wang-luo-jian-jie" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/99f8fc7dbe7cd0726f5271aae11b9872.png" alt></p>
<p>有了这样的<strong>Inception</strong>模块，你就可以输入某个量，因为它累加了所有数字，这里的最终输出为32+32+128+64=256。有了这样的<strong>Inception</strong>模块，你就可以输入某个量，因为它累加了所有数字，这里的最终输出为32+32+128+64=256。Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。</p>
<p> 1x1 的卷积层通常被称作<strong>瓶颈层（Bottleneck layer）</strong></p>
<p>计算量为 28x28x32x5x5x192 = 1.2亿</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/The-problem-of-computational-cost.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Using-1x1-convolution.png" alt></p>
<p>28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。</p>
<h3><span id="l7-inception-network-inception-wang-luo">L7 : Inception network(Inception 网络)</span><a href="#l7-inception-network-inception-wang-luo" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_24.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/example_2_25.png" alt></p>
<h3><span id="l8-using-open-source-implementations-shi-yong-kai-yuan-de-shi-xian-fang-an">L8 : Using open-source implementations( 使用开源的实现方案)</span><a href="#l8-using-open-source-implementations-shi-yong-kai-yuan-de-shi-xian-fang-an" class="header-anchor">#</a></h3><p>开源项目</p>
<h3><span id="l9-transfer-learning-qian-yi-xue-xi">L9 ： Transfer Learning（迁移学习）</span><a href="#l9-transfer-learning-qian-yi-xue-xi" class="header-anchor">#</a></h3><p>如果你下载别人已经训练好网络结构的权重，你通常能够进展的相当快，用这个作为预训练，然后转换到你感兴趣的任务上。</p>
<ol>
<li>只有很小数据集： 可以你只需要训练<strong>softmax</strong>层的权重，把前面这些层的权重都冻结。</li>
<li>稍微更大的数据集： 你应该冻结更少的层，比如只把这些层冻结，然后训练后面的层。如果你的输出层的类别不同，那么你需要构建自己的输出单元；或者你可以直接去掉这几层，换成你自己的隐藏单元和你自己的<strong>softmax</strong>输出层，这些方法值得一试。</li>
<li>大量数据： 你可以用下载的权重只作为初始化，用它们来代替随机初始化，接着你可以用梯度下降训练，更新网络所有层的所有权重。</li>
</ol>
<h3><span id="l10-data-augmentation-shu-ju-zeng-qiang">L10 ： Data augmentation（数据增强）</span><a href="#l10-data-augmentation-shu-ju-zeng-qiang" class="header-anchor">#</a></h3><p>数据量远远不够</p>
<ol>
<li>Mirroring</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring.png" alt></p>
<ol>
<li>Random Cropping</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_1.png" alt></p>
<ol>
<li><p>彩色转换color shifting</p>
<p>r,g,b数据改变</p>
</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_2.png" alt></p>
<p>除了随意改变RGB通道数值外，还可以更有针对性地对图片的RGB通道进行PCA color augmentation，也就是对图片颜色进行主成分分析，对主要的通道颜色进行增加或减少，可以采用高斯扰动做法。这样也能增加有效的样本数量。具体的PCA color augmentation做法可以查阅AlexNet的相关论文。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_3.png" alt></p>
<p>常用的实现数据扩充的方法是使用一个线程或者是多线程，这些可以用来加载数据，实现变形失真，然后传给其他的线程或者其他进程，来训练这个（编号2）和这个（编号1），可以并行实现。</p>
<h3><span id="l11-the-state-of-computer-vision-ji-suan-ji-shi-jue-xian-zhuang">L11：The state of computer vision(计算机视觉现状)</span><a href="#l11-the-state-of-computer-vision-ji-suan-ji-shi-jue-xian-zhuang" class="header-anchor">#</a></h3><ol>
<li>神经网络需要数据，不同的网络模型所需的数据量是不同的。Object dection，Image recognition，Speech recognition所需的数据量依次增加。一般来说，如果data较少，那么就需要更多的hand-engineering，对已有data进行处理。</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_4.png" alt></p>
<p>hand-engineering是一项非常重要也比较困难的工作。很多时候，hand-engineering对模型训练效果影响很大，特别是在数据量不多的情况下。</p>
<p>当你有少量的数据时，有一件事对你很有帮助，那就是迁移学习。在别人做好的基础上研究</p>
<ol>
<li><p>提升性能</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Mirroring_5.png" alt>*</p>
</li>
</ol>
<p>由于计算机视觉问题建立在小数据集之上，其他人已经完成了大量的网络架构的手工工程。一个神经网络在某个计算机视觉问题上很有效，但令人惊讶的是它通常也会解决其他计算机视觉问题。</p>
<p>所以，要想建立一个实用的系统，你最好先从其他人的神经网络架构入手。如果可能的话，你可以使用开源的一些应用，因为开放的源码实现可能已经找到了所有繁琐的细节，比如学习率衰减方式或者超参数。</p>
<h2><span id="summary">summary</span><a href="#summary" class="header-anchor">#</a></h2><font color="red">1. CNN的常见网络结构 重点说了一些残差网络 2.数据增加的方法 3. 多用开源框架，不用从头开始训练 </font>

<h1><span id="w3-object-detection-mu-biao-jian-ce">W3 Object detection(目标检测)</span><a href="#w3-object-detection-mu-biao-jian-ce" class="header-anchor">#</a></h1><h3><span id="l1-object-localization-mu-biao-ding-wei">L1 :Object localization(目标定位)</span><a href="#l1-object-localization-mu-biao-ding-wei" class="header-anchor">#</a></h3><p>目标定位和目标检测</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_1.png" alt></p>
<p>模型</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_2.png" alt></p>
<p>输入还包括位置信息</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_3.png" alt></p>
<p>损失函数</p>
<p>情况一：检测到了</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_4.png" alt></p>
<p>情况二：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_5.png" alt></p>
<h3><span id="l2-landmark-detection-te-zheng-dian-jian-ce">L2: Landmark detection(特征点检测)</span><a href="#l2-landmark-detection-te-zheng-dian-jian-ce" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_6.png" alt></p>
<p>该网络模型共检测人脸上64处特征点，加上是否为face的标志位，输出label共有64x2+1=129个值。通过检测人脸特征点可以进行情绪分类与判断，或者应用于AR领域等等。</p>
<p>除了人脸特征点检测之外，还可以检测人体姿势动作，如下图所示：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_7.png" alt></p>
<h3><span id="l3-object-detection-mu-biao-jian-ce">L3 :Object detection(目标检测)</span><a href="#l3-object-detection-mu-biao-jian-ce" class="header-anchor">#</a></h3><p>学过了对象定位和特征点检测，今天我们来构建一个对象检测算法。这节课，我们将学习如何通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_8.png" alt></p>
<p>训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下。</p>
<p>选定特定大小的窗口，窗口圈定输入卷积神经网络，卷积神经网络开始预测。</p>
<p>重复上述操作，不过这次我们选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，你可以根据卷积网络对输入大小调整这个区域，然后输入给卷积网络，输出0或<img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_10.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_11.png" alt></p>
<p>如果你这样做，不论汽车在图片的什么位置，总有一个窗口可以检测到它。</p>
<p>这种算法叫作滑动窗口目标检测，因为我们以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车。</p>
<p>滑动窗算法的优点是原理简单，且不需要人为选定目标区域（检测出目标的滑动窗即为目标区域）。但是其缺点也很明显，首先滑动窗的大小和步进长度都需要人为直观设定。滑动窗过小或过大，步进长度过大均会降低目标检测正确率。而且，每次滑动窗区域都要进行一次CNN网络计算，如果滑动窗和步进长度较小，整个目标检测的算法运行时间会很长。所以，滑动窗算法虽然简单，但是性能不佳，不够快，不够灵活。</p>
<h3><span id="l-4-convolutional-implementation-of-sliding-windows-hua-dong-chuang-kou-de-juan-ji-shi-xian">L 4 : Convolutional implementation of sliding windows(滑动窗口的卷积实现)</span><a href="#l-4-convolutional-implementation-of-sliding-windows-hua-dong-chuang-kou-de-juan-ji-shi-xian" class="header-anchor">#</a></h3><ol>
<li><p>全连接层转化为卷积层</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_12.png" alt></p>
</li>
</ol>
<p>单个窗口区域卷积网络结构建立完毕之后，对于待检测图片，即可使用该网络参数和结构进行运算。例如16 x 16 x 3的图片，步进长度为2，CNN网络得到的输出层为2 x 2 x 4。其中，2 x 2表示共有4个窗口结果。对于更复杂的28 x 28 x3的图片，CNN网络得到的输出层为8 x 8 x 4，共64个窗口结果。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_13.png" alt></p>
<p>之前的滑动窗算法需要反复进行CNN正向计算，例如16 x 16 x 3的图片需进行4次，28 x 28 x3的图片需进行64次。而利用卷积操作代替滑动窗算法，则不管原始图片有多大，只需要进行一次CNN正向计算，因为其中共享了很多重复计算部分，这大大节约了运算成本。值得一提的是，窗口步进长度与选择的MAX POOL大小有关。如果需要步进长度为4，只需设置MAX POOL为4 x 4即可。</p>
<h3><span id="l5-bounding-box-predictions-bounding-box-yu-ce">L5 ： Bounding box predictions（Bounding Box预测）</span><a href="#l5-bounding-box-predictions-bounding-box-yu-ce" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_14.png" alt></p>
<ol>
<li><p>YOLO（You Only Look Once）算法可以解决这类问题，生成更加准确的目标区域（如上图红色窗口）。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_16.png" alt></p>
</li>
<li><p>如果目标中心坐标(bx,by)不在当前网格内，则当前网格Pc=0；相反，则当前网格Pc=1（即只看中心坐标是否在当前网格内）。判断有目标的网格中，bx,by,bh,bw限定了目标区域。值得注意的是，当前网格左上角坐标设定为(0, 0)，右下角坐标设定为(1, 1)，(bx,by)范围限定在[0,1]之间，但是bh,bw可以大于1。因为目标可能超出该网格，横跨多个区域，如上图所示。目标占几个网格没有关系，目标中心坐标必然在一个网格之内。</p>
</li>
</ol>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_15.png" alt></p>
<h3><span id="l6-intersection-over-union-jiao-bing-bi">L6 ：Intersection over union（交并比)</span><a href="#l6-intersection-over-union-jiao-bing-bi" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_17.png" alt></p>
<p>一般约定，在计算机检测任务中，如果lou&gt;=0.5，就说检测正确，如果预测器和实际边界框完美重叠，<strong>loU</strong>就是1，因为交集就等于并集。但一般来说只要lou&gt;=0.5，那么结果是可以接受的，看起来还可以。一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这么约定，但如果你希望更严格一点，你可以将<strong>loU</strong>定得更高，比如说大于0.6或者更大的数字，但<strong>loU</strong>越高，边界框越精确。</p>
<h3><span id="l7-non-max-suppression-fei-ji-da-zhi-yi-zhi">L7: Non-max suppression(非极大值抑制)</span><a href="#l7-non-max-suppression-fei-ji-da-zhi-yi-zhi" class="header-anchor">#</a></h3><p>到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以确保你的算法对每个对象只检测一次，我们讲一个例子。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_18.png" alt></p>
<p>假设你需要在这张图片里检测行人和汽车，你可能会在上面放个19×19网格，理论上这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_19.png" alt></p>
<p>实际情况是格子1，2，3，4，5，6都认为里面有车。因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的pc,我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。</p>
<p>非最大值抑制（Non-max Suppression）做法很简单，图示每个网格的Pc值可以求出，Pc值反映了该网格包含目标中心坐标的可信度。首先选取Pc最大值对应的网格和区域，然后计算该区域与所有其它区域的IoU，剔除掉IoU大于阈值（例如0.5）的所有网格及区域。这样就能保证同一目标只有一个网格与之对应，且该网格Pc最大，最可信。接着，再从剩下的网格中选取Pc最大的网格，重复上一步的操作。最后，就能使得每个目标都仅由一个网格和区域对应。如下图所示：</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_20.png" alt></p>
<p>总结一下非最大值抑制算法的流程：</p>
<ol>
<li><strong>剔除Pc值小于某阈值（例如0.6）的所有网格；</strong></li>
<li><strong>选取Pc值最大的网格，利用IoU，摒弃与该网格交叠较大的网格；</strong></li>
<li><strong>对剩下的网格，重复步骤2。</strong></li>
</ol>
<p>到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果你想让一个格子检测出多个对象，你可以这么做，就是使用<strong>anchor box</strong>这个概念，我们从一个例子开始讲吧。方法是使用不同形状的Anchor Boxes。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_21.png" alt></p>
<p>这就是<strong>anchor box</strong>的概念，我们建立<strong>anchor box</strong>这个概念，是为了处理两个对象出现在同一个格子的情况，实践中这种情况很少发生</p>
<h3><span id="l9-yolo-suan-fa-putting-it-together-yolo-algorithm">L9 :  YOLO 算法（Putting it together: YOLO algorithm）</span><a href="#l9-yolo-suan-fa-putting-it-together-yolo-algorithm" class="header-anchor">#</a></h3><p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_22.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_23.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_24.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_25.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_26.png" alt></p>
<p>这就是<strong>YOLO</strong>对象检测算法，这实际上是最有效的对象检测算法之一，包含了整个计算机视觉对象检测领域文献中很多最精妙的思路</p>
<h3><span id="region-proposals-optional-hou-xuan-qu-yu-xuan-xiu">Region proposals (Optional)（候选区域（选修））</span><a href="#region-proposals-optional-hou-xuan-qu-yu-xuan-xiu" class="header-anchor">#</a></h3><p>之前介绍的滑动窗算法会对原始图片的每个区域都进行扫描，即使是一些空白的或明显没有目标的区域，例如下图所示。这样会降低算法运行效率，耗费时间。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_27.png" alt></p>
<p>为了解决这一问题，尽量避免对无用区域的扫描，可以使用Region Proposals的方法。具体做法是先对原始图片进行分割算法处理，然后支队分割后的图片中的块进行目标检测。</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_28.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/Detering_29.png" alt></p>
<p>Region Proposals共有三种方法：</p>
<ul>
<li><strong>R-CNN: 滑动窗的形式，一次只对单个区域块进行目标检测，运算速度慢。</strong></li>
<li><strong>Fast R-CNN: 利用卷积实现滑动窗算法，类似第4节做法。</strong></li>
<li><strong>Faster R-CNN: 利用卷积对图片进行分割，进一步提高运行速度。</strong></li>
</ul>
<h2><span id="w4-special-applications-face-recognition-amp-neural-style-transfer-te-shu-ying-yong-ren-lian-shi-bie-he-shen-jing-feng-ge-zhuan-huan">W4：Special applications: Face recognition &amp;Neural style transfer( 特殊应用：人脸识别和神经风格转换)</span><a href="#w4-special-applications-face-recognition-amp-neural-style-transfer-te-shu-ying-yong-ren-lian-shi-bie-he-shen-jing-feng-ge-zhuan-huan" class="header-anchor">#</a></h2><h3><span id="c1-what-is-face-recognition">C1 ： What is face recognition?</span><a href="#c1-what-is-face-recognition" class="header-anchor">#</a></h3><p>首先简单介绍一下人脸验证（face verification）和人脸识别（face recognition）的区别。</p>
<ul>
<li><strong>人脸验证：输入一张人脸图片，验证输出与模板是否为同一人，即一对一问题。</strong></li>
<li><strong>人脸识别：输入一张人脸图片，验证输出是否为K个模板中的某一个，即一对多问题。</strong></li>
</ul>
<h3><span id="l2-one-shot-learning">L2 ： One-shot learning</span><a href="#l2-one-shot-learning" class="header-anchor">#</a></h3><p>One-shot learning就是说数据库中每个人的训练样本只包含一张照片，然后训练一个CNN模型来进行人脸识别。若数据库有K个人，则CNN模型输出softmax层就是K维的。</p>
<p>但是One-shot learning的性能并不好，其包含了两个缺点：</p>
<ul>
<li><strong>每个人只有一张图片，训练样本少，构建的CNN网络不够健壮。</strong></li>
<li><strong>若数据库增加另一个人，输出层softmax的维度就要发生变化，相当于要重新构建CNN网络，使模型计算量大大增加，不够灵活。</strong></li>
</ul>
<p>为了解决One-shot learning的问题，我们先来介绍相似函数（similarity function）。相似函数表示两张图片的相似程度，用d(img1,img2)来表示。若d(img1,img2)较小，则表示两张图片相似；若d(img1,img2)较大，则表示两张图片不是同一个人。相似函数可以在人脸验证中使用：</p>
<ul>
<li><strong>d(img1,img2)≤τ : 一样</strong></li>
<li><strong>d(img1,img2)&gt;τ : 不一样</strong></li>
</ul>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_1.png" alt></p>
<p>现在你已经知道函数d是如何工作的，通过输入两张照片，它将让你能够解决一次学习问题。那么，下节视频中，我们将会学习如何训练你的神经网络学会这个函数。</p>
<h3><span id="l3-siamese-network">L3: Siamese network</span><a href="#l3-siamese-network" class="header-anchor">#</a></h3><p>最后一层去掉softmax单元做分类</p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_2.png" alt></p>
<p><img src="/2019/05/12/Deel%20Learning%20ai_Convolutional%20Neural%20Networks/congtion_3.png" alt></p>
<p>如果你要比较两个图片的话，例如这里的第一张（编号1）和第二张图片（编号2），你要做的就是把第二张图片喂给有同样参数的同样的神经网络，然后得到一个不同的128维的向量（编号3），这个向量代表或者编码第二个图片，我要把第二张图片的编码叫做$f(x^{(2)})$。这里我用$x^{(1)}$和$x^{(2)}$仅仅代表两个输入图片,</p>
<script type="math/tex; mode=display">
d(x^{(1)},x^{(2)})=||f(x^{(1)}-f(x^{(2)}||^2</script><p>不同的图片的CNN网络结构和参数都是一样的，目标就是利用梯度下降算法，调整网络参数</p>

    </div>

    
    
    


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Deep-learning/" rel="tag"><i class="fa fa-tag"></i> Deep learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/05/05/Deep%20Learning%20ai_Deep%20Learning%20Specialization/" rel="prev" title="The Deep Learning Specialization">
      <i class="fa fa-chevron-left"></i> The Deep Learning Specialization
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/05/28/Python-basic/" rel="next" title="Python Basics">
      Python Basics <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">C4 : Convolutional Neural Networks(卷积神经网络)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.1.</span> <span class="nav-text">W1 :Convolutional Neural Networks(卷积神经网络)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.1.</span> <span class="nav-text">L1: Computer Vision</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.2.</span> <span class="nav-text">L2: Edge detection example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.3.</span> <span class="nav-text">L3: Edge Detection Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.4.</span> <span class="nav-text">Padding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.5.</span> <span class="nav-text">L05: Strided convolution（卷积步长）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.6.</span> <span class="nav-text">L06: Convolution over volumes(三维卷积)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.7.</span> <span class="nav-text">L7 : One layer of a convolution network (单层神经网络)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.8.</span> <span class="nav-text">L8 : A simple convolution network example（简单卷积网络示例）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.9.</span> <span class="nav-text">L9: Pooling layers(池化层)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.10.</span> <span class="nav-text">L10: Convolutional neural network example (卷积神经网络实例)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.11.</span> <span class="nav-text">L11 Why convolution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.2.</span> <span class="nav-text">summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.3.</span> <span class="nav-text">W2 : Deep convolutional models: case studies(深度卷积网络：实例探究)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.1.</span> <span class="nav-text">L1 : Why look at case studies?(为什么要进行实例探究？)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.2.</span> <span class="nav-text">L2 : Classic networks(经典网络)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">1. LeNet-5</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">1. AlexNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">3. VGG-16</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.3.</span> <span class="nav-text">L3 : Residual Networks (ResNets)(残差网络(ResNets))</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.4.</span> <span class="nav-text">L4: Why ResNets work?(残差网络为什么有用？)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.5.</span> <span class="nav-text">L5 : Network in Network and 1×1 convolutions(网络中的网络以及 1×1 卷积)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.6.</span> <span class="nav-text">L6 : Inception network motivation(谷歌 Inception 网络简介)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.7.</span> <span class="nav-text">L7 : Inception network(Inception 网络)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.8.</span> <span class="nav-text">L8 : Using open-source implementations( 使用开源的实现方案)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.9.</span> <span class="nav-text">L9 ： Transfer Learning（迁移学习）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.10.</span> <span class="nav-text">L10 ： Data augmentation（数据增强）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.11.</span> <span class="nav-text">L11：The state of computer vision(计算机视觉现状)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.4.</span> <span class="nav-text">summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">W3 Object detection(目标检测)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.1.</span> <span class="nav-text">L1 :Object localization(目标定位)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.2.</span> <span class="nav-text">L2: Landmark detection(特征点检测)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.3.</span> <span class="nav-text">L3 :Object detection(目标检测)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.4.</span> <span class="nav-text">L 4 : Convolutional implementation of sliding windows(滑动窗口的卷积实现)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.5.</span> <span class="nav-text">L5 ： Bounding box predictions（Bounding Box预测）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.6.</span> <span class="nav-text">L6 ：Intersection over union（交并比)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.7.</span> <span class="nav-text">L7: Non-max suppression(非极大值抑制)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.8.</span> <span class="nav-text">L9 :  YOLO 算法（Putting it together: YOLO algorithm）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.0.9.</span> <span class="nav-text">Region proposals (Optional)（候选区域（选修））</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.1.</span> <span class="nav-text">W4：Special applications: Face recognition &amp;Neural style transfer( 特殊应用：人脸识别和神经风格转换)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.1.</span> <span class="nav-text">C1 ： What is face recognition?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.2.</span> <span class="nav-text">L2 ： One-shot learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">2.1.3.</span> <span class="nav-text">L3: Siamese network</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">XieMay</p>
  <div class="site-description" itemprop="description">wise</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">90</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/shiyichuixue" title="GitHub → https://github.com/shiyichuixue" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:2323020965@qq.com" title="E-Mail → mailto:2323020965@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XieMay</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">495k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">7:30</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共176.7k字</span>
</div>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '[object Object]';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/hibiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"tagMode":false});</script></body>
</html>
