<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="baidu-site-verification" content="E1Di33CelZ">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="WsojkhmMcOefku3B2Vxp02NtxlUt_JzBP1fVPrFk3Gw">


  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.0.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.4">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.4" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.0.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: 'WGLQQAQKBA',
      apiKey: '',
      indexName: 'xiemay',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  



<meta name="baidu-site-verification" content="z0tVDge8Pe">

  
  <meta name="keywords" content="Deep learning,">


<meta name="description" content="Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)C1W1C1W1L01: WelcomeAI is the new Electricity! Course 1: Neural Networks and Deep Learning Course 2: Improv">
<meta name="keywords" content="Deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Neural Network and Deep Learning">
<meta property="og:url" content="http://xiemaycherry.github.io/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/index.html">
<meta property="og:site_name" content="welcome">
<meta property="og:description" content="Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)C1W1C1W1L01: WelcomeAI is the new Electricity! Course 1: Neural Networks and Deep Learning Course 2: Improv">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/housr_prize_1.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/house_prize_2.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/house%20prize%203.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/supervised-learning-exmples.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/NeuralNetworkExamples.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/datastructure.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/scale.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/algorithm——rul.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/faster.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/w_piexl.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_blue_green_read.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_noation.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_nation_2.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_all_nation.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/2_all_nation_1.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_1.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_3.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_2.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_3.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log_cost_1.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/log__cost_2.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/GD1.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_1.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_2.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/gd_3jpg.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_3.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_1.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_2.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_3.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v_3.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/v-4.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing_4.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_5.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_6.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing_2.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/boradcasing_3.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordcasing_1.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/bordacasing.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/note_1.jpg">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_2png.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/cost_1png.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_2.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_3.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_5.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_6.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_4.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_7.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_8.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_9.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_10.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_11.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_12.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_13.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_14.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_15.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_16.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_17.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_18.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_19.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_20.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week3_13%20(1">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_1.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_2.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_3.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_5.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_6.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_8.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_7.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_9.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_10.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_11.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_12.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/L1_week4_13.png">
<meta property="og:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/C1.png">
<meta property="og:updated_time" content="2019-05-10T03:25:23.276Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning Neural Network and Deep Learning">
<meta name="twitter:description" content="Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)C1W1C1W1L01: WelcomeAI is the new Electricity! Course 1: Neural Networks and Deep Learning Course 2: Improv">
<meta name="twitter:image" content="http://xiemaycherry.github.io/2019/04/11/Deep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning/housr_prize_1.png">






  <link rel="canonical" href="http://xiemaycherry.github.io/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>Deep Learning Neural Network and Deep Learning | welcome</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<meta name="baidu-site-verification" content="3BmH9zSH5h"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">welcome</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>
        </li>
      
        
        <li class="menu-item menu-item-留言">
          <a href="/guestbook/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-commenting"></i> <br>留言</a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>Schedule</a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>Sitemap</a>
        </li>
      
        
        <li class="menu-item menu-item-baidusitmap">
          <a href="/baidusitmap.xml" rel="section">
            <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>baidusitmap</a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://xiemaycherry.github.io/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="XieMay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Deep Learning Neural Network and Deep Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-11T09:25:45+08:00">2019-04-11</time>
            

            
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article&#58;</span>
                
                <span title="Symbols count in article">
                22k字
              </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Course-one-Neural-Networks-and-Deep-Learning-Course-1-of-the-Deep-Learning-Specialization"><a href="#Course-one-Neural-Networks-and-Deep-Learning-Course-1-of-the-Deep-Learning-Specialization" class="headerlink" title="Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)"></a>Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)</h1><h1 id="C1W1"><a href="#C1W1" class="headerlink" title="C1W1"></a>C1W1</h1><h2 id="C1W1L01-Welcome"><a href="#C1W1L01-Welcome" class="headerlink" title="C1W1L01: Welcome"></a>C1W1L01: Welcome</h2><p>AI is the new Electricity!</p>
<p>Course 1: Neural Networks and Deep Learning</p>
<p>Course 2: Improving Deep Neural Networks: Hyperparameter tuning,Regularization and Optimization</p>
<p>Course 3: Structuring your Machine Learning project</p>
<p>Course 4: Convolutional Neural Networks</p>
<p>Course 5: Natural Langurge Processing: Building sequence models</p>
<h2 id="C1W1L02-What-is-Neural-Network"><a href="#C1W1L02-What-is-Neural-Network" class="headerlink" title="C1W1L02 : What is Neural Network"></a>C1W1L02 : What is Neural Network</h2><p>Deep Learning = training (very large) neural network</p>
<h3 id="For-example-of-house-prize-prediction-the-simplest-neural-network"><a href="#For-example-of-house-prize-prediction-the-simplest-neural-network" class="headerlink" title="For example of house prize prediction : the simplest neural network"></a>For example of house prize prediction : the simplest neural network</h3><p>如果现在有六栋房子的信息，分别是房子的大小(size of house)和对应的价格(prize),绘制出如下的。自然的想法：线性回归，得到拟合的直线。值得注意的是，房价不可能是负数吧！因此下图中蓝色的线，大致就是我们所需要的函数。这个对应一个最简单神经网络（neural network）</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/housr_prize_1.png" alt=""></p>
<p>上述是一个tiny little neural network，更大的，更复杂的神经网络是</p>
<p>把很多最简单的single neural堆积(stacking)到一起。</p>
<h3 id="For-example-of-house-prize-prediction-stacking-the-neural"><a href="#For-example-of-house-prize-prediction-stacking-the-neural" class="headerlink" title="For example of house prize prediction : stacking the  neural"></a>For example of house prize prediction : stacking the  neural</h3><p>上面这个例子，仅仅考虑特征是size,实际情况上，与房屋相关的特征还有number of bedrooms、zip code、wealth, number of bedrooms and size affect family size. The zip code is a feature that tells you you know walkability. The wealth tells you how good is the school quality</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/house_prize_2.png" alt=""></p>
<p>hidden layer 用输入层计算得到，因此说输入层与中间层紧密连接起来了</p>
<h3 id="The-actual-application-of-neural-networks"><a href="#The-actual-application-of-neural-networks" class="headerlink" title="The actual application of neural networks"></a>The actual application of neural networks</h3><p>hidden layer 与上一层的连接情况并不是手工确定，每一层都是上一层所有的输入函数，所以建立的神经网络如下：</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/house prize 3.png" alt=""></p>
<p>The remarkable thing about neural network</p>
<ol>
<li>Given enough data about X&amp;Y (x,y) which good at freaking out functions :map x to y</li>
<li>Most powerful in supervised learning</li>
</ol>
<h3 id="C2W1CL03-Supervised-Learning-with-Neural-Network"><a href="#C2W1CL03-Supervised-Learning-with-Neural-Network" class="headerlink" title="C2W1CL03 : Supervised Learning with Neural Network"></a>C2W1CL03 : Supervised Learning with Neural Network</h3><h3 id="常见的监督学习"><a href="#常见的监督学习" class="headerlink" title="常见的监督学习"></a>常见的监督学习</h3><p>截止到目前，<strong>Neural Network的成功应用基本都在Supervised Learning</strong>。比如：Ad，Images vision, Audio to Text, Machine translation, Autonomous Driving</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/supervised-learning-exmples.png" alt=""></p>
<h3 id="常见的神经网络的设计"><a href="#常见的神经网络的设计" class="headerlink" title="常见的神经网络的设计"></a>常见的神经网络的设计</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/NeuralNetworkExamples.png" alt=""></p>
<p>卷积神经网络：<strong>Convolutional Neural Network</strong> (CNN) 通常有用图像数据</p>
<p>递归神经网络： <strong>Recurrent Neural Network</strong> (RNN) 通常用于time series</p>
<p>对应复杂的应用中，定制一些复杂的混合的神经网络结构</p>
<h3 id="结构化和非结构化数据"><a href="#结构化和非结构化数据" class="headerlink" title="结构化和非结构化数据"></a>结构化和非结构化数据</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/datastructure.png" alt=""></p>
<p>处理非结构化数据是很难的，与结构化数据比较，让计算机理解非结构化数据很难</p>
<h3 id="C1W1L04-Why-is-deep-learning-taking-off"><a href="#C1W1L04-Why-is-deep-learning-taking-off" class="headerlink" title="C1W1L04: Why is deep learning taking off"></a>C1W1L04: Why is deep learning taking off</h3><p>Answer: scale</p>
<p>If you want to hit this very high level of performance ,firstly, you need to be able train a big enough neural network in order to take advantage of the huge amount of data and second you need to be out here on the x axes you do need a lot of data.</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/scale.jpg" alt=""></p>
<ol>
<li>If you do not have a lot training data is often up to your skill at hand engineering features that determines the foreman.在这个小的训练集中，各种算法的优先级事实上定义的也不是很明确，所以如果你没有大量的训练集，那效果会取决于你的特征工程能力，那将决定最终的性能。</li>
<li>这个图形区域的左边，各种算法之间的优先级并不是定义的很明确，最终的性能更多的是取决于你在用<strong>工程选择特征</strong>方面的能力以及<strong>算法处理方面</strong>的一些细节.</li>
<li>只是在某些大数据规模非常庞大的训练集，也就是在右边这个会非常的大时，我们能更加持续地看到更大的由神经网络控制其它方法.</li>
</ol>
<h3 id="The-Reason"><a href="#The-Reason" class="headerlink" title="The Reason"></a>The Reason</h3><ol>
<li><p>the scale of data</p>
</li>
<li><p>the speed of computation  such as GPUS</p>
</li>
<li><p>innovation of algorithm </p>
<p>许多算法方面的创新，一直是在尝试着使得神经网络运行的更快</p>
<p>switch sigmoid function to relu function</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/algorithm——rul.jpg" alt=""></p>
</li>
</ol>
<p>在这个区域，也就是这个<strong>sigmoid</strong>函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做<strong>ReLU</strong>的函数（修正线性单元），<strong>ReLU</strong>它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。</p>
<p>训练你的神经网络的过程，很多时候是凭借直觉的，往往你对神经网络架构有了一个想法，于是你尝试写代码实现你的想法，然后让你运行一个试验环境来告诉你，你的神经网络效果有多好，通过参考这个结果再返回去修改你的神经网络里面的一些细节，然后你不断的重复上面的操作，当你的神经网络需要很长时间去训练，需要很长时间重复这一循环，在这里就有很大的区别，根据你的生产效率去构建更高效的神经网络。当你能够有一个想法，试一试，看效果如何。在10分钟内，或者也许要花上一整天，如果你训练你的神经网络用了一个月的时间，有时候发生这样的事情，也是值得的，因为你很快得到了一个结果。在10分钟内或者一天内，你应该尝试更多的想法，那极有可能使得你的神经网络在你的应用方面工作的更好、更快的计算，在提高速度方面真的有帮助，那样你就能更快地得到你的实验结果。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/faster.jpg" alt=""></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><font color="red">Summary</font></h2><font color="green">早上花了2h小时学习第一周的视频，先看一遍视频的字幕，逐字逐句的理解，虽然很多时候都是自己乱猜的，大概清楚讲的什么！然后再看大牛的笔记，然后再看一篇结合PPT。下午也看了半个多小时。问题：1. 自己的英文水平不够，这个需要大大的提高讷。2. 其实只要看别人的笔记就可以知道内容，但是还是想听andow ng的讲解。3. 视频都比较短，每个视频设计的知识点或者内容不多，1到3个，分成知识点做笔记还是不错的</font>

<font color="blue">这一周的内容，也就是今天我学习的知识简单和容易理解。学习了神经网络的大致结构，神经网络的应用领域，深度学习为什么取得快速的发展的三点原因，尤其是数据scale与其他方法和神经网络规模的大致性能关系</font>

<h1 id="C1W2"><a href="#C1W2" class="headerlink" title="C1W2"></a>C1W2</h1><h3 id="C1W2L01-Binary-Classification"><a href="#C1W2L01-Binary-Classification" class="headerlink" title="C1W2L01: Binary Classification"></a>C1W2L01: Binary Classification</h3><p>In this week, we’re going to go over the basics of neural network programming. We are going to study handle data without for loop.</p>
<p>forward password for propagation </p>
<p> backward pass or what’s called a backward propagation step</p>
<p>Why the computations in learning in a neural network can be organized in this board propagation and a separate backward propagation by using logistic regression to convey(传达) theses ideas.</p>
<h3 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h3><p>Input； an image . three separate matrices corresponding red green and blue color channels of this image. 如果你的图片大小为64x64像素，那么你就有三个规模为64x64的矩阵，分别对应图片中红、绿、蓝三种像素的强度值</p>
<p>unroll all of these pixel intensity values into  a feature vector </p>
<p>pixel intensity values of this image </p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/w_piexl.jpg" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/2_blue_green_read.jpg" alt=""></p>
<p>notation</p>
<p>(x,y)： a pair X comma Y</p>
<p>$M_{train}$: M subscript train</p>
<p>每条测试集在矩阵中都是以列向量的形式存在</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/2_noation.png" alt=""></p>
<p>Matrix capital</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/2_nation_2.png" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/2_all_nation.jpg" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/2_all_nation_1.jpg" alt=""></p>
<h3 id="Model-hypothesis-Function-Logistic-Regression"><a href="#Model-hypothesis-Function-Logistic-Regression" class="headerlink" title="Model : hypothesis Function :Logistic Regression"></a>Model : hypothesis Function :Logistic Regression</h3><p>So given an input X and the parameters W and b, how do we generate the output Y hat? Well, one thing you could try, that doesn’t work, would be to have Y hat be w transpose X plus B, kind of a linear function of the input X. And in fact, this is what you use if you were doing linear regression. But this isn’t a very good algorithm for binary classification because you want Y hat to be the chance that Y is equal to one. </p>
<p>So,Y hat should really be between zero and one. This is what the sigmoid function looks like. </p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/log_1.png" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/log_3.jpg" alt=""></p>
<h3 id="sigmoid-function"><a href="#sigmoid-function" class="headerlink" title="sigmoid function"></a>sigmoid function</h3><script type="math/tex; mode=display">
\sigma(z) = \frac{1}{1+e^{-z}}</script><p>因为你想让$\hat{y}$表示实际值$y$等于1的机率的话， 应该在0到1之间。这是一个需要解决的问题，因为可能比1要大得多，或者甚至为一个负值。对于你想要的在0和1之间的概率来说它是没有意义的，因此在逻辑回归中，我们的输出应该是等于由上面得到的线性函数式子作为自变量的<strong>sigmoid</strong>函数中，公式如上图最下面所示，将线性函数转换为非线性函数。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/log_2.jpg" alt=""></p>
<p>注意：原来$w,b$是分开在，这里就合并，引入变量$x_0=1$,对应偏置$b$,</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/log_3.png" alt=""></p>
<h3 id="Strategy：Cost-function"><a href="#Strategy：Cost-function" class="headerlink" title="Strategy：Cost function"></a>Strategy：Cost function</h3><p>Firstly : Loss function</p>
<script type="math/tex; mode=display">
L(\hat{y},y)=\frac{1}{2}\sum{(y_i-\hat{y_i})^2}</script><p>这个优化问题不是凸优化问题(non-convex)，因此不选用这个</p>
<p>Secondly，</p>
<script type="math/tex; mode=display">
L(y,\hat{y})=-(ylog^{\hat{y}}+(1-y)log^{1-\hat{y}})</script><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/log_cost_1.jpg" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/log__cost_2.jpg" alt=""></p>
<h3 id="Algorithm-Gradient-Descent"><a href="#Algorithm-Gradient-Descent" class="headerlink" title="Algorithm: Gradient Descent"></a>Algorithm: Gradient Descent</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/GD1.jpg" alt=""></p>
<p>Gradient Descent算法步骤：</p>
<ol>
<li>Initialize $w$, $b$ to zero</li>
<li>repeat：</li>
</ol>
<p>$w :=w−\alpha \frac{∂J(w,b)}{∂w}​$</p>
<p>$b :=b-\alpha \frac{∂J(w,b)}{∂b}$</p>
<h3 id="C1W2L05-amp-C1W2L06-Derivatives"><a href="#C1W2L05-amp-C1W2L06-Derivatives" class="headerlink" title="C1W2L05 &amp; C1W2L06 Derivatives"></a>C1W2L05 &amp; C1W2L06 Derivatives</h3><p>求导，这个是微积分的内容，不用写了！</p>
<h3 id="C1W2L07：-Computation-Graph"><a href="#C1W2L07：-Computation-Graph" class="headerlink" title="C1W2L07： Computation Graph"></a>C1W2L07： Computation Graph</h3><h3 id="C1W2L08-Derivatives-with-compution-graphs"><a href="#C1W2L08-Derivatives-with-compution-graphs" class="headerlink" title="C1W2L08 : Derivatives with compution graphs"></a>C1W2L08 : Derivatives with compution graphs</h3><p>链式法则</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial v}=\frac{\partial L}{\partial u}\frac{\partial u}{\partial v}</script><h3 id="C1W2L09-Logistic-Regression-Gradient-Descent"><a href="#C1W2L09-Logistic-Regression-Gradient-Descent" class="headerlink" title="C1W2L09 : Logistic Regression Gradient Descent"></a>C1W2L09 : Logistic Regression Gradient Descent</h3><h4 id="single-training-example"><a href="#single-training-example" class="headerlink" title="single training example"></a>single training example</h4><p>You’ve seen the loss function that measures how well you’re doing on the single training example. You’ve also seen the cost function that measures how well your parameters w and b are doing on your entire training set.</p>
<p>You’ve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives.</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/gd_1.jpg" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/gd_2.png" alt=""></p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w}=\frac{\partial L}{\partial \alpha }\frac{\partial \alpha }{\partial z}\frac{\partial z}{\partial w}
\\=-(\frac{y}{a}+\frac{1-y}{1-a})a(1-a)x=(a-y)x</script><h3 id="C1W2L10-Gradient-Descent-on-m-example"><a href="#C1W2L10-Gradient-Descent-on-m-example" class="headerlink" title="C1W2L10 Gradient Descent on m example"></a>C1W2L10 Gradient Descent on m example</h3><script type="math/tex; mode=display">
\min L(w,b)=\sum_{i=1}^{m}L(\alpha_i,y_i)/m\\
\frac{\partial L }{\partial w}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial w})/m=(\sum_{i=1}^{m}(a-y_i)x_i)/m\\
\frac{\partial L }{\partial b}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial b})/m=(\sum_{i=1}^{m}(a-1)x_i)/m</script><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/gd_3jpg.jpg" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/cost_3.png" alt=""></p>
<p>上面的伪代码告诉我们，需要多次for loop完成代码，但是这会造成运算速度下降！因为我们越来越多地训练非常大的数据集，因此你真的需要你的代码变得非常高效。所以在接下来的几个视频中，我们会谈到向量化，以及如何应用向量化而连一个<strong>for</strong>循环都不使用。所以学习了这些，我希望你有关于如何应用逻辑回归，或是用于逻辑回归的梯度下降，事情会变得更加清晰</p>
<h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><font color="red">今天主要学习了以logistics regression 为例，如何通过链式求导的过程，简单的练习一下，以及再次了解什么是梯度下降法，以及训练学习算法的需要一个损失函数，训练的过程就是求损失函数最优值的过程</font>

<h3 id="C1W2L11-Vectorization"><a href="#C1W2L11-Vectorization" class="headerlink" title="C1W2L11: Vectorization"></a>C1W2L11: Vectorization</h3><h4 id="1-什么是Vectorization：将-for-loop-尽可能转换为矩阵运算"><a href="#1-什么是Vectorization：将-for-loop-尽可能转换为矩阵运算" class="headerlink" title="1. 什么是Vectorization：将 for loop 尽可能转换为矩阵运算"></a>1. 什么是Vectorization：将 for loop 尽可能转换为矩阵运算</h4><p>通过<strong>numpy</strong>内置函数和避开显式的循环(<strong>loop</strong>)的方式进行向量化，从而有效提高代码速度。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/v_1.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.dot(a,b)</span><br><span class="line">如果a,b是一维数组，则计算点积</span><br><span class="line">如果a,b是多维数据，则矩阵乘法</span><br></pre></td></tr></table></figure>
<h4 id="2-An-example-of-vectorization"><a href="#2-An-example-of-vectorization" class="headerlink" title="2. An example of vectorization"></a>2. An example of vectorization</h4><p>vectorization的好处：conciser code, but faster execution 一个简单的对比实验：1,000,000大小的两个向量内积计算，for loop要比Vectorization快300倍。 在Deep Learning时代，vectorization是一项重要的技能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入numpy库</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment">#创建一个数据a</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># [1 2 3 4]</span></span><br><span class="line"><span class="keyword">import</span> time <span class="comment">#导入时间库</span></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>) <span class="comment">#通过round随机得到两个一百万维度的数组</span></span><br><span class="line">tic = time.time() <span class="comment">#现在测量一下当前时间</span></span><br><span class="line"><span class="comment">#向量化的版本</span></span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line">print(“Vectorized version:” + str(<span class="number">1000</span>*(toc-tic)) +”ms”) <span class="comment">#打印一下向量化的版本的时间</span></span><br><span class="line">​</span><br><span class="line"><span class="comment">#继续增加非向量化的版本</span></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(“For loop:” + str(<span class="number">1000</span>*(toc-tic)) + “ms”)<span class="comment">#打印for循环的版本的时间</span></span><br></pre></td></tr></table></figure>
<h4 id="3-GPU-or-CPU"><a href="#3-GPU-or-CPU" class="headerlink" title="3. GPU or CPU"></a>3. GPU or CPU</h4><ol>
<li><p>大规模的深度学习再<strong>GPU</strong>或者图像处理单元运行”，<strong>CPU</strong>和<strong>GPU</strong>都有并行化的指令，他们有时候会叫做<strong>SIMD</strong>指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了<strong>built-in</strong>函数,像<code>np.function</code>或者并不要求你实现循环的函数，它可以让<strong>python</strong>的充分利用并行化计算。</p>
</li>
<li><p>只是在<strong>GPU</strong>和<strong>CPU</strong>上面计算，<strong>GPU</strong>更加擅长<strong>SIMD</strong>计算，但是<strong>CPU</strong>事实上也不是太差，可能没有<strong>GPU</strong>那么擅长吧。SIMD Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data)</p>
</li>
</ol>
<h3 id="C12L12-：-More-Vectorization-Example"><a href="#C12L12-：-More-Vectorization-Example" class="headerlink" title="C12L12 ： More Vectorization Example"></a>C12L12 ： More Vectorization Example</h3><h3 id="矩阵和向量乘法"><a href="#矩阵和向量乘法" class="headerlink" title="矩阵和向量乘法"></a>矩阵和向量乘法</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/v_2.png" alt=""></p>
<h3 id="向量函数"><a href="#向量函数" class="headerlink" title="向量函数"></a>向量函数</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/v_3.png" alt=""></p>
<ol>
<li>原则：whenever possible, avoid explict for-loops</li>
<li>使用Element wised的矩阵运算，将函数作用在每个矩阵元素上，比如：<ul>
<li>np.exp()</li>
<li>np.log()</li>
<li>np.abs()</li>
<li>np.maxium()</li>
<li>1/v</li>
<li>v**2</li>
</ul>
</li>
</ol>
<h3 id="C1W2L13-Vectorizing-Logistic-Regression"><a href="#C1W2L13-Vectorizing-Logistic-Regression" class="headerlink" title="C1W2L13: Vectorizing Logistic Regression"></a>C1W2L13: Vectorizing Logistic Regression</h3><h3 id="1-前向传播"><a href="#1-前向传播" class="headerlink" title="1. 前向传播"></a>1. 前向传播</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/v_3.jpg" alt=""></p>
<script type="math/tex; mode=display">
\hat{y}=σ(w^TX+b)=(a(1),a(2),...,a(m−1),a(m))=\\
(\alpha(z_1),\alpha(z_m),...,\alpha(z_m))=\\
(\alpha(w^Tx_1+b),\alpha(w^Tx_2+b),...,\alpha(w^Tx_m+b))=</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">z=np.dot(W^T,X)+b</span><br><span class="line"><span class="comment"># z这里就是python 巧妙的地方，b是实数，但是向量加上实数后，b扩展成向量，被称为广播（brosdcasting）</span></span><br></pre></td></tr></table></figure>
<p>个人经验：</p>
<ol>
<li>首先，熟悉每个变量的记号和维度，必要的话，可以画出来，更直观。</li>
<li>先从一个样本做向量化，再把m个样本的操作向量化。</li>
<li>for-loop里面是循环乘法，则向量化一定是一个乘法形式，若对于不确定乘法的左右关系，是否需转置，可以根据目标变量的维度推测。或者先乘起来，再根据目标变量看是否要转置。</li>
</ol>
<h3 id="C1W2L14-Vectorzing-Logistic-Regression’s-Gradient-Compution"><a href="#C1W2L14-Vectorzing-Logistic-Regression’s-Gradient-Compution" class="headerlink" title="C1W2L14 : Vectorzing Logistic Regression’s Gradient Compution"></a>C1W2L14 : Vectorzing Logistic Regression’s Gradient Compution</h3><ol>
<li><p>backforwd</p>
</li>
<li><script type="math/tex; mode=display">
\frac{∂J}{∂w}=\frac{1}{m}X(A−Y)T\\
\frac{∂J}{∂b}=\frac{1}{m}(a(i)−y(i))</script></li>
</ol>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/v-4.jpg" alt=""></p>
<p>重要的是弄清楚，里面的行列关系，代表的意思，运算时候，先自己理清楚。还有点积、等等运算性质对应的操作，或者对应的内置函数</p>
<h3 id="C1W2L15-Broadcasting-in-Python"><a href="#C1W2L15-Broadcasting-in-Python" class="headerlink" title="C1W2L15: Broadcasting in Python"></a>C1W2L15: Broadcasting in Python</h3><h3 id="One-Example"><a href="#One-Example" class="headerlink" title="One Example"></a>One Example</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/bordacasing_4.png" alt=""></p>
<p><code>A.sum(axis = 0)</code>中的参数<code>axis</code>。<strong>axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行。</strong></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/boradcasing_5.png" alt=""></p>
<p>第二个<code>A/cal.reshape(1,4)</code>指令则调用了<strong>numpy</strong>中的广播机制。这里使用 3 by 4的矩阵除以1 by 4 的矩阵。技术上来讲，其实并不需要再将矩阵 <code>reshape</code>(重塑)成 ，因为矩阵本身已经是 了。但是当我们写代码时不确定矩阵维度的时候，通常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作<code>reshape</code>是一个常量时间的操作，时间复杂度是，它的调用代价极低。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/boradcasing_6.png" alt=""></p>
<h3 id="Secondly-Example"><a href="#Secondly-Example" class="headerlink" title="Secondly Example"></a>Secondly Example</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/bordacasing_2.png" alt=""></p>
<p>python的广播机制会将常数扩展成4by 1的列向量</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/boradcasing_3.png" alt=""></p>
<p>其实是将1by*n 的矩阵复制成为mbyn的矩阵</p>
<h3 id="广播机制的举例"><a href="#广播机制的举例" class="headerlink" title="广播机制的举例"></a>广播机制的举例</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/bordcasing_1.png" alt=""></p>
<h3 id="axis"><a href="#axis" class="headerlink" title="axis"></a>axis</h3><p>补充：numpy中，类似sum的函数，经常涉及axis参数，可以取值为0或1，甚至其他。经常记不住，这里我查了了一下，是这样的（<a href="https://stackoverflow.com/questions/17079279/how-is-axis-indexed-in-numpys-array" target="_blank" rel="noopener">原文</a>）：</p>
<ol>
<li><strong>axis的数字，和数组的shape参数的索引是对应的</strong>。比如一个数组的shape是(5,6)，则代表5个row，6个column。即在shape中，row和column的个数的索引是0和1。也就第1个坐标，在shape中的第一个元素，<strong>索引是0，代表row的方向</strong>；第2个坐标，在shape中的第2个元素，<strong>索引是1，代表row的方向</strong>。</li>
<li>对于sum函数，axis指的是sum“<strong>沿着</strong>”的方向，经过计算，这个方向的维度因为求和后就消失了，比如sum(axis=0)代表是沿着“row”方向进行求和，</li>
<li>当然axis可以是一个tupe，那就相当于沿着多个多个方向求和。</li>
<li>sum如果不传入axis参数，默认是对所有维度求和。</li>
</ol>
<h3 id="broadcasting"><a href="#broadcasting" class="headerlink" title="broadcasting"></a>broadcasting</h3><p>  当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播（broadcasting）。</p>
<p>三种广播情况</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/bordacasing.png" alt=""></p>
<h3 id="C1W2L16-A-Note-on-Python-numpy-vectors"><a href="#C1W2L16-A-Note-on-Python-numpy-vectors" class="headerlink" title="C1W2L16 A Note on Python/numpy vectors"></a>C1W2L16 A Note on Python/numpy vectors</h3><p>本节主要讲<strong>Python</strong>中的<strong>numpy</strong>一维数组的特性，以及与行向量或列向量的区别</p>
<h3 id="1-一维数组的特性"><a href="#1-一维数组的特性" class="headerlink" title="1. 一维数组的特性"></a>1. 一维数组的特性</h3><p>首先设置a = np.array.random.randn(5)，这样会生成存储在数组a中的5个高斯随机数变量。之后输出 ，从屏幕上可以得知，此时a 的<strong>shape</strong>（形状）是一个的结构。这在<strong>Python</strong>中被称作<strong>一个一维数组</strong>。它既不是一个行向量也不是一个列向量，这也导致它有一些不是很直观的效果。举个例子，如果我输出一个转置阵，最终结果它会和看起来一样，所以和的转置阵最终结果看起来一样。而如果我输出和的转置阵的内积，你可能会想：乘以的转置返回给你的可能会是一个矩阵。但是如果我这样做，你只会得到一个数。</p>
<p>所以我建议当你编写神经网络时，不要在使用的<strong>shape</strong>(5,1)是还是(n,)或者一维数组。相反，如果你设置(5,1)，那么这就是5行1列向量。在先前的操作里a和a的转置看起来一样，而现在这样的 a变成一个新的a 的转置，并且它是一个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出a 的转置时有两对方括号，而之前只有一对方括号，所以这就是1行5列的矩阵和一维数组的差别。</p>
<h3 id="2-行向量和列向量"><a href="#2-行向量和列向量" class="headerlink" title="2. 行向量和列向量"></a>2. 行向量和列向量</h3><p><strong>rank 1 array问题</strong>：shape是(x,)的数组，既不是行向量，也不是列向量，没法参与正常的矩阵运算，应该总是使用(x,1)或(1,x)的shape来表示向量。但可以通过reshape方法将rank 1 array转换为行向量或列向量。（什么是rank，就是一个数组的维度）一维的数组既不是行向量也不是列向量，转置后，依然是本身。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/note_1.jpg" alt=""></p>
<h3 id="3-解决方法"><a href="#3-解决方法" class="headerlink" title="3. 解决方法"></a>3. 解决方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape=（<span class="number">5</span>，<span class="number">1</span>)）</span><br><span class="line"><span class="comment"># 为了确保你的矩阵或向量所需要的维数时，不要羞于 reshape 操作</span></span><br></pre></td></tr></table></figure>
<h3 id="C1W2L18-：Quick-Tour-of-Jupyter-iPython-Notebooks"><a href="#C1W2L18-：Quick-Tour-of-Jupyter-iPython-Notebooks" class="headerlink" title="C1W2L18 ：Quick Tour of Jupyter/iPython Notebooks"></a>C1W2L18 ：Quick Tour of Jupyter/iPython Notebooks</h3><h3 id="C1W2L18-Explanation-of-Logistic-Regression-Cost-Function"><a href="#C1W2L18-Explanation-of-Logistic-Regression-Cost-Function" class="headerlink" title="C1W2L18: Explanation of Logistic Regression Cost Function"></a>C1W2L18: Explanation of Logistic Regression Cost Function</h3><p>对应logistic regression，输出$\hat{y}=p(y=1|x)$,那么$p(y=0|x)=1-\hat{y}$</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/cost_2png.png" alt=""></p>
<p>综合上面</p>
<script type="math/tex; mode=display">
p(y|x)= \hat{y}^y*(1-\hat{y})^{1-y}</script><p>对于整个训练集，</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/cost_1png.png" alt=""></p>
<p>假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积:</p>
<script type="math/tex; mode=display">
p(labels \ in\  training\  set)=\Pi_{i=1}^mp(y_i|x_i)</script><p>如果利用极大似然法做，找到一组参数，使得样本观测值概率最大</p>
<script type="math/tex; mode=display">
\max log p(label \ in \ training \ set)=log \Pi_{i=1}^mp(y_i|x_i)=\sum -L(\hat{y^i},y^i)</script><script type="math/tex; mode=display">
\min cost J(w,b)=\frac{1}{m}L(\hat{y^i},y^i)</script><p>总结一下，为了最小化成本函数，我们从<strong>logistic</strong>回归模型的最大似然估计的角度出发，假设训练集中的样本都是独立同分布的条件下</p>
<h3 id="Day3-summary"><a href="#Day3-summary" class="headerlink" title="Day3 : summary"></a>Day3 : summary</h3><font color="red">主要学习了python编程的如何才能高效率，内置函数的具有并行性，simd指令，以及一维数组的使用注意事项，logistic regression的lost function的原理证明</font>

<h1 id="C1W3"><a href="#C1W3" class="headerlink" title="C1W3"></a>C1W3</h1><h2 id="C1W3L01-Neural-Network-Overview"><a href="#C1W3L01-Neural-Network-Overview" class="headerlink" title="C1W3L01 : Neural Network Overview"></a>C1W3L01 : Neural Network Overview</h2><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_2.png" alt=""></p>
<p>许多<strong>sigmoid</strong>单元堆叠起来形成一个神经网络。</p>
<p>正向传播：输入层到layer one</p>
<script type="math/tex; mode=display">
\left.\begin{array}{c}{x} \\ {W^{[1]}} \\ {b^{[1]}}\end{array}\right\} \Longrightarrow z^{[1]}=W^{[1]} x+b^{[1]} \Longrightarrow a^{[1]}=\sigma\left(z^{[1)}\right)</script><p>layer one 到layer two</p>
<script type="math/tex; mode=display">
\left.\begin{array}{r}{a^{(1]}=\sigma\left(z^{[1]}\right)} \\ {W^{[2]}} \\ {b^{[2]}}\end{array}\right\}\begin{array}{l}{\Longrightarrow z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \Longrightarrow a^{[2]}=\sigma\left(z^{[2]}\right)} \\ {\Longrightarrow L\left(a^{[2]}, y\right)}\end{array}</script><p>反向传播</p>
<script type="math/tex; mode=display">
\left.\begin{array}{r}{d a^{[1]}=d \sigma\left(z^{[1]}\right)} \\ {d W^{[2]}} \\ {d b^{[2]}}\end{array}\right\}\begin{array}{l}{\Longleftarrow d z^{[2]}=d\left(W^{[2]} \alpha^{[1]}+b^{[2]}\right) \Longleftarrow d a^{[2]}=d \sigma\left(z^{[2]}\right)} \\ {\Longleftarrow d L\left(a^{[2]}, y\right)}\end{array}</script><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_3.png" alt=""></p>
<p>$W$的行数是本次结点个数，列数是上层节点个数</p>
<h2 id="C1W3L02-Nerual-Network-Representations"><a href="#C1W3L02-Nerual-Network-Representations" class="headerlink" title="C1W3L02 : Nerual Network Representations"></a>C1W3L02 : Nerual Network Representations</h2><p>符号说明</p>
<h2 id="C1W3L03：-Computation-Neural-Network-Output"><a href="#C1W3L03：-Computation-Neural-Network-Output" class="headerlink" title="C1W3L03： Computation Neural Network Output"></a>C1W3L03： Computation Neural Network Output</h2><h3 id="A-simple-training-examples"><a href="#A-simple-training-examples" class="headerlink" title="A simple training examples"></a>A simple training examples</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_5.png" alt=""></p>
<p>其中，x表示输入特征，a表示每个神经元的输出，W表示特征的权重，上标表示神经网络的层数（隐藏层为1），下标表示该层的第几个神经元。这是神经网络的<strong>符号惯例</strong>，下同。</p>
<p><strong>神经网络的计算</strong></p>
<p>关于神经网络是怎么计算的，从我们之前提及的逻辑回归开始，如下图所示。用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先你按步骤计算出，然后在第二步中你以<strong>sigmoid</strong>函数为激活函数计算（得出），一个神经网络只是这样子做了好多次重复计算。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_6.png" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_4.png" alt=""></p>
<p>说明：$w_i^{[1]}$和$W^{[1]}$的关系，一个按照logistic regression ，一个是矩阵表示。</p>
<p><strong>向量化计算</strong> 如果你执行神经网络的程序，用for循环来做这些看起来真的很低效。所以接下来我们要做的就是把这四个等式向量化。向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的纵向堆积起来变成一个(4,3)的矩阵，用符号$W^{[1]}$表示。另一个看待这个的方法是我们有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的参数——向量，把这四个向量堆积在一起，你会得出这4×3的矩阵。</p>
<script type="math/tex; mode=display">
z^{[n]}=W^{[n]}X+b^{[n]}</script><script type="math/tex; mode=display">
a^{[1]}=\left[ \begin{array}{c}{a_{1}^{[1]}} \\ {a_{2}^{[1]}} \\ {a_{3}^{[1]}} \\ {a_{4}^{[1]}}\end{array}\right]=\sigma\left(z^{[1]}\right)</script><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_7.png" alt=""></p>
<p>Given input X（a single training set)</p>
<script type="math/tex; mode=display">
\begin{array}{c}{z^{[1]}=W^{[1]} a^{[0]}+b^{[1]}} \\ {a^{[1]}=\sigma\left(z^{[1]}\right)} \\ {z^{[2]}=W^{[2]} a^{[1]}+b^{[2]}} \\ {a^{[2]}=\sigma\left(z^{[2]}\right)}\end{array}</script><p>说明：</p>
<p>$W$的第$i$行表示，当前层到上一层的权重行向量，再计算单个的时候，由于是按照logristics regression的方式，所以认为$w_i$是列向量，所以转置成行向量。上面的图也说明了：如何从单个操作到矩阵操作，权重矩阵是怎么构造，怎么表示的。</p>
<p>b是列向量。</p>
<h2 id="C1W3L04-Vectorizing-Across-Mutilple-Example"><a href="#C1W3L04-Vectorizing-Across-Mutilple-Example" class="headerlink" title="C1W3L04: Vectorizing Across Mutilple Example"></a>C1W3L04: Vectorizing Across Mutilple Example</h2><p>Different training examples in different columns of the matrix </p>
<ol>
<li><p>for loop</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_8.png" alt=""></p>
</li>
<li><p>vectorizing : stacking training set in columns</p>
<script type="math/tex; mode=display">
x=\left[ \begin{array}{cccc}{\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {x^{(1)}} & {x^{(2)}} & {\dots} & {x} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots}\end{array}\right]</script></li>
</ol>
<p>就有</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}{A^{[1]}=\sigma\left(z^{[1]}\right)} \\ {z^{[2]}=W^{[2]} A^{[1]}+b^{[2]}} \\ {A^{[2]}=\sigma\left(z^{[2]}\right)}\end{array}\right.</script><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_9.png" alt=""></p>
<p>当垂直扫描，是索引到隐藏单位的数字。当水平扫描，将从第一个训练示例中从第一个隐藏的单元到第二个训练样本，第三个训练样本……直到节点对应于第一个隐藏单元的激活值，且这个隐藏单元是位于这个训练样本中的最终训练样本。</p>
<p>从水平上看，矩阵代表了各个训练样本。从竖直上看，矩阵的不同的索引对应于不同的隐藏单元。</p>
<h2 id="C1W3L05-Explanation-for-vectorized-implement"><a href="#C1W3L05-Explanation-for-vectorized-implement" class="headerlink" title="C1W3L05 : Explanation for vectorized implement"></a>C1W3L05 : Explanation for vectorized implement</h2><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_10.png" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_11.png" alt=""></p>
<h2 id="C1W3L06-Activation-Function"><a href="#C1W3L06-Activation-Function" class="headerlink" title="C1W3L06 : Activation Function"></a>C1W3L06 : Activation Function</h2><p>在讨论优化算法时，有一点要说明：基本已经不用<strong>sigmoid</strong>激活函数了，<strong>tanh</strong>函数在所有场合都优于<strong>sigmoid</strong>函数。</p>
<p><strong>sigmoid</strong>函数和<strong>tanh</strong>函数两者共同的缺点是，在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。</p>
<p>在机器学习另一个很流行的函数是：修正线性单元的函数（<strong>ReLu</strong>），<strong>ReLu</strong>函数图像是如下图。$ a = max(0,z)$：  所以，只要是正值的情况下，导数恒等于1，当是负值的时候，导数恒等于0。从实际上来说，当使用的导数时，=0的导数是没有定义的。但是当编程实现的时候，的取值刚好等于0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，是等于0的时候，假设一个导数是1或者0效果都可以。</p>
<p>如果输出是0、1值（二分类问题），则输出层选择<strong>sigmoid</strong>函数，然后其它的所有单元都选择<strong>Relu</strong>函数。</p>
<p>这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用<strong>Relu</strong>激活函数。有时，也会使用<strong>tanh</strong>激活函数，但<strong>Relu</strong>的一个缺点是：当z是负值的时候，导数等于0。</p>
<p>这里也有另一个版本的<strong>Relu</strong>被称为<strong>Leaky Relu</strong>。</p>
<p>当是负值时，这个函数的值不是等于0，而是轻微的倾斜。</p>
<p>两者的优点是：</p>
<p>第一，在的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个<strong>if-else</strong>语句，而<strong>sigmoid</strong>函数需要进行浮点四则运算，在实践中，使用<strong>ReLu</strong>激活函数神经网络通常会比使用<strong>sigmoid</strong>或者<strong>tanh</strong>激活函数学习的更快。</p>
<p>第二，<strong>sigmoid</strong>和<strong>tanh</strong>函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而<strong>Relu</strong>和<strong>Leaky ReLu</strong>函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，<strong>Relu</strong>进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而<strong>Leaky ReLu</strong>不会有这问题)</p>
<p>在<strong>ReLu</strong>的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_12.png" alt=""></p>
<p><strong>sigmoid</strong>激活函数：除了输出层是一个二分类问题基本不会用它。</p>
<p><strong>tanh</strong>激活函数：<strong>tanh</strong>是非常优秀的，几乎适合所有场合。</p>
<p><strong>ReLu</strong>激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用<strong>ReLu</strong>或者<strong>Leaky ReLu</strong>。</p>
<p>通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<h2 id="C1W3L07-Why-non-linear-activation-Functions"><a href="#C1W3L07-Why-non-linear-activation-Functions" class="headerlink" title="C1W3L07 : Why non-linear activation Functions"></a>C1W3L07 : Why non-linear activation Functions</h2><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_13.png" alt=""></p>
<p>通过推导可以得出，如果使用线性激活函数，相当于没有隐藏层。无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。当当然，在output layer是可以不用activation function，或者用linear activation function；这种情况一般是要求输出实数集结果（比如预测房价）。即便如此，在hidden layer还是要用non-linear activation function。</p>
<h3 id="sigmoid-activation-function"><a href="#sigmoid-activation-function" class="headerlink" title="sigmoid activation function"></a>sigmoid activation function</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_14.png" alt=""></p>
<script type="math/tex; mode=display">
\frac{d}{d z} g(z)=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)=g(z)(1-g(z))</script><h3 id="tanh-activation-function"><a href="#tanh-activation-function" class="headerlink" title="tanh activation function"></a>tanh activation function</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_15.png" alt=""></p>
<script type="math/tex; mode=display">
g(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{x}+e^{-z}}</script><script type="math/tex; mode=display">
\frac{d}{d z} g(z)=1-(\tanh (z))^{2}</script><h3 id="Rectified-linear-unit-RelU"><a href="#Rectified-linear-unit-RelU" class="headerlink" title="Rectified linear unit(RelU)"></a>Rectified linear unit(RelU)</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_16.png" alt=""></p>
<script type="math/tex; mode=display">
g(z)^{\prime}=\left\{\begin{array}{ll}{0} & {\text { if } z<0} \\ {1} & {\text { if } z>0} \\ {\text {undefined}} & {\text { if } z=0}\end{array}\right.</script><p>注：通常在z= 0的时候给定其导数1,0；当然=0的情况很少</p>
<h3 id="Leaky-linear-unit-Leaky-ReLU"><a href="#Leaky-linear-unit-Leaky-ReLU" class="headerlink" title="Leaky linear unit (Leaky ReLU)"></a><strong>Leaky linear unit (Leaky ReLU)</strong></h3><script type="math/tex; mode=display">
g(z)=\max (0.01 z, z)</script><script type="math/tex; mode=display">
g(z)^{\prime}=\left\{\begin{array}{ll}{0.01} & {\text { if } z<0} \\ {1} & {\text { if } z>0} \\ {\text {undefined}} & {\text { if } z=0}\end{array}\right.</script><p>注：通常在的z=0时候给定其导数1,0.01；当然的情况很少。</p>
<h2 id="C1W3L09-Gradient-Descent-For-Neural-Networks"><a href="#C1W3L09-Gradient-Descent-For-Neural-Networks" class="headerlink" title="C1W3L09 : Gradient Descent For Neural Networks"></a>C1W3L09 : Gradient Descent For Neural Networks</h2><ol>
<li><p>gradient descent的关键是求cost function对参数的偏导数</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_17.png" alt=""></p>
</li>
<li><p>求导过程使用的是Backpropagation</p>
<ol>
<li><p>首先做forward propagation，求解出每一层的输出A</p>
<script type="math/tex; mode=display">
(1) z^{[1]}=W^{[1]} x+b^{[1]}\\
(2) a^{[1]}=\sigma\left(z^{[1]}\right)\\(3) z^{[2]}=W^{[2]}=W^{[2]} a^{[1]}+b^{[2]}\\(4) a^{[2]}=g^{[2]}\left(z^{[z]}\right)=\sigma\left(z^{[2]}\right)</script></li>
<li><p>然后向后，逐层求解对每一层参数的偏导数</p>
</li>
</ol>
</li>
</ol>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_18.png" alt=""></p>
<p>sum，<code>keepdims</code>是防止<strong>python</strong>输出那些古怪的秩数(n,)，加上这个确保阵矩阵这个向量输出的维度为(n,1）这样标准的形式。</p>
<h2 id="C1WL10-Backpropagation-intuition-optional"><a href="#C1WL10-Backpropagation-intuition-optional" class="headerlink" title="C1WL10: Backpropagation intuition (optional)"></a>C1WL10: Backpropagation intuition (optional)</h2><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_19.png" alt=""></p>
<p>实现后向传播有个技巧，就是要保证矩阵的维度相互匹配</p>
<p>其实，对于一个神经元，输入部分：是权重和上一层输出的线性组合；输出：激活函数作用于输入，因此对$W$求偏导时，对激活函数求一次，再对线性组合求一次。对$b$求偏导是，对线性部分求偏导是1,这里用求和。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_20.png" alt=""></p>
<h2 id="C1W3L11-Random-Initialization"><a href="#C1W3L11-Random-Initialization" class="headerlink" title="C1W3L11: Random Initialization"></a>C1W3L11: Random Initialization</h2><p>`</p>
<p>与logistic regression不同，初始化参数不可固定为0，而是每个参数都要随机初始化。</p>
<p>主要原因是：<strong>如果每个参数w和b都是0，则同一层的每个neuron计算结果完全一样</strong>（输入一样a，参数一样w，则z一样,<strong>symmetry breaking problem</strong>）；接下来反向传播时的偏导数也一样，下一轮迭代同一层的每个neuron的w又是一样的。这样整个neural Network上每一层的neuron是同质的，自然不会有好的performance。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week3_13 (1" alt="">.png)</p>
<p>不过，对b参数，可以都初始化为0。</p>
<p>另外需要注意，虽然w是随机初始化，但最好使用较小的随机数。主要是避免让z的计算值过大，导致activation function对z的偏导数趋于0，导致Gradient descent下降较慢。 通常的做法是对random的值乘以一个比率，比如0.01（但具体怎么选这个比率，也要根据情况而定，这应该又是一个超参了）：</p>
<p>$W[1]=np.random.randn((2,2))∗0.01$</p>
<p>因为如果你用<strong>tanh</strong>或者<strong>sigmoid</strong>激活函数，或者说只在输出层有一个<strong>Sigmoid</strong>，如果（数值）波动太大，当你计算激活值时如果很大，就会很大或者很小，因此这种情况下你很可能停在<strong>tanh</strong>/<strong>sigmoid</strong>函数的平坦的地方，这些地方梯度很小也就意味着梯度下降会很慢，因此学习也就很慢。</p>
<p>事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能要试试0.01以外的常数。</p>
<h2 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h2><font color="red">如何建立一个一层的神经网络了，初始化参数，用前向传播预测，还有计算导数，结合反向传播用在梯度下降中。</font>

<font color="blue">

1. Define the neural network structure ( # of input units, # of hidden units, etc).
2. Initialize the model's parameters

1. Loop:
   - Implement forward propagation
   - Compute loss
   - Implement backward propagation to get the gradients
   - Update parameters (gradient descent)

</font>

<h1 id="C1W4"><a href="#C1W4" class="headerlink" title="C1W4"></a>C1W4</h1><h2 id="C1W4L01-Deep-layer-neural-network"><a href="#C1W4L01-Deep-layer-neural-network" class="headerlink" title="C1W4L01 Deep-layer neural network"></a>C1W4L01 Deep-layer neural network</h2><h3 id="1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network"><a href="#1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network" class="headerlink" title="1. logistics regression and shallow neural network and deep-layer neural network"></a>1. logistics regression and shallow neural network and deep-layer neural network</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_1.png" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_2.png" alt=""></p>
<h3 id="2-notation"><a href="#2-notation" class="headerlink" title="2. notation"></a>2. notation</h3><p>神经网络模型</p>
<script type="math/tex; mode=display">
\begin{array}{l}{X \in \mathbb{R}^{n_{x} \times m}} 代表输入的矩阵\\{x^{(i)} \in \mathbb{R}^{n_{x}}} 代表第 i 个样本的列向量\\
{Y \in \mathbb{R}^{n_{y} \times n}} 标记矩阵\\ {y^{(i)} \in \mathbb{R}^{n_{v}}}是第i样本的输出标签\\
W^{[l]} \in \mathbb{R}^{l \times(l-1)}代表第[l]层的权重矩阵\\ b^{[l]} \in \mathbb{R}^{l}代表第[l]层的偏差矩阵\\
 {\hat{y}^{(i)} \in \mathbb{R}^{n_{v}}}是预测输出向量\end{array}</script><script type="math/tex; mode=display">
通用激活公式：
a_{j}^{[l]}=g^{[l]}\left(z_{j}^{[l]}\right)=g^{ | l ]}\left(\sum_{k} w_{j k}^{[l]} a_{k}^{[l-1]}+b_{j}^{[l]}\right)</script><h2 id="C1W4L02：-Forward-and-Backward-propagation"><a href="#C1W4L02：-Forward-and-Backward-propagation" class="headerlink" title="C1W4L02： Forward and Backward propagation"></a>C1W4L02： Forward and Backward propagation</h2><h3 id="1-forward-propagation"><a href="#1-forward-propagation" class="headerlink" title="1. forward propagation"></a>1. forward propagation</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_3.png" alt=""></p>
<h3 id="2-backward-propagation"><a href="#2-backward-propagation" class="headerlink" title="2. backward propagation"></a>2. backward propagation</h3><script type="math/tex; mode=display">
\begin{array}{l}{d z^{[l]}=d a^{[l]} * g^{[l]}\left(z^{l l}\right)} \\ {d w^{[l]}=d z^{[l]} \cdot a^{[l-1]}}\\d b^{[l]}=d z^{[l]}\\
d a^{[l-1]}=w^{[l]} \cdot d z^{[l]}\\
d z^{[l]}=w^{[l+1] T} d z^{[l+1]} \cdot g^{[l]^{\prime}}\left(z^{[l]}\right)\end{array}</script><p>向量化</p>
<script type="math/tex; mode=display">
\begin{array}{l}{d Z^{[l]}=d A^{[l]} * g^{[l]}\left(Z^{[l]}\right)} \\ {d W^{[l]}=\frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T}}\\
\begin{array}{l}{d b^{[l]}=\frac{1}{m} n p \cdot \operatorname{sum}\left(d z^{[l]}, \text { axis }=1, \text {keepdims}=\text {True}\right)} \\ {d A^{[l-1]}=W^{[l] T} \cdot d Z^{[l]}}\end{array}\end{array}</script><p>summary</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_5.png" alt=""></p>
<h2 id="C1W4L03-Forward-Propagation-in-d-deep-network"><a href="#C1W4L03-Forward-Propagation-in-d-deep-network" class="headerlink" title="C1W4L03 : Forward Propagation in d deep network"></a>C1W4L03 : Forward Propagation in d deep network</h2><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_6.png" alt=""></p>
<p>这里只能用一个显式<strong>for</strong>循环，从1到，然后一层接着一层去计算。</p>
<h2 id="C1W4L04-Getting-matrix-dimension-right"><a href="#C1W4L04-Getting-matrix-dimension-right" class="headerlink" title="C1W4L04 Getting matrix dimension right"></a>C1W4L04 Getting matrix dimension right</h2><p>当实现深度神经网络的时候，其中一个常用的检查代码是否有错的方法就是拿出一张纸过一遍算法中矩阵的维数。</p>
<p>$d_w^{[l]}$和$w^{[l]}$维度相同，$db^{[l]}$和$b^{[l]}$维度相同，且w和b向量化维度不变，但z,a以及x的维度会向量化后发生变化。</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_8.png" alt=""></p>
<p>反向传播的维数检查</p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_7.png" alt=""></p>
<p>在你做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大大提高代码通过率。</p>
<h2 id="C1W4L05-Why-deep-representations"><a href="#C1W4L05-Why-deep-representations" class="headerlink" title="C1W4L05 Why deep representations?"></a>C1W4L05 Why deep representations?</h2><p>神经网络不需要很大，但是得有深度，也就是隐藏层需要很多，</p>
<h3 id="1-for-example-of-face-detector"><a href="#1-for-example-of-face-detector" class="headerlink" title="1. for example of face detector"></a>1. for example of face detector</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_9.png" alt=""></p>
<h2 id="C1W4L06-Building-blocks-of-a-deep-neural-network"><a href="#C1W4L06-Building-blocks-of-a-deep-neural-network" class="headerlink" title="C1W4L06 :Building blocks of a deep neural network"></a>C1W4L06 :Building blocks of a deep neural network</h2><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_10.png" alt=""></p>
<p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_11.png" alt=""></p>
<p>可以看得出，再反向传播的时候，需要用到$Z^{[L]},W^{[L]},b^{[L]}$,因此cash them</p>
<p>正向传播：$Z^{[1]},A^{[1]}…………$,反向传播：$dA^{[L]},dZ{[L]},dW^{[L]}dB^{[L]},dA^{[L-1]}$</p>
<h2 id="C1W4L07：Parameters-vs-Hyperparameters"><a href="#C1W4L07：Parameters-vs-Hyperparameters" class="headerlink" title="C1W4L07：Parameters vs Hyperparameters"></a>C1W4L07：Parameters vs Hyperparameters</h2><h3 id="1-What"><a href="#1-What" class="headerlink" title="1 What"></a>1 What</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_12.png" alt=""></p>
<h3 id="2-How"><a href="#2-How" class="headerlink" title="2 How"></a>2 How</h3><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/L1_week4_13.png" alt=""></p>
<p><strong>Idea—Code—Experiment—Idea</strong>这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代</p>
<p>今天的深度学习应用领域，还是很经验性的过程，通常你有个想法，比如你可能大致知道一个最好的学习率值，可能说最好，我会想先试试看，然后你可以实际试一下，训练一下看看效果如何。然后基于尝试的结果你会发现，你觉得学习率设定再提高到0.05会比较好。如果你不确定什么值是最好的，你大可以先试试一个学习率，再看看损失函数J的值有没有下降。然后你可以试一试大一些的值，然后发现损失函数的值增加并发散了。然后可能试试其他数，看结果是否下降的很快或者收敛到在更高的位置。你可能尝试不同的并观察损失函数这么变了，试试一组值，然后可能损失函数变成这样，这个值会加快学习过程，并且收敛在更低的损失函数值上（箭头标识），我就用这个值了。</p>
<p>在前面几页中，还有很多不同的超参数。然而，当你开始开发新应用时，预先很难确切知道，究竟超参数的最优值应该是什么。所以通常，你必须尝试很多不同的值，并走这个循环，试试各种参数。试试看5个隐藏层，这个数目的隐藏单元，实现模型并观察是否成功，然后再迭代。这页的标题是，应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。</p>
<p>所以我经常建议人们，特别是刚开始应用于新问题的人们，去试一定范围的值看看结果如何。然后下一门课程，我们会用更系统的方法，用系统性的尝试各种超参数取值。然后其次，甚至是你已经用了很久的模型，可能你在做网络广告应用，在你开发途中，很有可能学习率的最优数值或是其他超参数的最优值是会变的，所以即使你每天都在用当前最优的参数调试你的系统，你还是会发现，最优值过一年就会变化，因为电脑的基础设施，<strong>CPU</strong>或是<strong>GPU</strong>可能会变化很大。所以有一条经验规律可能每几个月就会变。如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。</p>
<p>有一条经验规律：经常试试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的直觉。</p>
<p>总结：超参数的设定，靠经验，尝试，并调，根据结果调，</p>
<h2 id="C1W4L08-What-does-this-have-to-do-with-the-brain"><a href="#C1W4L08-What-does-this-have-to-do-with-the-brain" class="headerlink" title="C1W4L08 : What does this have to do with the brain?"></a>C1W4L08 : What does this have to do with the brain?</h2><h1 id="summary-forward-prop-and-back-prop"><a href="#summary-forward-prop-and-back-prop" class="headerlink" title="# summary : forward prop and back prop"></a># summary : forward prop and back prop</h1><h2 id="1-logistics-regression-shallow-neural-network-and-deep-neural-network"><a href="#1-logistics-regression-shallow-neural-network-and-deep-neural-network" class="headerlink" title="1. logistics regression,shallow neural network and deep neural network"></a>1. logistics regression,shallow neural network and deep neural network</h2><p>logistics regression</p>
<script type="math/tex; mode=display">
Z = W^TX+B\\
A = \frac{1}{1+e^{-Z}}\\
L(A,Y)=-\frac{1}{m}(Ylog^A+(1-Y)log^{1-A}\\
\frac{\partial L}{\partial Z}=(A-Y)\\
\frac{\partial L}{\partial W}=X(A-Y)\\</script><p>说明：X是样本按列堆积，W是列向量</p>
<p>shallow neural network 以二分问题为例</p>
<script type="math/tex; mode=display">
Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}\\
A^{[1]}=g^{[1]}(Z^{[1]})\\
\ \\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=g^{[2]}(Z^{[2]})\\
\ \ \\
\ \\
L(A^{[2]},Y)=-\frac{1}{m}(Ylog^{A}+(1-Y)log^{1-A})\\
\frac{\partial L}{\partial Z^{[2]}}=(A^{[2]}-Y)\\
\frac{\partial L}{\partial W^{[2]}}=(A^{[2]}-Y)A^{[1]^T}\\
\frac{\partial L}{\partial b^{[2]}}=(A^{[2]}-Y)1_{1*m}^T\\
\frac{\partial L}{\partial a^{[1]}}=W^{[2]^T}(A^{[2]}-Y)\\
\ \\
\frac{\partial L}{\partial Z^{[1]}}=W^{[2]^T}(A^{[2]}-Y)* g^{'[1]}(Z^{[1]})\\</script><p>说明：W是按列排$W^{[L]}$是$n^{[L]}*n^{[L-1]}$矩阵，A,Z是按列堆积，记得检查矩阵维数就好了</p>
<h2 id="deep-neural-network"><a href="#deep-neural-network" class="headerlink" title="deep neural network"></a>deep neural network</h2><script type="math/tex; mode=display">
Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\
A^{[l]}=g^{[l]}(Z^{[l]})\\
\ \ \\
\ \\
\frac{\partial L}{\partial Z^{[l]}}=\partial A^*g^{'[l]}(Z^{l})\\
\frac{\partial L}{\partial W^{[l]}}=\partial Z^{[l]} A^{[1-1]^T}\\
\frac{\partial L}{\partial b^{[l]}}=\partial Z^{[l]}\\
\frac{\partial L}{\partial a^{[l-1]}}=W^{[l]^T}\partial Z^{[l]}\\
\ \\
\frac{\partial L}{\partial Z^{[l-1]}}=W^{[l]^T}\partial Z^{[l]}* g^{'[l-1]}(Z^{[l-1]})\\</script><h2 id="2-vectorization"><a href="#2-vectorization" class="headerlink" title="2. vectorization"></a>2. vectorization</h2><ol>
<li>推导的时候要向量化，注意矩阵维数表示，可以从单个推导到mutli</li>
<li>充分利用python的广播属性，和内置函数的并行化</li>
<li>python一维，二维数组的特性</li>
</ol>
<h2 id="3-知识结构"><a href="#3-知识结构" class="headerlink" title="3. 知识结构"></a>3. 知识结构</h2><p><img src="/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/C1.png" alt=""></p>

      
    </div>

    

    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author: </strong>XieMay</li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://xiemaycherry.github.io/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/" title="Deep Learning Neural Network and Deep Learning">http://xiemaycherry.github.io/2019/04/11/Deep Learning.ai_Neural Networks and Deep Learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li>
</ul>

      </div>
    
<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

  
</div>
    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-learning/" rel="tag"># Deep learning</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/11/deep-learning-ai深度学习笔记/" rel="next" title="deep_learning.ai深度学习笔记<Andrew Ng>">
                <i class="fa fa-chevron-left"></i> deep_learning.ai深度学习笔记<andrew ng="">
              </andrew></a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/11/tensorflow/" rel="prev" title="tensorflow">
                tensorflow <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">XieMay</p>
              <p class="site-description motion-element" itemprop="description">CAT_cat</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">45</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">36</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="http://www.cnblogs.com/shiyiandchuixue/" target="_blank" title="BLOGS"><i class="fa fa-fw fa-globe"></i>BLOGS</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:2323020965@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://github.com/shiyichuixue" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Course-one-Neural-Networks-and-Deep-Learning-Course-1-of-the-Deep-Learning-Specialization"><span class="nav-number">1.</span> <span class="nav-text">Course one :  Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#C1W1"><span class="nav-number">2.</span> <span class="nav-text">C1W1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W1L01-Welcome"><span class="nav-number">2.1.</span> <span class="nav-text">C1W1L01: Welcome</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W1L02-What-is-Neural-Network"><span class="nav-number">2.2.</span> <span class="nav-text">C1W1L02 : What is Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#For-example-of-house-prize-prediction-the-simplest-neural-network"><span class="nav-number">2.2.1.</span> <span class="nav-text">For example of house prize prediction : the simplest neural network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#For-example-of-house-prize-prediction-stacking-the-neural"><span class="nav-number">2.2.2.</span> <span class="nav-text">For example of house prize prediction : stacking the  neural</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-actual-application-of-neural-networks"><span class="nav-number">2.2.3.</span> <span class="nav-text">The actual application of neural networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C2W1CL03-Supervised-Learning-with-Neural-Network"><span class="nav-number">2.2.4.</span> <span class="nav-text">C2W1CL03 : Supervised Learning with Neural Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见的监督学习"><span class="nav-number">2.2.5.</span> <span class="nav-text">常见的监督学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见的神经网络的设计"><span class="nav-number">2.2.6.</span> <span class="nav-text">常见的神经网络的设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结构化和非结构化数据"><span class="nav-number">2.2.7.</span> <span class="nav-text">结构化和非结构化数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W1L04-Why-is-deep-learning-taking-off"><span class="nav-number">2.2.8.</span> <span class="nav-text">C1W1L04: Why is deep learning taking off</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Reason"><span class="nav-number">2.2.9.</span> <span class="nav-text">The Reason</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">2.3.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#C1W2"><span class="nav-number">3.</span> <span class="nav-text">C1W2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L01-Binary-Classification"><span class="nav-number">3.0.1.</span> <span class="nav-text">C1W2L01: Binary Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Binary-Classification"><span class="nav-number">3.0.2.</span> <span class="nav-text">Binary Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-hypothesis-Function-Logistic-Regression"><span class="nav-number">3.0.3.</span> <span class="nav-text">Model : hypothesis Function :Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid-function"><span class="nav-number">3.0.4.</span> <span class="nav-text">sigmoid function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Strategy：Cost-function"><span class="nav-number">3.0.5.</span> <span class="nav-text">Strategy：Cost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Algorithm-Gradient-Descent"><span class="nav-number">3.0.6.</span> <span class="nav-text">Algorithm: Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L05-amp-C1W2L06-Derivatives"><span class="nav-number">3.0.7.</span> <span class="nav-text">C1W2L05 &amp; C1W2L06 Derivatives</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L07：-Computation-Graph"><span class="nav-number">3.0.8.</span> <span class="nav-text">C1W2L07： Computation Graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L08-Derivatives-with-compution-graphs"><span class="nav-number">3.0.9.</span> <span class="nav-text">C1W2L08 : Derivatives with compution graphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L09-Logistic-Regression-Gradient-Descent"><span class="nav-number">3.0.10.</span> <span class="nav-text">C1W2L09 : Logistic Regression Gradient Descent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#single-training-example"><span class="nav-number">3.0.10.1.</span> <span class="nav-text">single training example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L10-Gradient-Descent-on-m-example"><span class="nav-number">3.0.11.</span> <span class="nav-text">C1W2L10 Gradient Descent on m example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary"><span class="nav-number">3.0.12.</span> <span class="nav-text">summary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L11-Vectorization"><span class="nav-number">3.0.13.</span> <span class="nav-text">C1W2L11: Vectorization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-什么是Vectorization：将-for-loop-尽可能转换为矩阵运算"><span class="nav-number">3.0.13.1.</span> <span class="nav-text">1. 什么是Vectorization：将 for loop 尽可能转换为矩阵运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-An-example-of-vectorization"><span class="nav-number">3.0.13.2.</span> <span class="nav-text">2. An example of vectorization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-GPU-or-CPU"><span class="nav-number">3.0.13.3.</span> <span class="nav-text">3. GPU or CPU</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C12L12-：-More-Vectorization-Example"><span class="nav-number">3.0.14.</span> <span class="nav-text">C12L12 ： More Vectorization Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵和向量乘法"><span class="nav-number">3.0.15.</span> <span class="nav-text">矩阵和向量乘法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#向量函数"><span class="nav-number">3.0.16.</span> <span class="nav-text">向量函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L13-Vectorizing-Logistic-Regression"><span class="nav-number">3.0.17.</span> <span class="nav-text">C1W2L13: Vectorizing Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-前向传播"><span class="nav-number">3.0.18.</span> <span class="nav-text">1. 前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L14-Vectorzing-Logistic-Regression’s-Gradient-Compution"><span class="nav-number">3.0.19.</span> <span class="nav-text">C1W2L14 : Vectorzing Logistic Regression’s Gradient Compution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L15-Broadcasting-in-Python"><span class="nav-number">3.0.20.</span> <span class="nav-text">C1W2L15: Broadcasting in Python</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#One-Example"><span class="nav-number">3.0.21.</span> <span class="nav-text">One Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Secondly-Example"><span class="nav-number">3.0.22.</span> <span class="nav-text">Secondly Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#广播机制的举例"><span class="nav-number">3.0.23.</span> <span class="nav-text">广播机制的举例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#axis"><span class="nav-number">3.0.24.</span> <span class="nav-text">axis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#broadcasting"><span class="nav-number">3.0.25.</span> <span class="nav-text">broadcasting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L16-A-Note-on-Python-numpy-vectors"><span class="nav-number">3.0.26.</span> <span class="nav-text">C1W2L16 A Note on Python/numpy vectors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-一维数组的特性"><span class="nav-number">3.0.27.</span> <span class="nav-text">1. 一维数组的特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-行向量和列向量"><span class="nav-number">3.0.28.</span> <span class="nav-text">2. 行向量和列向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-解决方法"><span class="nav-number">3.0.29.</span> <span class="nav-text">3. 解决方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L18-：Quick-Tour-of-Jupyter-iPython-Notebooks"><span class="nav-number">3.0.30.</span> <span class="nav-text">C1W2L18 ：Quick Tour of Jupyter/iPython Notebooks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C1W2L18-Explanation-of-Logistic-Regression-Cost-Function"><span class="nav-number">3.0.31.</span> <span class="nav-text">C1W2L18: Explanation of Logistic Regression Cost Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Day3-summary"><span class="nav-number">3.0.32.</span> <span class="nav-text">Day3 : summary</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#C1W3"><span class="nav-number">4.</span> <span class="nav-text">C1W3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L01-Neural-Network-Overview"><span class="nav-number">4.1.</span> <span class="nav-text">C1W3L01 : Neural Network Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L02-Nerual-Network-Representations"><span class="nav-number">4.2.</span> <span class="nav-text">C1W3L02 : Nerual Network Representations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L03：-Computation-Neural-Network-Output"><span class="nav-number">4.3.</span> <span class="nav-text">C1W3L03： Computation Neural Network Output</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-simple-training-examples"><span class="nav-number">4.3.1.</span> <span class="nav-text">A simple training examples</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L04-Vectorizing-Across-Mutilple-Example"><span class="nav-number">4.4.</span> <span class="nav-text">C1W3L04: Vectorizing Across Mutilple Example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L05-Explanation-for-vectorized-implement"><span class="nav-number">4.5.</span> <span class="nav-text">C1W3L05 : Explanation for vectorized implement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L06-Activation-Function"><span class="nav-number">4.6.</span> <span class="nav-text">C1W3L06 : Activation Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L07-Why-non-linear-activation-Functions"><span class="nav-number">4.7.</span> <span class="nav-text">C1W3L07 : Why non-linear activation Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid-activation-function"><span class="nav-number">4.7.1.</span> <span class="nav-text">sigmoid activation function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tanh-activation-function"><span class="nav-number">4.7.2.</span> <span class="nav-text">tanh activation function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rectified-linear-unit-RelU"><span class="nav-number">4.7.3.</span> <span class="nav-text">Rectified linear unit(RelU)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Leaky-linear-unit-Leaky-ReLU"><span class="nav-number">4.7.4.</span> <span class="nav-text">Leaky linear unit (Leaky ReLU)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L09-Gradient-Descent-For-Neural-Networks"><span class="nav-number">4.8.</span> <span class="nav-text">C1W3L09 : Gradient Descent For Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1WL10-Backpropagation-intuition-optional"><span class="nav-number">4.9.</span> <span class="nav-text">C1WL10: Backpropagation intuition (optional)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W3L11-Random-Initialization"><span class="nav-number">4.10.</span> <span class="nav-text">C1W3L11: Random Initialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-1"><span class="nav-number">4.11.</span> <span class="nav-text">summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#C1W4"><span class="nav-number">5.</span> <span class="nav-text">C1W4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W4L01-Deep-layer-neural-network"><span class="nav-number">5.1.</span> <span class="nav-text">C1W4L01 Deep-layer neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-logistics-regression-and-shallow-neural-network-and-deep-layer-neural-network"><span class="nav-number">5.1.1.</span> <span class="nav-text">1. logistics regression and shallow neural network and deep-layer neural network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-notation"><span class="nav-number">5.1.2.</span> <span class="nav-text">2. notation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W4L02：-Forward-and-Backward-propagation"><span class="nav-number">5.2.</span> <span class="nav-text">C1W4L02： Forward and Backward propagation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-forward-propagation"><span class="nav-number">5.2.1.</span> <span class="nav-text">1. forward propagation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-backward-propagation"><span class="nav-number">5.2.2.</span> <span class="nav-text">2. backward propagation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W4L03-Forward-Propagation-in-d-deep-network"><span class="nav-number">5.3.</span> <span class="nav-text">C1W4L03 : Forward Propagation in d deep network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W4L04-Getting-matrix-dimension-right"><span class="nav-number">5.4.</span> <span class="nav-text">C1W4L04 Getting matrix dimension right</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W4L05-Why-deep-representations"><span class="nav-number">5.5.</span> <span class="nav-text">C1W4L05 Why deep representations?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-for-example-of-face-detector"><span class="nav-number">5.5.1.</span> <span class="nav-text">1. for example of face detector</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W4L06-Building-blocks-of-a-deep-neural-network"><span class="nav-number">5.6.</span> <span class="nav-text">C1W4L06 :Building blocks of a deep neural network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W4L07：Parameters-vs-Hyperparameters"><span class="nav-number">5.7.</span> <span class="nav-text">C1W4L07：Parameters vs Hyperparameters</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-What"><span class="nav-number">5.7.1.</span> <span class="nav-text">1 What</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-How"><span class="nav-number">5.7.2.</span> <span class="nav-text">2 How</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C1W4L08-What-does-this-have-to-do-with-the-brain"><span class="nav-number">5.8.</span> <span class="nav-text">C1W4L08 : What does this have to do with the brain?</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#summary-forward-prop-and-back-prop"><span class="nav-number">6.</span> <span class="nav-text"># summary : forward prop and back prop</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-logistics-regression-shallow-neural-network-and-deep-neural-network"><span class="nav-number">6.1.</span> <span class="nav-text">1. logistics regression,shallow neural network and deep neural network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-neural-network"><span class="nav-number">6.2.</span> <span class="nav-text">deep neural network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-vectorization"><span class="nav-number">6.3.</span> <span class="nav-text">2. vectorization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-知识结构"><span class="nav-number">6.4.</span> <span class="nav-text">3. 知识结构</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="powered-by">
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">XieMay</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Symbols count total&#58;</span>
    
    <span title="Symbols count total">252k</span>
  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Pisces</a> v6.0.4</div>



</div>
        








        
      </div>
    </footer>

    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


















  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.0.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.0.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.4"></script>



  



	





  









  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
    
  
  <script src="[object Object]"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '1Dfsb4DwGQPCIlbyCKt9egUR-gzGzoHsz',
        appKey: '8OKqJPeMRRQnxx2vaAwIkM8y',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"model":"wanko","bottom":-30,"mobileShow":false,"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body>
</html>
