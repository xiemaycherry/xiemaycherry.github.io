<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2020%2F09%2F03%2F%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB%2FFeatureExtraction%2F</url>
    <content type="text"><![CDATA[Principal Component Analysis** : ä¸»æˆåˆ†åˆ†æ Â¶å½¢è±¡ç†è§£ å¦‚å›¾ï¼Œä¸‹é¢æ˜¯ä¸€å¼ 3dçš„å›¾ç‰‡ï¼Œä»ä¸åŒçš„æ–¹å‘æŠ•å½±å‡ºæ¥çš„äºŒç»´å›¾ï¼Œå¯ä»¥çœ‹å‡ºå³å¾€å·¦æŠ•å½±çš„å«æœ‰æ›´å¤šä¿¡æ¯ã€‚ å¦‚å›¾ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼ŒäºŒç»´ç‚¹å¾€ä¸¤ä¸ªæ­£äº¤çš„æ–¹å‘æŠ•å½±ï¼Œé•¿è½´å«æœ‰çš„ä¿¡æ¯æ›´å¤šã€‚ Â¶PCAçš„è¿‡ç¨‹ç¤ºæ„ **Step 1 ** : å»ä¸­å¿ƒã€‚ä¸­å¿ƒåœ¨åæ ‡è½´åœ¨ï¼ˆ0,0)ï¼Œå‡å€¼åœ¨åæ ‡è½´åŸç‚¹ Step 2 : Remove correlation(å»é™¤ç›¸å…³æ€§) é€šè¿‡åæ ‡å˜åŒ–ï¼Œåæ ‡æ—‹è½¬ï¼ŒçŸ©é˜µä½œç”¨ Â¶æ•°å­¦æ¨å¯¼ ç›®æ ‡ï¼šå˜æ¢åçš„çŸ©é˜µï¼Œå¯¹è§’éé›¶ï¼Œéå¯¹è§’çº¿å…¨ä¸ºé›¶ã€‚S(Y)æœ‰éé›¶çš„å¯¹è§’å…ƒç´ ï¼Œæ‰€æœ‰éå¯¹è§’å…ƒç´ éƒ½æ˜¯é›¶ Â¶ç†è®ºæ¨å¯¼ Â¶PCA Examples Â¶PCA bias]]></content>
  </entry>
  <entry>
    <title><![CDATA[å¿ƒè·¯å†ç¨‹]]></title>
    <url>%2F2020%2F09%2F03%2Frecording%20of%20master%2FMyMain%2F</url>
    <content type="text"><![CDATA[From 2019-09-31 to 2019-11-10 è¿‡å®Œäº†ç ”ä¸€çš„ç¬¬ä¸€ä¸ªåå‘¨ï¼Œå†…å¿ƒå¾ˆç©ºè™šï¼Œæåº¦çš„ç©ºè™šï¼Œä¸çŸ¥é“æ€ä¹ˆå›äº‹ï¼Ÿæˆ‘è§‰å¾—åŸå› æ˜¯ï¼šè‡ªå·±å¯¹è‡ªå·±çš„è¦æ±‚å¤ªé«˜äº†ï¼Œè¦æ±‚è‡ªå·±åšçš„äº‹æƒ…å¤ªå¤šäº†ï¼Ÿç»“æœä»€ä¹ˆéƒ½æ²¡æœ‰åšå¥½ã€‚ å¯¹äºè‡ªå·±çš„å»ºè®¾ï¼Œæˆ‘å°è¯•äº†å¾ˆå¤šçš„ä¸œè¥¿ï¼ŒåŒ–å¦†å’Œæ‰“æ‰®ã€‚æˆ‘è§‰å¾—æˆ‘è¿˜æ²¡æœ‰æ¼‚äº®çš„èµ„æœ¬ï¼Œæ²¡é’±ï¼Œæ²¡æˆ¿ã€‚æ‰€ä»¥ä¸€å®šè¦éå¸¸çš„åŠªåŠ›æ‰è¡Œã€‚æˆ‘åªå¸Œæœ›æˆ‘æœ€çˆ±çš„äººï¼Œé…å¾—ä¸Šæˆ‘ï¼Œæˆ‘å¸Œæœ›æˆ‘ä»¬æ˜¯å…±åŒå¥‹æ–—çš„çŠ¶æ€äº†ï¼Œè€Œä¸æ˜¯ä¾é ã€‚æ‰€ä»¥å°½é‡ä¸åŒ–å¦†å‡ºé—¨ï¼Œé™¤äº†é‡è¦åœºåˆã€‚ å¯¹äºå­¦ä¹ æ–¹é¢ï¼Œäº†è§£äº†é«˜é“èƒŒæ™¯ï¼Œè¿˜æ˜¯æ²¡æœ‰è¿›å…¥çŠ¶æ€ï¼Œæ‰€ä»¥è¦æ›´åŠªåŠ›æ‰è¡Œã€‚æœ€è¿‘çš„è¯¾ç¨‹å¤ªå¤šäº†ï¼Œå¹³è¡¡å•Šï¼æˆ‘å¿ƒæœ‰ä½™è€ŒåŠ›ä¸è¶³çš„ã€‚è¿˜æœ‰ä¸€å®šè¦ä¼šè¯´ï¼Œä¸å¥½è¯´ã€‚æ¥ä¸‹æ¥ä¸€å®šè¦é©¬åŠ›åè¶³çš„å‡ºå‘ã€‚ å¯¹äºäººï¼Œå¯èƒ½è¿™é‡Œåªæ˜¯ä¸‰å¹´çš„ä¸€æ®µæ—¶å…‰ï¼Œå¾ˆå¥½ç›¸å¤„å°±å¥½äº†ï¼Œæœ€é‡è¦çš„æ˜¯ç”·æœ‹å‹å’Œå®¤å‹ã€‚ å¯¹ä¸èµ·ï¼Œæˆ‘è¿˜æœ‰å¥‹æ–—ï¼Œæˆ‘è¦å¥‹æ–—ï¼Œæˆ‘è¦å¥‹æ–—ã€‚è¿˜æœ‰å¥èº«ã€‚ æˆ‘çš„æ—¶é—´éƒ½åˆ†æ•£åœ¨å…¶ä»–è§†é¢‘çš„äº‹æƒ…ä¸Šäº†ï¼Œæœç»å•Šï¼æ¯å¤©æœ€å¤šä¸¤å°æ—¶ã€‚å°½é‡ä¸ç©æ‰‹æœºäº†ï¼ï¼ï¼ï¼]]></content>
      <tags>
        <tag>æˆé•¿</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç›®å½•â€”â€”æ•°æ®åˆ†æ]]></title>
    <url>%2F2020%2F08%2F28%2F%E7%9B%AE%E5%BD%95%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"></content>
      <categories>
        <category>æ•°æ®åˆ†æ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ç›®å½•â€”â€”æ·±åº¦å­¦ä¹ ]]></title>
    <url>%2F2020%2F08%2F28%2F%E7%9B%AE%E5%BD%95%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"></content>
      <categories>
        <category>æ·±åº¦å­¦ä¹ </category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ç›®å½•â€”â€”æœºå™¨å­¦ä¹ ]]></title>
    <url>%2F2020%2F08%2F28%2F%E7%9B%AE%E5%BD%95%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ç”Ÿæ´»æ”»ç•¥]]></title>
    <url>%2F2020%2F08%2F28%2F%E7%94%9F%E6%B4%BB%E6%94%BB%E7%95%A5%2F</url>
    <content type="text"><![CDATA[Â¶2020 Â¶202008 Â¶ç”µè„‘æ–‡ä»¶ç®¡ç† ç¬¬ä¸€æ¬¡å°±è¦æŠŠæ–‡ä»¶ä½ç½®ç¡®å®šã€‚ ä¸»è¦æ˜¯æ¢³ç†ç”µè„‘æ–‡ä»¶ç®¡ç†çš„æŠ€å·§ï¼Œæ–¹ä¾¿ä»¥åæé«˜å·¥ä½œæ•ˆç‡ã€‚ å‘½åæ–¹æ³•ã€‚é‡‡ç”¨3W1Xçš„ç†å¿µï¼Œwhen-work-who-X(å¤‡æ³¨)ï¼Œâ€œ3Wâ€åˆ†åˆ«æŒ‡ï¼šWhenæ—¶é—´â€”â€”20200630ï¼›Workäº‹é¡¹â€”â€”é”€å”®éƒ¨è¥é”€è®¡åˆ’ï¼›Whoä¸»ä½“â€”â€”å®¢æˆ·Aã€‚ â€œ1Xâ€æ˜¯æŒ‡ï¼šXå¤‡æ³¨â€”â€”ç¬¬ä¸‰ç‰ˆã€‚ åŒä¸€æ–‡ä»¶ï¼Œå¤šä¸ªä¿®æ”¹ç‰ˆæœ¬ã€‚æ–‡ä»¶åå‰é¢åŠ ä¸Šä¿®æ”¹äººï¼Œåœ¨æœ€ååŠ ä¸Šç‰ˆæœ¬å·ã€‚ åŒä¸€ä»»åŠ¡ï¼ˆä¸»é¢˜ï¼‰æ–‡ä»¶å¤¹ç®¡ç†ã€‚åŒçº§æœ€å¤š7ä¸ªæ–‡ä»¶å¤¹ï¼Œæœ€å¤š5å±‚ã€‚ åŠæ—¶å¤ç›˜å¾ˆå…³é”®ã€‚ 1. æ¯æ˜ŸæœŸè¦æ•´ç†ä¸€æ¬¡æ–‡ä»¶å¤¹ã€‚ 2. é‡è¦æ–‡ä»¶è¦åŒæ­¥ã€‚]]></content>
      <categories>
        <category>ç”Ÿæ´»æ”»ç•¥</category>
      </categories>
      <tags>
        <tag>æŠ€å·§</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BLOG ã® ç›®å½•]]></title>
    <url>%2F2020%2F08%2F28%2Fcategories%2F</url>
    <content type="text"><![CDATA[BLOG ã® ç›®å½• ç»Ÿä¸€ç”¨ä¸­æ–‡ æ•°å­¦ (Mathematics) æ•°å­¦çŸ¥è¯† æœºå™¨å­¦ä¹ ï¼ˆMachine Learning) æ€»ç›®å½• ç®—æ³•ï¼Œæ¨¡å‹ å®ç° æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰ 1. æ€»ç›®å½• ç®—æ³• å®ç° ç¼–ç¨‹è¯­è¨€ ï¼ˆProgram Languageï¼‰ æ€»ç›®å½• C Python è®¡ç®—æœºç›¸å…³çŸ¥è¯† æ•°æ®åˆ†æ æ€»ç›®å½• å¯è§†åŒ– ä¸šåŠ¡çŸ¥è¯† åˆ†ææ€ç»´ æ•°æ®åº“ æ•°æ®æŒ–æ˜ é¡¹ç›®å®æˆ˜ å­¦ä¹ æ—¥å¸¸ ï¼ˆStudying Diaryï¼‰ ä¸»è¦æ˜¯æ¯å¤©çš„å­¦ä¹ è®°å½• æ‚ä¸ƒæ‚å…«ï¼ˆMixed) å…¶ä»–é¢†åŸŸçš„çŸ¥è¯† ç”Ÿæ´»æ”»ç•¥ å­¦ä¹ å†ç¨‹ï¼ˆJourney of Studying) ä¸»è¦æ˜¯ä¹¦ç±èµ„æ–™çš„å­¦ä¹ ï¼ˆä¸€æœ¬ä¸€ç¯‡ï¼‰è¯¦ç»†çš„è®°å½•åœ¨æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ é‡Œé¢]]></content>
      <categories>
        <category>Categories</category>
      </categories>
      <tags>
        <tag>Categories</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ•°æ®åˆ†æä¹‹æ•°æ®åˆ†æèŒè´£]]></title>
    <url>%2F2020%2F08%2F27%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%81%8C%E8%B4%A3%2F</url>
    <content type="text"><![CDATA[æŠ€æœ¯+ä¸šåŠ¡ ä¸šåŠ¡+æŠ€æœ¯ï¼Œè‡³å°‘æ‡‚è¿™äº› æœ¯ä¸šæœ‰ä¸“æ”»ï¼ŒçŸ¥è¯†è¦å¹¿æ³›ï¼Œæ˜¯èŒä¸šå‘å±•çš„åŸºæœ¬å‡†åˆ™ã€‚ç‰¹åˆ«å¯¹æ•°æ®åˆ†æå¸ˆè¿™æ ·ä¸€ä¸ªå¤šé¢æ‰‹å‹è§’è‰²ã€‚é‚£ä¹ˆæˆ‘ä»¬åº”è¯¥äº†è§£åˆ°ä»€ä¹ˆç¨‹åº¦å‘¢ï¼Ÿè¿™é‡Œæœ‰ä¸ªå»ºè®®ï¼š **ä¸šåŠ¡æ–¹å‘åˆ†æå¸ˆï¼š**æ•°æ®é‡‡é›†æ–¹å¼ã€æ•°æ®å­—æ®µæ ¼å¼ã€æŒ‡æ ‡çš„è®¡ç®—å£å¾„ä¸æ›´æ–°æ—¶é—´è¿™ä¸‰ä¸ªæ˜¯å¿…é¡»å¿…é¡»çŸ¥é“çš„ã€‚å› ä¸ºè¿™ä¸‰ç‚¹æ¶‰åŠåˆ°æ•°æ®çœŸå®æ€§ä¸å¯é æ€§ã€‚æ²¡æœ‰æ•°æ®è´¨é‡åšä¿è¯ï¼Œä»€ä¹ˆåˆ†æéƒ½æ˜¯ç©ºè°ˆã€‚å¯¹åŸºç¡€æ•°é‡è¶Šäº†è§£ï¼Œè¶Šèƒ½ä»ç»†èŠ‚ä¸­æ‰¾åˆ°æ€è·¯ï¼›ç®—æ³•æ¨¡å‹çš„ç§ç±»ä¸åº”ç”¨åœºæ™¯æ˜¯å¿…é¡»äº†è§£çš„ã€‚å› ä¸ºè¿™æ¶‰åŠåˆ°å¦‚ä½•é€‰æ‹©åˆ†ææ–¹æ³•ï¼Œå¦‚ä½•æå‡åˆ†æè´¨é‡ã€‚å…·ä½“ä»£ç æ€ä¹ˆå†™ï¼Œå¼„æ‡‚å°±æ‡‚ã€‚ **æŠ€æœ¯æ–¹å‘åˆ†æå¸ˆï¼š**ä¸šåŠ¡éƒ¨é—¨åˆ†å·¥ã€èŒè´£ã€æµç¨‹å¿…é¡»è¦äº†è§£ã€‚è‡³å°‘èŒè´£æ¸…æ™°ï¼ŒçŸ¥é“è‡ªå·±è¦å¯¹æ¥çš„äººåˆ°åº•æ˜¯å¹²ä»€ä¹ˆã€‚è‡ªå·±å¯¹åº”éƒ¨é—¨å¸¸è§çš„ä¸šåŠ¡éœ€æ±‚ï¼Œå¦‚é”€å”®åˆ†æã€ç»è¥åˆ†æã€ä¿ƒé”€åˆ†æã€å•†å“ç®¡ç†çš„æ–¹æ³•è¦æœ‰æ‰€äº†è§£ã€‚åœ¨é¢å¯¹ä¸šåŠ¡éƒ¨é—¨éœ€æ±‚çš„æ—¶å€™ï¼Œå¤§æ¦‚çŸ¥é“ä»–ä»¬åœ¨æƒ³ä»€ä¹ˆï¼Œæœ‰ä»€ä¹ˆå¥—è·¯ã€‚å¸®åŠ©è‡ªå·±æ›´å¥½çš„ç†è§£éœ€æ±‚ï¼Œè§„é¿éœ€æ±‚å¤§å‘ã€‚]]></content>
      <categories>
        <category>æ•°æ®åˆ†æ</category>
      </categories>
      <tags>
        <tag>èŒä½è¦æ±‚</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è‹±è¯­Daily]]></title>
    <url>%2F2020%2F08%2F25%2F%E8%8B%B1%E8%AF%ADDaily%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[æ•°æ®åˆ†ææŠ€èƒ½]]></title>
    <url>%2F2020%2F08%2F25%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD%2F</url>
    <content type="text"><![CDATA[https://ask.hellobi.com/blog/qinlu/sitemap/ SQL Â¶å­¦ä¼šç›´æ¥æŠŠåˆ«äººçš„çŸ¥è¯†è£…è¿›è‡ªå·±çš„è„‘å­é‡Œ https://www.zhihu.com/xen/market/personal-works-all/houziliaorenwu?zh_hide_tab_bar=true Â¶LeetCode &amp; NowCoder https://leetcode.com/problemset/database/ https://www.nowcoder.com/ta/sql Â¶å…³ç³»å‹æ•°æ®åº“ Â¶MySQLæ•°æ®ç±»å‹ ä¸»è¦æä¾›äº†ä¸‰ç§ç±»å‹ï¼Œåˆ†åˆ«æ˜¯æ–‡æœ¬ï¼Œæ•°å­—å’Œæ—¥æœŸã€‚ Â¶MySQL æ•°æ®ç±»å‹ åœ¨ MySQL ä¸­ï¼Œæœ‰ä¸‰ç§ä¸»è¦çš„ç±»å‹ï¼šæ–‡æœ¬ã€æ•°å­—å’Œæ—¥æœŸ/æ—¶é—´ç±»å‹ã€‚ Â¶Text ç±»å‹ï¼š æ•°æ®ç±»å‹ æè¿° CHAR(size) ä¿å­˜å›ºå®šé•¿åº¦çš„å­—ç¬¦ä¸²ï¼ˆå¯åŒ…å«å­—æ¯ã€æ•°å­—ä»¥åŠç‰¹æ®Šå­—ç¬¦ï¼‰ã€‚åœ¨æ‹¬å·ä¸­æŒ‡å®šå­—ç¬¦ä¸²çš„é•¿åº¦ã€‚æœ€å¤š 255 ä¸ªå­—ç¬¦ã€‚ VARCHAR(size) ä¿å­˜å¯å˜é•¿åº¦çš„å­—ç¬¦ä¸²ï¼ˆå¯åŒ…å«å­—æ¯ã€æ•°å­—ä»¥åŠç‰¹æ®Šå­—ç¬¦ï¼‰ã€‚åœ¨æ‹¬å·ä¸­æŒ‡å®šå­—ç¬¦ä¸²çš„æœ€å¤§é•¿åº¦ã€‚æœ€å¤š 255 ä¸ªå­—ç¬¦ã€‚æ³¨é‡Šï¼šå¦‚æœå€¼çš„é•¿åº¦å¤§äº 255ï¼Œåˆ™è¢«è½¬æ¢ä¸º TEXT ç±»å‹ã€‚ TINYTEXT å­˜æ”¾æœ€å¤§é•¿åº¦ä¸º 255 ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ã€‚ TEXT å­˜æ”¾æœ€å¤§é•¿åº¦ä¸º 65,535 ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ã€‚ BLOB ç”¨äº BLOBs (Binary Large OBjects)ã€‚å­˜æ”¾æœ€å¤š 65,535 å­—èŠ‚çš„æ•°æ®ã€‚ MEDIUMTEXT å­˜æ”¾æœ€å¤§é•¿åº¦ä¸º 16,777,215 ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ã€‚ MEDIUMBLOB ç”¨äº BLOBs (Binary Large OBjects)ã€‚å­˜æ”¾æœ€å¤š 16,777,215 å­—èŠ‚çš„æ•°æ®ã€‚ LONGTEXT å­˜æ”¾æœ€å¤§é•¿åº¦ä¸º 4,294,967,295 ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸²ã€‚ LONGBLOB ç”¨äº BLOBs (Binary Large OBjects)ã€‚å­˜æ”¾æœ€å¤š 4,294,967,295 å­—èŠ‚çš„æ•°æ®ã€‚ ENUM(x,y,z,etc.) å…è®¸ä½ è¾“å…¥å¯èƒ½å€¼çš„åˆ—è¡¨ã€‚å¯ä»¥åœ¨ ENUM åˆ—è¡¨ä¸­åˆ—å‡ºæœ€å¤§ 65535 ä¸ªå€¼ã€‚å¦‚æœåˆ—è¡¨ä¸­ä¸å­˜åœ¨æ’å…¥çš„å€¼ï¼Œåˆ™æ’å…¥ç©ºå€¼ã€‚æ³¨é‡Šï¼šè¿™äº›å€¼æ˜¯æŒ‰ç…§ä½ è¾“å…¥çš„é¡ºåºå­˜å‚¨çš„ã€‚å¯ä»¥æŒ‰ç…§æ­¤æ ¼å¼è¾“å…¥å¯èƒ½çš„å€¼ï¼šENUM(â€˜Xâ€™,â€˜Yâ€™,â€˜Zâ€™) SET ä¸ ENUM ç±»ä¼¼ï¼ŒSET æœ€å¤šåªèƒ½åŒ…å« 64 ä¸ªåˆ—è¡¨é¡¹ï¼Œä¸è¿‡ SET å¯å­˜å‚¨ä¸€ä¸ªä»¥ä¸Šçš„å€¼ã€‚ Â¶Number ç±»å‹ï¼š æ•°æ®ç±»å‹ æè¿° TINYINT(size) -128 åˆ° 127 å¸¸è§„ã€‚0 åˆ° 255 æ— ç¬¦å·*ã€‚åœ¨æ‹¬å·ä¸­è§„å®šæœ€å¤§ä½æ•°ã€‚ SMALLINT(size) -32768 åˆ° 32767 å¸¸è§„ã€‚0 åˆ° 65535 æ— ç¬¦å·*ã€‚åœ¨æ‹¬å·ä¸­è§„å®šæœ€å¤§ä½æ•°ã€‚ MEDIUMINT(size) -8388608 åˆ° 8388607 æ™®é€šã€‚0 to 16777215 æ— ç¬¦å·*ã€‚åœ¨æ‹¬å·ä¸­è§„å®šæœ€å¤§ä½æ•°ã€‚ INT(size) -2147483648 åˆ° 2147483647 å¸¸è§„ã€‚0 åˆ° 4294967295 æ— ç¬¦å·*ã€‚åœ¨æ‹¬å·ä¸­è§„å®šæœ€å¤§ä½æ•°ã€‚ BIGINT(size) -9223372036854775808 åˆ° 9223372036854775807 å¸¸è§„ã€‚0 åˆ° 18446744073709551615 æ— ç¬¦å·*ã€‚åœ¨æ‹¬å·ä¸­è§„å®šæœ€å¤§ä½æ•°ã€‚ FLOAT(size,d) å¸¦æœ‰æµ®åŠ¨å°æ•°ç‚¹çš„å°æ•°å­—ã€‚åœ¨æ‹¬å·ä¸­è§„å®šæœ€å¤§ä½æ•°ã€‚åœ¨ d å‚æ•°ä¸­è§„å®šå°æ•°ç‚¹å³ä¾§çš„æœ€å¤§ä½æ•°ã€‚ DOUBLE(size,d) å¸¦æœ‰æµ®åŠ¨å°æ•°ç‚¹çš„å¤§æ•°å­—ã€‚åœ¨æ‹¬å·ä¸­è§„å®šæœ€å¤§ä½æ•°ã€‚åœ¨ d å‚æ•°ä¸­è§„å®šå°æ•°ç‚¹å³ä¾§çš„æœ€å¤§ä½æ•°ã€‚ DECIMAL(size,d) ä½œä¸ºå­—ç¬¦ä¸²å­˜å‚¨çš„ DOUBLE ç±»å‹ï¼Œå…è®¸å›ºå®šçš„å°æ•°ç‚¹ã€‚ * è¿™äº›æ•´æ•°ç±»å‹æ‹¥æœ‰é¢å¤–çš„é€‰é¡¹ UNSIGNEDã€‚é€šå¸¸ï¼Œæ•´æ•°å¯ä»¥æ˜¯è´Ÿæ•°æˆ–æ­£æ•°ã€‚å¦‚æœæ·»åŠ  UNSIGNED å±æ€§ï¼Œé‚£ä¹ˆèŒƒå›´å°†ä» 0 å¼€å§‹ï¼Œè€Œä¸æ˜¯æŸä¸ªè´Ÿæ•°ã€‚ Â¶Date ç±»å‹ï¼š æ•°æ®ç±»å‹ æè¿° DATE() æ—¥æœŸã€‚æ ¼å¼ï¼šYYYY-MM-DDæ³¨é‡Šï¼šæ”¯æŒçš„èŒƒå›´æ˜¯ä» â€˜1000-01-01â€™ åˆ° â€˜9999-12-31â€™ DATETIME() *æ—¥æœŸå’Œæ—¶é—´çš„ç»„åˆã€‚æ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSæ³¨é‡Šï¼šæ”¯æŒçš„èŒƒå›´æ˜¯ä» â€˜1000-01-01 00:00:00â€™ åˆ° â€˜9999-12-31 23:59:59â€™ TIMESTAMP() *æ—¶é—´æˆ³ã€‚TIMESTAMP å€¼ä½¿ç”¨ Unix çºªå…ƒ(â€˜1970-01-01 00:00:00â€™ UTC) è‡³ä»Šçš„æè¿°æ¥å­˜å‚¨ã€‚æ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSæ³¨é‡Šï¼šæ”¯æŒçš„èŒƒå›´æ˜¯ä» â€˜1970-01-01 00:00:01â€™ UTC åˆ° â€˜2038-01-09 03:14:07â€™ UTC TIME() æ—¶é—´ã€‚æ ¼å¼ï¼šHH:MM:SS æ³¨é‡Šï¼šæ”¯æŒçš„èŒƒå›´æ˜¯ä» â€˜-838:59:59â€™ åˆ° â€˜838:59:59â€™ YEAR() 2 ä½æˆ– 4 ä½æ ¼å¼çš„å¹´ã€‚æ³¨é‡Šï¼š4 ä½æ ¼å¼æ‰€å…è®¸çš„å€¼ï¼š1901 åˆ° 2155ã€‚2 ä½æ ¼å¼æ‰€å…è®¸çš„å€¼ï¼š70 åˆ° 69ï¼Œè¡¨ç¤ºä» 1970 åˆ° 2069ã€‚ * å³ä¾¿ DATETIME å’Œ TIMESTAMP è¿”å›ç›¸åŒçš„æ ¼å¼ï¼Œå®ƒä»¬çš„å·¥ä½œæ–¹å¼å¾ˆä¸åŒã€‚åœ¨ INSERT æˆ– UPDATE æŸ¥è¯¢ä¸­ï¼ŒTIMESTAMP è‡ªåŠ¨æŠŠè‡ªèº«è®¾ç½®ä¸ºå½“å‰çš„æ—¥æœŸå’Œæ—¶é—´ã€‚TIMESTAMP ä¹Ÿæ¥å—ä¸åŒçš„æ ¼å¼ï¼Œæ¯”å¦‚ YYYYMMDDHHMMSSã€YYMMDDHHMMSSã€YYYYMMDD æˆ– YYMMDDã€‚ Â¶Day Ox00 å…¥é—¨ï¼š https://www.w3school.com.cn/sql/sql_syntax.asp é€šè¿‡SQLä½¿å¾—æ•°æ®æ“ä½œå‘˜æœ‰èƒ½åŠ›æŸ¥è¯¢ï¼Œä¿®æ”¹æ•°æ®åº“ã€‚ Â¶What SQLï¼šç»“æ„åŒ–æŸ¥è¯¢è¯­è¨€ã€‚SQLå¯¹å¤§å°å†™ä¸æ•æ„Ÿã€‚ SQLè¯­å¥åé¢çš„åˆ†å·ï¼Ÿæœ‰äº›æ•°æ®åº“ç³»ç»Ÿè¦æ±‚æ¯æ¡SQLå‘½ä»¤çš„æœ«ç«¯ä½¿ç”¨åˆ†å·ï¼Œæ¥è¡¨ç¤ºè¯­å¥çš„ç»“æŸã€‚ åˆ†ç±»ï¼š æ•°æ®æ“ä½œè¯­è¨€ï¼ˆDML) è¿™éƒ¨åˆ†åŒ…æ‹¬æŸ¥è¯¢å’Œæ›´æ–°æŒ‡ä»¤ã€‚ SELECT - ä»æ•°æ®åº“è¡¨ä¸­è·å–æ•°æ® UPDATE - æ›´æ–°æ•°æ®åº“è¡¨ä¸­çš„æ•°æ® DELETE - ä»æ•°æ®åº“è¡¨ä¸­åˆ é™¤æ•°æ® INSERT INTO - å‘æ•°æ®åº“è¡¨ä¸­æ’å…¥æ•°æ® æ•°æ®å®šä¹‰è¯­è¨€ (DDL) è¿™éƒ¨åˆ†åŒ…æ‹¬åˆ›å»ºå’Œåˆ é™¤è¡¨æ ¼ï¼Œè¿˜æœ‰å®šä¹‰ç´¢å¼•(é”®)ï¼Œè§„å®šè¡¨ä¹‹é—´çš„é“¾æ¥ï¼Œå’Œè¡¨é—´çš„çº¦æŸã€‚ SQL ä¸­æœ€é‡è¦çš„ DDL è¯­å¥: â€‹ CREATE DATABASE - åˆ›å»ºæ–°æ•°æ®åº“ â€‹ ALTER DATABASE - ä¿®æ”¹æ•°æ®åº“ â€‹ CREATE TABLE - åˆ›å»ºæ–°è¡¨ â€‹ ALTER TABLE - å˜æ›´ï¼ˆæ”¹å˜ï¼‰æ•°æ®åº“è¡¨ â€‹ DROP TABLE - åˆ é™¤è¡¨ â€‹ CREATE INDEX - åˆ›å»ºç´¢å¼•ï¼ˆæœç´¢é”®ï¼‰ â€‹ DROP INDEX - åˆ é™¤ç´¢å¼• Â¶Day Ox00 ç®€å•æŸ¥è¯¢ï¼š Â¶SELECT : æŸ¥è¯¢è¯­å¥ ä¸å¸¦æ¡ä»¶æŸ¥è¯¢è¯­å¥ 123SELECT åˆ—åç§°1, åˆ—åç§°2 FROM è¡¨åç§°SELECT * FROM è¡¨åç§°SELECT DISTINCT åˆ—åç§° FROM è¡¨åç§° //å»é™¤é‡å¤å€¼ æœ‰æ¡ä»¶æŸ¥è¯¢è¯­å¥ 1SELECT åˆ—åç§° FROM è¡¨åç§° WHERE åˆ— è¿ç®—ç¬¦ å€¼ WHERE è¿‡æ»¤å•æ¡è®°å½• 12345678910æ“ä½œç¬¦ æè¿°= ç­‰äº&lt;&gt; ä¸ç­‰äº&gt; å¤§äº&lt; å°äº&gt;= å¤§äºç­‰äº&lt;= å°äºç­‰äºBETWEEN åœ¨æŸä¸ªèŒƒå›´å†…LIKE æœç´¢æŸç§æ¨¡å¼æ³¨é‡Šï¼šåœ¨æŸäº›ç‰ˆæœ¬çš„ SQL ä¸­ï¼Œæ“ä½œç¬¦ &lt;&gt; å¯ä»¥å†™ä¸º !=ã€‚ WHERE è¿‡æ»¤ä¸¤ä¸ªä»¥ä¸Šçš„æ¡ä»¶è®°å½• AND / OR]]></content>
      <categories>
        <category>æ•°æ®åˆ†æ</category>
      </categories>
      <tags>
        <tag>æ•°æ®åˆ†ææŠ€èƒ½</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ•°æ®åˆ†æå®æˆ˜é¡¹ç›®]]></title>
    <url>%2F2020%2F08%2F25%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[ç ”äºŒã®å­¦ä¹ ]]></title>
    <url>%2F2020%2F08%2F15%2F%E7%A0%94%E4%BA%8C%E3%81%AE%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[è®°å½•æ—¥å¸¸ï¼Œæ—¥å¸¸ç•™å¿µã€‚ é—­å…³ä¿®ç‚¼ï¼Œä¿®ç‚¼æˆé‡‘åˆšä¸åä¹‹èº«ï¼ ç«‹ä¸‹Flag, ä¸‰ä¸ªæœˆåï¼Œçœ‹çœ‹è‡ªå·±ï¼ŒåŠªåŠ›ğŸ˜€ğŸ˜€ æ¸…å•ğŸ˜€ğŸ˜€ what how æ—¥å¿— å…¬ä¼—å·ã€åšå®¢ã€CSDNã€Github ä¸»è¦è¾“å‡º æ¬ç –+é¡¹ç›® ä¸»è¦è¾“å…¥ æ•°æ®åˆ†æ+æ–°ç»æµ 8:00~11:20 2h 14:10-17:20 3h 19:00-22:00 3h æ¯å¤© æ¯å¤©çš„è§„åˆ’ï¼›æ¯å¤©åŠå°æ—¶ç‘œä¼½ï¼ˆå“‘é“ƒ)ï¼› æ¥ä¸‹æ¥æœ‰æ„ä¹‰çš„äº‹æƒ…ï¼Œè½¬è¿‡å»è½¬è¿‡æ¥åˆå›åˆ°æœ€åˆçš„èµ·ç‚¹ï¼š 1. @å¯è§†åŒ–ã€‚å°è£…ä¸€ä¸‹matplotlibä¿®æ”¹ä¸€äº›é»˜è®¤å‚æ•°ã€‚ä¸»è¦å°±æ˜¯ä¿®æ”¹å­—ä½“ï¼Œå­—å·ï¼Œä¸»é¢˜ï¼ŒèƒŒæ™¯ã€‚ 2. @å¡«å……ç®€å†ã€‚a)åŸºæœ¬ä¿¡æ¯ b) é¡¹ç›®ç»å†(ä¸‰å››ä¸ªå§) c)å®ä¹ ç»å† d) çŸ¥è¯†æŠ€èƒ½(è¦è¿›ä¸€æ­¥è¡¥å……)ã€‚è¾ƒå¼±çš„åœ°æ–¹:è½åœ°çš„é¡¹ç›®(å¥½æƒ³å¥½æƒ³å®ä¹ )ï¼›è‹±è¯­å•Šï¼› 3. @ç ”ç©¶ç”Ÿå®Œæ»¡çš„ç»“æŸã€‚æ¯•ä¸šè®¾è®¡æ¯ç« å¯¹åº”å‘ä¸€ç¯‡æ–‡ç« å§ï¼ˆäº§ä¸š)ã€‚åŠ æ²¹ï¼ŒåŠ æ²¹ï¼è¿™åŠå¹´è¦åšå®Œï¼Œå†æ‰¾æ•°æ® 4. @è‹±è¯­ã€‚æ–°æ¦‚å¿µè‹±è¯­äºŒå’Œæ–°æ¦‚å¿µè‹±è¯­ä¸‰å¤©å¤©å­¦ä¹ å–” 5. @åˆ»æ„ç»ƒä¹ ã€‚å¦‚æœç‹ ä¸ä¸‹å¿ƒï¼Œé‚£å°±æ¯æ—¶æ¯åˆ»çš„åˆ»æ„ï¼Œæ¯æ—¶æ¯åˆ»çš„æ®šç²¾ç«­è™‘ 6. @å…¬æ–‡å†™ä½œã€‚å—¯å—¯å—¯ã€‚åˆ»æ„çš„è®­ç»ƒè‡ªå·± æ¯å¤©è¿½è¸ªå…¬ä¼—å·æ–‡ç« ã€‚ çŒ´å­æ•°æ®åˆ†æã€‚ Â¶20200903 ç»ˆäºè€ƒå®Œäº†å•Š é«˜é«˜å…´å…´ï¼Œè®¤è®¤çœŸçœŸæç§‘ç ”äº†ï¼ è¿™ä¸ªæœˆæŠŠé«˜é“é‡æ–°æ›´æ–°ä¸€ä¸‹ è¿™ä¸ªæœˆæŠŠæ—¶é—´åºåˆ—è·‘å‡ºæ¥ç»“æœ è¿™ä¸ªæœˆå¯è§†åŒ–èµ„æ–™å¼„ä¸€ä¸‹ è¿™ä¸ªæœˆ å¿…ä¿®ç¯èŠ‚ 6400006003 å­¦æœ¯æ´»åŠ¨ ç ”ç©¶ç”Ÿé™¢ 0 1 æ˜¥ä¸ç§‹ å…¶ä»– è€ƒæŸ¥ æ˜¯ å¿…ä¿®ç¯èŠ‚ 6400006009 è®ºæ–‡å¼€é¢˜æŠ¥å‘ŠåŠæ–‡çŒ®é˜…è¯»ç»¼è¿°II ç ”ç©¶ç”Ÿé™¢ 0 1 æ˜¥ä¸ç§‹ å…¶ä»– è€ƒæŸ¥ æ˜¯ å®è·µæ•™å­¦ç¯èŠ‚ï¼šè¿˜æœ‰äº”ä¸ªå­¦åˆ†ã€‚ å®è·µæ•™å­¦ç¯èŠ‚6ä¸ªå­¦åˆ†ä¸­ï¼ŒåŸºåœ°å®è·µå¿…é¡»å®Œæˆ2-4ä¸ªå­¦åˆ†ï¼ŒæŒ‰ç…§å®è·µæ—¶é—´1-3ä¸ªæœˆã€4-6ä¸ªæœˆã€7-12ä¸ªæœˆåŠä»¥ä¸Šä½œä¸ºå®è·µæ—¶é—´å•ä½ï¼Œåˆ†åˆ«è®¤å®šä¸º2å­¦åˆ†ã€3å­¦åˆ†å’Œ4å­¦åˆ†ã€‚è¦æ±‚æäº¤å®è·µæ€»ç»“æŠ¥å‘Šï¼Œå®è·µåŸºåœ°ï¼ˆå•ä½ï¼‰å°±å­¦ç”Ÿæäº¤çš„æŠ¥å‘Šç»™äºˆç›¸å…³æ”¯æ’‘ä¹¦é¢ææ–™è¯æ˜ï¼Œæ ¹æ®å®é™…å®è·µæ—¶é—´ï¼Œç»å¯¼å¸ˆå®¡æ ¸é€šè¿‡ï¼Œå¯è·å¾—2-4ä¸ªå­¦åˆ†ã€‚ å®è·µæ•™å­¦è¯¾ç¨‹ä¸»è¦æŒ‡çªå‡ºå®è·µè®­ç»ƒçš„å®éªŒè¯¾ç¨‹ï¼Œå…¨æ ¡å¯é€šé€‰ï¼Œå®Œæˆè€…å–å¾—ç›¸åº”å­¦åˆ†ã€‚ 3ä¸ªå­¦åˆ†(å®ä¹ )+å­¦ä½è®ºæ–‡å†™ä½œè§„èŒƒ(2)ä¸ªå­¦åˆ† è™½ç„¶æˆ‘å¾ˆå–œæ¬¢å¤©å¤©ç¡å¤§è§‰ï¼Œèººåœ¨åºŠä¸Šç¡è§‰ã€‚ä½†æ˜¯å‘¢ï¼Ÿå¥½å¥½å·¥ä½œï¼Œè§„å¾‹çš„ç”Ÿæ´»ä½œæ¯ï¼Œè¿™è¿˜æ˜¯æˆ‘çš„è¿½æ±‚ã€‚ è™½ç„¶åœ¨æˆä¸ºé‚£ä¸ªç‹¬ç‰¹çš„è‡ªå·±çš„æ—¶å€™ï¼Œä¼šèµ°å¾ˆå¤šçš„å¼¯è·¯ï¼Œç„¶åæ‰çŸ¥é“æ€ä¹ˆæ ·çš„è‡ªå·±é€‚åˆè‡ªå·±ã€‚ä»€ä¹ˆçš„ä¸œè¥¿æ˜¯åº”è¯¥ä¸¢æ‰çš„ã€‚ ä¿®ç‚¼ï¼Œä¿®ç‚¼ï¼Œä¿®ç‚¼ ä¸èƒ½å†åƒå¤–å–äº†ï¼ Â¶20200902 å¤ä¹  ä»Šå¤©å¤ä¹ è½¯ä»¶å®‰å…¨æ€§åˆ†æ Â¶20200901 å¤ä¹  ä»Šå¤©æ—©ä¸Šå¤ä¹ èƒŒè¯µ ä»Šå¤©ä¸‹åˆæ•¢æŠ¥å‘Š ä»Šå¤©æ™šä¸ŠAndroidæ¼æ´ï¼Œå“ï¼Œè™½ç„¶å†™è¿‡android apk,ä½†æ˜¯å®‰å…¨é—®é¢˜å¥½éš¾å•Š 202008 Â¶2020-831 å¤ä¹ +ç©è€ ä»Šå¤©æ—©ä¸Šå¤ä¹ è½¯ä»¶å®‰å…¨æ€§åˆ†æ ä»Šå¤©ä¸‹åˆå’Œé¹å“¥å‡ºå»çœ‹ç”µå½±äº† ä»Šå¤©æ™šä¸Šå¤ä¹ è½¯ä»¶å®‰å…¨æ€§åˆ†æ Â¶20200830 å¤ä¹ ing ä»Šå¤©ç»§ç»­è½¯ä»¶å®‰å…¨æ€§åˆ†æï¼ˆæ— èŠï¼Œä½æ•ˆç‡) å†³å®šæ™šä¸ŠæŠŠæŠ¥å‘Šå®Œæˆäº† çªç„¶æƒ³çœ‹è®ºæ–‡äº†ï¼Œæˆ‘å‘ç°æ˜¯çš„çš„ç¡®ç¡®çˆ±ä¸Šç¤¾ä¼šç»æµå­¦çš„ä¸œè¥¿äº†ï¼Œç³Ÿäº†å•Šã€‚æ„Ÿè§‰è¦æ–­é€å¤šå¹´å»äº’è”ç½‘å…¬å¸çš„æ¢¦æƒ³äº†ï¼ˆå…¶å®è‡ªå·±å­¦çš„ä¹Ÿä¸å¥½)ï¼Œæ˜¯ä¸æ˜¯è¦å»ä½“åˆ¶å†…å·¥ä½œäº†å•Šï¼æ„Ÿè§‰å»ä½“åˆ¶å†…å·¥ä½œè¦æ··ä¸ªåšå£«å­¦ä½æ‰è¡Œå•Šã€‚çœ‹äº†å®éªŒå®¤åšå£«æ¯•ä¸šçš„æ¡ä»¶å’Œå¥–åŠ±è§„åˆ™ï¼Œçªç„¶å‘ç°è¯»åšå¥½éš¾å•Šï¼(ä¸»è¦æ˜¯æˆ‘æƒ³3-4å¹´è¾¾æ ‡å•Šï¼‰ã€‚é‚£æˆ‘æ–­ä¸æ–­é€å»äº’è”ç½‘å·¥ä½œçš„æƒ³æ³•å•Šï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ æŒ‰ç…§ç¡•å£«è¦æ±‚å¹²ï¼Ÿ 1**ï¼‰åšå£«ç”Ÿå››å¹´æ—¶é—´åº”è¯¥æœ‰ä¸€ä¸ªæ¯”è¾ƒå¥½çš„ç ”ç©¶è§„åˆ’ã€‚åšå£«å¤§è®ºæ–‡ä¸€èˆ¬åªéœ€è¦6-7ç« èŠ‚ï¼ŒåŸºæœ¬ä¸Šæ¯ä¸€å­¦æœŸå®Œæˆä¸€ç« èŠ‚å°±å¯ä»¥äº†ã€‚åšå£«ç”Ÿç¬¬ä¸€å¹´ï¼Œå­¦æ ¡å®‰æ’å¤§å®¶ä¸Šè¯¾ã€‚æœ‰å¾ˆå¤šå­¦ç”Ÿåœ¨è¿™ä¸€å¹´æ—¶é—´é‡Œé¢å»¶ç»­æœ¬ç§‘ç”Ÿå­¦ä¹ ä¹ æƒ¯ï¼Œå°†è€ƒè¯•æˆç»©æ”¾åœ¨ç¬¬ä¸€ä½ä¸Šã€‚å¯¹äºå–å¾—å¥½çš„å­¦ä¹ æˆç»©ã€æ‰“å¥½æ‰å®çš„å­¦ç§‘åŸºç¡€ï¼Œå›ºç„¶æœ‰å¿…è¦ï¼Œ ä½†åšå£«ç”Ÿç¬¬ä¸€å­¦å¹´ä¸€å®šä¸èƒ½å¤Ÿåªåšâ€œå°é•‡è¯»ä¹¦é’å¹´â€ï¼Œé™¤äº†ä¹¦æœ¬è€ƒè¯•ï¼Œä¸€å®šè¦å°½å¯èƒ½æ—©ç‚¹åŠ¨æ‰‹å¼€å±•ç ”ç©¶å·¥ä½œã€‚ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä¸€ç›´è§‰å¾—åšå£«ç”Ÿç¬¬ä¸€å­¦å¹´ç»“æŸæ—¶ï¼Œåº”è¯¥æ’°å†™å®Œæˆåšå£«è®ºæ–‡ç¬¬ä¸€ç« â€œç ”ç©¶èƒŒæ™¯å’Œå›½å†…å¤–ç ”ç©¶è¿›å±•â€95%ä»¥ä¸Šçš„å·¥ä½œï¼ŒåŒæ—¶ï¼Œæ’°å†™å®Œæˆåšå£«è®ºæ–‡ç¬¬äºŒç« â€œå®éªŒæ–¹æ³•ä¸åŸºæœ¬ç†è®ºâ€çš„åŸºæœ¬å†…å®¹ã€‚ åšå£«ç¬¬äºŒå­¦å¹´ï¼ŒåŠªåŠ›å®Œæˆç¬¬ä¸‰ã€å››ç« çš„ç ”ç©¶å†…å®¹ï¼Œå¹¶æ ¹æ®ç ”ç©¶å†…å®¹æ’°å†™1-2ç¯‡ä¸­ç­‰æ°´å‡†çš„å­¦æœ¯è®ºæ–‡ã€‚ åšå£«ç¬¬ä¸‰å­¦å¹´ï¼Œå®Œæˆåšå£«è®ºæ–‡ç¬¬äº”ç« ã€ç¬¬å…­ç« çš„ç ”ç©¶å†…å®¹ï¼Œæ ¹æ®ç ”ç©¶ç»“æœå‘è¡¨2-3ç¯‡è¾ƒé«˜æ°´å‡†çš„å­¦æœ¯è®ºæ–‡ã€‚ åšå£«ç¬¬å››å­¦å¹´ä¸ŠåŠå¹´ï¼Œè¡¥å……ã€å®Œå–„åšå£«è®ºæ–‡ç¬¬äºŒç« -ç¬¬å…­ç« ç ”ç©¶å†…å®¹ï¼Œæ’°å†™ç¬¬ä¸ƒç« ç»“è®ºéƒ¨åˆ†ã€‚é€šè¿‡å¯¹åšå£«ç ”ç©¶å†…å®¹è¿›è¡Œç³»ç»Ÿæ€§æ€»ç»“ï¼Œå‘è¡¨ä¸€ç¯‡é«˜æ°´å¹³çš„å­¦æœ¯è®ºæ–‡ã€‚ä¸æ­¤åŒæ—¶ï¼Œåšå£«ç”Ÿåœ¨ç¬¬å››å¹´ä¸ŠåŠå¹´å°†è‡ªå·±çš„åšå£«è®ºæ–‡ç¬¬ä¸€ç¨¿é€’äº¤ç»™å¯¼å¸ˆï¼Œè®©å…¶æœ‰å……è¶³çš„æ—¶é—´å¸®åŠ©ä½ ä¿®æ”¹ã€‚å¦‚æœå¯¼å¸ˆå‘ç°æœ‰ä¸é€‚åˆçš„éƒ¨åˆ†ï¼Œå¯ä»¥æŠ“ç´§æ—¶é—´è¿›è¡Œè¡¥å……å’Œå®Œå–„ã€‚åˆ°ç¬¬å››å­¦å¹´ä¸‹åŠå¹´ï¼Œå°±æ˜¯å®‰æ’é€å®¡ã€å‡†å¤‡ç­”è¾©ï¼Œè¿˜æœ‰è¶³å¤Ÿçš„æ—¶é—´æ‰¾å·¥ä½œã€‚ å¦‚æœåšå£«ç”Ÿèƒ½å¤ŸæŒ‰ç…§ä¸Šé¢çš„è¿™ä¸ªè®¡åˆ’å»å¼€å±•å·¥ä½œï¼Œåšå£«æœŸé—´å¯ä»¥å‘è¡¨5ç¯‡ä»¥ä¸Šå­¦æœ¯è®ºæ–‡ï¼Œåº”è¯¥éƒ½ä¼šè¶…è¿‡å­¦æ ¡çš„æ¯•ä¸šè¦æ±‚ï¼Œåšå£«ä¸æ­£å¸¸æ¯•ä¸šæ˜¯æ²¡æœ‰ç†ç”±çš„ã€‚** çªç„¶å‘ç°è¯»ç ”è¯»åšè·Ÿå·¥ä½œæ€§è´¨æ˜¯å·®ä¸å¤šçš„ï¼Œéƒ½æ˜¯é è‡ªå·±çš„æœ¬äº‹ç»™è€æ¿æœåŠ¡ã€‚å—¯å—¯ï¼Œå°±æ˜¯è¿™ç§ç›¸å¤„æ¨¡å¼ã€‚ Â¶20200829 å¤ä¹ ing ä»Šå¤©æ—©ä¸Šå’ŒåŒå­¦èŠå¤©ï¼Œå±…ç„¶å‡ºå»å®ä¹ äº† ä»Šå¤©æ™šä¸Šå’Œä¸‹åˆå¤ä¹ è½¯ä»¶å®‰å…¨æ€§åˆ†æï¼ˆéš¾ï¼‰ Â¶202008028 å¯†ç å­¦è€ƒè¯• ä»Šå¤©æ—©ä¸Šé å¯†ç å­¦ ä»Šå¤©ä¸‹åˆæ”¶æ‹¾åšå®¢æ–‡ç«  ä»Šå¤©æ™šä¸Šï¼Œç”±äºä¸‹å¤§é›¨ï¼Œå›å¯å®¤çœ‹å‰§äº† Â¶20200827 å·¥ç¨‹ä¼¦ç† ä»Šå¤©æ—©ä¸Šï¼Œæœç´¢å¾ˆå¤šæ•°æ®åˆ†æçš„èµ„æ–™ï¼Œæ„Ÿè§‰ä¸šåŠ¡å‹æ•°æ®åˆ†æå¸ˆè¦æ¥è§¦å¤ªå¤šä¸šåŠ¡ï¼ˆè½¬è¡Œçš„äººå¤šå•Šï¼Œå¤§å‚èƒ½å‘æŒ¥ä½œç”¨)ï¼ŒæŠ€æœ¯å‹æ•°æ®åˆ†æå¸ˆè¦ç®—æ³•å­¦çš„å¾ˆå¥½ï¼ˆæˆ‘åˆæ²¡æœ‰å¾ˆç‰¹åˆ«çš„å¤´è„‘ï¼Œé¡¹ç›®ç»å†ï¼‰ã€‚ä½†æ˜¯å‘¢ï¼Ÿæ•°æ®å¤„ç†ï¼Œæ•°æ®å»ºæ¨¡ï¼Œæ•°æ®å¯è§†åŒ–æ˜¯å¹²ä»€ä¹ˆéƒ½è¦ç”¨åˆ°çš„ã€‚è¿˜æ˜¯æˆ‘æœ¬ç§‘çš„æ°´å¹³ã€‚ä½†æ˜¯æˆ‘è¿˜æ˜¯æƒ³è·¨ç•Œå•Šï¼Œæœç„¶æ˜¯çˆ±ä¸€è¡Œï¼Œçˆ±ä¸Šä¸€è¡Œã€‚é±¼å’Œç†ŠæŒä¸å¯å…¼å¾—å•Šï¼Œè¦å–èˆã€‚ https://ask.hellobi.com/blog/qinlu/10261 ä»Šå¤©ä¸‹åˆï¼Œè€ƒå·¥ç¨‹ä¼¦ç†ã€‚å¿ƒå¡å¡ã€‚ ä»Šå¤©æ™šä¸Šï¼Œçœ‹äº†æˆ‘åœ¨é¢å’Œå›­ã€‚å†å²æ–‡ç‰©ï¼Œå›­æ—é£æ™¯å¤ªæ£’äº†ã€‚è‹±è¯­ï¼šã€Šbreakfast or lunchã€‹ã€‚åˆè¿›ä¸€æ­¥è¿›è¡Œäº†èŒä¸šè§„åˆ’ï¼› å¤ä¹ å¯†ç å­¦ï¼›å­¦ä¹ äº†tableauï¼Œæœç„¶æ¼‚äº®å•Šï¼ æˆ‘æƒ³äº†æƒ³ï¼ŒæŠŠæ¯•ä¸šè®¾è®¡åšå®Œäº†ï¼ŒæŠŠæ—¶é—´åºåˆ—é¡¹ç›®åšå¥½ï¼ŒæŠŠæ‰¾å®ä¹ ç›¸å…³å‡†å¤‡åšå¥½äº†ï¼Œçœ‹çœ‹å‰§ï¼Œå…»å…»èŠ±ï¼Œè¿™æ ·çš„ç ”ç©¶ç”Ÿå¤šå¥½å•Šï¼Œå¹²å˜›è¦ä¸è‡ªå·±æçš„é‚£ä¹ˆç´¯ã€‚åƒå¥½å–å¥½ï¼Œæ—¶ä¸æ—¶ç»™å®¶äººä¹°ç¤¼ç‰©ã€‚ å•¦å•¦å•¦å•¦å•¦å•¦å•¦å•¦å•¦å•¦å•¦å•¦å•¦å•¦ã€‚ä¸è¦è¿½ä¸€åŒ¹é©¬ï¼Œä½ ç”¨è¿½é©¬çš„æ—¶é—´å»ç§è‰ï¼Œå¾…æ˜¥æš–èŠ±å¼€æ—¶ï¼Œèƒ½å¸å¼•ä¸€æ‰¹éªé©¬æ¥ä¾›ä½ é€‰æ‹©ï¼›æ‰€ä»¥åªè¦è‡ªå·±è¶³å¤Ÿä¼˜ç§€ï¼Œä½ å°±æœ‰å¤šå¤§çš„æƒåŠ›é€‰æ‹©è‡ªå·±çš„å–œå¥½ã€‚å°±è‡ªå·±åŠªåŠ›çš„åŠ¨åŠ›å•Šï¼ Â¶202007826 Deadline ä»Šå¤©æ—©ä¸Šï¼Œå¤ä¹ äº†å…¨éƒ¨çš„å¯†ç å­¦ã€‚ ä»Šå¤©ä¸‹åˆï¼Œå¤ä¹ äº†å·¥ç¨‹ä¼¦ç†çš„åˆ†æé¢˜ã€‚ ä»Šå¤©æ™šä¸Šï¼Œå­¦ä¹ äº†æ•°æ®åˆ†æä¸­çš„é€»è¾‘æ€ç»´ï¼Œå¦‚ä½•ç²¾å‡†åŒ–äº§å“è®¾è®¡ï¼Œäº§ä¸šè¿è¥ã€‚ æˆ‘å·²ç»å—å¤Ÿäº†è¿™æ ·çš„ç”Ÿæ´»äº†ã€‚å°½æ—©å®ä¹ ï¼Œæœ‰äººå†…æ¨ï¼Œå°±çœ‹è‡ªå·±äº†ã€‚ @@@@@@@ æˆ‘ä¸æ˜¯ä¸€ä¸ªçƒ­è„¸ç‰¹å†·å±è‚¡çš„äººã€‚ Â¶20200825 ä¸ƒå¤•å¿«ä¹ ä»Šå¤©æ—©ä¸Šï¼Œèµ·åºŠå‘ç°è‡€éƒ¨å¥½ç–¼å•Š ä»Šå¤©ä¸‹åˆç»§ç»­å¤ä¹ å¯†ç å­¦ å¯è¯æ˜çš„åŠ å¯†å®‰å…¨æ€§ï¼›æ•°å­—ç­¾åå®‰å…¨æ€§ç›¸å…³æ¦‚å¿µã€‚å…¬é’¥ä½“åˆ¶çš„å®‰å…¨æ€§ ä»Šå¤©ä¸‹åˆå­¦ä¹ äº†A/B(whatï¼Œwhy, how)ï¼Œç”¨äºè¯„ä¼°æŸç§äº§å“å’Œè®¾è®¡æ˜¯å¦æœ‰æ•ˆçš„æä¾›äº†æŸé¡¹æŒ‡æ ‡ï¼Œè¿›è€Œæœ‰åŠ©äºè¾…åŠ©å†³ç­–ã€‚ æ— è®ºæ˜¯å…¬å¸ï¼ŒåŒçº§ï¼Œæ™®é€šæœ‹å‹ï¼Œç”·å¥³æœ‹å‹â€”â€”åˆ©ç›Šç¬¬ä¸€ï¼Œæ„Ÿæƒ…å€’æ•°ç¬¬ä¸€ã€‚èƒ½å®ç°åŒèµ¢æ˜¯æœ€å¥½ã€‚ è®ºèµšé’±çš„é‡è¦æ€§ï¼Œæˆ‘åˆä¸æ˜¯éä½ ä¸å¯ã€‚ä¸å–œæ¬¢çš„ä¸œè¥¿ï¼Œæˆ‘æ€ä¹ˆéƒ½ä¸ä¼šå°†å°±ã€‚ Â¶20200824 å¤ä¹ ing ä»Šå¤©æ—©ä¸Šåœ¨å¯å®¤å¤ä¹ æ§åˆ¶æµå®Œæ•´æ€§ ä»Šå¤©ä¸‹åˆå¤ä¹ æ¨¡ç³Šæµ‹è¯•ï¼Œfuzzingï¼Œå„ç§ç»†èŠ‚ ä»Šå¤©æ™šä¸Šä½æ•ˆç‡å¤ä¹ å¯†ç å­¦åˆ†ç»„å¯†ç å’ŒåŸºæœ¬æ¦‚å¿µï¼Œè¿˜å¯è¯æ˜å®‰å…¨æ€§ï¼Œç¬¬ä¸ƒç« çš„åŸºæœ¬æ¦‚å¿µï¼ŒåŒºåˆ«å…¬é’¥å¯†ç ä½“åˆ¶ï¼ˆä¸‰ç§ï¼‰ï¼Œ ã€Šåƒé±¼ã€‹å¥½å¬ 5W2Håˆ†ææ³•åœ¨æ•°æ®åˆ†æä¸­çš„åº”ç”¨ æœŸæœ«è€ƒè¯•å’‹å›äº‹èƒŒè¯µå‘¢ï¼Ÿè¿˜æ˜¯æ•°å­¦å¥½ï¼Œä¸€ä¸ªå®šç†åšå‡ åé“é¢˜ï¼Œè¿™ç§å­¦ç§‘å‡ ä¸ªç‚¹éƒ½ä¸ä¸€å®šè€ƒ ä¸‹å‘¨ï¼Œå…¨æ–°å…¨æ„çš„çœ‹è§†é¢‘ï¼Œçœ‹PPTï¼Œæ‰“å°è®ºæ–‡èƒŒè¯µ èƒŒè¯µï¼Œå®Œæˆè¯¾ç¨‹æŠ¥å‘Šã€‚ Â¶20200823 å¤ä¹ ing ä»Šå¤©æ—©ä¸Šå¯å®¤æ´—è¡£æœï¼ˆå‘¨æœ«å˜›ï¼Œå¤šç¡ä¼š) ä»Šå¤©ä¸‹åˆç»§ç»­å¤ä¹ å·¥ç¨‹ä¼¦ç†ï¼Œã€‚ã€‚ã€‚æ–‡ç§‘ã€‚ã€‚ã€‚è¿˜çœ‹äº†æˆ‘åœ¨é¢å’Œå›­ç­‰ä½ ã€‚æˆ‘è¿˜åœ¨æƒ³æ€ä¹ˆç”¨å·¥ç¨‹ä¼¦ç†åˆ†æç”Ÿæ´»ä¸­çš„å°ä¸‰ã€‚ ä»Šå¤©æ™šä¸Šå†™äº†å·¥ç¨‹ä¼¦ç†çš„è¯¾ä»¶ï¼Œ å›å¯å®¤å¥èº«ï¼Œå“‘é“ƒåˆ°äº† ååƒ»ä¹‹åœ°ï¼Œååƒ»ä¹‹åœ°ï¼Œååƒ»ä¹‹åœ° åˆ»æ„çš„ç»ƒä¹ ï¼ä¸ä¼šä»€ä¹ˆï¼Œå°±è¡¥å……ä»€ä¹ˆã€‚å›¢ç»“å°±æ˜¯åŠ›é‡ Â¶20200822 å¹³æ·¡æ— å¥‡çš„ç”Ÿæ´» ä»Šå¤©æ—©ä¸Šä½æ•ˆç‡çš„å¤ä¹ äº†å¯†ç å­¦å’Œè½¯ä»¶å®‰å…¨æ€§åˆ†æï¼Œå¤ªéš¾è®°å¿†äº†ã€‚çœ‹å‰§æ—¥å¸¸ ä»Šå¤©ä¸‹åˆå¤ä¹ ç»§ç»­ ä»Šå¤©æ™šä¸Šç»§ç»­å¤ä¹ ï¼Œå•Šï¼Œä¸ºä»€ä¹ˆé‚£ä¹ˆå¤šè¦èƒŒçš„ä¸œè¥¿å•Š Â¶20200821 æ— èŠçš„ç”Ÿæ´» æ˜¨å¤©æ™šä¸Šç¡ä¸å¥½ï¼Œç›–è¢«å­å¤ªçƒ­ï¼Œä¸ç›–åˆå¤ªå†·ï¼ŒæŠ˜è…¾ç€ç¡ä¸ç€ï¼Œæˆ‘çš„å¤©å•Šã€‚ ä»Šå¤©æ—©ä¸Šå¤‡ä»½èµ„æ–™ã€‚ ä»Šå¤©ä¸‹åˆå’Œæ™šä¸Šç»§ç»­å¤ä¹ ã€‚å¥½å¿ƒå¡å•Šï¼Œå¤ªéš¾èƒŒäº†ï¼Œæ„Ÿè§‰åœ¨è€ƒæ–‡ç§‘å•Šã€‚ä¸‹é¢æ—¶é—´å…¨éƒ¨å¤ä¹ äº†ã€‚ çœ‹ä¸‹å¦‚ä½•é€šè¿‡Pythonçˆ¬å–å¾®ä¿¡å…¬ä¼—å·ä¸Šçš„å…¨éƒ¨æ–‡ç«  å¯¹ä¸èµ·ï¼Œå…¶å®æ˜¯æœ€æ²¡æœ‰ç”¨çš„è¯è¯­äº†ã€‚ ä¸ºäº†æ¯å¤©å°‘å¾€è¿”å®éªŒå®¤ä¸€ä¸ªæ¥å›ï¼Œæˆ‘å†³å®šæ—©ä¸Šåœ¨å¯å®¤å­¦ä¹ äº†ï¼Œè¿™æ ·è¿˜å¯ä»¥åœ¨åºŠä¸Šèººåˆ°7:45ï¼Œä¹Ÿä¸æ‰“æ‰°å®¤å‹èµ·åºŠã€‚ Â¶20200820 è¿”æ ¡è®° æƒ³ä¸åˆ°æˆ‘é©¬ä¸Šç ”äºŒäº†ï¼Œæ„Ÿè§‰è¿˜æ˜¯ä¸€äº‹æ— æˆçš„æ ·å­ã€‚å¾—ç»™è‡ªå·±é€‰å®šæ–¹å‘ï¼Œå¿«é€Ÿå‘å±•äº† Â¶20200819 è·¯æ¼«æ¼«å…¶ä¿®è¿œå…® å†ä¸åŠªåŠ›ï¼Œä½ å°±ç”Ÿé”ˆäº†ã€‚äº²çˆ±çš„ï¼Œå¥³å­©å­ã€‚ ä½ è§ æˆ–è€…ä¸è§æˆ‘ æˆ‘å°±åœ¨é‚£é‡Œ ä¸æ‚²ä¸å–œ ä½ å¿µ æˆ–è€…ä¸å¿µæˆ‘ æƒ…å°±åœ¨é‚£é‡Œ ä¸æ¥ä¸å» ä½ çˆ± æˆ–è€…ä¸çˆ±æˆ‘ çˆ±å°±åœ¨é‚£é‡Œ ä¸å¢ä¸å‡ ä½ è·Ÿ æˆ–è€…ä¸è·Ÿæˆ‘ æˆ‘çš„æ‰‹å°±åœ¨ä½ æ‰‹é‡Œ ä¸èˆä¸å¼ƒ æ¥æˆ‘çš„æ€€é‡Œ æˆ–è€… è®©æˆ‘ä½è¿›ä½ çš„å¿ƒé‡Œ é»˜ç„¶ ç›¸çˆ± å¯‚é™ å–œæ¬¢]]></content>
      <categories>
        <category>å­¦ä¹ ã®å†ç¨‹(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>Daily</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ•°æ®åˆ†æä¸æœºå™¨å­¦ä¹ ä¹‹è¯­è¨€ç¯‡]]></title>
    <url>%2F2020%2F08%2F11%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%AF%AD%E8%A8%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[ç¥ç»ç½‘ç»œPytorch Â¶åŸºç¡€è¯­æ³• Â¶å¸¸è§ç¥ç»ç½‘ç»œçš„å®ç° Â¶çº¿æ€§æ¨¡å‹ Â¶å¤šå±‚æ„ŸçŸ¥æœº Â¶CNN Â¶LSTM Â¶GAN]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ ä¹‹æ¨¡å‹æ€§èƒ½]]></title>
    <url>%2F2020%2F08%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æ­£åˆ™åŒ–</tag>
        <tag>äº¤å‰éªŒè¯</tag>
        <tag>ç‰¹å¾å·¥ç¨‹</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Book-Learning]]></title>
    <url>%2F2020%2F07%2F29%2FBook-Learning%2F</url>
    <content type="text"><![CDATA[åˆ†äº«çœ‹ä¹¦ç¬”è®° æ•°æ®æŒ–æ˜â€”â€”æ¦‚å¿µä¸æŠ€æœ¯ Â¶æ•°æ®æŒ–æ˜çš„æ¦‚å¿µ æ•°æ®æŒ–æ˜æ˜¯ä»å¤§é‡æ•°æ®ä¸­æå–æˆ–æŒ–æ˜çŸ¥è¯†ã€‚å¼ºè°ƒï¼šä»å¤§é‡çš„ï¼ŒæœªåŠ å·¥çš„ææ–™ä¸­å‘ç°å°‘é‡é‡‘å—è¿™ä¸€è¿‡ç¨‹ã€‚ åŸºæœ¬è¿‡ç¨‹ï¼š æ•°æ®æ¸…ç†ã€‚(æ¶ˆé™¤å™ªéŸ³æˆ–è€…ä¸ä¸€è‡´çš„æ•°æ®) æ•°æ®é›†æˆï¼ˆå¤šç§æ•°æ®é›†æˆå¯ä»¥ç»„åˆåœ¨ä¸€èµ·ï¼‰ æ•°æ®é€‰æ‹©ï¼ˆä»æ•°æ®åº“ä¸­æå–ä¸åˆ†æä»»åŠ¡ç›¸å…³çš„æ•°æ®) æ•°æ®å˜æ¢ã€‚(æ•°æ®å˜æ¢æˆ–ç»Ÿä¸€æˆé€‚åˆæŒ–æ˜å½¢å¼) æ•°æ®æŒ–æ˜(ä½¿ç”¨æ™ºèƒ½æ–¹æ³•æå–æ•°æ®) æ¨¡å¼è¯„ä¼° çŸ¥è¯†è¡¨ç¤ºï¼ˆå¯è§†åŒ–ç­‰æ‰‹æ®µï¼Œå±•ç¤ºæŒ–æ˜çš„çŸ¥è¯†) Â¶æ•°æ®é¢„å¤„ç† é—®é¢˜ï¼šç°å®ä¸­æ•°æ®ææ˜“å—å™ªéŸ³æ•°æ®ã€é—æ¼æ•°æ®å’Œä¸ä¸€è‡´æ€§æ•°æ®çš„å¹²æ‰°ã€‚ ä¸å®Œæ•´ï¼ˆå±æ€§å€¼ç¼ºå¤±ï¼‰ å™ªéŸ³(é”™è¯¯å±æ€§å€¼ï¼Œç¦»ç¾¤å€¼) ä¸ä¸€è‡´æ€§ï¼ˆç¼–ç ) æ•°æ®æ¸…ç†ã€‚åŒ…æ‹¬å¡«å†™é—æ¼å€¼ï¼Œå¹³æ»‘å™ªéŸ³æ•°æ®ï¼Œè¯†åˆ«ã€åˆ é™¤å±€å¤–è€…ï¼Œå¹¶è§£å†³ä¸ä¸€è‡´æ¥â€œæ¸…ç†æ•°æ®â€ã€‚è„æ•°æ®è½¬åŒ–ä¸ºå¹²å‡€æ•°æ®ã€‚ æ•°æ®é›†æˆã€‚ä¸åŒæ•°æ®æºçš„æ•°æ®é›†æˆã€‚ Â¶æ•°æ®æ¸…æ´— Â¶é—æ¼å€¼ å¿½ç•¥ äººå·¥å¡«å†™ å…¨å±€å˜é‡ å¹³å‡å€¼ åˆ†ç»„å¡«å…… Â¶å™ªéŸ³æ•°æ® å™ªéŸ³æ˜¯æŒ‡æµ‹é‡å˜é‡çš„éšæœºè¯¯å·®æˆ–åå·®ã€‚å¹³æ»‘æ•°æ®ã€‚ åˆ†ç®±ã€‚å­˜å‚¨å€¼è¢«åˆ†å¸ƒåˆ°ä¸€äº›ç®±å­ä¸­ï¼Œç„¶åå±€éƒ¨å¹³æ»‘ã€‚æŒ‰å¹³å‡å€¼å¹³æ»‘ã€‚æŒ‰ä¸­å€¼å¹³æ»‘ã€‚æŒ‰è¾¹ç•Œå¹³æ»‘ã€‚ èšç±»ã€‚ å›å½’ Â¶ä¸ä¸€è‡´æ•°æ® Â¶æ•°æ®é›†æˆ å®ä½“è¯†åˆ«ã€‚ å†—ä½™ã€‚ Â¶æ•°æ®å˜æ¢ å°†æ•°æ®è½¬æ¢æˆé€‚ç”¨äºæŒ–æ˜çš„å½¢å¼ å¹³æ»‘ã€‚ èšé›†ã€‚å¯¹æ•°æ®è¿›è¡Œæ±‡æ€»å’Œèšé›†ã€‚æ—¥ã€æœˆå’Œå¹´é”€å”®é¢ã€‚å¤šç²’åº¦æ•°æ®åˆ†ææ„é€ æ•°æ®æ–¹ æ•°æ®æ³›åŒ–ã€‚street-&gt;city; ageâ€“&gt;young, middle-age,senior è§„èŒƒåŒ–ã€‚ å±æ€§æ•°æ®æŒ‰æ¯”ä¾‹ç¼©æ”¾ï¼Œç¼©æ”¾åˆ°ç‰¹å®šåŒºé—´ã€‚ å±æ€§æ„é€ ã€‚ æœ€å°-æœ€å¤§è§„èŒƒåŒ– $$ v = \frac{v-min}{max-min}(new_max-new_min)+new_min $$ z-scoreè§„èŒƒåŒ– $$ v = \frac{v-mean}{\delta} $$ Â¶æ•°æ®å½’çº¦ æ•°æ®æ–¹èšé›† ç»´å½’çº¦ æ•°æ®å‹ç¼© æ•°å€¼å‹ç¼© æ¦‚å¿µæè¿°ï¼šç‰¹å¾ä¸æ¯”è¾ƒ æ•°æ®æŒ–æ˜åˆ†ä¸ºä¸¤ç±»: æè¿°å¼å’Œé¢„æµ‹å¼æŒ–æ˜ã€‚æè¿°å¼æä¾›æ•°æ®çš„æœ‰è¶£çš„ä¸€èˆ¬æ€§è´¨ã€‚å»ºç«‹ä¸€ä¸ªæˆ–ä¸€ç»„æ¨¡å‹ï¼Œå¹¶è¯•å›¾é¢„æµ‹æ–°æ•°æ®é›†çš„è¡Œä¸ºã€‚ æè¿°å¼æ•°æ®æŒ–æ˜ç§°ä¸ºæ¦‚å¿µæè¿°ã€‚ä¸åŒçš„ç²’åº¦å’Œè§’åº¦æè¿°æ•°æ®é›†ã€‚ åº¦é‡å¯ä»¥æ ¹æ®å…¶æ‰€ç”¨çš„èšé›†å‡½æ•°åˆ†æˆä¸‰ç±»ï¼š åˆ†å¸ƒçš„ï¼šä¸€ä¸ªèšé›†å‡½æ•°æ˜¯åˆ†å¸ƒçš„ï¼Œå¦‚æœå®ƒèƒ½ä»¥å¦‚ä¸‹åˆ†å¸ƒæ–¹å¼è¿›è¡Œè®¡ç®—ï¼šè®¾æ•°æ®è¢«åˆ’åˆ†ä¸º n ä¸ªé›†åˆï¼Œå‡½æ•°åœ¨æ¯ä¸€éƒ¨åˆ†ä¸Šçš„è®¡ç®—å¾—åˆ°ä¸€ä¸ªèšé›†å€¼ã€‚å¦‚æœå°†å‡½æ•°ç”¨äº n ä¸ªèšé›†å€¼å¾—åˆ°çš„ç»“æœï¼Œä¸å°†å‡½æ•° ç”¨äºæ‰€æœ‰æ•°æ®å¾—åˆ°çš„ç»“æœä¸€æ ·ï¼Œåˆ™è¯¥å‡½æ•°å¯ä»¥ç”¨åˆ†å¸ƒæ–¹å¼è®¡ç®—ã€‚ ä»£æ•°çš„ï¼šave(); æ•´ä½“çš„ï¼šrank(),median() Â¶æè¿°æ€§ç»Ÿè®¡åº¦é‡ Â¶åº¦é‡ä¸­å¿ƒè¶‹åŠ¿ å¹³å‡å€¼ åŠ æƒç®—æœ¯å¹³å‡ï¼ˆåŠ æƒå¹³å‡) ä¸­ä½æ•° åˆ†ä½æ•° æŒ–æ˜å¤§å‹æ•°æ®åº“ä¸­çš„å…³è”è§„åˆ™ å…³è”è§„åˆ™æŒ–æ˜å‘ç°å¤§é‡æ•°æ®ä¸­é¡¹é›†çš„å…³è”æˆ–è€…ç›¸å…³è”ç³»ã€‚å¯ç”¨äºäºåˆ†ç±»è®¾è®¡ï¼Œäº¤å‰è´­ç‰©å’Œè´±å–åˆ†æã€‚ è´­ä¹°è®¡ç®—æœºä¹Ÿè¶‹å‘äºåŒæ—¶è´­ä¹°è´¢åŠ¡ç®¡ç†è½¯ä»¶çš„å…³è”è§„åˆ™ $$ computer =&gt; finanical_management_software\ [support = 2%, confidence = 60%] $$ æ”¯æŒåº¦ï¼šæœ‰ç”¨æ€§ã€‚åŒæ—¶è´­ä¹°è®¡ç®—æœºå’Œè´¢åŠ¡ç®¡ç†è½¯ä»¶ã€‚ ç½®ä¿¡åº¦ï¼šç¡®å®šæ€§ã€‚è´­ä¹°è®¡ç®—æœºçš„é¡¾å®¢æœ‰å¤šå°‘è´­ä¹°è´¢åŠ¡ç®¡ç†è½¯ä»¶ã€‚ å¯æœ€å°æ”¯æŒåº¦é˜™å€¼å’Œæœ€å°ç½®ä¿¡åº¦é˜™å€¼ã€‚ Â¶Apriori ç®—æ³•ï¼š ä½¿ç”¨å€™é€‰é¡¹é›†æ‰¾é¢‘ç¹é¡¹é›† åˆ†ç±»å’Œé¢„æµ‹ åˆ†ç±»å’Œé¢„æµ‹æ˜¯æ•°æ®åˆ†æçš„ä¸¤ç§å½¢å¼ï¼Œå¯ä»¥ç”¨äºæå–æè¿°é‡è¦æ•°æ®ç±»çš„æ¨¡å‹æˆ–é¢„æµ‹æœªæ¥çš„æ•°æ®è¶‹åŠ¿ã€‚ åˆ†ç±»é¢„æµ‹åˆ†ç±»æ ‡å·ï¼ˆç±»ï¼‰ï¼Œè€Œé¢„æµ‹å»ºç«‹è¿ç»­å€¼å‡½æ•°æ¨¡å‹ã€‚ é¢„æµ‹çš„å‡†ç¡®ç‡ã€è®¡ç®—é€Ÿåº¦ã€é²æ£’æ€§ã€å¯è§„æ¨¡æ€§å’Œå¯è§£é‡Šæ€§æ˜¯è¯„ä¼°åˆ†ç±»å’Œé¢„æµ‹æ–¹æ³•çš„äº”æ¡æ ‡å‡†ã€‚ èšç±»åˆ†æ æ•°æ®å¯¹è±¡çš„é›†åˆè¿›è¡Œåˆ†æï¼Œä½†ä¸åˆ†ç±»ä¸åŒçš„æ˜¯ï¼Œèšç±»åˆ†æ (clustering) å±äºéç›‘ç£å­¦ä¹ ï¼Œä¹Ÿå°±æ˜¯ä¸çŸ¥é“è¦åˆ’åˆ†ç±»æ˜¯æœªçŸ¥çš„ã€‚èšç±»åˆ†æå°±æ˜¯è¦å°†æ•°æ®å¯¹è±¡åˆ†ç»„æˆä¸ºå¤šä¸ªç±»æˆ–ç°‡(cluster)ã€‚æ¯ä¸€ä¸ªç°‡ä¸­çš„å¯¹è±¡ä¹‹é—´å…·æœ‰è¾ƒé«˜çš„ç›¸ä¼¼åº¦ï¼Œè€Œä¸åŒç°‡å¯¹è±¡å·®åˆ«è¾ƒå¤§ã€‚å¸¸å¸¸é‡‡ç”¨è·ç¦»ä½œä¸ºç›¸å¼‚åº¦çš„è¡¡é‡ã€‚]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>æ•°æ®æŒ–æ˜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å‘ç€æ°¸æ’å‡ºå‘]]></title>
    <url>%2F2020%2F07%2F28%2Fprepare-for-work%2F</url>
    <content type="text"><![CDATA[ç¼–ç¨‹è¯­è¨€çš„å­¦ä¹  ç¬¬ä¸€å‘¨ï¼šPythonåŸºç¡€çŸ¥è¯† https://github.com/jackfrued/Python-100-Days/tree/master/Day01-15 Â¶Day Ox00 åŸºæœ¬çš„æ•°æ®ç»“æ„ç±»å‹ï¼Œä»¥åŠæä¾›çš„å¸¸ç”¨æ–¹æ³•ã€‚ ç¼–ç çš„é£æ ¼å’Œè§„èŒƒæ€§ Â¶åˆè¯†python Pythonç®€ä»‹ - Pythonçš„å†å² / Pythonçš„ä¼˜ç¼ºç‚¹ / Pythonçš„åº”ç”¨é¢†åŸŸ æ­å»ºç¼–ç¨‹ç¯å¢ƒ - Windowsç¯å¢ƒ / Linuxç¯å¢ƒ / MacOSç¯å¢ƒ ä»ç»ˆç«¯è¿è¡ŒPythonç¨‹åº - Hello, world / printå‡½æ•° / è¿è¡Œç¨‹åº ä½¿ç”¨IDLE - äº¤äº’å¼ç¯å¢ƒ(REPL) / ç¼–å†™å¤šè¡Œä»£ç  / è¿è¡Œç¨‹åº / é€€å‡ºIDLE æ³¨é‡Š - æ³¨é‡Šçš„ä½œç”¨ / å•è¡Œæ³¨é‡Š / å¤šè¡Œæ³¨é‡Š ä»£ç æ³¨é‡Šé£æ ¼ 12345678910111213141516def func(arg1, arg2): &quot;&quot;&quot;åœ¨è¿™é‡Œå†™å‡½æ•°çš„ä¸€å¥è¯æ€»ç»“(å¦‚: è®¡ç®—å¹³å‡å€¼). è¿™é‡Œæ˜¯å…·ä½“æè¿°. å‚æ•° ---------- arg1 : int arg1çš„å…·ä½“æè¿° arg2 : int arg2çš„å…·ä½“æè¿° è¿”å›å€¼ ------- int è¿”å›å€¼çš„å…·ä½“æè¿° å˜é‡çš„å®šä¹‰ 12345678910æ¨¡å—å°½é‡ä½¿ç”¨å°å†™å‘½åï¼Œé¦–å­—æ¯ä¿æŒå°å†™ï¼Œå°½é‡ä¸è¦ç”¨ä¸‹åˆ’çº¿(é™¤éå¤šä¸ªå•è¯ï¼Œä¸”æ•°é‡ä¸å¤šçš„æƒ…å†µ)ç±»åä½¿ç”¨é©¼å³°(CamelCase)å‘½åé£æ ¼ï¼Œé¦–å­—æ¯å¤§å†™ï¼Œç§æœ‰ç±»å¯ç”¨ä¸€ä¸ªä¸‹åˆ’çº¿å¼€å¤´å‡½æ•°åä¸€å¾‹å°å†™ï¼Œå¦‚æœ‰å¤šä¸ªå•è¯ï¼Œç”¨ä¸‹åˆ’çº¿éš”å¼€å˜é‡åå°½é‡å°å†™, å¦‚æœ‰å¤šä¸ªå•è¯ï¼Œç”¨ä¸‹åˆ’çº¿éš”å¼€å¸¸é‡é‡‡ç”¨å…¨å¤§å†™ï¼Œå¦‚æœ‰å¤šä¸ªå•è¯ï¼Œä½¿ç”¨ä¸‹åˆ’çº¿éš”å¼€ Â¶è¯­è¨€å…ƒç´  ç¨‹åºå’Œè¿›åˆ¶ - æŒ‡ä»¤å’Œç¨‹åº / å†¯è¯ºä¾æ›¼æœº / äºŒè¿›åˆ¶å’Œåè¿›åˆ¶ / å…«è¿›åˆ¶å’Œåå…­è¿›åˆ¶ å˜é‡å’Œç±»å‹ - å˜é‡çš„å‘½å / å˜é‡çš„ä½¿ç”¨ / inputå‡½æ•° / æ£€æŸ¥å˜é‡ç±»å‹ / ç±»å‹è½¬æ¢ æ•°å­—å’Œå­—ç¬¦ä¸² - æ•´æ•° / æµ®ç‚¹æ•° / å¤æ•° / å­—ç¬¦ä¸² / å­—ç¬¦ä¸²åŸºæœ¬æ“ä½œ / å­—ç¬¦ç¼–ç  è¿ç®—ç¬¦ - æ•°å­¦è¿ç®—ç¬¦ / èµ‹å€¼è¿ç®—ç¬¦ / æ¯”è¾ƒè¿ç®—ç¬¦ / é€»è¾‘è¿ç®—ç¬¦ / èº«ä»½è¿ç®—ç¬¦ / è¿ç®—ç¬¦çš„ä¼˜å…ˆçº§ Â¶åˆ†æ”¯ç»“æ„ åˆ†æ”¯ç»“æ„çš„åº”ç”¨åœºæ™¯ - æ¡ä»¶ / ç¼©è¿› / ä»£ç å— / æµç¨‹å›¾ ifè¯­å¥ - ç®€å•çš„if / if-elseç»“æ„ / if-elif-elseç»“æ„ / åµŒå¥—çš„if Â¶å¾ªç¯ç»“æ„ å¾ªç¯ç»“æ„çš„åº”ç”¨åœºæ™¯ - æ¡ä»¶ / ç¼©è¿› / ä»£ç å— / æµç¨‹å›¾ whileå¾ªç¯ - åŸºæœ¬ç»“æ„ / breakè¯­å¥ / continueè¯­å¥ forå¾ªç¯ - åŸºæœ¬ç»“æ„ / rangeç±»å‹ / å¾ªç¯ä¸­çš„åˆ†æ”¯ç»“æ„ / åµŒå¥—çš„å¾ªç¯ / æå‰ç»“æŸç¨‹åº Â¶å‡½æ•°å’Œæ¨¡å—çš„ä½¿ç”¨ å‡½æ•°çš„ä½œç”¨ - ä»£ç çš„åå‘³é“ / ç”¨å‡½æ•°å°è£…åŠŸèƒ½æ¨¡å— å®šä¹‰å‡½æ•° - defè¯­å¥ / å‡½æ•°å / å‚æ•°åˆ—è¡¨ / returnè¯­å¥ / è°ƒç”¨è‡ªå®šä¹‰å‡½æ•° è°ƒç”¨å‡½æ•° - Pythonå†…ç½®å‡½æ•° / å¯¼å…¥æ¨¡å—å’Œå‡½æ•° å‡½æ•°çš„å‚æ•° - é»˜è®¤å‚æ•° / å¯å˜å‚æ•° / å…³é”®å­—å‚æ•° / å‘½åå…³é”®å­—å‚æ•° å‡½æ•°çš„è¿”å›å€¼ - æ²¡æœ‰è¿”å›å€¼ / è¿”å›å•ä¸ªå€¼ / è¿”å›å¤šä¸ªå€¼ ä½œç”¨åŸŸé—®é¢˜ - å±€éƒ¨ä½œç”¨åŸŸ / åµŒå¥—ä½œç”¨åŸŸ / å…¨å±€ä½œç”¨åŸŸ / å†…ç½®ä½œç”¨åŸŸ / å’Œä½œç”¨åŸŸç›¸å…³çš„å…³é”®å­— ç”¨æ¨¡å—ç®¡ç†å‡½æ•° - æ¨¡å—çš„æ¦‚å¿µ / ç”¨è‡ªå®šä¹‰æ¨¡å—ç®¡ç†å‡½æ•° / å‘½åå†²çªçš„æ—¶å€™ä¼šæ€æ ·ï¼ˆåŒä¸€ä¸ªæ¨¡å—å’Œä¸åŒçš„æ¨¡å—ï¼‰ Lambdaè¡¨è¾¾å¼ Â¶å­—ç¬¦ä¸²å’Œå¸¸ç”¨çš„æ•°æ®ç»“æ„ è¿™ä¸€å¥éå¸¸é‡è¦ï¼Œpythonæä¾›çš„æ•°æ®ç»“æ„ï¼Œå’Œå†…ç½®çš„æ–¹æ³•å¾ˆå®ç”¨ã€‚ å­—ç¬¦ä¸²çš„ä½¿ç”¨ - è®¡ç®—é•¿åº¦ / ä¸‹æ ‡è¿ç®— / åˆ‡ç‰‡ / å¸¸ç”¨æ–¹æ³• åˆ—è¡¨åŸºæœ¬ç”¨æ³• - å®šä¹‰åˆ—è¡¨ / ç”¨ä¸‹è¡¨è®¿é—®å…ƒç´  / ä¸‹æ ‡è¶Šç•Œ / æ·»åŠ å…ƒç´  / åˆ é™¤å…ƒç´  / ä¿®æ”¹å…ƒç´  / åˆ‡ç‰‡ / å¾ªç¯éå† åƒ insert ï¼Œremove æˆ–è€… sort æ–¹æ³•ï¼Œåªä¿®æ”¹åˆ—è¡¨ï¼Œæ²¡æœ‰æ‰“å°å‡ºè¿”å›å€¼â€”â€”å®ƒä»¬è¿”å›é»˜è®¤å€¼ None ã€‚è¿™æ˜¯Pythonä¸­æ‰€æœ‰å¯å˜æ•°æ®ç»“æ„çš„è®¾è®¡åŸåˆ™ã€‚ åˆ—è¡¨å¸¸ç”¨æ“ä½œ - è¿æ¥ / å¤åˆ¶(å¤åˆ¶å…ƒç´ å’Œå¤åˆ¶æ•°ç»„) / é•¿åº¦ / æ’åº / å€’è½¬ / æŸ¥æ‰¾ ç”Ÿæˆåˆ—è¡¨ - ä½¿ç”¨rangeåˆ›å»ºæ•°å­—åˆ—è¡¨ / ç”Ÿæˆè¡¨è¾¾å¼ / ç”Ÿæˆå™¨ å…ƒç»„çš„ä½¿ç”¨ - å®šä¹‰å…ƒç»„ / ä½¿ç”¨å…ƒç»„ä¸­çš„å€¼ / ä¿®æ”¹å…ƒç»„å˜é‡ / å…ƒç»„å’Œåˆ—è¡¨è½¬æ¢ é›†åˆåŸºæœ¬ç”¨æ³• - é›†åˆå’Œåˆ—è¡¨çš„åŒºåˆ« / åˆ›å»ºé›†åˆ / æ·»åŠ å…ƒç´  / åˆ é™¤å…ƒç´  / æ¸…ç©º é›†åˆå¸¸ç”¨æ“ä½œ - äº¤é›† / å¹¶é›† / å·®é›† / å¯¹ç§°å·® / å­é›† / è¶…é›† å­—å…¸çš„åŸºæœ¬ç”¨æ³• - å­—å…¸çš„ç‰¹ç‚¹ / åˆ›å»ºå­—å…¸ / æ·»åŠ å…ƒç´  / åˆ é™¤å…ƒç´  / å–å€¼ / æ¸…ç©º å­—å…¸å¸¸ç”¨æ“ä½œ - keys()æ–¹æ³• / values()æ–¹æ³• / items()æ–¹æ³• / setdefault()æ–¹æ³• 12list(d1.keys())in not in python list å¸¸ç”¨æ–¹æ³•æ€»ç»“ 12345678910111213141516171819201. åˆ›å»º 1.1 l = [1, 2] 1.2 list = []2. æ·»åŠ  2.1 æœ«å°¾å¢åŠ ä¸€ä¸ª append 2.2 æŒ‡å®šä½ç½®å¢åŠ ä¸€ä¸ª insert 2.3 å¢åŠ list extend +3. æŸ¥ [start: stop: step]4. æ”¹5. åˆ  1. pop 2. remove(å…ƒç´ ) 3. del åˆ—è¡¨6. æ’åºå’Œåè½¬ reverse(); sort(); sort(reverse = True)7. å±æ€§ len max/min8. å¤åˆ¶ python string 12345678910111213141516171819202122231. æ£€æµ‹ str æ˜¯å¦åŒ…å«åœ¨ mysträ¸­ï¼Œå¦‚æœæ˜¯ï¼Œè¿”å›å¼€å§‹çš„ç´¢å¼•å€¼ï¼›å¦åˆ™è¿”å›-1ã€‚ä¹Ÿå¯ä»¥æŒ‡å®šåœ¨ä¸€å®šçš„èŒƒå›´å†…ã€‚mystr.find(str, start=0, end=len(mystr))2. è·Ÿfind()æ–¹æ³•ä¸€æ ·ï¼Œåªä¸è¿‡å¦‚æœsträ¸åœ¨ mysträ¸­ä¼šæŠ¥ä¸€ä¸ªå¼‚å¸¸.mystr.index(str, start=0, end=len(mystr))3. æŠŠ mystr ä¸­çš„ str1 æ›¿æ¢æˆ str2,å¦‚æœ count æŒ‡å®šï¼Œåˆ™æ›¿æ¢ä¸è¶…è¿‡ count æ¬¡.mystr.replace(str1, str2, mystr.count(str1))4. ä»¥ str ä¸ºåˆ†éš”ç¬¦åˆ‡ç‰‡ mystrï¼Œå¦‚æœ maxsplitæœ‰æŒ‡å®šå€¼ï¼Œåˆ™ä»…åˆ†éš” maxsplit ä¸ªå­å­—ç¬¦ä¸²mystr.split(str=&quot; &quot;, 2)5. åˆ é™¤ mystr å·¦è¾¹çš„ç©ºç™½å­—ç¬¦mystr.lstrip()6. åˆ é™¤ mystr å­—ç¬¦ä¸²æœ«å°¾çš„ç©ºç™½å­—ç¬¦mystr.rstrip()7. åˆ é™¤mystrå­—ç¬¦ä¸²ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦ Â¶Day Ox01 Â¶å®šä¹‰ç±» 12classclass classname(object): __init__æ˜¯åˆ›å»ºå¯¹è±¡æ—¶è¿›è¡Œåˆå§‹åŒ–æ“ä½œ 12345678910111213141516171819202122232425262728 def __init__(self, name, age): self.name = name self.age = ageåˆ›å»ºå®åˆ—std = classname(&apos;xiemay&apos;,&apos;24&apos;)ç§æœ‰å’Œå…¬å¼€çš„å±æ€§å’Œå‡½æ•°ï¼šç”¨__å¼€å¤´@property è£…é¥°å™¨å¦‚æœæƒ³è®¿é—®å±æ€§å¯ä»¥é€šè¿‡å±æ€§çš„getterï¼ˆè®¿é—®å™¨ï¼‰å’Œsetterï¼ˆä¿®æ”¹å™¨ï¼‰æ–¹æ³•è¿›è¡Œå¯¹åº”çš„æ“ä½œã€‚ # è®¿é—®å™¨ - getteræ–¹æ³• @property def age(self): return self._ageé™æ€æ–¹æ³•å’Œç±»æ–¹æ³• # ä¿®æ”¹å™¨ - setteræ–¹æ³• @age.setter def age(self, age): self._age = age @staticmethod def is_valid(a, b, c): return a + b &gt; c and b + c &gt; a and a + c &gt; b## ç»§æ‰¿å’Œå¤šæ€class person(): class woman(person): super.__init__() Â¶æ–‡ä»¶æ“ä½œ æ–‡ä»¶æ“ä½œåŸºæœ¬ä¸Šæ²¡ä»€ä¹ˆé—®é¢˜äº† æ“ä½œæ¨¡å¼ å…·ä½“å«ä¹‰ a+ æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶ç”¨äºè¯»å†™ã€‚å¦‚æœè¯¥æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ–‡ä»¶æŒ‡é’ˆå°†ä¼šæ”¾åœ¨æ–‡ä»¶çš„ç»“å°¾ã€‚æ–‡ä»¶æ‰“å¼€æ—¶ä¼šæ˜¯è¿½åŠ æ¨¡å¼ã€‚å¦‚æœè¯¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶ç”¨äºè¯»å†™ã€‚ w+ æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶ç”¨äºè¯»å†™ã€‚å¦‚æœè¯¥æ–‡ä»¶å·²å­˜åœ¨åˆ™æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶ä»å¼€å¤´å¼€å§‹ç¼–è¾‘ï¼Œå³åŸæœ‰å†…å®¹ä¼šè¢«åˆ é™¤ã€‚å¦‚æœè¯¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶ã€‚ 'r' è¯»å– ï¼ˆé»˜è®¤ï¼‰ 'w' å†™å…¥ï¼ˆä¼šå…ˆæˆªæ–­ä¹‹å‰çš„å†…å®¹ï¼‰ 'x' å†™å…¥ï¼Œå¦‚æœæ–‡ä»¶å·²ç»å­˜åœ¨ä¼šäº§ç”Ÿå¼‚å¸¸ 'a' è¿½åŠ ï¼Œå°†å†…å®¹å†™å…¥åˆ°å·²æœ‰æ–‡ä»¶çš„æœ«å°¾ 'b' äºŒè¿›åˆ¶æ¨¡å¼ 't' æ–‡æœ¬æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ '+' æ›´æ–°ï¼ˆæ—¢å¯ä»¥è¯»åˆå¯ä»¥å†™ï¼‰ r+ æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶ç”¨äºè¯»å†™ã€‚æ–‡ä»¶æŒ‡é’ˆå°†ä¼šæ”¾åœ¨æ–‡ä»¶çš„å¼€å¤´ã€‚ 12with open(filename, moder, encoding) as f1: for line in f1: Â¶è¯»å–æ–¹æ³• Â¶æŒ‰å­—èŠ‚ 12fileObject.read([count]); åœ¨è¿™é‡Œï¼Œè¢«ä¼ é€’çš„å‚æ•°æ˜¯è¦ä»å·²æ‰“å¼€æ–‡ä»¶ä¸­è¯»å–çš„å­—èŠ‚è®¡æ•°ã€‚è¯¥æ–¹æ³•ä»æ–‡ä»¶çš„å¼€å¤´å¼€å§‹è¯»å…¥ï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥countï¼Œå®ƒä¼šå°è¯•å°½å¯èƒ½å¤šåœ°è¯»å–æ›´å¤šçš„å†…å®¹ï¼Œå¾ˆå¯èƒ½æ˜¯ç›´åˆ°æ–‡ä»¶çš„æœ«å°¾ã€‚ Â¶å•ç‹¬ä¸€è¡Œ 123456readline()æ–¹æ³•f.readline() ä¼šä»æ–‡ä»¶ä¸­è¯»å–å•ç‹¬çš„ä¸€è¡Œã€‚æ¢è¡Œç¬¦ä¸º &apos;\n&apos;ã€‚f.readline() å¦‚æœè¿”å›ä¸€ä¸ªç©ºå­—ç¬¦ä¸², è¯´æ˜å·²ç»å·²ç»è¯»å–åˆ°æœ€åä¸€è¡Œã€‚with open() as f1: while True: lines = f.readline() if lines: Â¶å…¨éƒ¨è¡Œ 12345readlines()æ–¹æ³• f.readlines() å°†ä»¥åˆ—è¡¨çš„å½¢å¼è¿”å›è¯¥æ–‡ä»¶ä¸­åŒ…å«çš„æ‰€æœ‰è¡Œï¼Œåˆ—è¡¨ä¸­çš„ä¸€é¡¹è¡¨ç¤ºæ–‡ä»¶çš„ä¸€è¡Œã€‚å¦‚æœè®¾ç½®å¯é€‰å‚æ•° sizehint, åˆ™è¯»å–æŒ‡å®šé•¿åº¦çš„å­—èŠ‚, å¹¶ä¸”å°†è¿™äº›å­—èŠ‚æŒ‰è¡Œåˆ†å‰²ã€‚with open() as f1: lines = f1.readlines() Â¶json æ–‡ä»¶è¯»å– 12345678910jsonæ¨¡å—ä¸»è¦æœ‰å››ä¸ªæ¯”è¾ƒé‡è¦çš„å‡½æ•°ï¼Œåˆ†åˆ«æ˜¯ï¼šdump - å°†Pythonå¯¹è±¡æŒ‰ç…§JSONæ ¼å¼åºåˆ—åŒ–åˆ°æ–‡ä»¶ä¸­dumps - å°†Pythonå¯¹è±¡å¤„ç†æˆJSONæ ¼å¼çš„å­—ç¬¦ä¸² infor = json.dumps(neww) f.write(infor+&apos;\n&apos;)load - å°†æ–‡ä»¶ä¸­çš„JSONæ•°æ®ååºåˆ—åŒ–æˆå¯¹è±¡loads - å°†å­—ç¬¦ä¸²çš„å†…å®¹ååºåˆ—åŒ–æˆPythonå¯¹è±¡ b = json.loads(lines) Â¶excel å•ç‹¬æ“ä½œexcelçš„åº“ åˆ›å»º/æ‰“å¼€(å¯¹è±¡)ï¼Œå®šä½(sheet)ï¼Œå®šä½è¡Œåˆ—, ä¿å­˜ å°±æ˜¯è¿™å‡ ä¸ªæ–¹æ³• Â¶xlrd 12345678910111213141516171819202122232425262728293031323334353637383940### å¯¼å…¥ xlrd åº“import xlrd### æ‰“å¼€åˆšæ‰æˆ‘ä»¬å†™å…¥çš„ test_w.xls æ–‡ä»¶wb = xlrd.open_workbook(&quot;test_w.xls&quot;)### è·å–å¹¶æ‰“å° sheet æ•°é‡print( &quot;sheet æ•°é‡:&quot;, wb.nsheets)### è·å–å¹¶æ‰“å° sheet åç§°print( &quot;sheet åç§°:&quot;, wb.sheet_names())### æ ¹æ® sheet ç´¢å¼•è·å–å†…å®¹sh1 = wb.sheet_by_index(0)### æˆ–è€…### ä¹Ÿå¯æ ¹æ® sheet åç§°è·å–å†…å®¹### sh = wb.sheet_by_name(&apos;æˆç»©&apos;)### è·å–å¹¶æ‰“å°è¯¥ sheet è¡Œæ•°å’Œåˆ—æ•°print( u&quot;sheet %s å…± %d è¡Œ %d åˆ—&quot; % (sh1.name, sh1.nrows, sh1.ncols))### è·å–å¹¶æ‰“å°æŸä¸ªå•å…ƒæ ¼çš„å€¼print( &quot;ç¬¬ä¸€è¡Œç¬¬äºŒåˆ—çš„å€¼ä¸º:&quot;, sh1.cell_value(0, 1))### è·å–æ•´è¡Œæˆ–æ•´åˆ—çš„å€¼rows = sh1.row_values(0) # è·å–ç¬¬ä¸€è¡Œå†…å®¹cols = sh1.col_values(1) # è·å–ç¬¬äºŒåˆ—å†…å®¹### æ‰“å°è·å–çš„è¡Œåˆ—å€¼print( &quot;ç¬¬ä¸€è¡Œçš„å€¼ä¸º:&quot;, rows)print( &quot;ç¬¬äºŒåˆ—çš„å€¼ä¸º:&quot;, cols)### è·å–å•å…ƒæ ¼å†…å®¹çš„æ•°æ®ç±»å‹print( &quot;ç¬¬äºŒè¡Œç¬¬ä¸€åˆ—çš„å€¼ç±»å‹ä¸º:&quot;, sh1.cell(1, 0).ctype)### éå†æ‰€æœ‰è¡¨å•å†…å®¹for sh in wb.sheets(): for r in range(sh.nrows): # è¾“å‡ºæŒ‡å®šè¡Œ print( sh.row(r)) Â¶xlwt 123456789101112131415161718192021222324252627### excel_w.py### å¯¼å…¥ xlwt åº“import xlwt### åˆ›å»º xls æ–‡ä»¶å¯¹è±¡wb = xlwt.Workbook()### æ–°å¢ä¸¤ä¸ªè¡¨å•é¡µsh1 = wb.add_sheet(&apos;æˆç»©&apos;)sh2 = wb.add_sheet(&apos;æ±‡æ€»&apos;)### ç„¶åæŒ‰ç…§ä½ç½®æ¥æ·»åŠ æ•°æ®,ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¡Œï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯åˆ—### å†™å…¥ç¬¬ä¸€ä¸ªsheetsh1.write(0, 0, &apos;å§“å&apos;)sh1.write(0, 1, &apos;æˆç»©&apos;)sh1.write(1, 0, &apos;å¼ ä¸‰&apos;)sh1.write(1, 1, 88)sh1.write(2, 0, &apos;æå››&apos;)sh1.write(2, 1, 99.5)### å†™å…¥ç¬¬äºŒä¸ªsheetsh2.write(0, 0, &apos;æ€»åˆ†&apos;)sh2.write(1, 0, 187.5)### æœ€åä¿å­˜æ–‡ä»¶å³å¯wb.save(&apos;test_w.xls&apos;) pandas ä¹Ÿå¯ä»¥æ“ä½œ 123456789101112ExcelWriter: Class for writing DataFrame objects into excel sheets.with ExcelWriter(&apos;path_to_file.xlsx&apos;, mode=&apos;a&apos;) as writer: df.to_excel(writer, sheet_name=&apos;Sheet3&apos;) writer = pd.ExcelWriter(&apos;exam.xlsx&apos;) for i in all_grad_pair: inflowdata = pd.read_csv(newfiledir+i+precitypair+&apos;.txt&apos;,\ sep = &apos;\t&apos;,header = 0, index_col = 0,encoding = &apos;utf-8&apos;) inflowdata.to_excel(writer,sheet_name = CityName[j])writer.save() â€‹12345678910111213import pandas as pd #è¯»å–ä¸¤ä¸ªè¡¨æ ¼data1=pd.read_excel('æ–‡ä»¶è·¯å¾„')data2=pd.read_excel('æ–‡ä»¶è·¯å¾„') #å°†ä¸¤ä¸ªè¡¨æ ¼è¾“å‡ºåˆ°ä¸€ä¸ªexcelæ–‡ä»¶é‡Œé¢writer=pd.ExcelWriter('D:æ–°è¡¨.xlsx')data1.to_excel(writer,sheet_name='sheet1')data2.to_excel(writer,sheet_name='sheet2') #å¿…é¡»è¿è¡Œwriter.save()ï¼Œä¸ç„¶ä¸èƒ½è¾“å‡ºåˆ°æœ¬åœ°writer.save() ç¬¬äºŒå‘¨ï¼šnumpy https://cloudxlab.com/blog/numpy-pandas-introduction/ Â¶å¯¹è±¡ Numpy æä¾›äº†nç»´æ•°ç»„å¯¹è±¡ï¼ˆndarray, A multidimensional array object) Â¶åˆ›å»º 1234np.arange(6)array([0,1, 2, 3, 4, 5])np.zeros(10) # 1-nnp.zeros((3, 6)) # 2-n Â¶types 123arr1 = np.array([1, 2, 3], dtype=np.float64)arr1.dtypefloat_arr = arr.astype(np.float64) Â¶ç´¢å¼• å°±æ˜¯ç”¨ä¸­æ‹¬å·ï¼Œæ³¨æ„åˆ‡ç‰‡æ˜¯ä¸€ç»´æ•°ç»„è¿˜æ˜¯å¤šå°‘ç»´æ•°ç»„ï¼[, ], å¦‚æœï¼Œåé¢çœç•¥è¡¨ç¤ºå…¨éƒ¨ç´¢å¼•ã€‚ å¸ƒå°”ç±»å‹ç´¢å¼• 12345data[names == &apos;Bob&apos;]è¡¨ç¤ºç´¢å¼•è¡Œdata[names == &apos;Bob&apos;, 2:]è¡¨ç¤ºè¡Œåˆ—ç´¢å¼• Â¶å¢ Â¶æ”¹ 1result = np.where(cond, xarr, yarr) Â¶ç´¢å¼• Â¶å€¼ Â¶åˆ  Â¶è®¡ç®—å‡½æ•° Â±*/ **éƒ½æ˜¯å…ƒç´ å¯¹é½ np.sqrt() np.exp() pandas Â¶å¯¹è±¡ Â¶åˆ›å»º Â¶ç´¢å¼• Â¶å¢ Â¶æ”¹ Â¶ç´¢å¼• Â¶å€¼ Â¶åˆ  Â¶è®¡ç®—å‡½æ•° matplotlib seaborn ç¬¬ä¸‰å‘¨ï¼šsklearn ç¬¬äº”å‘¨ï¼šsql ç¬¬å…­å‘¨ï¼š Linux ç¬¬ä¸ƒå‘¨å’Œå…«å‘¨ï¼šæœºå™¨å­¦ä¹ ç®—æ³•å’Œæ•°æ®æŒ–æ˜]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æœºå™¨å­¦ä¹ ]]></title>
    <url>%2F2020%2F07%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[çº¿æ€§å›å½’ é€»è¾‘æ–¯ç‰¹å›å½’ å†³ç­–æ ‘]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>ç›‘ç£å­¦ä¹ ä¸éç›‘ç£å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data-Analysis]]></title>
    <url>%2F2020%2F07%2F25%2FData-Analysis%2F</url>
    <content type="text"></content>
      <categories>
        <category>æ•°æ®åˆ†æ</category>
      </categories>
      <tags>
        <tag>æ•°æ®åˆ†æ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç»Ÿè®¡å­¦]]></title>
    <url>%2F2020%2F07%2F17%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[ç»Ÿè®¡å­¦ ç»Ÿè®¡å­¦ day Ox00 day Ox00 é¦–å…ˆä»‹ç»éšæœºå®éªŒçš„åŸºæœ¬æ¦‚å¿µï¼ŒåŒ…æ‹¬éšæœºå®éªŒï¼Œæ ·æœ¬ç‚¹ï¼Œæ ·æœ¬ç©ºé—´ï¼ŒåŸºæœ¬äº‹ä»¶ï¼Œéšæœºäº‹ä»¶ï¼›å…¶æ¬¡ä»‹ç»æ¦‚ç‡è®ºçš„åŸºæœ¬æ¦‚å¿µï¼ŒåŒ…æ‹¬æ¦‚ç‡çš„å…¬ç†åŒ–å®šä¹‰ï¼Œå¤å…¸æ¦‚ç‡ï¼Œæ¡ä»¶æ¦‚ç‡ï¼Œå…¨æ¦‚ç‡ï¼Œè´å¶æ–¯å…¬å¼ç­‰ç­‰ã€‚ç‰¹åˆ«æ³¨æ„ä¸¤ä¸ªå®¹æ˜“æ··æ·†çš„æ¦‚å¿µï¼šäº‹ä»¶çš„ç‹¬ç«‹æ€§å’Œäº’æ–¥ã€‚ day Ox01 é¦–å…ˆå¼• å‡ºéšæœºå˜é‡çš„å®šä¹‰ï¼Œä»ç¦»æ•£éšæœºå˜é‡å’Œè¿ç»­éšæœºå˜é‡ä¸¤ä¸ªç»´åº¦ï¼Œä»‹ç»å…¸å‹çš„åˆ†å¸ƒå‡½æ•°ã€‚å…¶ä¸­æ¦‚ç‡å‡½æ•°å’Œåˆ†å¸ƒå‡½æ•°æ˜¯éå¸¸é‡è¦çš„æ¦‚å¿µã€‚ Â¶åŸºæœ¬æ¦‚å¿µ éšæœºè¯•éªŒï¼šè®°ä½œ$E$ æ ·æœ¬ç‚¹ï¼š éšæœºè¯•éªŒä¸­å‡ºç°çš„å¯èƒ½ç»“æœç§°ä¸ºæ ·æœ¬ç‚¹ï¼Œè®°ä½œ $\omega$ æ ·æœ¬ç©ºé—´ï¼š æ‰€æœ‰æ ·æœ¬ç‚¹ç»„æˆçš„é›†åˆç§°ä¸ºæ ·æœ¬ç©ºé—´ï¼Œéšæœºå®éªŒæ‰€æœ‰çš„ç»“æœçš„é›†åˆï¼Œè®°ä½œ$\Omega$ äº‹ä»¶ï¼š æ ·æœ¬ç©ºé—´çš„å­é›†ï¼Œå«åšéšæœºäº‹ä»¶ï¼Œè®°ä½œA,B,Cã€‚ â€‹ åˆ†ç±»ï¼šåŸºæœ¬äº‹ä»¶ï¼ˆç”±ä¸€ä¸ªæ ·æœ¬ç‚¹æ„æˆï¼‰ï¼Œä¸å¯èƒ½äº‹ä»¶ï¼ˆä¸åŒ…å«ä»»ä½•æ ·æœ¬ç‚¹ï¼‰ï¼Œå¿…ç„¶äº‹ä»¶ï¼ˆæ ·æœ¬ç©ºé—´çš„æ‰€æœ‰æ ·æœ¬ç‚¹ç»„æˆï¼‰ äº‹ä»¶çš„å…³ç³»å’Œè¿ç®— â€‹ Aä¸Bäº’æ–¥ï¼ˆäº’ä¸ç›¸å®¹ï¼‰ï¼Œå¹¶ä¸ºç©ºé›†ã€‚ä¸å¯èƒ½åŒæ—¶å‘ç”Ÿã€‚ â€‹ å¯¹ç«‹ï¼ˆäº’é€†ï¼‰ï¼šA,Båœ¨ä¸€æ¬¡å®éªŒä¸­æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªå‘ç”Ÿã€‚ Â¶äº‹ä»¶é—´çš„å…³ç³» åŒ…å« ç›¸ç­‰ äº’ä¸ç›¸å®¹æ€§ï¼šä¸å¯èƒ½åŒæ—¶å‘ç”Ÿï¼Œæ²¡æœ‰äº¤é›† Â¶äº‹ä»¶çš„æ¦‚ç‡ Â¶æ¦‚ç‡çš„å…¬ç†åŒ–å®šä¹‰ Â¶è®¡ç®—æ–¹æ³• Â¶å¤å…¸æ–¹æ³• éšæœºäº‹ä»¶çš„è¦æ±‚ï¼š(1). æ¶‰åŠçš„éšæœºç°è±¡åªæœ‰æœ‰é™ä¸ªåŸºæœ¬ç»“æœï¼ˆ2). æ¯ä¸ªåŸºæœ¬ç»“æœå‡ºç°çš„å¯èƒ½æ€§æ˜¯ç›¸åŒçš„ï¼ˆç­‰å¯èƒ½æ€§ï¼‰ äº‹ä»¶çš„åŸºæœ¬ç»“æœï¼š $$ P(A) = \frac{k}{n} = \frac{äº‹ä»¶åŒ…å«çš„åŸºæœ¬äº‹ä»¶çš„ä¸ªæ•°}{å…¨ç©ºé—´åŒ…å«çš„åŸºæœ¬ç»“æœæ€»æ•°} $$ Â¶äº‹ä»¶çš„ç‹¬ç«‹æ€§ ä¸¤ä¸ªäº‹ä»¶çš„ç‹¬ç«‹æ€§æ˜¯æŒ‡ä¸€ä¸ªäº‹ä»¶çš„å‘ç”Ÿä¸å½±å“å¦ä¸€ä¸ªäº‹ä»¶çš„å‘ç”Ÿï¼Œ $$ P(AB) = P(A)P(B) $$ å¤šä¸ªäº‹ä»¶çš„ç‹¬ç«‹æ€§ $$ P(A_iA_j) = P(A_i)P(A_j)\ P(A_iA_jA_k) = P(A_i)P(A_j)P(A_k)\ \vdots P(A_1A_2\cdots A_n) = P(A_1)P(A_2)\cdots P(A_n) $$ Â¶å®éªŒçš„ç‹¬ç«‹æ€§ å®éªŒ$E_1$çš„ä»»æ„ä¸€ä¸ªç»“æœï¼ˆäº‹ä»¶ï¼‰ä¸å®éªŒ$E_2$çš„ä»»ä¸€ä¸ªç»“æœéƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„äº‹ä»¶ï¼Œåˆ™ç§°å®éªŒç›¸äº’ç‹¬ç«‹ Â¶æ¡ä»¶æ¦‚ç‡ $$ P(A|B) = \frac{P(AB)}{P(B)} $$ Â¶ä¹˜æ³•å…¬å¼ $$ P(A_1A_2A_3) = P(A_1)P(A_2|A_1)P(A_3|A_1A_2) $$ Â¶å…¨æ¦‚ç‡å…¬å¼ $$ P(A) = P(A|B)P(B)+P(A|\hat{B})P(\hat{B}) $$ $$ P(A) = \sum_{i = 1}^nP(A|B_i)P(B_i) $$ Â¶è´å¶æ–¯å…¬å¼ $$ P(B_k|A) = \frac{P(A|B_k)P(B_k)}{\sum_{i = 1}^nP(A|B_k)P(B_k)} $$ éšæœºå˜é‡ day Ox01 éšæœºå˜é‡è¡¨ç¤ºéšæœºç°è±¡ç»“æœï¼Œä¸€èˆ¬å¤§å†™å­—æ¯X,Y,Z, éšæœºå˜é‡å–å€¼ç”¨å°å†™å­—æ¯x,y,zç­‰è¡¨ç¤ºã€‚ ç”¨ç­‰å·æˆ–è€…ä¸ç­‰å·æŠŠXä¸xè”ç³»èµ·æ¥å°±å¾ˆå¤šæœ‰è¶£çš„äº‹ä»¶ï¼ŒX=x,Y&lt;y,ç­‰ç­‰æ„æˆäº†äº‹ä»¶ã€‚ Â¶éšæœºå˜é‡ å®šä¹‰åœ¨åŸºæœ¬ç©ºé—´$\Omega$ä¸Šçš„å®å€¼å‡½æ•°$X = X(w)$æˆä¸ºéšæœºç©ºé—´ $$ X: w-&gt;å®æ•°åŸŸï¼ˆæ˜ å°„) $$ Â¶éšæœºå˜é‡çš„åˆ†å¸ƒå‡½æ•° åˆ†å¸ƒå‡½æ•°çš„å®šä¹‰ $$ F(x) = P(X&lt;=x) $$ ç¦»æ•£éšæœºå˜é‡ï¼šåˆ†æ®µå‡½æ•° è¿ç»­éšæœºå˜é‡ï¼šè¿ç»­å‡½æ•° Â¶ç¦»æ•£éšæœºå˜é‡ ${p(x)}$ æˆä¸ºéšæœºå˜é‡Xçš„åˆ†å¸ƒåˆ—ï¼Œæˆ–æ¦‚ç‡åˆ†å¸ƒ $X ~ {p(x_i)}$ å…¶åˆ†å¸ƒå‡½æ•°$F(x) = \sum_{x_i&lt;x}p(x_i)$ Â¶å¸¸è§åˆ†å¸ƒ Â¶äºŒé¡¹åˆ†å¸ƒ ç‰¹ç‚¹ é‡å¤è¿›è¡Œ$n$æ¬¡ç›¸äº’ç‹¬ç«‹çš„å®éªŒ æ¯æ¬¡å®éªŒåªå¯èƒ½æœ‰ä¸¤ä¸ªç»“æœ æ¯æ¬¡å®éªŒå‡ºç°æˆåŠŸçš„æ¦‚ç‡ç›¸åŒï¼Œä¸”ä¸ºp ç¬¦å·è¯´æ˜ $ B_{n,k}$è¡¨ç¤ºné‡è´åŠªé‡Œå®éªŒä¸­æˆåŠŸå‡ºç°çš„kæ¬¡ X:né‡è´åŠªé‡Œå®éªŒä¸­æˆåŠŸçš„æ¬¡æ•°ï¼Œåˆ™æœ‰ $B_{n,k} =â€˜X=kâ€™$ $$ P(X=x)=\left ( \begin{matrix} n\ x \end{matrix}\right )px(1-p){n-x} $$ Â¶æ³Šæ¾åˆ†å¸ƒ $P(X = x) = \frac{\lambdax}{x!}e{-\lambda}$ Â¶è¿ç»­éšæœºå˜é‡ æ¦‚ç‡å¯†åº¦å‡½æ•° Â¶å‡åŒ€åˆ†å¸ƒ $$ p(x) = \left{\begin{array}{c} \frac{1}{b-a}&amp; a&lt;=x&lt;=b\ 0, others \end{array} \right. $$ åˆ†å¸ƒå‡½æ•° $$ F(x)=\left{\begin{array}{c} 0, x&lt;a\ \frac{x-a}{b-a}, a&lt;=x&lt;=b\ 1, x&gt;b \end{array}\right. $$ Â¶æŒ‡æ•°åˆ†å¸ƒ X~Exp($\lambda$) $$ p(x) = \left{\begin{array}{c} \lambda e^{-\lambda x},x&gt;=0\ 0, x&lt;0 \end{array}\right. $$ $$ F(x) = \left{\begin{array}{c} 0,x&lt;0\ 1-e^{-\lambda x}, x&gt;=0 \end{array} \right. $$ Â¶æ­£å¤ªåˆ†å¸ƒX~N(u,$\sigma^2$) æ¦‚ç‡å¯†åº¦å‡½æ•° $$ p(x) = \frac{1}{\sqrt{2\pi}\sigma}exp{-\frac{(x-u)2}{2\sigma2}} $$ Â¶ä¼½é©¬åˆ†å¸ƒGa(a,$\lambda$) $$ p(x)=\left{\begin{array}{c} \frac{\lambda{a}}{\Gamma(a)}x{a-1}e^{-\lambda x},x&gt;0\ 0,x&lt;=0 \end{array}\right. $$ å«ä¹‰ä¸¤ä¸ªå‚æ•°$a$å’Œ$\lambda$,$a&gt;0$ç§°ä¸ºå½¢çŠ¶å‚æ•°ï¼Œ$\lambda&gt;0$ ç§°ä¸ºå°ºåº¦å‚æ•°ã€‚å½“$a&gt;1$å¯†åº¦å‡½æ•°æ˜¯å•å³°ï¼Œå³°å€¼ä½äº$x = (a-1)/\lambda$ ; å½“$1&lt;a&lt;=2$,å…¶å¯†åº¦å‡½æ•°æ˜¯å…ˆä¸Šå‡¸ï¼Œåä¸‹å‡¸ï¼›å¯¹$a&gt;2$, å…¶å¯†åº¦æ˜¯å…ˆä¸‹å‡¸ï¼Œä¸­é—´ä¸Šå‡¸ï¼Œæœ€ååˆä¸‹å‡¸ éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾ day Ox02 Â¶æœŸæœ› E(X) $$ E(X) = \left{ \begin{array}{c} \sum_{i}x_ip(x_i)\ \int_{-\infty}^{+\infty}xp(x)dx \end{array}\right. $$ Â¶æ–¹å·® Var(X) åå·®å¹³æ–¹çš„æ•°å­¦æœŸæœ› $$ Var(X) = E(X-E(X))^2 $$ Â¶æ ‡å‡†å·® $\sigma_X$ æ–¹å·®çš„æ­£å¹³æ–¹æ ¹ï¼ˆå¼€æ ¹å·ï¼‰ Â¶ké˜¶ï¼ˆåŸç‚¹ï¼‰çŸ©($u_k$) $$ u_k = E(X^k) $$ Â¶ké˜¶ä¸­å¿ƒçŸ© $v_k$ $$ v_k = E(X-E(X))^k $$ Â¶å˜å¼‚ç³»æ•°$C_v$ $$ C_V = \frac{\sqrt{Var(X)}}{E(X)} $$ Â¶ååº¦ $$ \beta_s = \frac{E(X-E(X))3}{[Var(X)]{3/2}} $$ ç§°ä¸ºååº¦ç³»æ•°ã€‚$\beta_s&gt;0$ ç§°è¯¥åˆ†å¸ƒä¸ºæ­£åï¼Œæˆ–å³åã€‚$\beta_s&lt;0$ ç§°è¯¥åˆ†å¸ƒä¸ºè´Ÿåï¼Œåˆç§°å·¦åã€‚åˆ»ç”»çš„æ˜¯æè¿°åˆ†å¸ƒåç¦»å¯¹ç§°æ€§ç¨‹åº¦çš„ä¸€ä¸ªç‰¹å¾æ•°ã€‚ Â¶å³°åº¦ $$ \beta_k = \frac{v_4}{v_2^2}-3 = \frac{E(X-EX)4}{var(X)2}-3 $$ å«å³°åº¦ç³»æ•°ï¼Œç®€ç§°å³°åº¦ã€‚å³°åº¦æ˜¯æè¿°åˆ†å¸ƒå°–æ¢¢ç¨‹åº¦å’Œ/æˆ–å°¾éƒ¨ç²—ç»†çš„ä¸€ä¸ªç‰¹å¾æ•°ã€‚ç›¸å¯¹äºæ­£æ€åˆ†å¸ƒè€Œè¨€çš„è¶…å‡ºé‡ã€‚ $\beta_k&gt;0$ è¡¨ç¤ºæ ‡å‡†åŒ–åçš„åˆ†å¸ƒæ¯”æ ‡å‡†æ­£æ€åˆ†å¸ƒæ›´å°–æ¢¢å’Œ/æˆ–å°¾éƒ¨æ›´ç²— $\beta_k &lt; 0 $ è¡¨ç¤ºæ ‡å‡†åŒ–åçš„åˆ†å¸ƒæ¯”æ ‡å‡†åŒ–çŠ¶æ€åˆ†å¸ƒæ›´å¹³å¦å’Œ/æˆ–å°¾éƒ¨æ›´ç»† ååº¦å’Œå³°åº¦éƒ½æ˜¯æè¿°åˆ†å¸ƒå½¢çŠ¶çš„ç‰¹å¾æ•° Â¶ä¸­ä½æ•° Â¶åˆ†ä½æ•° æ¦‚ç‡ç´¯ç§¯ Â¶ä¼—æ•° å¤šç»´éšæœºå˜é‡åŠå…¶è”åˆåˆ†å¸ƒ day Ox03 Â¶nç»´éšæœºå‘é‡ $$ X(\omega) = (X_1(\omega),X_2(\omega),â€¦,X_n(\omega)) $$ Â¶è”åˆåˆ†å¸ƒå‡½æ•° $$ F(x_1,x_2,â€¦,x_n) = \P(X_1&lt;=x_1,X_2&lt;=x_2,X_3&lt;=x_3,â€¦,X_n&lt;=x_n) $$ Â¶è¾¹é™…å¯†åº¦å‡½æ•° $$ p_Y(y) = \int_{-\infty}^{+\infty}p(x,y)dx $$ Â¶ç‹¬ç«‹æ€§ $$ F(x_1,x_2,â€¦,x_n) = F(x_1)F(x_2)â€¦F(x_3) $$ Â¶å·ç§¯å…¬å¼ Xä¸Yç›¸äº’ç‹¬ç«‹çš„è¿ç»­éšæœºå˜é‡ï¼Œå…¶å¯†åº¦å‡½æ•°$P_X(x)$å’Œ$P_Y(y)$ï¼Œåˆ™å…¶å’Œ$Z = X+Y$ çš„å¯†åº¦å‡½æ•°ä¸º $$ p_Z(z) = \int_{-\infty}^{+\infty}p_X(z-y)p_Y(y)dy $$ Â¶æ•°å­—ç‰¹å¾ æœŸæœ› Â¶åæ–¹å·® $$ Cov(X,Y) = E[(X-E(X))(Y-E(Y))] $$ Â¶ç›¸å…³ç³»æ•° $$ Corr(X,Y) = \frac{Cov(X,Y)}{\sigma_X \sigma_Y} $$ Â¶æ¡ä»¶åˆ†å¸ƒ $$ P(X=x_i|Y = y_i) = \frac{P(X=x_i,Y = y_i)}{P(Y = y_i)} = \frac{p_{ij}}{p_{.j}} $$ $$ p(x|y) = \frac{p(x,y)}{P_Y(y)} $$ æ•°ç†ç»Ÿè®¡ day Ox04 æ¦‚ç‡è®ºç ”ç©¶çš„æ˜¯æ¦‚ç‡ã€å„ç§åˆ†å¸ƒçš„æ€§è´¨ã€‚æ•°é‡ç»Ÿè®¡åˆ™æ˜¯å¯¹éšæœºç°è±¡çš„è§‚å¯Ÿæˆ–è¯•éªŒæ¥è·å–æ•°æ®ï¼Œå¯¹å·²çŸ¥çš„æ•°æ®è¿›è¡Œåˆ†æï¼Œå¹¶æ¨æ–­éšè—åœ¨æ•°æ®èƒŒåçš„ç»Ÿè®¡è§„å¾‹æ€§ã€‚ Â¶ç›¸å…³æœ¯è¯­ ç ”ç©¶å¯¹è±¡çš„å…¨ä½“æˆä¸ºæ€»ä½“ï¼›æ„æˆæ€»ä½“çš„æ¯ä¸ªæˆå‘˜å«ä¸ªä½“ã€‚ Â¶ä»æ ·æœ¬å»è®¤è¯†æ€»ä½“ Â¶é¢‘æ•°é¢‘ç‡åˆ†å¸ƒ æ ·æœ¬ç›´æ–¹å›¾æè¿°æ€»ä½“æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å¤§è‡´å½¢çŠ¶ Â¶ç»éªŒåˆ†å¸ƒå‡½æ•° æè¿°æ€»ä½“åˆ†å¸ƒå‡½æ•°çš„å¤§è‡´å½¢çŠ¶ Â¶ç»Ÿè®¡é‡ä¸æŠ½æ ·åˆ†å¸ƒ ç»Ÿè®¡é‡ï¼ˆæŠ½æ ·åˆ†å¸ƒï¼‰,$X = (X_1,X_2,â€¦,X_n)$æ˜¯æ¥è‡ªæ€»ä½“çš„ä¸€ä¸ªå®¹é‡$n$çš„æ ·æœ¬ã€‚ $$ T = T(X_1,X_2,â€¦,X_n) $$ åˆ™$T$ä¸ºç»Ÿè®¡é‡ï¼Œç»Ÿè®¡é‡çš„åˆ†å¸ƒç§°ä¸ºæŠ½æ ·åˆ†å¸ƒã€‚ è§‚æµ‹å€¼ï¼Œ$x = x_1,x_2,â€¦,x_n$åï¼Œå¸¦å…¥ç»Ÿè®¡é‡ $$ t = T(x) = T(x_1,x_2,â€¦,x_n) $$ ç»Ÿè®¡é‡ä¸èƒ½å«æœ‰æœªçŸ¥å‚æ•°ã€‚ Â¶æ ·æœ¬å‡å€¼åŠå…¶åˆ†å¸ƒ $$ \hat{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i $$ Â¶æ ·æœ¬æ–¹å·®ä¸æ ·æœ¬æ ‡å‡†å·® $$ S_n^2 = \frac{1}{n}\sum_{i = 1}n(X_i-\hat{X})2 $$ Â¶æ ·æœ¬çš„é«˜é˜¶çŸ© $$ A_k = \frac{1}{n}\sum_{i = 1}nX_ik $$ $$ B_k = \frac{1}{n}\sum_{i = 1}n(X_i-\hat{X})k $$ Â¶åˆ†ä½æ•° $m_d$ä¸ºæ ·æœ¬ä¸­ä½æ•° $X_1,X_2,â€¦,X_n$æ˜¯æ¥è‡ªæ€»ä½“çš„ä¸€ä¸ªæ ·æœ¬ï¼Œå…¶æ¬¡åºç»Ÿè®¡é‡ä¸º$X_{(1)}&lt;=X_{(2)}&lt;=â€¦&lt;=X_{(n)}$ï¼Œæ ·æœ¬çš„påˆ†ä½æ•°$m_p$ $$ m_p=\left{\begin{array}{c} X_{(k)},\frac{k}{n+1} = p\ X_{(k)}+[X_{k+1}-X_{k}][(n+1)p-k],\frac{k}{n+1}&lt;p&lt;\frac{k+1}{n+1} \end{array}\right. $$ Â¶ç®±çº¿å›¾ Â¶çŸ©ä¼°è®¡ Ox05 Â¶çŸ©ä¼°è®¡ æƒ³æ³•:æ ·æœ¬çŸ©ä¼°è®¡ä»£æ›¿æ€»ä½“çŸ© Â¶æ— åä¼°è®¡ $$ \hat{\theta} = \hat{\theta}(X_1,X_2,â€¦,X_n) $$ ä¸ºå‚æ•°$\theta$çš„ä¼°è®¡é‡ $$ E(\hat{\theta}) = \theta $$ Â¶æå¤§ä¼¼ç„¶ä¼°è®¡ Â¶åŒºé—´ä¼°è®¡ $$ p(|\bar{x}-\mu|&lt;\Delta)=1-\alfha $$ å¦‚æœ$\hat{x}$çš„åˆ†å¸ƒå·²çŸ¥ï¼Œåˆ™å¯ä»¥æ±‚è§£å‡º$\Delta$,å°±æ˜¯åŒºé—´æ¦‚ç‡ï¼Œæ±‚è§£è¾¹ç•Œ Â¶ç‚¹ä¼°è®¡ Â¶è´å¶æ–¯ä¼°è®¡ Â¶å‡è®¾æ£€éªŒ Step 1: å»ºç«‹å‡è®¾ åŸå‡è®¾å’Œå¤‡æ‹©å‡è®¾ Step 2: é€‰æ‹©æ£€éªŒç»Ÿè®¡é‡ Step3: é€‰æ‹©æ˜¾è‘—æ€§æ°´å¹³ ç¬¬ä¸€ç±»é”™è¯¯ï¼šåŸå‡è®¾H_0ä¸ºçœŸï¼Œæ ·æœ¬è§‚å¯Ÿå€¼è½å…¥æ‹’ç»åŸŸW,å…¶å‘ç”Ÿçš„æ¦‚ç‡ä¸ºï¼Œ$\alfha$ Step 4: ç¡®å®šä¸´ç•Œå€¼ï¼Œç»™å‡ºæ‹’ç»åŸŸ ä¸“é¢˜ç³»åˆ— Â¶ä¸“é¢˜ä¸€ï¼šå‚æ•°ä¼°è®¡ æ•°æ®-&gt; æ¨æ–­ï¼šæ¦‚ç‡å¯†åº¦å‡½æ•°-&gt;å…ˆç¡®å®šï¼šæ¦‚ç‡å¯†åº¦å‡½æ•°çš„å½¢å¼å’Œå‚æ•°ã€‚ å‚æ•°ä¼°è®¡å¤šå¤§æ–¹æ³•: æå¤§ä¼¼ç„¶ä¼°è®¡ã€æœ€å¤§åéªŒä¼°è®¡ã€è´å¶æ–¯ä¼°è®¡ã€æœ€å¤§ç†µä¼°è®¡ã€æ··åˆæ¨¡å‹ä¼°è®¡ã€‚ Â¶æå¤§ä¼¼ç„¶ä¼°è®¡(MLE) å½¢å¼: æ¨¡å‹å·²å®šï¼Œå‚æ•°æœªçŸ¥ å±äºé¢‘ç‡æ´¾-ã€‹ä¼˜åŒ–é—®é¢˜ã€‹ç‚¹ä¼°è®¡ å‡è®¾ï¼š é‡‡æ ·ç‹¬ç«‹åŒåˆ†å¸ƒ æƒ³æ³•ï¼šä¼¼ç„¶å‡½æ•°æœ€å¤§ï¼Œå°±æ˜¯ä¸æ–­æ”¹å˜å‚æ•°ï¼Œä½¿å¾—äº‹ä»¶é›†å‘ç”Ÿçš„æ¦‚ç‡æœ€å¤§ã€‚$\theta$æ˜¯å®šå€¼ã€‚ $$ p(x|\theta) $$ å¦‚æœ$\theta$ç¡®å®šï¼Œ$x$æ˜¯å˜é‡ï¼Œè¿™ä¸ªå‡½æ•°å«åšæ¦‚ç‡å‡½æ•°ï¼ˆprobability function) å¦‚æœ$x$å·²çŸ¥ï¼Œ$\theta$æœªçŸ¥ï¼Œå«ä¼¼ç„¶å‡½æ•°ï¼ˆlikelihood function),æè¿°å¯¹äºä¸åŒå‚æ•°çš„æ¨¡å‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡ºç°è¿™ä¸ªæ ·æœ¬ç‚¹çš„æ¦‚ç‡æ˜¯å¤šå°‘ $$ L(\theta) = P(x_1,x_2,â€¦,x_n|\theta)=\Pi_{i=1}^{n}P(x_i|\theta) $$ $$ \theta = arg max_{\theta}\Pi_{i = 1}^{n}P(x_i|\theta) $$ Â¶æœ€å¤§åéªŒä¼°è®¡ï¼ˆMAP) $\theta$æ˜¯éšæœºå˜é‡ï¼Œæœ€å¤§åéªŒä¼°è®¡è¦æ±‚$P(\theta|X)$æœ€å¤§ã€‚ $$ P(\theta|X) = \frac{P(\theta)P(X|\theta)}{p(X)} $$ å› ä¸º$P(X)$å’Œ$\theta$æ˜¯ç‹¬ç«‹çš„ï¼Œæ‰€ä»¥ç›´æ¥å¿½ç•¥$p(X)$ $$ \theta_{max}= argmax_{\theta}p(\theta)p(X|\theta) $$ $p(\theta)$ç§°ä¸ºå…ˆéªŒæ¦‚ç‡ã€‚å¦‚æœæœä»å‡åŒ€åˆ†å¸ƒï¼Œ$p(\theta)$æœä»å‡åŒ€åˆ†å¸ƒï¼Œ Â¶ä¸“é¢˜äºŒï¼š å¸¸è§åˆ†å¸ƒ https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&amp;mid=2247510542&amp;idx=1&amp;sn=40a49b52f65ea475a71c3756bc3443d5&amp;chksm=e8737b43df04f25565b4c1d58a8c0e7e5985271da20a60c289ac3c4a7d4427ebe33bc9efc48d&amp;scene=0&amp;xtrack=1#rd Â¶ä¸“é¢˜ä¸‰ ï¼šå‡è®¾æ£€éªŒ https://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&amp;mid=2648938280&amp;idx=2&amp;sn=dc6184ab880a735be9aadd0bce3383ed&amp;chksm=87940502b0e38c1499e6247e91d65b15e8354b202aba53790859d2d0ffc9081c4cb4b7ae8369&amp;scene=0&amp;xtrack=1#rd https://mp.weixin.qq.com/s?__biz=Mzg2NzIzNzg4NQ==&amp;mid=2247483764&amp;idx=1&amp;sn=5ad227215a9c634cb7114c36289e2cea&amp;chksm=cebfebe6f9c862f0ff6c2bb71648740c21129494b34da747ddd0a4e2028404401b1487d44577&amp;scene=21#wechat_redirect]]></content>
      <categories>
        <category>æ•°å­¦</category>
      </categories>
      <tags>
        <tag>ç»Ÿè®¡å­¦</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è¥¿ç“œä¹¦]]></title>
    <url>%2F2020%2F07%2F17%2F%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%91%A8%E5%BF%97%E5%8D%8E%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Â¶é˜…è¯»ç›®å½• [TOC] Â¶ç¬¬ä¸€ç«  What is the machine learning? éå¸¸å®˜æ–¹çš„å®šä¹‰ï¼š Tom mitchell(1998) Well-posed Learning Problem:A compute program is said to learn from experience E with respect to same task T and some performance measure P,if its performance on T,as measured by P, improves with experience Eã€‚ï¼ˆè¿™ä¸ªæˆ‘è«æ³•ç¿»è¯‘å–”ï¼‰ å¤§æ¦‚æ„æ€æ˜¯å¼ºå¤§çš„è®¡ç®—æœºèƒ½å¤Ÿäº‹å…ˆåœ°å®Œæˆäººä¸ºéæ˜¾ç¤ºç¼–ç¨‹å¥½çš„ä»»åŠ¡ï¼Œæ€ä¹ˆå®Œæˆå‘¢ï¼Ÿå¯¹äºæŸä¸ªä»»åŠ¡T,ç»™å®šä¸€ä¸ªæ€§èƒ½åº¦é‡æ–¹æ³•P,åœ¨ç»éªŒEçš„å½±å“ä¸‹ï¼Œå¦‚æœPå¯¹Tçš„æµ‹é‡ç»“æœå¾—åˆ°äº†æ”¹è¿›ï¼Œåˆ™è¯´æ˜è¯¥ç¨‹åºä»Eä¸­å­¦ä¹ äº† æœºå™¨å­¦ä¹ çš„è¿‡ç¨‹å¤§è‡´å¦‚æ­¤ï¼šè®©è®¡ç®—æœºä»æ•°æ®ä¸­äº§ç”Ÿæ¨¡å‹(model)ï¼Œé¦–å…ˆæä¾›ç»éªŒæ•°æ®ï¼Œç»™å®šå­¦ä¹ ç®—æ³•(learning algorithm)å’Œæ€§èƒ½æµ‹é‡æ–¹æ³•ï¼Œå®ƒå°±èƒ½æ ¹æ®æ•°æ®äº§ç”Ÿæ¨¡å‹ã€‚ æ¨¡å‹ï¼š æ³›æŒ‡ä»æ•°æ®ä¸­å­¦å¾—çš„ç»“æœ æ¨¡å¼ï¼š å±€éƒ¨æ€§çš„ç»“æœ åŸºæœ¬æœ¯è¯­ æ•°æ®é›†: data set æ ·æœ¬ï¼š sample å±æ€§ï¼ˆç‰¹å¾ï¼‰ï¼š attributeï¼ˆfeature) å±æ€§å€¼ï¼š attribute value å±æ€§ç©ºé—´ï¼ˆç‰¹å¾ç©ºé—´ï¼‰ï¼š attribute space ï¼ˆ sample spaceï¼‰ ç‰¹å¾å‘é‡ï¼š feature vector å­¦ä¹ ï¼ˆè®­ç»ƒï¼‰ï¼šlearningï¼ˆtrainingï¼‰ è®­ç»ƒæ•°æ®ï¼š training data è®­ç»ƒé›†ï¼š training set å‡è®¾ï¼šhypothesis å­¦å¾—æ¨¡å‹å¯¹åº”äº†å…³äºæ•°æ®çš„æŸç§æ½œåœ¨è§„å¾‹ æ³›å‡½èƒ½åŠ›: generalization å‡è®¾ç©ºé—´ å½’çº³ï¼ˆinductionï¼‰ï¼š ä»ç‰¹æ®Šåˆ°ä¸€èˆ¬çš„â€œæ³›åŒ–â€(generalization)è¿‡ç¨‹ æ¼”ç»ï¼ˆdeduction)ï¼š ä»ä¸€èˆ¬åˆ°ç‰¹æ®Šçš„â€œç‰¹åŒ–â€(specialization)è¿‡ç¨‹ æœºå™¨å­¦ä¹ æ˜¾ç„¶æ˜¯å½’çº³å­¦ä¹ ï¼ˆinductive learning) å½’çº³å­¦ä¹ åˆ†ç‹­ä¹‰ä¸å¹¿ä¹‰ï¼Œç‹­ä¹‰æ˜¯æŒ‡è¦æ±‚ä»training set ä¸­å­¦å¾—æ¦‚å¿µï¼Œå¹¿ä¹‰æ˜¯æŒ‡ä»sampleä¸­å­¦ä¹  å­¦ä¹ è¿‡ç¨‹ï¼ˆè®­ç»ƒè¿‡ç¨‹ï¼‰çœ‹ä½œæ˜¯åœ¨æ‰€ä»¥å‡è®¾ç»„æˆçš„ç©ºé—´ä¸­è¿›è¡Œæœç´¢çš„è¿‡ç¨‹ï¼Œæœç´¢ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸training setåŒ¹é…çš„å‡è®¾ã€‚å¦‚æœå‡è®¾çš„è¡¨ç¤ºä¸€æ—¦ç¡®å®šï¼Œå‡è®¾ç©ºé—´ä¸å…¶è§„æ¨¡å°±ç¡®å®šäº†ã€‚æƒ³æ›´è¯¦ç»†äº†è§£å‡è®¾ç©ºé—´ï¼Œæˆ³æˆ‘å•¦5.2 ç°å®é—®é¢˜ä¸­å¸¸é¢ä¸´å¾ˆå¤§çš„å‡è®¾ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥å¯»æ‰¾ä¸€ä¸ªä¸è®­ç»ƒé›†ä¸€è‡´çš„å‡è®¾é›†åˆï¼Œç§°ä¹‹ä¸ºç‰ˆæœ¬ç©ºé—´ã€‚ç‰ˆæœ¬ç©ºé—´ä»å‡è®¾ç©ºé—´å‰”é™¤äº†ä¸æ­£ä¾‹ä¸ä¸€è‡´å’Œä¸åä¾‹ä¸€è‡´çš„å‡è®¾ï¼Œå®ƒå¯ä»¥çœ‹æˆæ˜¯å¯¹æ­£ä¾‹çš„æœ€å¤§æ³›åŒ–ã€‚ å½’çº³åå¥½ æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹æŸç§ç±»å‹å‡è®¾çš„åå¥½ï¼Œç§°ä¸ºâ€œå½’çº³åå¥½â€ï¼ˆinductive bias),ä¹Ÿå°±æ˜¯å­¦ä¹ ç®—æ³•åœ¨ä¸€ä¸ªå¯èƒ½å¾ˆåºå¤§çš„å‡è®¾ç©ºé—´ä¸­å¯¹å‡è®¾è¿›è¡Œé€‰æ‹©çš„å¯å‘å¼æˆ–è€…â€œä»·å€¼è§‚â€ å¥¥å¡å§†å‰ƒåˆ€å®šå¾‹ï¼š è‹¥æœ‰å¤šä¸ªå‡è®¾ä¸è§‚æµ‹ä¸€è‡´ï¼Œåˆ™é€‰æ‹©åšç®€å•çš„å“ªä¸ªã€‚ æ²¡æœ‰å…è´¹çš„æ— é¤å®šç†ï¼ˆNo Free Lunch Theorem[NFL]) åœ¨æ‰€ä»¥é—®é¢˜å‡ºç°çš„æœºä¼šç›¸åŒï¼Œæˆ–è€…æ‰€ä»¥é—®é¢˜åŒç­‰é‡è¦ä¸‹ï¼Œæ‰€æœ‰ç®—æ³•çš„æœŸæœ›ä¸€æ ·ã€‚ä½†åœ¨å®é™…é—®é¢˜ä¸­ï¼Œé’ˆå¯¹å…·ä½“çš„é—®é¢˜ï¼Œä¸åŒçš„ç®—æ³•æ‰ä¼šå‡ºç°ç›¸å¯¹ä¼˜åŠ£ã€‚ å‘å±•å†ç¨‹ æ¨ç†æœŸï¼šäºŒåä¸–çºªäº”åå¹´ä»£åˆ°ä¸ƒåå¹´ä»£åˆï¼ŒAIå¤„äºæ¨ç†åŒºï¼Œä»£è¡¨æ€§å·¥ä½œä¸»è¦æ˜¯A.Newell å’ŒH.Simonçš„â€œé€»è¾‘ç†è®ºå®¶â€ç¨‹åºå’Œæ­¤åçš„â€œé€šç”¨é—®é¢˜æ±‚è§£â€ç¨‹åºç­‰ã€‚â€œé€»è¾‘ç†è®ºå®¶â€ç¨‹åºè¯æ˜äº†æ•°å­¦å®¶ç½—ç´ å’Œæ€€ç‰¹æµ·çš„ã€Šæ•°å­¦åŸç†ã€‹é‡Œé¢çš„æŸäº›å®šç†ï¼Œè·å¾—å›¾çµå¥–ã€‚ çŸ¥è¯†æœŸï¼šä»äºŒåä¸–çºªä¸ƒåå¹´ä»£ä¸­æœŸå¼€å§‹ï¼ŒAIçš„ç ”ç©¶è¿›å…¥äº†â€œçŸ¥è¯†æœŸâ€ï¼Œå¤§é‡çš„ä¸“å®¶ç³»ç»Ÿå‡ºç°ï¼ŒE.A.Feigenbaumï¼ˆçŸ¥è¯†å·¥ç¨‹ä¹‹çˆ¶ï¼‰åœ¨1994è·å¾—å›¾çµå¥–ã€‚äººä»¬æ„è¯†åˆ°ï¼Œä¸“å®¶ç³»ç»Ÿé¢ä¸´â€œçŸ¥è¯†å·¥ç¨‹ç“¶é¢ˆ&quot;,åœ¨é‚£ä¸ªæ—¶å€™ï¼Œæœ‰äººæŠŠçŸ¥è¯†æ€»ç»“å‡ºæ¥å†æ•™ç»™è®¡ç®—æœºæ˜¯ç›¸å½“å›°éš¾çš„ã€‚ 1950å¹´ï¼Œå›¾çµå†å…³äºå›¾çµæµ‹è¯•çš„æ–‡ç« ä¸­ï¼Œæ›¾æåˆ°æœºå™¨å­¦ä¹ çš„å¯èƒ½ äºŒåä¸–çºªäº”åå¹´ä»£åˆï¼ŒA.Samuelè‘—åè·³æ£‹ç¨‹åºã€‚äº”åå¹´ä»£ä¸­åæœŸï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„â€è¿æ¥ä¸»ä¹‰â€œå­¦ä¹ ï¼Œå¦‚F.Rosenblattçš„æ„ŸçŸ¥å™¨ï¼ˆPerceptroï¼‰ï¼ŒB.Widremçš„Adaline,å…­ä¸ƒåå¹´ä»£ï¼ŒåŸºäºé€»è¾‘è¡¨ç¤ºçš„â€ç¬¦å·ä¸»ä¹‰å­¦ä¹ æŠ€æœ¯è“¬å‹ƒå‘å±• å­¦ä¹ æœŸï¼šäºŒåä¸–çºªå…«åå¹´ä»£æ˜¯æœºå™¨å­¦ä¹ ç™¾èŠ±åˆæ”¾çš„æ—¶æœŸã€‚ä¸€å¤§ä¸»æµæ˜¯ç¬¦å·ä¸»ä¹‰å­¦ä¹ ï¼Œä»£è¡¨å†³ç­–æ ‘ï¼ˆdecision tree).äºŒåä¸–çºªä¹åå¹´ä»£ä¸­æœŸä¹‹å‰ï¼Œå¦å¤–ä¸€å¤§ä¸»æµæŠ€æœ¯æ˜¯åŸºäºç¥ç»ç½‘ç»œçš„è¿æ¥ä¸»ä¹‰å­¦ä¹ ã€‚äºŒåä¸–çºªä¹åå¹´ä»£ä¸­æœŸï¼Œâ€ç»Ÿè®¡å­¦ä¹ â€œå æ®ä¸»æµï¼Œä»£è¡¨æ”¯æŒå‘é‡æœºã€‚äºŒåä¸€ä¸–çºªåˆï¼Œè¿æ¥ä¸»ä¹‰å­¦ä¹ æ€èµ·äº†â€æ·±åº¦å­¦ä¹ â€œä¸ºåçš„çƒ­æ½®ã€‚ Â¶ç¬¬äºŒç«  ï¼š æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹© Â¶ç»éªŒè¯¯å·®ä¸è¿‡æ‹Ÿåˆã€æ¬ æ‹Ÿåˆ è®­ç»ƒè¯¯å·®ï¼ˆtraining error) or ç»éªŒè¯¯å·®ï¼ˆempirical error): å­¦ä¹ å™¨åœ¨è®­ç»ƒé›†ä¸Šçš„è¾“å‡ºä¸è®­ç»ƒé›†ä¹‹é—´çš„å·®å¼‚ è¿‡æ‹Ÿåˆï¼ˆover fittingï¼‰ï¼šåœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°éå¸¸å¥½ï¼Œæ³›åŒ–èƒ½åŠ›å¤ªå·®ï¼Œæœ€å¸¸è§çš„æƒ…å†µæ˜¯å­¦ä¹ èƒ½åŠ›å¤ªå¼ºå­¦ä¹ åˆ°ä¸å¤ªä¸€èˆ¬çš„ç‰¹æ€§ï¼Œæ— æ³•å½»åº•é¿å…ï¼Œåªèƒ½â€œç¼“è§£â€ æ¬ æ‹Ÿåˆï¼ˆunder fittingï¼‰ï¼šè¿™ç§æƒ…å†µå®¹æ˜“å…‹æœ æ¨¡å‹é€‰æ‹©(model selection): ä¸åŒçš„å‚æ•°é…ç½®ï¼Œäº§ç”Ÿä¸åŒçš„æ¨¡å‹ã€‚ç†è®ºä¸Šæœ€å¥½çš„æ¨¡å‹æ˜¯å¯¹æ³›åŒ–èƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼Œæœ€å¥½çš„å°±æ˜¯æ³›åŒ–è¯¯å·®æœ€å°çš„ï¼Œæ³›åŒ–è¯¯å·®æ˜¯æ— æ³•ç›´æ¥è·å–çš„ Â¶è¯„ä¼°æ–¹æ³• è®¾ç½®ä¸€ä¸ª&quot;æµ‹è¯•é›†ï¼ˆtesting set)&quot;æ¥æµ‹è¯•å­¦ä¹ å™¨åœ¨æ–°æ ·æœ¬çš„åˆ¤æ–­èƒ½åŠ›ï¼Œç”¨æµ‹è¯•è¯¯å·®è¿‘ä¼¼æ³›åŒ–è¯¯å·® è¦æ±‚ï¼š æµ‹è¯•æ ·æœ¬ä¸è®­ç»ƒæ ·æœ¬ç‹¬ç«‹åŒåˆ†å¸ƒçš„ æµ‹è¯•é›†åº”è¯¥å°½å¯èƒ½ä¸è®­ç»ƒé›†äº’æ–¥ï¼Œæµ‹è¯•æ ·æœ¬å°½é‡ä¸å‡ºç°åœ¨è®­ç»ƒé›†ä¸­ Â¶å¦‚ä½•äº§ç”Ÿtraining set å’Œ testing set ç•™å‡ºæ³•ï¼ˆhold-out) è¦æ±‚ï¼šæ•°æ®é›†($D$)åˆ’åˆ†æˆä¸¤ä¸ªäº’æ–¥çš„é›†åˆï¼ˆè®­ç»ƒé›†($S$,æµ‹è¯•é›†$T$),éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåˆ’åˆ†åï¼Œå°½é‡å¯èƒ½çš„ä¿æŒæ•°æ®åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚ ä¸åŒçš„åˆ’åˆ†ç»“æœï¼Œå¾—åˆ°ä¸åŒçš„æµ‹è¯•è¯¯å·®ã€‚å•æ¬¡ä½¿ç”¨ç•™å‡ºæ³•å¾—åˆ°çš„ç»“æœæ˜¯ä¸å¤Ÿç¨³å®šçš„ï¼Œæ‰€ä»¥ä¸€èˆ¬é‡‡ç”¨è‹¥å¹²æ¬¡çš„éšæœºåˆ’åˆ†ï¼Œé‡å¤è¿›è¡Œå®éªŒè¯„ä¼°åå»å¹³å‡å€¼ äº¤å‰éªŒè¯æ³•ï¼ˆcross validation) I. å°†æ•°æ®($D$)åˆ’åˆ†æˆ$k$ä¸ªå¤§å°ç›¸ä¼¼çš„äº’æ–¥å­é›†ï¼Œæ¯ä¸ªå­é›†$D_i$éƒ½å°½å¯èƒ½ä¿æŒæ•°æ®åˆ†å¸ƒçš„ä¸€è‡´æ€§ II. æ¯æ¬¡éƒ½ç”¨$k-1$ä½œä¸ºè®­ç»ƒé›†ï¼Œä½™ä¸‹çš„å“ªä¸ªå­é›†ä½œä¸ºæµ‹è¯•é›†ï¼Œäºæ˜¯ä¹éƒ½åˆ°äº†kä¸ªæµ‹è¯•ç»“æœçš„å‡å€¼ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ$k$çš„å–å€¼å¯¹ç»“æœçš„ç¨³å®šæ€§å’Œä¿çœŸæ€§æœ‰å¾ˆå¤§çš„å½±å“ï¼Œå› æ­¤ä¹Ÿå«kè€…äº¤å‰éªŒè¯ï¼ˆk-flold cross validation) kçš„é€šå¸¸å–å€¼æ˜¯10 åŒæ ·çš„ï¼Œæ•°æ®é›†$D$åˆ’åˆ†ä¸º$k$ä¸ªå­é›†æœ‰å¾ˆå¤šçš„åˆ’åˆ†æ–¹å¼ï¼Œå¯é‡å¤$P$æ¬¡$k$æŠ˜äº¤å‰éªŒè¯ã€‚ è‡ªåŠ©æ³• (bootstrapping) æ³¨æ„çš„æ˜¯æˆ‘ä»¬å¸Œæœ›é€šè¿‡æ‰€ä»¥çš„è®­ç»ƒé›†ï¼ˆ$D$)è®­ç»ƒå‡ºæ¨¡å‹ï¼Œä½†æ˜¯æµå‡ºæ³•å’Œäº¤å‰éªŒè¯çš„æ–¹æ³•ï¼Œéƒ½ä¿ç•™ä¸€éƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†ï¼Œå› æ­¤å®é™…è¯„ä¼°çš„æ¨¡å‹æ‰€ä½¿ç”¨çš„è®­ç»ƒé›†æ›´ä¸‹ï¼Œè¿™ä¹Ÿè®¸ä¼šå¯¼è‡´ä¼°è®¡åå·®ã€‚ è‡ªåŠ©æ³•ï¼š å¯é‡å¤é‡‡æ ·æˆ–è€…æœ‰æ”¾å›é‡‡æ · è®°é‡‡æ ·äº§ç”Ÿçš„æ•°æ®é›†ï¼ˆ$Dâ€™$),æ¯æ¬¡ä»$D$ä¸­æŒ‘é€‰åº”è¯¥æ ·æœ¬ï¼Œå°†å…¶æ‹·è´è‡³($Dâ€™$),å¹¶å†å°†é‡‡æ ·çš„æ ·æœ¬æ”¾å›æ•°æ®é›†($D$),é‡å¤($m$)æ¬¡ä»¥åï¼Œå¾—åˆ°äº†åŒ…å«($m$)ä¸ªæ ·æœ¬çš„æ•°æ®é›†($Dâ€™$) å¯¹äºå¯é‡å¤é‡‡æ ·ï¼Œæ ·æœ¬å§‹ç»ˆä¸é‡‡åˆ°çš„æ¦‚ç‡æ˜¯$(1-\frac{1}{m})^m$,å–æé™å¾—åˆ°ï¼šåˆå¼æ•°æ®é›†ä¸­$36.8%$ä¸ºå‡ºç°åœ¨é‡‡æ ·æ•°æ®é›†ä¸­ï¼Œå› æ­¤å¯å°†($D$)ä½œä¸ºè®­ç»ƒé›†ï¼Œ($D\Dâ€™$)ä½œä¸ºæµ‹è¯•é›†ï¼Œåˆç§°å¤–åŒ…ä¼°è®¡(out-of-bag estimate) è‡ªåŠ©æ³•é€‚ç”¨äºæ•°æ®é‡å°‘ï¼Œéš¾åŒºåˆ«æµ‹è¯•é›†å’Œè®­ç»ƒé›†æ—¶ï¼Œè‡ªåŠ©æ³•ä¼šæ”¹å˜åˆå§‹æ•°æ®çš„åˆ†å¸ƒï¼Œåœ¨åˆå§‹æ•°æ®è¶³å¤Ÿçš„æƒ…å†µä¸‹ï¼Œæµå‡ºæ³•å’Œäº¤å‰éªŒè¯æ›´å¸¸ç”¨ä¸€äº› Â¶è°ƒå‚å’Œæœ€ç»ˆçš„æ¨¡å‹ å­¦ä¹ ç®—æ³•éƒ½æœ‰å‚æ•°(parameter),ä¸åŒçš„å‚æ•°é…ç½®ï¼Œå­¦å¾—æ¨¡å‹çš„æ€§èƒ½ä¹Ÿå¾€å¾€ä¸åŒ éªŒè¯é›†(validation set): æ¨¡å‹è¯„ä¼°å’Œé€‰æ‹©ä¸­ç”¨äºä¼°è®¡æµ‹è¯•çš„æ•°æ®é›†ç§°ä¸ºçš„æ•°æ®é›† å¾€å¾€å°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ŒåŸºäºéªŒè¯é›†ä¸Šçš„æ€§èƒ½æ¥è¿›è¡Œæ¨¡å‹é€‰æ‹©å’Œè°ƒå‚ Â¶æ€§èƒ½åº¦é‡(performance measure) Â¶å‡è®¾æ£€éªŒ ï¼ˆå…¶å®æˆ‘ä¸€ç›´éƒ½å¹¶ä¸æ˜¯ç‰¹åˆ«äº†è§£ï¼‰ å‡è®¾æ£€éªŒçš„åŸºæœ¬åŸç† æ˜¯é‡è¦çš„ç»Ÿè®¡æ¨æ–­é—®é¢˜ä¹‹ä¸€ï¼Œæ ¹æ®æ ·æœ¬æä¾›çš„ä¿¡æ¯ï¼Œæ£€éªŒå…³äºæ€»ä½“æŸä¸ªå‡è®¾æ˜¯å¦æ­£ç¡®ã€‚åŒ…æ‹¬å‚æ•°çš„å‡è®¾æ£€éªŒï¼ˆå‡å€¼ã€æ–¹å·®ç­‰ï¼‰å’Œéå‚æ•°ï¼ˆåˆ†å¸ƒå•Šï¼‰çš„å‡è®¾æ£€éªŒã€‚ å‚æ•°æ£€éªŒï¼š æå‡ºå‡è®¾Hâ€”&gt;åœ¨æ„é€ ç»Ÿè®¡é‡ï¼Œç¡®å®šç»Ÿè®¡é‡çš„åˆ†å¸ƒâ€”&gt; ç¡®å®šæ‹’ç»åŸŸå’Œæ¥å—åŸŸçš„åˆ†ç•Œçº¿â€”&gt; åœ¨æ ¹æ®æ ·æœ¬è®¡ç®—ç»Ÿè®¡é‡çš„å€¼u â€”&gt; æ¨æ–­ åˆ†å¸ƒæ‹Ÿåˆæ£€éªŒ Â¶åå·®å’Œæ–¹å·® é€šè¿‡æ¦‚ç‡è®ºåˆ†æå¯¹å­¦ä¹ ç®—æ³•çš„æœŸæœ›æ³›åŒ–é”™è¯¯ç‡è¿›è¡Œæ‹†è§£ $x$: æµ‹è¯•æ ·æœ¬ $y_D$ï¼š $x$åœ¨æ•°æ®é›†ä¸­çš„æ ‡è®° $y$: $x$çš„çœŸå®æ ‡è®° $f(x:D)$: åœ¨è®­ç»ƒé›†ä¸Šå­¦å¾—çš„æ¨¡å‹$f$åœ¨$x$ä¸Šé¢„æµ‹è¾“å‡º ä»¥å›å½’ä»»åŠ¡ä¸ºä¾‹å­ï¼š å­¦ä¹ ç®—æ³•çš„æœŸæœ›é¢„æµ‹ä¸ºï¼š $$ \hat{f}(x) = E_D[f(x;D)]$$ æ–¹å·®ï¼šåº¦é‡åŒæ ·çš„æ ·æœ¬å¤§å°çš„è®­ç»ƒé›†çš„å˜åŠ¨æ‰€å¯¼è‡´çš„å­¦ä¹ æ€§èƒ½çš„å˜åŒ–ï¼Œå³åˆ»ç”»æ•°æ®æ‰°åŠ¨æ‰€é€ æˆçš„å½±å“ $$ var(x)= E_D[(f(x;D)-\hat{f}(x))^2]$$ å™ªå£°ï¼š è¡¨è¾¾äº†å½“å‰ä»»åŠ¡ä¸Šä»»åŠ¡å­¦ä¹ ç®—æ³•æ‰€èƒ½è¾¾åˆ°çš„æœŸæœ›æ³›åŒ–è¯¯å·®çš„ä¸‹ç•Œï¼Œå³åˆ»ç”»äº†å­¦ä¹ é—®é¢˜æœ¬èº«çš„éš¾åº¦ã€‚ $$ \epsilon2=E_D[(y_D-y)2] $$ æœŸæœ›è¾“å‡ºå’ŒçœŸå®æ ‡è®°çš„å·®åˆ«ç§°ä¸ºåå·®(bias): åº¦é‡äº†å­¦ä¹ ç®—æ³•çš„æœŸæœ›é¢„æµ‹ä¸çœŸå®ç»“æœçš„åç¦»ç¨‹åº¦ï¼Œå³åˆ»ç”»äº†å­¦ä¹ ç®—æ³•æœ¬èº«çš„æ‹Ÿåˆèƒ½åŠ› $$ bias2(x)=(f(x)-y)2$$ è‹¥å‡è®¾å™ªå£°æœŸæœ›ä¸ºé›¶ï¼Œé‚£ä¹ˆç®—æ³•çš„æœŸæœ›æ³›åŒ–è¯¯å·®ï¼š $$ E(f;D)=E_D[(f(x;D)-y)^2]\ =â€¦=E_D[(f(x;D)-\hat{f}(x))2]+(\hat{f}(x)-y)2+E_D[(y_D-y)^2] $$ $$E(f;D)=bias2(x)+var(x)+\epsilon2$$ ç”±ä¸Šå¼å¯çŸ¥ï¼Œæ³›åŒ–èƒ½åŠ›ç”±å­¦ä¹ ç®—æ³•çš„èƒ½åŠ›ã€æ•°æ®çš„å……åˆ†æ€§ã€å­¦ä¹ ä»»åŠ¡æœ¬èº«çš„éš¾åº¦å…±åŒå†³å®šçš„ã€‚ underfitting: åå·®ä¸»å¯¼æ³›åŒ–è¯¯å·® over fittingï¼š è®­ç»ƒæ•°æ®å‘ç”Ÿçš„æ‰°åŠ¨æ¸æ¸è¢«å­¦ä¹ åˆ°ï¼Œæ–¹å·®ä¸»å¯¼äº†æ³›åŒ–è¯¯å·® Â¶ç¬¬ä¸‰ç«  çº¿æ€§æ¨¡å‹ æˆ‘è‡ªå·±å…¶å®æ˜¯ä¸€ç›´åœç•™åœ¨çº¿æ€§æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ï¼Œå› ä¸ºæ¯æ¬¡å¼€å¤´éƒ½æ˜¯è¿™ä¸€å¼ ï¼Œæ‰€ä»¥æˆ‘å°±å­¦ä¹ äº†å¾ˆå¤šæ¬¡ã€‚è¿™æ¬¡ä¸å‡†å¤‡å†ç»†çœ‹äº†ã€‚ Â¶çº¿æ€§åˆ¤åˆ«åˆ†æ Linear Discriminant Analysis (LDA) åŸºæœ¬æ€æƒ³ï¼š åœ¨è®­ç»ƒæ ·ä¾‹é›†ä¸Šï¼Œè®¾æ³•å°†æ ·æœ¬ä¾‹å­æŠ•å½±åˆ°ä¸€æ¡ç›´çº¿ä¸Šä½¿å¾—åŒç±»æ ·ä¾‹çš„æŠ•å½±å°½å¯èƒ½æ¥è¿‘ã€å¼‚ç±»æŠ•å½±ç‚¹å°½å¯èƒ½è¿œç¦»ã€‚ æ•°å­¦è¡¨è¾¾ï¼š $D={(x_i,y_i)}{i=1}^{m}$: data set $X_i$: ç¬¬$i$ç±»é›†åˆ $u_i$: ç¬¬$i$ç±»é›†åˆå‡å€¼å‘é‡ $\sum{i}$: ç¬¬$i$ç±»é›†åˆåæ–¹å·®çŸ©é˜µ $ w^Tu_i$ï¼š ç¬¬$i$ç±»é›†åˆåœ¨ç›´çº¿ä¸Šçš„æŠ•å½± $ w^T\sum{i}w$: æ ·æœ¬ç‚¹çš„åœ¨ç›´çº¿ä¸Šçš„æŠ•å½± å­¦ä¹ ç®—æ³•ï¼š åŒç±»æ›´è¿‘ï¼š $\min \sum_{i=1}{n}(wT\sum_{i}w)$ ç±»ä¸­å¿ƒè¶Šå¤§ï¼š $\max ||w{T}u_1-(\sum_{i=2}(w{T}u_i))||_2^2$ å› æ­¤ï¼Œæƒ³æœ€å¤§åŒ–çš„ç›®æ ‡ è€ƒè™‘$i = 2$çš„æƒ…å†µ $$J = \frac{||wTu_0-wTu_1||22}{wT\sum{i=1}w+w^T\sum_{i=2}w} =\frac{wT(u_0-u_1)(u_0-u_1)Tw}{w^T(\sum_1+\sum_2)w} $$ $$ åº”ç”¨ç©ºé—´å‡ ä½•å’ŒçŸ©é˜µçš„å…³ç³»æè¿° ç±»å†…æ•£åº¦çŸ©é˜µ($S_W$) $$ $$\sum_1+\sum_2$$ $$ ç±»é—´æ•£åº¦çŸ©é˜µï¼š $$ $$(u_0-u_1)(u_0-u_1)^T$$ $$ æ‰€ä»¥ï¼Œæˆ‘ä»¬æƒ³ä¼˜åŒ–ç›®æ ‡å¦‚ä¸‹ï¼š $$ $$J = \frac{wT_Sbw}{wTS_ww}$$ å¦‚ä½•ç¡®å®š$w$å‘¢ï¼Ÿæ³¨æ„åˆ°åˆ†å­åˆ†æ¯éƒ½æ˜¯å…³äº$w$çš„äºŒæ¬¡å‹ï¼Œå› æ­¤è§£è¿™å’Œwçš„æ–¹å‘æœ‰å…³ç³»ï¼Œå› æ­¤ï¼Œå¯ä»¤ $w^TS_ww=1$ ,ä¼˜åŒ–é—®é¢˜å¯æ˜¯å¦‚ä¸‹ï¼š $$\min -w^TS_bw \ s.t. w^TS_ww = 1$$ æ„é€ lagrange å‡½æ•° $$ L = -wTS_bw+r(wTS_ww-1)$$ å¯¹$w$æ±‚å¯¼å¯å¾—ï¼š $$S_bw =rS_ww$$ $S_b w$å’Œ$ u_0 - u_1 $ æ–¹å‘æ˜¯$u_0-u_1$,ä¸å¦¨è®¾ $$ S_nw=r(u_0-u_1)$$ so,$$w = s_w^{-1}(u_0-u_1)$$ è¿™é‡Œè€ƒè™‘åˆ°æ•°å€¼è§£çš„ç¨³å®šæ€§ï¼Œå› æ­¤å¾€å¾€æŠŠ$S_w$è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ Â¶ç¬¬å››ç«  å†³ç­–æ ‘ å†³ç­–æ ‘æ˜¯ä¸€ç§ç‰¹åˆ«æ™®é€šçš„ç¬¦åˆç”Ÿæ´»åšå†³ç­–çš„è¿‡ç¨‹ã€‚ Â¶ç¬¬äº”ç«  ç¥ç»ç½‘ç»œ ç¥ç»ç½‘ç»œæœ€å¼€å§‹å‡ºç°æ˜¯æ ¹æ®ç”Ÿç‰©ç¥ç»ç½‘ç»œæ¥çš„ã€‚ Â¶æœ€ç®€å•çš„ç¥ç»ç½‘ç»œï¼šç¥ç»å…ƒæ¨¡å‹(neuron|unit) McCulloch and PittsæŠ½è±¡å‡ºâ€œM-Pç¥ç»å…ƒæ¨¡å‹&quot; Â¶æ„ŸçŸ¥å™¨ï¼ˆPerceptron) è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ï¼Œè¾“å‡ºå±‚ï¼šM-Pç¥ç»å…ƒ æ„ŸçŸ¥å™¨çš„å­¦ä¹ è¿‡ç¨‹ä¸€å®šæ˜¯æ”¶æ•›çš„ Â¶å¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œ ï¼ˆmulti-layer feddforward neural networks) å‰é¦ˆï¼šç½‘ç»œçš„æ‹“æ‰‘ç»“æ„ä¸å­˜åœ¨ç¯æˆ–è€…å›è·¯ ç¥ç»å…ƒçš„å­¦ä¹ è¿‡ç¨‹ï¼šå°±æ˜¯æ ¹æ®è®­ç»ƒæ•°æ®æ¥è°ƒæ•´ç¥ç»å…ƒä¹‹é—´çš„â€è¿æ¥æƒ&quot;(connection weight),ä»¥åŠæ¯ä¸ªåŠŸèƒ½ç¥ç»å…ƒçš„é˜™å€¼ Â¶è¯¯å·®é€†ä¼ æ’­ç®—æ³•ï¼š error BackPropagation (BP) Â¶å…¨å±€æœ€å°å’Œå±€éƒ¨æœ€å° ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹å…¶å®ä¹Ÿå°±æ˜¯å‚æ•°å¯»ä¼˜çš„è¿‡ç¨‹ï¼ŒåŸºäºæ¢¯åº¦çš„æœç´ æ˜¯ä½¿ç”¨æœ€ä¸ºå¹¿æ³›çš„å‚æ•°å¯»ä¼˜æ–¹æ³•ï¼Œä½†æ˜¯å¦‚æœè¯¯å·®å‡½æ•°åœ¨å½“å‰ç‚¹çš„æ¢¯åº¦ä¸ºé›¶ï¼Œåˆ™å¾ˆæœ‰å¯èƒ½è¾¾åˆ°å±€éƒ¨æå°ã€‚ Â¶ç¬¬å…­ç«  æ”¯æŒå‘é‡æœº æ”¯æŒå‘é‡æœºçš„å­¦ä¹ åŸç†å¾ˆç®€å•ä¹Ÿå¾ˆæœ‰è¶£ï¼Œä»åˆ†ç±»é—®é¢˜ï¼Œæ€ä¹ˆä¸€æ­¥ä¸€æ­¥å»ºç«‹çš„ä¼˜åŒ–é—®é¢˜ï¼Œä¸€æ­¥ä¸€æ­¥çš„å®Œå–„ä¼˜åŒ–é—®é¢˜ä»¥åŠæ±‚è§£ï¼Œä»ç¡¬é—´éš”åˆ°è½¯é—´éš”ï¼Œåˆ†ç±»é—®é¢˜æ˜¯è€ƒè™‘åˆ†å¯¹ï¼Œè€Œå›å½’é—®é¢˜å¸Œæœ›é¢„æµ‹å€¼å’ŒåŸå§‹å€¼å°½å¯èƒ½çš„æ¥è¿‘ï¼Œè¿™æ ·å°±é€ æˆäº†çº¦æŸæ¡ä»¶ï¼Œç›®æ ‡æ€§çš„ä¸åŒã€‚ æœ€é‡è¦çš„æ˜¯å¼•å…¥äº†æ ¸æ–¹æ³•ï¼Œä½ç»´ç©ºé—´çš„éçº¿æ€§å…³ç³»æ˜ å°„æˆäº†é«˜ç»´ç©ºé—´çº¿æ€§å…³ç³»ï¼Œè¿™æ˜¯ç‰¹åˆ«é‡è¦çš„æ€æƒ³ ç¬¬å…«ç«  é›†æˆå­¦ä¹  Â¶åŸºæœ¬æ€æƒ³ æ„å»ºä¸€ç»„åŸºå­¦ä¹ å™¨ï¼ˆbase learner)ï¼Œåœ¨ç»“åˆ a. å¦‚æœé›†æˆä¸­æ˜¯ç›¸åŒç±»å‹çš„ä¸ªä½“å­¦ä¹ å™¨ï¼Œå¦‚å†³ç­–æ ‘ï¼Œå…¨æ˜¯ç¥ç»ç½‘ç»œçš„é›†æˆâ€œåŒè´¨â€ï¼ˆhomogeneous),ä¸ªä½“å­¦ä¹ å™¨å«åŸºå­¦ä¹ å™¨ b. ä¸åŒçš„å­¦ä¹ å™¨ï¼Œå¼‚è´¨ï¼ˆheterogeneous)ï¼Œä¸ªä½“å­¦ä¹ å™¨å«ç»„ä»¶å­¦ä¹ å™¨ Â¶ä¸ºä»€ä¹ˆæœ‰æ•ˆ å¤šæ ·æ€§çš„åŸºå­¦ä¹ å™¨ ä¸åŒçš„æ¨¡å‹å–é•¿è¡¥çŸ­ æ¯ä¸ªåŸºå­¦ä¹ å™¨éƒ½çŠ¯é”™è¯¯ï¼Œç»¼åˆèµ·æ¥å¯èƒ½æ€§ä¸å¤§ ä¸¾ä¸ªæ —å­ ä¹Ÿè®¸ä¸€ä¸ªçº¿æ€§æ¨¡å‹ä¸èƒ½ç®€å•åˆ†ç±»ï¼Œä½†æ˜¯å¤šä¸ªçº¿æ€§æ¨¡å‹ç»¼åˆï¼Œå¯å°†æ•°æ®é›†æˆåŠŸåˆ†ç±» Â¶æ„å»ºä¸åŒçš„æœºå™¨å­¦ä¹  Q 1: å¦‚ä½•å»ºç«‹åŸºå­¦ä¹ å™¨ å°½é‡æ»¡è¶³å¤šæ ·æ€§ M1: ä¸åŒçš„å­¦ä¹ ç®—æ³• M2: ç›¸åŒå­¦ä¹ ç®—æ³•ã€ä¸åŒçš„å‚æ•° M3: ä¸åŒçš„æ•°æ®é›†ï¼ˆä¸åŒçš„æ ·æœ¬å­é›†ã€æ•°æ®é›†ä¸Šä¸åŒçš„ç‰¹å¾ï¼‰ homogenous ensemble é‡‡ç”¨ç›¸åŒçš„å­¦ä¹ ç®—æ³•ã€ä¸åŒçš„è®­ç»ƒé›† Bagging Boosting ç›¸åŒç®—æ³•ï¼Œä¸åŒçš„å‚æ•°è®¾ç½® ç›¸åŒçš„è®­ç»ƒé›†ï¼Œä¸åŒçš„å­¦ä¹ ç®—æ³• Q2: å¦‚ä½•ç»¼åˆå‘¢ï¼Ÿ tæŠ•ç¥¨æ³•ï¼šmajority voting weighted voting è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹ç¡®å®šå¦‚ä½•ç»¼åˆ Stacking åå¥½çš„ç®€å•æ¨¡å‹ Â¶ç»¼åˆ Â¶Bagging = Boostrap AGGregatING æœ‰æ”¾å›é‡‡æ ·ï¼ŒåŒè´¨å­¦ä¹ å™¨ Â¶ç®—æ³• 1234567891011Input : è®­ç»ƒé›† D=&#123;(x1,y1)&#125; åŸºå­¦ä¹ ç®—æ³•A è®­ç»ƒè½®æ•° Tè¿‡ç¨‹ for t = 1,2,...,T do h_t= A(D,Dt) // Dtç¬¬tæ¬¡é‡‡æ ·çš„åˆ†å¸ƒ end forè¾“å‡º å›å½’ï¼šAverage åˆ†ç±»ï¼šæŠ•ç¥¨æ³• Â¶ä¼˜ç‚¹ æ²¡æœ‰ç”¨äºå»ºæ¨¡çš„æ ·æœ¬ï¼Œå¯ä»¥ç”¨ä½œéªŒè¯é›†æ¥å¯¹æ³›åŒ–èƒ½åŠ›è¿›è¡ŒåŒ…å¤–ä¼°è®¡ï¼Œå¯ä»¥å¾—å‡ºBaggingæ³›åŒ–è¯¯å·®çš„åŒ…å¤–ä¼°è®¡ Â¶random forestï¼ˆRF) è¾“å…¥ä¸ºæ ·æœ¬é›†$D={(x,y1),(x2,y2),â€¦(xm,ym)}$ï¼Œå¼±åˆ†ç±»å™¨è¿­ä»£æ¬¡æ•°Tã€‚ è¾“å‡ºä¸ºæœ€ç»ˆçš„å¼ºåˆ†ç±»å™¨f(x)f(x) 1ï¼‰å¯¹äºt=1,2â€¦,T: a)å¯¹è®­ç»ƒé›†è¿›è¡Œç¬¬tæ¬¡éšæœºé‡‡æ ·ï¼Œå…±é‡‡é›†mæ¬¡ï¼Œå¾—åˆ°åŒ…å«mä¸ªæ ·æœ¬çš„é‡‡æ ·é›†$Dt$ b)ç”¨é‡‡æ ·é›†$Dt$è®­ç»ƒç¬¬tä¸ªå†³ç­–æ ‘æ¨¡å‹$Gt(x)$ï¼Œåœ¨è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹çš„èŠ‚ç‚¹çš„æ—¶å€™ï¼Œ åœ¨èŠ‚ç‚¹ä¸Šæ‰€æœ‰çš„æ ·æœ¬ç‰¹å¾ä¸­é€‰æ‹©ä¸€éƒ¨åˆ†æ ·æœ¬ç‰¹å¾ï¼Œ åœ¨è¿™äº›éšæœºé€‰æ‹©çš„éƒ¨åˆ†æ ·æœ¬ç‰¹å¾ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜çš„ç‰¹å¾æ¥åšå†³ç­–æ ‘çš„å·¦å³å­æ ‘åˆ’åˆ† å¦‚æœæ˜¯åˆ†ç±»ç®—æ³•é¢„æµ‹ï¼Œåˆ™Tä¸ªå¼±å­¦ä¹ å™¨æŠ•å‡ºæœ€å¤šç¥¨æ•°çš„ç±»åˆ«æˆ–è€…ç±»åˆ«ä¹‹ä¸€ä¸ºæœ€ç»ˆç±»åˆ«ã€‚å¦‚æœæ˜¯å›å½’ç®—æ³•ï¼ŒTä¸ªå¼±å­¦ä¹ å™¨å¾—åˆ°çš„å›å½’ç»“æœè¿›è¡Œç®—æœ¯å¹³å‡å¾—åˆ°çš„å€¼ä¸ºæœ€ç»ˆçš„æ¨¡å‹è¾“å‡ºã€‚ å‚æ•°è®¾ç½® åˆ©ç”¨00Bæ ·æœ¬è¯„ä¼°å˜é‡çš„é‡è¦æ€§ Â¶Boosting æé«˜ é¡ºæ¬¡å»ºç«‹å­¦ä¹ å™¨ï¼Œå°±æ˜¯å…ˆä»è®­ç»ƒé›†ä¸Šè®­ç»ƒä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼Œå†æ ¹æ®å­¦ä¹ å™¨çš„è¡¨ç°å¯¹è®­ç»ƒé›†åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œè®©å…ˆå­¦ä¹ å™¨é”™è¯¯è®­ç»ƒçš„æ ·æœ¬åœ¨åç»­æ”¶åˆ°æ›´å¤šçš„å…³æ³¨ï¼Œç„¶ååŸºäºè°ƒæ•´çš„åˆ†å¸ƒè®­ç»ƒä¸‹ä¸€ä¸ªå­¦ä¹ å™¨ï¼Œæœ€åï¼Œåœ¨å°†è¿™Tä¸ªå­¦ä¹ å™¨è¿›è¡ŒåŠ æƒç»“åˆ åŸºå­¦ä¹ å™¨çš„çº¿æ€§ç»„åˆ $$ H_N(x;P)=\sum_{t=1}^{N}\alpha_th_t(x;a_t) $$ $a_t$æ˜¯ç¬¬$i$ä¸ªå¼±å­¦ä¹ å™¨çš„æœ€ä¼˜å‚æ•°ï¼Œ$\alpha_t$æ˜¯åœ¨å¼ºåˆ†ç±»å™¨ä¸­çš„æ¯”é‡ï¼Œ$P$æ˜¯$a_t$å’Œ$\alpha_t$çš„ç»„åˆ æœ€å°åŒ–æŒ‡æ•°æŸå¤±å‡½æ•° $$ l_{exp}(H|D)=E_{x~D}[e^{-f(x)H(x)}] $$ $$ H_n(x)=H_{n-1}(x)+\alpha_{n}h_{n}(x,a_n) $$ $$l(h_i(x,a_t)|D)=E_{x~D}(exp(-f(x)h_i(x)))\=p(f(x)=1)exp(-h_i(x))+p(f(x)=-1)exp(h_i(x))$$ $$ \frac{\partial l(h_i(x,a_t)|D)}{\partial h_i(x,a_t)}=\ -p(f(x)=1)exp(-h_i(x))+p(f(x)=-1)exp(h_i(x))=0 $$ $$h(x)=\frac{1}{2}ln\frac{P(f(x)=1)}{P(f(x)=-1)}$$ é‡‡å–ä¸åŒçš„æŸå¤±å‡½æ•°ï¼Œå¾—åˆ°ä¸åŒçš„ç±»å‹ https://blog.csdn.net/luanpeng825485697/article/details/79383492 Â¶GBDT Â¶Stacking ä¸åŒå­¦ä¹ å™¨ï¼Œç›¸åŒæ•°æ®é›† ç¬¬ä¸€å±‚ ç¬¬äºŒå±‚ï¼šä¸ç”¨ç¬¬ä¸€å±‚çš„æ•°æ® å¯ç”¨äº¤å‰éªŒè¯ æ³¨æ„äº‹é¡¹ï¼š è¿‡æ‹Ÿåˆé—®é¢˜ï¼šç¬¬äºŒå±‚çº¿æ€§å›å½’ ç¬¬ä¸€å±‚å°½å¯èƒ½çš„å¤šæ ·æ€§ï¼š ç»¼åˆå¥½çš„æ¨¡å‹ é˜²æ­¢è¿‡æ‹Ÿåˆ 1. éšæœºæ€§ 2. Bagging Boosting Stacking Â¶æå¤§ä¼¼ç„¶ä¼°è®¡ ä¼¼ç„¶ï¼š ç›¸ä¼¼çš„æ ·å­ å¯¹äºä¸€ç»„æ•°æ®ï¼Œå‡è®¾ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œå¸Œæœ›å·²çŸ¥ç‚¹åœ¨è¿™ä¸ªæ­£æ€åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰ç‚¹å¯¹äºçš„æ¦‚ç‡ä¹‹å’Œæˆ–è€…ç§¯æœ€å¤§ï¼Œ ï¼Œè“è‰²è¡¨ç¤ºæ•°æ®ï¼Œçº¢è‰²å°±æ˜¯åšå¾—æ­£æ€åˆ†å¸ƒ Â¶ç¬¬åç«  é™ç»´ä¸åº¦é‡å­¦ä¹  Â¶kè¿‘é‚»å­¦ä¹  k-Nearest Neighbor åŸç†ï¼š åŸºäºæŸç§è·ç¦»åº¦é‡æ‰¾å‡ºè®­ç»ƒé›†ä¸­ä¸å…¶æœ€é è¿‘çš„kä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ ¹æ®kä¸ªé‚»å±…çš„ä¿¡æ¯è¿›è¡Œé¢„æµ‹ã€‚ ç»™å®šæµ‹è¯•æ ·æœ¬$x$,å¦‚æœæœ€é‚»è¿‘æ ·æœ¬$z$,æœ€é‚»è¿‘åˆ†ç±»å™¨å‡ºé”™çš„æ¦‚ç‡å°±æ˜¯$x$ä¸$z$ä¸å†åŒä¸€ç±» $$ p(err) = 1-\sum_{c \in y}p(c|x)P(c|z) $$ Â¶ä½ç»´åµŒå…¥ ç¼“è§£ç»´æ•°ç¾éš¾çš„é‡è¦é€”ç»ä¹‹ä¸€æ˜¯é™ç»´ï¼ˆdimension reductionï¼‰è¿™æ ·ä½¿å¾—å­ç©ºé—´ä¸­æ ·æœ¬å¯†åº¦å¤§å¹…åº¦æé«˜ï¼Œè·ç¦»è®¡ç®—å˜å¾—æ›´å®¹æ˜“ï¼Œ Â¶å¤šç»´ç¼©æ”¾ï¼ˆMultiple Dimensional,Scalingï¼‰ MDS å‡å®šmä¸ªæ ·æœ¬åœ¨åŸå§‹ç©ºé—´çš„è·ç¦»çŸ©é˜µ$D$,åœ¨ä½ç»´ç©ºé—´ä¸­ï¼Œä¸¤ä¸ªæ ·æœ¬æ¬§å¼è·ç¦»ç­‰äºåŸç©ºé—´çš„è·ç¦»ï¼Œ$||z_i-z_j|| = dist_{ij}$, ä»¤$B=Z^TZ$ä¸ºé™ç»´åæ ·æœ¬çš„å†…ç§¯çŸ©é˜µ, $$ dist_{ij}2=||z_i||2+||z_j||^2-2z_iz_j=b_{ii}+b_{jj}-2b_{ij} $$ å¯¹é™ç»´åæ•°æ®ä¸­å¿ƒåŒ–ï¼Œå‡å€¼ä¸º0,$\sum_{i=1}{m}z_i$,äºæ˜¯ä¹å°±æœ‰$\sum_{i=1}{M}b_{ij}=z_j(z_1+z_2+â€¦+z_m)=0=\sum_{j=1}^{m}x_{ij}$ ,å¯å¾— $$ \sum_{i=1}{m}dist_{ij}2=\sum_{i=1}^{m}(b_{ii}+b_{jj}-2b_{ij})=tr(B)mb{jj}\ \sum_{i=1}{m}\sum_{j=1}{m}dist_{ij}^2 = 2m tr(B)\ tr(B)=\sum_{i=1}{m}||z_i||2 $$ å¯å¾— $$ b_{ij}=-\frac{1}{2}(dist_{ij}2-dist_{i.}2-dist_{.j}2+dist{â€¦}2) $$ å¯¹çŸ©é˜µBåšç‰¹å¾å€¼åˆ†è§£(eigenvalue decomposition)ï¼Œ$B = V \land V$,åˆ™ $$ Z = \land_{}^{1/2}V_{} $$ æ¬²è·å¾—ä½ç»´å­ç©ºé—´ï¼Œæœ€ç®€å•æ˜¯å¯¹åŸå§‹é«˜ç»´ç©ºé—´è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œ$Z = W^TX$,ç‰¹åˆ«çš„ï¼Œ$W$å–æ­£äº¤å˜æ¢ï¼Œ$W={w_1,w_2,â€¦,w_{dâ€™}}$Wæ˜¯dâ€™ä¸ªdç»´åŸºå‘é‡ï¼Œ Â¶ä¸»æˆåˆ†åˆ†æ Principal Component Analysis ï¼šPCA åœ¨æ­£äº¤ç©ºé—´é‡Œé¢çš„æ ·æœ¬ï¼Œç”¨ä¸€ä¸ªè¶…å¹³é¢å¯¹æ ·æœ¬è¿›è¡Œæ°å½“çš„è¡¨è¾¾ï¼Œè‡³å°‘è¿™ä¸ªæ ·æœ¬ç‚¹æ»¡è¶³ æœ€è¿‘é‡æ„æ€§ï¼š æ ·æœ¬ç‚¹åˆ°è¿™ä¸ªè¶…å¹³é¢çš„è·ç¦»è¶³å¤Ÿè¿‘ æœ€å¤§å¯åˆ†æ€§ï¼š æ ·æœ¬ç‚¹åœ¨è¿™è¶…å¹³é¢ä¸Šçš„æŠ•å½±å°½å¯èƒ½åˆ†å¼€ å¯¹äºæœ€è¿‘é‡æ„æ€§ï¼š å‡è®¾æ ·æœ¬å»ä¸­å¿ƒåŒ–ï¼Œå†å‡è®¾æŠ•å½±å˜æ¢åå¾—åˆ°æ¬£çš„æ­£äº¤åæ ‡ç³»${w_1,w_2,â€¦,w_d}$,dç»´ç©ºé—´é‡Œé¢çš„ä¸€ç»„å•ä½æ­£äº¤åŸºï¼Œ$||w_i||2=0$,$||w_iTw_j||=0$,å¦‚æœå†æ–°åæ ‡ç³»ä¸­ä¸¢æ‰ä¸€éƒ¨åˆ†åæ ‡ï¼Œæ ·æœ¬ç‚¹åœ¨æ–°åæ ‡çš„æŠ•å½±æ˜¯$z_i={w_1Tx{i1}},â€¦,w_{dâ€™}^Tx_{i}$,äºæ˜¯åˆ$z_{ij} =w_{j}Tx_i$,$\hat{x_i}=\sum_{j}{dâ€™}w_jx_i$ $$ \sum_{i=1}{m}||\sum_{j=1}{dâ€™}z_{ij}w_j-x_i||22=\sum_{i=1}{m}z_iTz_i-2\sum_{i=1}{m}z_iTWTx_i+x_i^Tx_i\ =\sum{i=1}{m}x_iTWWTx_i-2\sum_{i=1}{m}x_iTWWTx_i+x_i^Tx_i\ min -\sum_{i=1}{m}z_iTz_i=-tr(Z^TZ)\ min -tr(\sum_{i=1}{m}WTx_ix_iTW)=-tr(WT(\sum_{i=1}{m}x_iTx_i)Wï¼‰=-tr(WTXXTW)\ s.t W^TW = I $$ å¯¹äºæœ€å¤§å¯åˆ†æ€§$(W^T\hat{X}=0)$ $$ max tr(WTXXTW)\s.t W^TW = I $$ æ ¹æ®lagrange $$ L(W,\lambda)=-tr(WTXXTW)-\lambda(W^TW-I)\ \frac{\partial L}{\partial w_i}=-2w_iXX^T-2\lambda_i w_i=0\ XX^Tw_i = \lambda w_i $$ $XX^T$æ˜¯åæ–¹å·®çŸ©é˜µ,$\lambda$æ˜¯ç‰¹å¾å€¼ï¼Œ$w_i$æ˜¯ç‰¹å¾å‘é‡ ç‰¹åˆ«æç¤ºï¼Œ$x$éœ€è¦ä¸­å¿ƒåŒ– å¯¹äºçº¿æ€§PCAé™ç»´æ–¹æ³•æ˜¯ä»é«˜ç»´ç©ºé—´æ˜ å°„åˆ°ä½ç»´ç©ºé—´ï¼Œ$Z= W^TX$,ç„¶è€Œä¸å°‘æƒ…å†µï¼Œåˆ™éœ€è¦éçº¿æ€§æ˜ å°„æ‰èƒ½æ‰¾åˆ°æ°å½“çš„ä½ç»´åµŒå…¥ï¼Œ $\phi(x)$ $$ \max tr(\phi(X)\phi(X)^T)=tr( WT\varphi(x)\varphi(x)TW)\ W^TW = I $$ äºæ˜¯æœ‰ $$ \varphi(x)^T\varphi(x)w_i=\lambda_iw_i\ w_i=\frac{tr(\varphi(x)^T\varphi(x))}{\lambda_iw_i} $$ $$ z_j = \frac{\sum_{i=1}{m}\varphi(x)T\varphi(x)}{\lambda_iw_i}\varphi(x_i) =\frac{\sum_{i=1}^{m}\varphi(x_i)K(x_i,x)}{\lambda_iw_i} $$ Â¶æµå½¢å­¦ä¹ ï¼ˆè¡¨ç¤ºå­¦ä¹ æœ‰ç‚¹å›°éš¾) ç¬¬åä¸€ç«  ç‰¹å¾é€‰æ‹©ä¸ç¨€ç–å­¦ä¹  å¯¹äºä¸€ä¸ªå­¦ä¹ ä»»åŠ¡ï¼Œå¯¹ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾,ç§°ä¸º&quot;relevant feature&quot;ï¼Œå¯¹äºæ²¡æœ‰ç”¨çš„å±æ€§â€irrelevant feature&quot;,å› æ­¤ä»ç»™å®šç‰¹å¾é›†é€‰æ‹©å‡ºç›¸å…³ç‰¹å¾å­é›†çš„è¿‡ç¨‹ï¼Œç‰¹å¾é€‰æ‹©ï¼ˆfeature selection),åŸå› ä¸€ï¼Œé™ç»´ï¼›åŸå› äºŒï¼šé™ä½å­¦ä¹ çš„ä»»åŠ¡ã€‚ æ— å…³ç‰¹å¾ï¼ŒåŒ…æ‹¬ä¸€ç±»å†—ä½™ç‰¹å¾ï¼ˆredundant featureï¼‰ï¼Œèƒ½å¤Ÿä»å…¶ä»–ç‰¹å¾é‡Œé¢æ¨æ¼”å‡ºæ¥ã€‚ Â¶ç‰¹å¾æœç´¢ Â¶å‰å‘ï¼ˆforward)æœç´¢ å¯¹äºç‰¹å¾é›†åˆ${a_1,a_2,â€¦,a_d }$,æ¯ä¸ªç‰¹å¾çœ‹ä½œä¸€ä¸ªå€™é€‰é›†ï¼Œå¯¹è¿™$d$å€™é€‰çš„å•ç‰¹å¾å­é›†è¿›è¡Œè¯„ä»·ï¼Œå¯é€‰å‡ºæœ€ä¼˜å­é›†ï¼Œç„¶åï¼Œå†ä¸‹ä¸€è½®å­é›†ä¸­ï¼Œæ„æˆäº†ä¸¤ä¸ªç‰¹å¾å€™é€‰çš„å­é›†ï¼Œ Â¶åå‘ (backward) æœç´¢ æ¯æ¬¡å°è¯•å»æ‰ä¸€ä¸ªæ— å…³ç‰¹å¾ Â¶åŒå‘(bidirectional)æœç´¢ ä¸Šè¿°æ“ä½œåªæ˜¯è´ªå¿ƒç­–ç•¥ï¼Œä»…ä»…è€ƒè™‘äº†æœ¬è½®é€‰å®šé›†åˆæœ€ä¼˜ â€‹ Â¶å­é›†è¯„ä»·ï¼ˆsubset evaluation) å·²çŸ¥ä¸€ä¸ªæ•°æ®é›†$D$,å‡å®šç¬¬$i$ç±»æ ·æœ¬æ‰€å æ¯”ä¾‹$p_i$,å¯¹äºå±æ€§å­é›†$A$,å‡è®¾æ ¹æ®å–å€¼Dåˆ†æˆVä¸ªå­é›†${D1,D2,â€¦,D^V}$,åˆ™å­é›†Açš„ä¿¡å¿ƒ å¢ç›Š $$ Gain(A) = Ent(D)-\sum_{i=1}V\frac{|Di|}{|D|}Ent(D^i)\ Ent(D)=\sum_{i=1}{|y|}p_ilog{-p_i} $$ â€‹ ä¿¡æ¯å¢ç›ŠGain(A)è¶Šå¤§ï¼Œè¯´æ˜ç‰¹å¾å­é›†AåŒ…å«çš„æœ‰åŠ©äºåˆ†ç±»çš„ä¿¡æ¯è¶Šå¤šï¼Œç‰¹å¾å­é›†Aæ˜¯å¯¹æ•°æ®é›†Dçš„ä¸€ä¸ªåˆ’åˆ†ï¼Œæ ·æœ¬Dçš„æ ‡è®°ä¿¡æ¯Yåˆ™å¯¹åº”ç€Dçš„çœŸå®åˆ’åˆ†ï¼Œå°±èƒ½å¯¹Aè¿›è¡Œè¯„ä»·ï¼Œå¯¹Yå¯¹åº”çš„åˆ’åˆ†çš„å·®å¼‚è¶Šå°ï¼Œåˆ™è¯´æ˜Aè¶Šå¥½ï¼Œ Â¶è¿‡æ»¤å¼é€‰æ‹© Relief ï¼ˆRelevant Featureï¼‰ è®¾è®¡ä¸€ä¸ªâ€œç›¸å…³ç»Ÿè®¡é‡â€æ¥æè¿°åº¦é‡ç‰¹å¾çš„é‡è¦æ€§ï¼Œè¯¥ç»Ÿè®¡é‡æ˜¯ä¸€ä¸ªå‘é‡ï¼Œæ¯ä¸ªåˆ†é‡å¯¹åº”ä¸€ä¸ªåˆå¼ç‰¹å¾ï¼Œè€Œç‰¹å¾å­é›†çš„é‡è¦æ€§åˆ™æ˜¯æ¯ä¸ªç‰¹å¾å¯¹åº”ç»Ÿè®¡é‡åˆ†é‡ä¹‹å’Œæ¥å†³å®šï¼Œæœ€ç»ˆåªéœ€æŒ‡å®šä¸€ä¸ªé˜™å€¼ï¼Œæ ¹æ®é˜™å€¼é€‰æ‹©ç»Ÿè®¡é‡åˆ†é‡å¯¹åº”çš„ç‰¹å¾å³å¯ å¦‚ä½•ç¡®å®šç›¸å…³ç»Ÿè®¡é‡ ç»™å®šè®­ç»ƒé›†$(x_i,y_i)$,å¯¹äºå®ä¾‹$x_i$,åœ¨å…¶åŒç±»æ ·æœ¬ä¸­æ‰¾æœ€è¿‘é‚»ï¼ˆnear-hit),åœ¨ä»å¼‚ç±»æ ·æœ¬ä¸­å¯»æ‰¾å…¶æœ€è¿‘é‚»$x_{x,nm}$ç§°ä¸ºâ€œçŒœé”™è¿‘é‚»â€ï¼Œ $$ \delta^j =\sum_i-diff(x_ij,x_{i.nh}j)2+diff(x_ij,x_{i,nm}j)2 $$ åˆ†å€¼è¶Šå¤§ï¼Œè¯´æ˜å¯¹åº”å±æ€§çš„åˆ†ç±»èƒ½åŠ›è¶Šå¼º å¯¹äºå¤šåˆ†ç±»é—®é¢˜ $$ \delta^j = \sum_i-diff(x_ij,x_{i,nh}j)^2+\sum_{l \neq k}p_l\ diff(x_ij,x_{i,l,nm}j) $$ è¿™ç§æ–¹æ³•çœ‹ä¸€ä¸ªå±æ€§ï¼ˆç‰¹å¾ï¼‰é‡ä¸é‡è¦ï¼Œå…ˆè®¡ç®—å‡ºæ¯ä¸ªå±æ€§çš„ç»Ÿè®¡åˆ†é‡ï¼ŒæŒ‰ç…§å…¬å¼ï¼Œå­é›†çš„è¯„ä»·å°±æ˜¯å¯¹äºåˆ†é‡çš„å’Œ Â¶åŒ…è£¹å¼é€‰æ‹© ç›´æ¥æŠŠæœ€ç»ˆå°†è¦ä½¿ç”¨çš„å­¦ä¹ å™¨çš„æ€§èƒ½ä½œä¸ºç‰¹å¾å­é›†çš„è¯„ä»·å‡†åˆ™ï¼Œç‰¹å¾é€‰æ‹©çš„ç›®çš„å°±æ˜¯ä¸ºç»™å®šå­¦ä¹ æœŸé€‰æ‹©æœ‰åˆ©å…¶æ€§èƒ½çš„ç‰¹å¾å­é›†ã€‚ LVWï¼ˆLas Vegas Wrapperï¼‰æ˜¯å…¸å‹çš„åŒ…è£¹å¼ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œæ‹‰æ–¯ç»´åŠ æ–¯æ–¹æ³•ï¼ˆLas Vegas methodï¼‰æ¡†æ¶ä¸‹ä½¿ç”¨éšæœºç­–ç•¥æ¥è¿›è¡Œå­é›†æœç´¢ï¼Œå¹¶ä»¥æœ€ç»ˆåˆ†ç±»å™¨çš„è¯¯å·®ä¸ºç‰¹å¾å­é›†è¯„ä»·å‡†åˆ™ ç®—æ³• Â¶åµŒå…¥å¼é€‰æ‹© å­¦ä¹ å™¨è‡ªåŠ¨åœ°è¿›è¡Œç‰¹å¾é€‰æ‹© L-PèŒƒæ•° $$ L_P = ||X||P = p\sqrt{\sum{i=1}{n}x_ip} $$ L0èŒƒæ•° $$ ||X||_0=å‘é‡ä¸­éé›¶å…ƒç´ çš„ä¸ªæ•° $$ L1èŒƒæ•° $$ ||x||_1 = \sum|x_i| $$ L2èŒƒæ•°ï¼Œæœ€å¸¸ç”¨ $$ ||X||_2=\sqrt{x_i^2} $$ æ— ç©·èŒƒæ•° $$ ||x||=max|x_i| $$ å¯¹äºçº¿æ€§å›å½’æ¨¡å‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¦‚æœä½¿ç”¨L2,ç§°ä¸ºå²­å›å½’(ridge regression),å¦‚æœé‡‡å–L1èŒƒæ•°ï¼Œåˆ™æœ‰ç§°ä¸ºLASSOï¼ŒL1æ¯”L2æ›´æ˜“äºç¨€ç–è§£ï¼Œå¯ä»¥çœ‹å¾—å‡ºL1èŒƒæ•°æ­£åˆ™åŒ–çš„è¿‡ç¨‹å¾—åˆ°äº†ä»…é‡‡ç”¨ä¸€éƒ¨åˆ†åˆå§‹åŒ–ç‰¹å¾çš„æ¨¡å‹ã€‚ L1æ­£åˆ™åŒ–æ±‚è§£å¯ä½¿ç”¨è¿‘ç«¯æ¢¯åº¦ä¸‹é™æ³•(Proximal Gradient Descent)PGD L-Lipschitzæ¡ä»¶ è®¾å‡½æ•°$Î¦(x)$åœ¨æœ‰é™ åŒºé—´$[a,b]$ä¸Šæ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š (1) å½“$xâˆˆ[a,b]$æ—¶ï¼Œ$Î¦(x)âˆˆ[a,b]$ï¼Œå³$aâ‰¤Î¦(x)â‰¤b$. (2) å¯¹ä»»æ„çš„$x1ï¼Œx2âˆˆ[a,b]$ï¼Œ æ’æˆç«‹ï¼š$|Î¦(x1)-Î¦(x2)|â‰¤L|x1-x2|$. å¦‚æœ$f(x)$å¯å¯¼ï¼Œå¹¶ä¸”$\nabla f$æ»¡è¶³L-Lipschitzæ¡ä»¶ï¼Œ $$ ||\nabla f(xâ€™)-\nabla f(x)||_22&lt;L||xâ€™-x||_22 $$ åœ¨$x_k$é™„è¿‘ $$ \hat{f}(x)=f(x_k)+f{â€™}(x_k)(x-x_k)+\frac{L}{2}||x-x_k||2\ =\frac{L}{2}||x-(x_k)-\frac{1}{L}\nabla f(x_k)||2^2+const $$ å¯çŸ¥ $$ x{k+1}=x_k-\frac{1}{L}\nabla f(x_k) $$ å¯¹äºåŸå§‹é—®é¢˜ï¼Œå¯å…ˆè®¡ç®—$z=x_k-\frac{1}{L}\nabla f(x_k)$, $x_{k+1}=arg \ \ min_{x} \frac{L}{2}||x-z||_2^2+\lambda||x||_1$ ç”±äºå„ä¸ªåˆ†é‡ç›¸äº’ä¸å½±å“ $$ x_{k+1}i=\begin{cases}zi-\frac{\lambda}{L}, \frac{\lambda}{L}&lt;z^i\ 0, |z^i| &lt;= \frac{\lambda}{L} \ zi+\frac{\lambda}{L},zi&lt;-\frac{\lambda}{L}\end{cases} $$ Â¶ç¨€ç–å­¦ä¹ ]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>è¥¿ç“œä¹¦</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PageRankç®—æ³•]]></title>
    <url>%2F2020%2F07%2F15%2FPageRank%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[PageRankæ˜¯ä¸€ç§ç½‘é¡µæ’åºç®—æ³•ï¼ŒåŸºäºé¡µé¢çš„è´¨é‡å’Œæ•°é‡ã€‚å¯åº”ç”¨äºè¯„ä¼°ç½‘é¡µèŠ‚ç‚¹é‡è¦æ€§ã€‚ PageRankç®—æ³• PageRank,å³****ç½‘é¡µæ’å*ï¼Œåˆç§°ç½‘é¡µçº§åˆ«*ã€Googleå·¦ä¾§æ’åæˆ–**ä½©å¥‡æ’åã€‚**PageRankæ˜¯Googleç”¨äºç”¨æ¥æ ‡è¯†ç½‘é¡µçš„ç­‰çº§/é‡è¦æ€§çš„ä¸€ç§æ–¹æ³•ï¼Œæ˜¯Googleç”¨æ¥è¡¡é‡ä¸€ä¸ªç½‘ç«™çš„å¥½åçš„å”¯ä¸€æ ‡å‡†ã€‚ å‡è®¾ æ•°é‡å‡è®¾: å¦‚æœä¸€ä¸ªé¡µé¢èŠ‚ç‚¹å…¥é“¾æ•°é‡è¶Šå¤šï¼Œåˆ™è¿™ä¸ªé¡µç è¶Šé‡è¦ã€‚ è´¨é‡å‡è®¾ï¼šæŒ‡å‘é¡µé¢Açš„å…¥é“¾è´¨é‡ä¸åŒï¼Œè€ƒè™‘æƒé‡çš„å½±å“ï¼Œåˆ™è¿™ä¸ªé¡µé¢è¶Šæ˜¯é‡è¦ã€‚ Â¶ç®—æ³•æ±‚è§£ ç¬¬ä¸€é˜¶æ®µï¼šé€šè¿‡ç½‘é¡µé“¾æ¥å…³ç³»æ„å»ºèµ·Webå›¾ï¼Œåˆå§‹æ¯ä¸ªé¡µé¢ç›¸åŒçš„PageRankå€¼ï¼Œå†é€šè¿‡è‹¥å¹²è½®å¾—åˆ°æ¯ä¸ªé¡µé¢çš„æœ€ç»ˆpagerank. æ¯ä¸€è½®æ›´æ–°é¡µé¢PageRankå¾—åˆ†çš„è®¡ç®—æ–¹æ³• Â¶æƒé‡ $$ PR(T)/L(T)\ where PR(T)çš„PageRankå€¼ï¼ŒL(T)ä¸ºTçš„å‡ºé“¾æ•°ç›® $$ Â¶ä¿®æ­£ $L(T)$ä¸º0çš„æƒ…å†µï¼Œå­¤ç«‹ç½‘é¡µï¼Œä½¿å¾—å¾ˆå¤šç½‘é¡µèƒ½è¢«è®¿é—®åˆ°ã€‚$q = 0.85$ $$ PR(A) = (\frac{PR(B)}{L(B)}+\frac{PRÂ©}{LÂ©}+\dots)q+1-q $$ Â¶å…¶ä»–ç½‘ç»œå±æ€§åº¦é‡æ–¹æ³• Centrality indices: degree, betweenness, and closeness. Â¶reference æå‡ºè€…ï¼š The anatomy of a large-scale hypertextual Web search engine https://en.wikipedia.org/wiki/PageRank]]></content>
      <categories>
        <category>ç§‘ç ”</category>
      </categories>
      <tags>
        <tag>ç®—æ³•</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rè¯­è¨€]]></title>
    <url>%2F2020%2F07%2F03%2FR%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[https://bookdown.org/qiyuandong/intro_r/-r-basics-2.html#section-3.3 å…¥é—¨ï¼š https://rc2e.com/ http://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/intro.html å…¨é¢ï¼š https://github.com/harryprince/R-Tutor è§†é¢‘ï¼š ä¸­æ–‡ï¼š https://www.youtube.com/watch?v=rPj5FsTRboE è‹±æ–‡ï¼šhttps://www.youtube.com/watch?v=32o0DnuRjfg è¿™ä¸ªæ•™ç¨‹å¥½ï¼š https://sites.google.com/site/econometricsacademy/econometrics-models/linear-regression https://www.youtube.com/watch?v=YMt5K68ZvjQ&amp;list=PLRW9kMvtNZOh7Xt1m5Mlhhz2wtr0tCUEE]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xgboost]]></title>
    <url>%2F2020%2F07%2F03%2FXgboost%2F</url>
    <content type="text"><![CDATA[ç†è®ºéƒ¨åˆ† è¯¥ç®—æ³•æ€æƒ³å°±æ˜¯ä¸æ–­åœ°æ·»åŠ æ ‘ï¼Œä¸æ–­åœ°è¿›è¡Œç‰¹å¾åˆ†è£‚æ¥ç”Ÿé•¿ä¸€æ£µæ ‘ï¼Œæ¯æ¬¡æ·»åŠ ä¸€ä¸ªæ ‘ï¼Œå…¶å®æ˜¯å­¦ä¹ ä¸€ä¸ªæ–°å‡½æ•°ï¼Œå»æ‹Ÿåˆä¸Šæ¬¡é¢„æµ‹çš„æ®‹å·®ã€‚å½“æˆ‘ä»¬è®­ç»ƒå®Œæˆå¾—åˆ°kæ£µæ ‘ï¼Œæˆ‘ä»¬è¦é¢„æµ‹ä¸€ä¸ªæ ·æœ¬çš„åˆ†æ•°ï¼Œå…¶å®å°±æ˜¯æ ¹æ®è¿™ä¸ªæ ·æœ¬çš„ç‰¹å¾ï¼Œåœ¨æ¯æ£µæ ‘ä¸­ä¼šè½åˆ°å¯¹åº”çš„ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæ¯ä¸ªå¶å­èŠ‚ç‚¹å°±å¯¹åº”ä¸€ä¸ªåˆ†æ•°ï¼Œæœ€ååªéœ€è¦å°†æ¯æ£µæ ‘å¯¹åº”çš„åˆ†æ•°åŠ èµ·æ¥å°±æ˜¯è¯¥æ ·æœ¬çš„é¢„æµ‹å€¼ã€‚ boosting: https://zhuanlan.zhihu.com/p/38329631 Xgboost å°±æ˜¯å›å½’æ ‘çš„é›†æˆ https://www.csuldw.com/2019/07/20/2019-07-20-xgboost-theory/ https://blog.csdn.net/github_38414650/article/details/76061893?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare https://blog.csdn.net/qq_24519677/article/details/81809157 æœ‰ç©ºå†æ¨å¯¼äº† è°ƒç”¨åº“ Python æä¾›äº†ä¸¤ç§åº“ xgboost xgboost sklearnæ¥å£ æ­å»ºæ¨¡å‹ å‚æ•°è®¾ç½® GridSearchCV è°ƒå‚(ç½‘æ ¼æ³•) è°ƒå‚æ­¥éª¤ï¼Œå‚æ•°èŒƒå›´ https://blog.csdn.net/han_xiaoyang/article/details/52665396 12345678import xgboost as xgbfrom xgboost import XGBRegressorfrom sklearn.metrics import mean_absolute_error,make_scorerfrom sklearn.grid_search import GridSearchCVfrom sklearn.cross_validation import KFold, train_test_splitfrom sklearn.datasets import load_boston https://blog.csdn.net/s09094031/article/details/94871596?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare 1sklearn.model_selection.``train_test_split test_size train_sizeï¼š â€‹ ä¸‰ç§ç±»å‹ã€‚floatï¼Œintï¼ŒNoneã€‚ floatï¼š0.0-1.0ä¹‹é—´ï¼Œä»£è¡¨è®­ç»ƒæ•°æ®é›†å æ€»æ•°æ®é›†çš„æ¯”ä¾‹ã€‚ intï¼šä»£è¡¨è®­ç»ƒæ•°æ®é›†å…·ä½“çš„æ ·æœ¬æ•°é‡ã€‚ Noneï¼šè®¾ç½®ä¸ºtest_sizeçš„è¡¥ã€‚ defaultï¼šé»˜è®¤ä¸ºNoneã€‚ random_stateï¼šä¸‰ç§ç±»å‹ã€‚intï¼Œrandomstate instanceï¼ŒNoneã€‚ intï¼šæ˜¯éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ã€‚æ¯æ¬¡åˆ†é…çš„æ•°æ®ç›¸åŒã€‚ randomstateï¼šrandom_stateæ˜¯éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ã€‚ï¼ˆè¿™é‡Œæ²¡å¤ªç†è§£ï¼‰ Noneï¼šéšæœºæ•°ç”Ÿæˆå™¨æ˜¯ä½¿ç”¨äº†np.randomçš„randomstateã€‚ ç§å­ç›¸åŒï¼Œäº§ç”Ÿçš„éšæœºæ•°å°±ç›¸åŒã€‚ç§å­ä¸åŒï¼Œå³ä½¿æ˜¯ä¸åŒçš„å®ä¾‹ï¼Œäº§ç”Ÿçš„ç§å­ä¹Ÿä¸ç›¸åŒã€‚ shuffleï¼šå¸ƒå°”å€¼ï¼Œå¯é€‰å‚æ•°ã€‚é»˜è®¤æ˜¯Noneã€‚åœ¨åˆ’åˆ†æ•°æ®ä¹‹å‰å…ˆæ‰“ä¹±æ•°æ®ã€‚å¦‚æœshuffle=FALSEï¼Œåˆ™stratifyå¿…é¡»æ˜¯Noneã€‚ stratifyï¼šarray-likeæˆ–è€…Noneï¼Œé»˜è®¤æ˜¯Noneã€‚å¦‚æœä¸æ˜¯Noneï¼Œå°†ä¼šåˆ©ç”¨æ•°æ®çš„æ ‡ç­¾å°†æ•°æ®åˆ†å±‚åˆ’åˆ†ã€‚ è‹¥ä¸ºNoneæ—¶ï¼Œåˆ’åˆ†å‡ºæ¥çš„æµ‹è¯•é›†æˆ–è®­ç»ƒé›†ä¸­ï¼Œå…¶ç±»æ ‡ç­¾çš„æ¯”ä¾‹ä¹Ÿæ˜¯éšæœºçš„ã€‚ è‹¥ä¸ä¸ºNoneæ—¶ï¼Œåˆ’åˆ†å‡ºæ¥çš„æµ‹è¯•é›†æˆ–è®­ç»ƒé›†ä¸­ï¼Œå…¶ç±»æ ‡ç­¾çš„æ¯”ä¾‹åŒè¾“å…¥çš„æ•°ç»„ä¸­ç±»æ ‡ç­¾çš„æ¯”ä¾‹ç›¸åŒï¼Œå¯ä»¥ç”¨äºå¤„ç†ä¸å‡è¡¡çš„æ•°æ®é›†ã€‚ x_train, y_train, x_test, y_test = train_test_split(x, y, test_size=0.23, random_state=2) https://blog.csdn.net/qq_43288098/article/details/105407204?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare å‚æ•°ï¼šåˆ†å¼€è°ƒ https://blog.csdn.net/zc02051126/article/details/46711047 https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn Â¶æ¨¡å‹ä¿å­˜ https://www.fatrabbids.com/2018/10/19/xgboost%e7%9a%84%e4%bf%9d%e5%ad%98%e6%a8%a1%e5%9e%8b%e3%80%81%e5%8a%a0%e8%bd%bd%e6%a8%a1%e5%9e%8b%e3%80%81%e7%bb%a7%e7%bb%ad%e8%ae%ad%e7%bb%83/#more-235 XGBoostçš„ç‰¹æ€§é‡è¦æ€§å’Œç‰¹æ€§é€‰æ‹© æ¨¡å‹å¤æ‚åº¦ ç‰¹å¾æ•°é‡è¡¡é‡ï¼šç‰¹å¾é‡è¦æ€§é˜™å€¼çš„å¢åŠ ï¼Œé€‰æ‹©ç‰¹å¾æ•°é‡å‡å°‘ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡ä¼šä¸‹é™ã€‚å½“ç„¶ï¼Œç‰¹å¾æ•°é‡çš„å‡å°‘åè€Œä¼šæ˜¯å‡†ç¡®ç‡å‡é«˜ï¼Œå› ä¸ºè¿™äº›è¢«å‰”é™¤ç‰¹å¾æ˜¯å™ªå£°ã€‚]]></content>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[English-Daily]]></title>
    <url>%2F2020%2F06%2F23%2FEnglish-Daily%2F</url>
    <content type="text"><![CDATA[2020-7-6 coincide with v. ä¸â€¦ç›¸ç¬¦ stalk v. æ½œè¿‘ï¼ˆçŒç‰©æˆ–äººï¼‰ï¼›ï¼ˆéæ³•ï¼‰è·Ÿè¸ªï¼›æ€’å†²å†²åœ°èµ°ï¼›è¶¾é«˜æ°”æ‰¬åœ°èµ° n. ç§†ï¼›æŸ„ï¼›ï¼ˆå¶ï¼‰æŸ„ï¼›ï¼ˆèŠ±ï¼‰æ¢— verge Bella was on the verge of tears when she heard the news. å¬åˆ°è¿™ä¸ªæ¶ˆæ¯æ—¶ï¼Œè´æ‹‰å·®ç‚¹å°±è¦å“­äº†ã€‚ resistant adj. æŠµåˆ¶çš„ï¼ŒåæŠ—çš„ï¼ŒæŠ—æ‹’çš„ï¼›æœ‰æŠµæŠ—åŠ›çš„ï¼›æŠµæŠ—â€¦çš„ï¼›ä¸å—â€¦â€¦æŸå®³çš„ People are usually resistant to change. äººä»¬é€šå¸¸æŠ—æ‹’æ”¹å˜ã€‚ liar The tall guy was a notorious liar. é‚£ä¸ªé«˜ä¸ªå­æ˜¯ä¸ªè‡­åæ˜­è‘—çš„éª—å­ã€‚ politics n. æ”¿æ²»ï¼›æ”¿æ²»äº‹ç‰©ï¼ˆæ´»åŠ¨ï¼‰ï¼›æ”¿è§ï¼›æƒæœ¯ oblige (ä»¥æ³•å¾‹ã€ä¹‰åŠ¡ç­‰)å¼ºè¿«, è¿«ä½¿; å¸®å¿™, æ•ˆåŠ³; [å¸¸ç”¨è¢«åŠ¨]ä½¿æ„Ÿæ¿€; ä½¿(è¡Œä¸ºç­‰)æˆä¸ºå¿…è¦ phrase. (feel obliged to do sth.)è§‰å¾—æœ‰ä¹‰åŠ¡åšï¼›ä¸å¾—ä¸åš I felt obliged to leave after such an unpleasant quarrel. å‘ç”Ÿäº†è¿™æ ·ä¸æ„‰å¿«çš„äº‰åµä¹‹åï¼Œæˆ‘è§‰å¾—æœ‰å¿…è¦ç¦»å¼€ã€‚ 2020-7-1 jelly n. æœå†»ï¼›è‚‰å†»ï¼›æœé…±ï¼›èƒ¶çŠ¶ç‰©ï¼Œèƒ¶å‡ç‰©ï¼›è½»ä¾¿å¡‘æ–™é‹ oval adj. æ¤­åœ†å½¢çš„ï¼›åµå½¢çš„ n. æ¤­åœ†å½¢ï¼›åµå½¢ rigorous /'rÉªÉ¡É™rÉ™s/ adj. è°¨æ…çš„ï¼Œç»†è‡´çš„ï¼›ä¸¥æ ¼çš„ï¼Œä¸¥å‰çš„ He makes a rigorous study of the plants in the area. ä»–å¯¹è¯¥åœ°çš„æ¤ç‰©è¿›è¡Œäº†ç¼œå¯†çš„ç ”ç©¶ã€‚ ultimately UK/'ÊŒltÉªmÉ™tli/ adv. æœ€ç»ˆ, æœ€å, å½’æ ¹ç»“åº•, ç»ˆç©¶ Everything will ultimately depend on what is said at the meeting. ä¸€åˆ‡å°†æœ€ç»ˆå–å†³äºä¼šè®®çš„å†…å®¹ã€‚ sturdy UK/'stÉœËdi/ adj. ç»“å®çš„ï¼Œåšå›ºçš„ï¼›å¼ºå£®çš„ï¼›å¥å£®çš„ï¼›åšå†³çš„ï¼Œé¡½å¼ºçš„ broaden UK/'brÉ”Ëdn/ You should broaden your experience by travelling more. ä½ åº”è¯¥å¤šåˆ°å„åœ°èµ°èµ°ä»¥å¢å¹¿è§è¯†. broaden the horizon å¼€æ‹“è§†é‡ propel UK/prÉ™â€™pel/ v. æ¨è¿›ï¼Œæ¨åŠ¨ï¼›é©±ä½¿ï¼›è¿«ä½¿ voyage UK/'vÉ”ÉªÉªdÊ’/ n. èˆªè¡Œ, ï¼ˆå°¤æŒ‡ï¼‰èˆªæµ· v. èˆªè¡Œ, è¿œè¡Œ, ï¼ˆå°¤æŒ‡ï¼‰è¿œèˆª ä¾‹å¥ The voyage from England to India used to take 3 weeks. ä»è‹±æ ¼å…°åˆ°å°åº¦çš„èˆªè¡Œæ›¾ç»éœ€è¦ä¸‰å‘¨ã€‚ 2020-6-28 moist UK/mÉ”Éªst/ adj. å¾®æ¹¿çš„, æ¹¿æ¶¦çš„ insult UK/Éªnâ€™sÊŒlt/v. ä¾®è¾±ï¼Œè¾±éª‚ n. ä¾®è¾±ï¼Œè¾±éª‚ spontaneous UK/spÉ’nâ€™teÉªniÉ™s/ They greeted him with spontaneous applause. ä»–ä»¬è‡ªå‘åœ°é¼“èµ·æŒæ¥æ¬¢è¿ä»–ã€‚ slender UK/'slendÉ™Â®/ perimeter UK/pÉ™â€™rÉªmÉªtÉ™Â®/ n. å‘¨é•¿ï¼›å¤–ç¼˜ï¼Œè¾¹ç¼˜ blouse UK/blaÊŠz/ He pointed out a woman passing by who was wearing a skirt and blouse. ä»–æŒ‡å‡ºäº†ä¸€ä¸ªç©¿ç€è£™å­å’Œè¡¬è¡«çš„è¿‡è·¯å¥³å­ã€‚ perfume UK/'pÉœËfjuËm/ n. é¦™æ°´, é¦™æ–™, èŠ³é¦™ v. ä½¿â€¦å‘å‡ºé¦™æ°”, æ´’é¦™æ°´ 2020-6-27 2020-6-26 Functional foods are food products that have a potentially positive effect on health beyond basic nutritional benefits. Functional foods aim to solve not only all the needs that regular foods provide, but also to address functional needs, which can range from maintaining and improving physical or mental health to adjusting energy levels and moods. Food has been historically used as preventive medicine in many cultures around the world, but the recent rise of functional foods can be directly linked to the rise of the wellness economy, which, in turn, is largely driven by influencer marketing and social media use. 2020-6-25 IT IS A truth universally acknowledged that inequalityï¼ˆä¸å¹³ç­‰ï¼‰in the rich worldï¼ˆå‘è¾¾å›½å®¶ï¼‰is high and rising. Or, at least, it used to be. A growing band of economists are challenging the receivedï¼ˆè¢«å…¬è®¤çš„ï¼‰wisdom, pointing out that trends in the distributionï¼ˆåˆ†å¸ƒï¼Œåˆ†é…ï¼‰of income and wealth may not be as bad as is often thought. ä¼—æ‰€å‘¨çŸ¥ï¼Œå¯Œè£•å›½å®¶çš„ä¸å¹³ç­‰ç°è±¡éå¸¸ä¸¥é‡ï¼Œè€Œä¸”è¿˜åœ¨åŠ å‰§ã€‚æˆ–è€…è¯´ï¼Œè‡³å°‘æ›¾ç»æ˜¯è¿™æ ·çš„ã€‚è¶Šæ¥è¶Šå¤šçš„ç»æµå­¦å®¶å¼€å§‹è´¨ç–‘æ—¢æœ‰çš„è§‚ç‚¹ï¼Œä»–ä»¬æŒ‡å‡ºæ”¶å…¥å’Œè´¢å¯Œçš„åˆ†å¸ƒè¶‹åŠ¿å¯èƒ½ä¸æ˜¯åƒé€šå¸¸è¢«è®¤ä¸ºçš„é‚£ä¹ˆç³Ÿç³•ã€‚ Â¶2020-6-24 imaginary adj. æƒ³è±¡ä¸­çš„, å¹»æƒ³çš„, è™šæ„çš„ carriage n. è¿è¾“ï¼›è¿è´¹ï¼Œï¼ˆæ—§æ—¶ï¼‰é©¬è½¦ï¼›ç«è½¦è½¦å¢ï¼›ä»ªæ€ï¼Œå§¿æ€ï¼Œä¸¾æ­¢ message messenger n. ä¿¡ä½¿, é€ä¿¡äºº, é€šä¿¡å‘˜, é‚®é€’å‘˜ pavement n. äººè¡Œé“ postpone v. å»¶æœŸ, å»¶è¿Ÿ, æš‚ç¼“ Weâ€™ll have to postpone the meeting until next week. æˆ‘ä»¬å°†ä¸å¾—ä¸æŠŠä¼šè®®æ¨è¿Ÿåˆ°ä¸‹å‘¨ä¸¾è¡Œã€‚ velocity n. é€Ÿåº¦ï¼Œé€Ÿç‡ï¼›é«˜é€Ÿ reconcile v. ä½¿å’Œè°ä¸€è‡´ï¼Œè°ƒå’Œï¼›ä½¿å’Œè§£ï¼›å°†å°±ï¼Œå¦¥å Itâ€™s difficult to reconcile these two different points of view. å¾ˆéš¾å…¼é¡¾è¿™ä¸¤ç§ä¸åŒçš„è§‚ç‚¹ã€‚ 2020-6-23 ï¿¼The success of the brand wasnâ€™t built through big marketing campaigns, but through a savvy digital marketing strategy that increased brand awareness and generated high engagement, traffic, and conversions. è¯¥å“ç‰Œçš„æˆåŠŸå¹¶ä¸å»ºç«‹äºå¤§å‹è¥é”€æ´»åŠ¨ï¼Œè€Œæ˜¯å»ºç«‹äºç²¾å‡†çš„æ•°å­—è¥é”€ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æé«˜äº†å“ç‰Œçš„çŸ¥ååº¦ï¼Œè·å¾—äº†å¾ˆé«˜çš„å‚ä¸åº¦ã€æµé‡å’Œè½¬åŒ–ç‡ã€‚ traffic: ä¿¡æ¯æµé‡ï¼Œé€šä¿¡é‡ With only 40 physical stores, which are mostly used to drive consumers to e-commerce portals, Perfect Diary maintains momentum primarily through its digital footprint. Currently, it has a powerful presence on Little Red Book, Bilibili, Weibo, WeChat, Tmall, and Douyin. Thereafter she wrote articles for papers and magazines for a living. æ­¤åå¥¹ç»™æŠ¥çº¸å’Œæ‚å¿—æ’°ç¨¿è°‹ç”Ÿã€‚ adv. æ­¤å, ä¹‹å, ä»¥å spur n. åˆºæ¿€, æ¿€åŠ±, é­ç­–; è¸¢é©¬åˆº, é´åˆº; éª¨åˆº; å±±å˜´, å°–å¡ v. åˆºæ¿€, æ¿€åŠ±, ä¿ƒè¿›, é­ç­– stick adj. é»ï¼ˆæ€§ï¼‰çš„, ä¸€é¢å¸¦é»èƒ¶çš„, é—·çƒ­çš„, æ„Ÿåˆ°çƒ­å¾—éš¾å—çš„ n. å‘Šäº‹è´´ I have to take a shower before going out because the sweat had made my skin sticky. å‡ºé—¨å‰æˆ‘å¾—å†²ä¸ªæ¾¡ï¼Œå› ä¸ºæ±—æ°´è®©æˆ‘çš„çš®è‚¤é»ä¹ä¹çš„ devotion n. å…³çˆ±ï¼Œå…³ç…§ï¼›å¥‰çŒ®ï¼›å¿ è¯šï¼›å®—æ•™ç¤¼æ‹œ The career needs our devotion for all our lives. è¿™é¡¹äº‹ä¸šéœ€è¦æˆ‘ä»¬æ¯•ç”Ÿçš„å¥‰çŒ®ã€‚ reckless adj. é²è½çš„ï¼›ä¸è®¡åæœçš„ï¼›æ— æ‰€é¡¾å¿Œçš„ wag v. æ‘‡åŠ¨ï¼›æ‘†ï¼ˆå°¾å·´ï¼‰ï¼Œï¼ˆå°¾å·´ï¼‰æ‘‡ï¼Œæ‘†åŠ¨ n. æ‘‡æ‘†ï¼Œæ‘†åŠ¨ï¼›è€å¼€ç©ç¬‘çš„äººï¼Œçˆ±é—¹ç€ç©çš„äºº keen adj. çƒ­è¡·çš„, çƒ­æƒ…çš„; æ¸´æœ›çš„; æ•æ·çš„; çµæ•çš„; é”‹åˆ©çš„; å¼ºçƒˆçš„ n. æ¸å“­; æŒ½æ­Œ v. (ä¸ºæ­»è€…)æ¸å“­ be keen on sthå¯¹ æ„Ÿå…´è¶£ be keen to do æ¸´æœ›åšæŸäº‹ offspring n. å­å¥³ï¼Œåä»£ï¼›å¹¼å´½ï¼›å¹¼è‹— receipt n. æ”¶æ®ï¼Œæ”¶å…¥]]></content>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[day]]></title>
    <url>%2F2020%2F06%2F22%2Ftime-series-01%2F</url>
    <content type="text"><![CDATA[æ—¶é—´åºåˆ—åŠå…¶åˆ†è§£ Â¶æ—¶é—´åºåˆ—åˆ†ç±» Â¶å¹³ç¨³åºåˆ—ï¼ˆstationary series) åºåˆ—ä¸­çš„å„è§‚å¯Ÿå€¼åŸºæœ¬ä¸Šåœ¨æŸä¸ªå›ºå®šçš„æ°´å¹³ä¸Šæ³¢åŠ¨ï¼Œåœ¨ä¸åŒæ—¶Fé—´æ®µæ³¢åŠ¨ç¨‹åº¦ä¸åŒï¼Œä½†ä¸å­˜åœ¨æŸç§è§„å¾‹ã€‚å¹³ç¨³æ€§æ—¶é—´åºåˆ—çš„å‡å€¼å’Œæ–¹å·®éƒ½æ˜¯å¸¸æ•°ã€‚ æ–¹æ³•ï¼ša) çœ‹åŸå›¾ã€‚æ˜¯å¦åœ¨æŸä¸ªå¸¸æ•°é™„è¿‘æ³¢åŠ¨ï¼Œä¸”æ³¢åŠ¨èŒƒå›´æœ‰ç•Œã€‚å¦‚æœæœ‰æ˜æ˜¾çš„è¶‹åŠ¿æ€§æˆ–è€…å‘¨æœŸæ€§ï¼Œåˆ™ä¸æ˜¯ã€‚b) ADFå•ä½æ ¹æ£€æµ‹ã€‚på€¼ã€‚ Â¶éå¹³ç¨³åºåˆ—ï¼ˆnon-stationary series) æ¶‰åŠè¶‹åŠ¿ã€å­£èŠ‚æ€§å’Œå‘¨æœŸä¸‰ç§ç‰¹æ€§ï¼ŒåŒ…å«å…¶ä¸­ä¸€ç§æˆ–è€…å¤šç§æˆåˆ†ã€‚ Â¶è¶‹åŠ¿(trend) æ—¶é—´åºåˆ—åœ¨é•¿æ—¶æœŸå†…å‘ˆç°å‡ºæ¥çš„æŸç§ä¸Šå‡æˆ–è€…ä¸‹é™çš„è¶‹åŠ¿ã€‚åˆ†ä¸ºçº¿æ€§å’Œéçº¿æ€§ã€‚ Â¶å­£èŠ‚æ€§ï¼ˆseasonality) æ˜¯æŒ‡æ—¶é—´åºåˆ—åœ¨ä¸€å¹´å†…é‡å¤å‡ºç°çš„å‘¨æœŸæ³¢åŠ¨ã€‚å› å­£èŠ‚ä¸åŒè€Œå‘ç”Ÿå˜åŒ–ï¼Œå¦‚æ—…æ¸¸æ—ºå­£ï¼Œæ—…æ¸¸æ·¡å­£ã€‚ Â¶å‘¨æœŸæ€§ï¼ˆcyclicityï¼‰ æ˜¯æŒ‡æ—¶é—´åºåˆ—å‘ˆç°å‡ºçš„é•¿æœŸè¶‹åŠ¿ã€‚å‘¨æœŸæ€§ä¸åŒäºè¶‹åŠ¿å˜åŠ¨ï¼Œå®ƒæ˜¯æ¶¨è½ç›¸é—´çš„äº¤æ›¿æ³¢åŠ¨ã€‚ä¸åŒæ„å­£èŠ‚å˜åŠ¨ï¼Œå®ƒæ— å›ºå®šè§„å¾‹ï¼Œå˜åŠ¨å‘¨æœŸå¤šåœ¨ä¸€å¹´ä»¥ä¸Šï¼Œä¸”å‘¨æœŸé•¿çŸ­ä¸ä¸€ã€‚å‘¨æœŸæ€§é€šå¸¸æ˜¯ç”±ç»æµç¯å¢ƒçš„å˜åŒ–å¼•èµ·çš„ã€‚ Â¶å¶ç„¶æ€§å› ç´  å…¶å¯¼è‡´æ—¶é—´åºåˆ—å‘ˆç°å‡ºæŸç§éšæœºæ³¢åŠ¨ã€‚ æ—¶é—´åºåˆ—çš„æˆåˆ†å¯åˆ†ä¸ºï¼šè¶‹åŠ¿ï¼ˆT),å­£èŠ‚æ€§ï¼ˆS),å‘¨æœŸæ€§ï¼ˆC),éšæœºæ€§ï¼ˆI)ã€‚ Â¶å¹³ç¨³æ—¶é—´åºåˆ—åˆ†æ Â¶ARæ¨¡å‹ è‡ªå›å½’æ¨¡å‹AR è‡ªå›å½’æ¨¡å‹æè¿°å½“å‰å€¼ä¸å†å²å€¼ä¹‹é—´çš„å…³ç³»ï¼Œç”¨å˜é‡è‡ªèº«çš„å†å²æ—¶é—´æ•°æ®å¯¹è‡ªèº«è¿›è¡Œé¢„æµ‹ã€‚è‡ªå›å½’æ¨¡å‹å¿…é¡»æ»¡è¶³å¹³ç¨³æ€§çš„è¦æ±‚ã€‚ ç§»åŠ¨å¹³å‡æ¨¡å‹MA ç§»åŠ¨å¹³å‡æ¨¡å‹å…³æ³¨çš„æ˜¯è‡ªå›å½’æ¨¡å‹ä¸­çš„è¯¯å·®é¡¹çš„ç´¯åŠ  è‡ªå›å½’ç§»åŠ¨å¹³å‡æ¨¡å‹ARMA è‡ªå›å½’æ¨¡å‹ARå’Œç§»åŠ¨å¹³å‡æ¨¡å‹MAæ¨¡å‹ç›¸ç»“åˆï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†è‡ªå›å½’ç§»åŠ¨å¹³å‡æ¨¡å‹ARMA(p,q) å·®åˆ†è‡ªå›å½’ç§»åŠ¨å¹³å‡æ¨¡å‹ARIMA å°†è‡ªå›å½’æ¨¡å‹ã€ç§»åŠ¨å¹³å‡æ¨¡å‹å’Œå·®åˆ†æ³•ç»“åˆï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†å·®åˆ†è‡ªå›å½’ç§»åŠ¨å¹³å‡æ¨¡å‹ARIMA(p,d,q) Â¶å‚æ•°ç¡®å®š æ‹–å°¾å’Œæˆªå°¾ æ‹–å°¾æŒ‡åºåˆ—ä»¥æŒ‡æ•°ç‡å•è°ƒé€’å‡æˆ–éœ‡è¡è¡°å‡ï¼Œè€Œæˆªå°¾æŒ‡åºåˆ—ä»æŸä¸ªæ—¶ç‚¹å˜å¾—éå¸¸å°ã€‚ Â¶ARIMAå»ºæ¨¡è¿‡ç¨‹ å°†åºåˆ—å¹³ç¨³ï¼ˆå·®åˆ†æ³•ç¡®å®šdï¼‰ på’Œqé˜¶æ•°ç¡®å®šï¼šACFä¸PACF ARIMAï¼ˆp,d,qï¼‰ æ¨¡å‹ ACF PACF ARï¼ˆpï¼‰ è¡°å‡è¶‹äºé›¶ï¼ˆå‡ ä½•å‹æˆ–æŒ¯è¡å‹ï¼‰ pé˜¶åæˆªå°¾ MAï¼ˆqï¼‰ qé˜¶åæˆªå°¾ è¡°å‡è¶‹äºé›¶ï¼ˆå‡ ä½•å‹æˆ–æŒ¯è¡å‹ï¼‰ ARMAï¼ˆp,qï¼‰ qé˜¶åè¡°å‡è¶‹äºé›¶ï¼ˆå‡ ä½•å‹æˆ–æŒ¯è¡å‹ï¼‰ pé˜¶åè¡°å‡è¶‹äºé›¶ï¼ˆå‡ ä½•å‹æˆ–æŒ¯è¡å‹ï¼‰ Â¶å‚æ•° p,q çš„è‡ªåŠ¨ç¡®å®šæ–¹å¼ Â¶ä¿¡æ¯å‡†åˆ™ åœ¨å‚æ•°ä¼°è®¡çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¼¼ç„¶å‡½æ•°ä½œä¸ºç›®æ ‡å‡½æ•°ã€‚å¯ä»¥é€šè¿‡åŠ å…¥æ¨¡å‹å¤æ‚åº¦çš„æƒ©ç½šé¡¹é¿å…è¿‡æ‹Ÿåˆé—®é¢˜ã€‚æ¯”å¦‚èµ¤æ± ä¿¡æ¯å‡†åˆ™ï¼ˆAIC)å’Œè´å¶æ–¯ä¿¡æ¯å‡†åˆ™(BIC) $$ AIC=2kâˆ’2ln(L) $$ ä¸€æ–¹é¢å¼•å…¥æƒ©ç½šé¡¹ï¼Œä½¿å¾—æ¨¡å‹å‚æ•°å°½å¿«å°‘ï¼Œå‡å°‘è¿‡æ‹Ÿåˆã€‚å¦ä¸€æ–¹é¢ï¼Œä¹Ÿå¸Œæœ›æé«˜æ¨¡å‹çš„æ‹Ÿåˆåº¦ï¼ˆæå¤§ä¼¼ç„¶ï¼‰ $$ BIC=kLn(n)âˆ’2ln(L) $$ kä¸ºæ¨¡å‹å‚æ•°ä¸ªæ•°ï¼Œnä¸ºæ ·æœ¬æ•°é‡ï¼ŒLä¸ºä¼¼ç„¶å‡½æ•°ã€‚å¼•å…¥$Kln(n)$æƒ©ç½šé¡¹åœ¨ç»´åº¦è¿‡å¤§ä¸”æ ·æœ¬æ•°æ®ç›¸å¯¹è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥æœ‰æ•ˆé¿å…å‡ºç°ç»´åº¦ç¾éš¾ã€‚ Â¶æ—¶é—´åºåˆ—çš„åˆ†è§£ åŠ æ³•æ¨¡å‹ $$ X_t = T_t + C_t+S_t + I_t ,t = 1,2,â€¦,n $$ æ¯ä¸ªæ—¶é—´åºåˆ—çœ‹æˆæ˜¯ä¸‰ä¸ªéƒ¨åˆ†çš„å åŠ ï¼Œåˆ†åˆ«æ˜¯è¶‹åŠ¿é¡¹ã€å¾ªç¯é¡¹ï¼Œå­£èŠ‚é¡¹ï¼Œéšæœºé¡¹ ä¹˜æ³•æ¨¡å‹ $$ X_t = T_tC_tS_t*I_t $$ Â¶è¶‹åŠ¿åˆ†æ è¶‹åŠ¿æ‹Ÿåˆæ³•å°±æ˜¯æŠŠæ—¶é—´ä½œä¸ºè‡ªå˜é‡ï¼Œç›¸åº”çš„åºåˆ—è§‚å¯Ÿå€¼ä½œä¸ºå› å˜é‡ï¼Œå»ºç«‹åºåˆ—å€¼éšæ—¶é—´å˜åŒ–çš„å›å½’æ¨¡å‹ã€‚å¯åˆ†ä¸ºçº¿æ€§æ‹Ÿåˆå’Œæ›²çº¿æ‹Ÿåˆã€‚ Â¶çº¿æ€§æ‹Ÿåˆ å¦‚æœé•¿æœŸè¶‹åŠ¿å‘ˆç°å‡ºçº¿æ€§ç‰¹å¾ï¼Œå¯ç”¨çº¿æ€§æ¨¡å‹æ‹Ÿåˆï¼Œ $$ \left{\begin{array}{c} x_t = a+bt+I_t\ E(I_t) = 0,Var(I_t) = \sigma^2 \end{array} \right. $$ å…¶ä¸­ï¼Œ$T_t = a+bt$å°±æ˜¯æ¶ˆé™¤éšæœºæ³¢åŠ¨å½±å“åçš„è¯¥åºåˆ—çš„é•¿æœŸè¶‹åŠ¿ã€‚ Â¶æ›²çº¿æ‹Ÿåˆ å¦‚æœé•¿æœŸè¶‹åŠ¿å‘ˆç°å‡ºçº¿æ€§ç‰¹å¾ï¼Œå¯ç”¨æ›²çº¿æ¨¡å‹æ¥æ‹Ÿåˆ $$ \left{ \begin{array}{c|c|c} äºŒæ¬¡å‹&amp; T_t = a+bt+ct^2&amp; å˜æ¢åï¼Œçº¿æ€§æœ€å°äºŒä¹˜æ³•\ æŒ‡æ•°å‹&amp;T_t = ab^t&amp; å¯¹æ•°å˜åŒ– &amp; æœ€å°äºŒä¹˜æ³•\ ä¿®æ­£æŒ‡æ•°å‹&amp;T_t = a+bc^t&amp; &amp;è¿­ä»£æ³•\ Gompertzå‹&amp; T_t = e{a+bct}&amp; &amp; è¿­ä»£æ³•\ Logistic &amp; T_t = \frac{1}{a+bc^t}&amp; è¿­ä»£æ³• \end{array} \right. $$ Â¶å¹³æ»‘æ³• Â¶ç§»åŠ¨å¹³å‡æ³• å‡è®¾åœ¨æ¯”è¾ƒçŸ­çš„æ—¶é—´é—´éš”é‡Œï¼Œåºåˆ—çš„å–å€¼æ˜¯è¾ƒç¨³å®šçš„ï¼Œè¿™ç§å·®å¼‚æ˜¯ç”±éšæœºæ³¢åŠ¨é€ æˆçš„ã€‚ç”±æ­¤ï¼Œå¯ç”¨ä¸€å®šæ—¶é—´é—´éš”å†…çš„å¹³å‡å€¼ä½œä¸ºæŸä¸€æœŸçš„ä¼°è®¡å€¼ã€‚ næœŸä¸­å¿ƒç§»åŠ¨å¹³å‡ $$ \widetilde{x_t} = \frac{1}{n}(\frac{1}{2}x_{t-\frac{n}{2}}+x_{t-\frac{n}{2}+1}+\dots+x_{t+\frac{n}{2}-1}+\frac{1}{2}x_{t+\frac{n}{2}}) $$ næœŸç§»åŠ¨å¹³å‡ $$ \widetilde{x_t} = \frac{1}{n}(x_t+x_{t-1}+\dots+x_{t-n+1}) $$ Â¶æŒ‡æ•°å¹³æ»‘æ³• ç®€å•æŒ‡æ•°å¹³æ»‘ $$ \widetilde{x_t} = \alpha x_t+\alpha (1-\alpha )x_{t-1}+\dots) $$ Â¶å­£èŠ‚æ•ˆåº” å­£èŠ‚æ€§æ•ˆåº”çš„å­˜åœ¨ï¼Œä½¿å¾—æ°”æ¸©ä¼šåœ¨ä¸åŒå¹´ä»½çš„ç›¸åŒæœˆä»½å‘ˆç°å‡ºç›¸ä¼¼çš„æ€§è´¨ã€‚ å¦‚æœåªæ˜¯å­˜åœ¨å­£èŠ‚æ€§å’Œéšæœºæ³¢åŠ¨æ€§ $$ x_{ij} = \hat{x}S_j+I_{ij} $$ å…¶ä¸­$S_j$è¡¨ç¤ºç¬¬jä¸ªæœˆçš„å­£èŠ‚æŒ‡æ•°ï¼Œ$\hat{x}$ä¸ºå„æœˆå¹³å‡æ°”æ¸©ã€‚ å­£èŠ‚æŒ‡æ•°çš„è®¡ç®—: Step1: è®¡ç®—å‘¨æœŸå†…å„æœŸçš„å¹³å‡æ•° $$ \hat{x}k = \frac{\sum{i= 1}^{n}x_{ik}}{n}ï¼ˆk = 1,2,â€¦,m) $$ å…¶ä¸­ï¼Œmè¡¨ç¤ºå‘¨æœŸï¼Œnè¡¨ç¤ºå‘¨æœŸçš„æ•°é‡ Step2: è®¡ç®—æ€»å¹³å‡æ•° $$ \hat{x} = \frac{\sum_{i = 1}^{n}\sum_{k = 1}^{m}x_{ik}}{nm} $$ Step3: è®¡ç®—å­£èŠ‚æŒ‡æ•° $$ S_k = \frac{\hat{x}_k}{\hat{x}} $$ Â¶æ··åˆæ•ˆåº” åŠ æ³•æ¨¡å‹ $$ x_t = T_t + S_t + I_t $$ ä¹˜æ³•æ¨¡å‹ $$ x_t = T_tS_tI_t $$ æ··åˆæ¨¡å‹ $$ x_t = S_tT_t+I_t\ x_t = S_t(T_t+I_t) $$ å¦‚æœå­£èŠ‚æ³¢åŠ¨çš„æŒ¯å¹…ä¸å—è¶‹åŠ¿å˜åŠ¨çš„å½±å“ï¼Œåˆ™è¯´æ˜å­£èŠ‚æ€§ä¸è¶‹åŠ¿ä¹‹é—´æ²¡æœ‰ç›¸äº’ä½œç”¨å…³ç³»ï¼Œå¯åŠ ã€‚å¦‚æœå­£èŠ‚æ³¢åŠ¨çš„æŒ¯å¹…éšè¶‹åŠ¿çš„å˜åŒ–è€Œå˜åŒ–ï¼Œæ˜¯ç›¸äº’ä½œç”¨çš„å…³ç³»ï¼Œå¯å°è¯•æ··åˆæ¨¡å‹å’Œä¹˜æ³•æ¨¡å‹ã€‚ Tool in Python: xfresh Â¶ç‰¹å¾æå– å®˜ç½‘ï¼š https://tsfresh.readthedocs.io/en/latest/text/quick_start.html ä¸­æ–‡ï¼š https://github.com/SimaShanhe/tsfresh-feature-translation Â¶Data Formats column_id: Features will be extracted individually for each entity(id); one row per id. column_sort: sorting the time series. ç‰¹å¾æå–: å¯ä»¥ä¸€æ¬¡æ€§æå–å®Œï¼›ä¹Ÿå¯ä»¥å•ç‹¬æå–kind_to_parameters è®¾ç½®å‚æ•°ï¼›è¿˜å¯ä»¥æå– å¯åˆ†å¸ƒå¼è®¡ç®— the rolling mechanism é¦–å…ˆç¡®å®šæ»‘åŠ¨çª—å£ Step1 : å®ç°å•å˜é‡ç‰¹å¾çš„æå– Step2 : å®ç°å¤šå˜é‡ç‰¹å¾çš„æå– Â¶Day Ox 01 çŸ¥è¯†æ¸…å•: ç‰¹å¾æå–ï¼šå¤§æ¦‚ä¸Šåƒç§ç‰¹å¾ï¼ˆå‡ åç§æ–¹æ³•ï¼‰ tsfresh.feature_extraction.extraction.extract_features(timeseries_container,default_fc_parameters=None**,** kind_to_fc_parameters=None**,** column_id=None**,** column_sort=None**,** column_kind=None**,** column_value=None**,** chunksize=None**,** n_jobs=1**,** show_warnings=False**,** disable_progressbar=False**,** impute_function=None**,** profile=False**,** profiling_filename=â€˜profile.txtâ€™, profiling_sorting=â€˜cumulativeâ€™, distributor=None**)** pandas.DataFrame containing the different time series column_id (str) â€“ The name of the id column to group by. column_sort (str) â€“ The name of the sort column. n_jobs (int) â€“ The number of processes to use for parallelization. æ—¶é—´åºåˆ—çš„æ»‘åŠ¨çª—å£ï¼ˆå•åºåˆ—åˆ’åˆ†æˆå¤šåºåˆ—ï¼‰ tsfresh.utilities.dataframe_functions.``roll_time_series(df_or_dict, column_id**,** column_sort=None**,** column_kind=None**,** rolling_direction=1**,** max_timeshift=None**,** min_timeshift=0**,** chunksize=None**,** n_jobs=1**,** show_warnings=False**,** disable_progressbar=False**,** distributor=None**)** max_timeshift (int) â€“ If not None, the cut-out window is at maximum max_timeshift large. If none, it grows infinitely. min_timeshift (int) â€“ Throw away all extracted forecast windows smaller or equal than this. Must be larger than or equal 0. n_jobs (int) â€“ The number of processes to use for parallelization. If zero, no parallelization is used. show_warnings=False ï¼ˆæŒ‡å®šï¼‰ç‰¹å¾æå– æ˜¾è‘—æ€§æ£€æµ‹ https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_selection.html?highlight=select_features#tsfresh.feature_selection.selection.select_features ç›¸å…³æ€§æ£€æµ‹ https://tsfresh.readthedocs.io/en/latest/text/parallelization.html#parallelization-of-feature-selection 123456789101112131415161718192021222324252627282930313233from tsfresh import extract_features, select_features,extract_relevant_featuresfrom tsfresh.utilities.dataframe_functions import imputefrom tsfresh.utilities.dataframe_functions import roll_time_series, make_forecasting_frameimport pandas as pdimport tsfresh as tsf fc_parameters_value1 = &#123;"length": None, "sum_values": None&#125;fc_parameters_value2 = &#123;"maximum": None, "minimum": None&#125;kind_to_fc_parameters = &#123; "value1": fc_parameters_value1, "value2": fc_parameters_value2&#125;if __name__ == '__main__': # ceate data rawdata = &#123;'id1': [0,0,0,0,0,1,1,1,1,1],'time': [1,2,3,4,5,10,11,12,13,14],\ 'value1': [1,2,3,4,5,6,7,8,9,10], 'value2': [1,2,3,4,5,6,7,8,9,10] &#125; df = pd.DataFrame(rawdata)# è®¾ç½®é•¿åº¦+1 = çœŸå®é•¿åº¦,æ˜¯å½“å‰ç¼–å·å¾€ä¸Šæ•°. df_rolled = roll_time_series(df, column_id="id1", column_sort="time", max_timeshift=1, min_timeshift=0)# roll_time_seriesçš„è¿”å›å€¼ print(df_rolled) df_rolled = df_rolled.drop('id1',axis = 1)# column_id: èšåˆåˆ— column_sort:æ’åºï¼Œä¸€ä¸ªcolumn_idå°±å¯¹åº”ä¸€ä¸ªç‰¹å¾ extracted_features = extract_features(df_rolled, column_id='id', column_sort='time', kind_to_fc_parameters = kind_to_fc_parameters, show_warnings=False) print(extracted_features) Â¶Day Ox 02 æŸ¥çœ‹æå–ç‰¹å¾ å¯æ ¹æ®æ­¤æå–è‡ªåŠ¨æå–çš„ç‰¹å¾ï¼Œç”¨äºé¢„æµ‹æ—¶å€™çš„æå–ç‰¹å¾ 1kind_to_fc_parameters = tsf.feature_extraction.settings.from_columns(extracted_features) 1234# 5. ç‰¹å¾æŠ½å–ä¸è¿‡æ»¤åŒæ—¶è¿›è¡Œï¼ˆä¸€æ­¥åˆ°ä½ï¼Œçœå»å¤šä½™è®¡ç®—ï¼‰# column_id: group by #features_filtered_direct = extract_relevant_features(timeseries, y, column_id='id', column_sort='time')#print(features_filtered_direct.head()) å­¦ä¹ è·¯å¾„ï¼š 1. æ•°æ®æ ¼å¼ 2. æ»‘åŠ¨çª—å£è®¾ç½® 3. ç‰¹å¾æå– 4. ç‰¹å¾é€‰æ‹© ä¸“é¢˜ æ—¶é—´åºåˆ—çš„ç«èµ›æ–¹æ¡ˆ https://mp.weixin.qq.com/s?__biz=MzU1Nzc1NjI0Nw==&amp;mid=2247485604&amp;idx=1&amp;sn=6283ec080344665bfad90570bf1504a4&amp;chksm=fc31b29ccb463b8acac7acf4d89494aaad0c76620becb2b07c370ccbfaff850edc3c1ad4e0fd&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1593390448780&amp;sharer_shareid=fb5716a8ad12ea6329433df53d4cbf64#rd https://www.zhihu.com/question/21229371/answer/533770345 Prophet å·¥å…·]]></content>
  </entry>
  <entry>
    <title><![CDATA[å›å½’åˆ†æ]]></title>
    <url>%2F2020%2F06%2F20%2F%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[[TOC] å›å½’åˆ†æ æœ€ç®€å•çš„çº¿æ€§å›å½’ï¼Œé¿å…å¤šé‡å…±çº¿æ€§ï¼Œè¿‡æ‹Ÿåˆï¼Œå¼•å…¥æ­£åˆ™é¡¹çš„çº¿æ€§å›å½’æ¨¡å‹ã€‚æ¶‰åŠåˆ°çš„æ•°å­¦çŸ¥è¯†ï¼šä¸€èŒƒæ•°ï¼ŒäºŒèŒƒæ•°ï¼Œå¤šå…ƒå‡½æ•°æ±‚æå€¼ã€‚æ¨¡å‹çš„å«ä¹‰ï¼Œå‚æ•°æ±‚è§£ç®—æ³•ï¼Œç›®æ ‡å‡½æ•°ï¼Œä»¥åŠå„ç§æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚ Â¶å®šä¹‰ å›å½’åˆ†ææ˜¯å¯»æ‰¾è‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´çš„æ•°é‡å…³ç³»ï¼Œç”¨äºé¢„æµ‹å»ºæ¨¡çš„æ–¹æ³•ã€‚å…¶ä¸€ï¼Œå®ƒå¯ä»¥æ­ç¤ºè‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´çš„æ˜¾è‘—æ€§æ£€æµ‹ã€‚å…¶äºŒï¼Œæ­ç¤ºå¤šä¸ªè‡ªå˜é‡å¯¹ä¸€ä¸ªå› å˜é‡çš„å½±å“ç¨‹åº¦å¤§å°ã€‚ Â¶å›å½’ç±»å‹ 1ï¼‰ç‹¬ç«‹å˜é‡çš„æ•°é‡ 2ï¼‰åº¦é‡å˜é‡çš„ç±»å‹ 3ï¼‰å›å½’çº¿çš„å½¢çŠ¶ Â¶1. çº¿æ€§å›å½’ï¼ˆLinear Regression) å› å˜é‡ï¼šè¿ç»­ï¼› è‡ªå˜é‡ï¼šè¿ç»­æˆ–è€…ç¦»æ•£ Â¶æ¨¡å‹çš„å½¢å¼ $$ Y = a+bX+ğœ€\ \left(\begin{array}{c} y_{1} \ y_{2} \ \vdots \ y_{n} \end{array}\right)=\left(\begin{array}{cccc} 1 &amp; x_{11} &amp; \cdots &amp; x_{1(p-1)} \ 1 &amp; x_{21} &amp; \cdots &amp; x_{2(p-1)} \ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \ 1 &amp; x_{n 1} &amp; \cdots &amp; x_{n(p-1)} \end{array}\right) \beta+\left(\begin{array}{c} e_{1} \ e_{2} \ \vdots \ e_{n} \end{array}\right)\ Y_{n1} = X_{np}\beta+ğœ€ $$ where $a$ and $b$ are the regression coefficients, and ğœ€ is the random error. Â¶ç›®æ ‡å‡½æ•° $$ min SSR = \sum_{i}(y_i-f(x_i))^2\ min_{w}||Xw-y||_2^2 $$ Â¶å‚æ•°ä¼°è®¡ æœ€å°äºŒä¹˜æ³•ï¼ˆLease Square Method)ï¼ˆOLS) This approach is called the method of ordinary least squares. Â¶æ¨¡å‹è¯„ä¼° Â¶æ‹Ÿåˆä¼˜åº¦ R-square , coefficient of determination Larger $R^2$ indicates a better fit and means that the model can better explain the variation of the output with different inputs. https://realpython.com/linear-regression-in-python/ Â¶è¦æ±‚ è‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´å¿…é¡»æ»¡è¶³çº¿æ€§å…³ç³»ã€‚ å¤šå…ƒå›å½’å­˜åœ¨å¤šé‡å…±çº¿æ€§ï¼Œè‡ªç›¸å…³æ€§å’Œå¼‚æ–¹å·®æ€§ã€‚ çº¿æ€§å›å½’å¯¹å¼‚å¸¸å€¼éå¸¸æ•æ„Ÿã€‚å¼‚å¸¸å€¼ä¼šä¸¥é‡å½±å“å›å½’çº¿å’Œæœ€ç»ˆçš„é¢„æµ‹å€¼ã€‚ å¤šé‡å…±çº¿æ€§ä¼šå¢åŠ ç³»æ•°ä¼°è®¡çš„æ–¹å·®ï¼Œå¹¶ä¸”ä½¿å¾—ä¼°è®¡å¯¹æ¨¡å‹ä¸­çš„å¾®å°å˜åŒ–éå¸¸æ•æ„Ÿã€‚ç»“æœæ˜¯ç³»æ•°ä¼°è®¡ä¸ç¨³å®šã€‚ åœ¨å¤šä¸ªè‡ªå˜é‡çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨æ­£å‘é€‰æ‹©ã€å‘åæ¶ˆé™¤å’Œé€æ­¥é€‰æ‹©çš„æ–¹æ³•æ¥é€‰æ‹©æœ€é‡è¦çš„è‡ªå˜é‡ã€‚ Â¶é€»è¾‘å›å½’ï¼ˆLogistic Regression) Logistic å›å½’çš„æœ¬è´¨æ˜¯ï¼šå‡è®¾æ•°æ®æœä»è¿™ä¸ªåˆ†å¸ƒï¼Œç„¶åä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡åšå‚æ•°çš„ä¼°è®¡ã€‚ Â¶Logistic åˆ†å¸ƒ $$ F(x) = P(X&lt;=x) = \frac{1}{1+e^{-(x-u)/\gamma}} $$ $$ f(x) = Fâ€™(X&lt;=x) = \frac{e{-(x-u)/\gamma}}{\gamma(1+e{-1(x-u)/\gamma})^2} $$ where $u$ è¡¨ç¤ºä½ç½®å‚æ•°ï¼Œ$\gamma$æ˜¯å½¢çŠ¶å‚æ•° Â¶æ¨¡å‹å½¢å¼ $$ y = \frac{1}{1+e{-(wTx+b)}} $$ $$ P(Y=1|x) = \frac{1}{1+e{-(wTx+b)}} $$ Â¶æŸå¤±å‡½æ•° $$ P(Y=1|x)=p(x)\ p(Y=0|x) = 1-p(x) $$ ä¼¼ç„¶å‡½æ•° $$ L(w) $$ Â¶å¤šé¡¹å¼å›å½’ï¼ˆPolynomial Regressionï¼‰ Â¶é€æ­¥å›å½’ï¼ˆStepwise Regrssion) Â¶å²­å›å½’ï¼ˆRidge Regression) L2æ­£åˆ™åŒ–(The ridge coefficients minimize a penalized residual sum of squares) æƒ©ç½šå‡½æ•° Â¶æŸå¤±å‡½æ•° $$ argmin_{w}||y-X\beta||_22+\lambda||\beta||_22 $$ å²­å›å½’åˆ†ææ˜¯ä¸€ç§ç”¨äºå­˜åœ¨å¤šé‡å…±çº¿æ€§ï¼ˆè‡ªå˜é‡é«˜åº¦ç›¸å…³ï¼‰æ•°æ®çš„æŠ€æœ¯ã€‚åœ¨å¤šé‡å…±çº¿æ€§æƒ…å†µä¸‹ï¼Œå°½ç®¡æœ€å°äºŒä¹˜æ³•ï¼ˆOLSï¼‰å¯¹æ¯ä¸ªå˜é‡å¾ˆå…¬å¹³ï¼Œä½†å®ƒä»¬çš„å·®å¼‚å¾ˆå¤§ï¼Œä½¿å¾—è§‚æµ‹å€¼åç§»å¹¶è¿œç¦»çœŸå®å€¼ã€‚å²­å›å½’é€šè¿‡ç»™å›å½’ä¼°è®¡ä¸Šå¢åŠ ä¸€ä¸ªåå·®åº¦ï¼Œæ¥é™ä½æ ‡å‡†è¯¯å·®ã€‚ Â¶å¥—ç´¢å›å½’ï¼ˆ Lasso Regressionï¼‰ Â¶L1æ­£åˆ™åŒ– æŸå¤±å‡½æ•° $$ argmin_{w}||y-X\beta||_2^2+\lambda||\beta||_1 $$ The larger the value of $\lambda$ , the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. Â¶å¼¹æ€§å›å½’ ElasticNet Regression Â¶æŸå¤±å‡½æ•° $$ argmin_{w}||y-X\beta||_22+\lambda_1||\beta||_22+\lambda_2||\beta||_1 $$ Â¶è´å¶æ–¯å›å½’ é¢‘ç‡æ´¾ï¼ˆä¼˜åŒ–é—®é¢˜ï¼‰ è´å¶æ–¯æ´¾ åœ¨æå¤§ä¼¼ç„¶ä¼°è®¡çº¿æ€§å›å½’ä¸­æˆ‘ä»¬æŠŠå‚æ•°çœ‹æˆæ˜¯ä¸€ä¸ªæœªçŸ¥çš„å›ºå®šå€¼ï¼Œè€Œè´å¶æ–¯å­¦æ´¾åˆ™æŠŠçœ‹æˆæ˜¯ä¸€ä¸ªéšæœºå˜é‡ã€‚ Model $$ f(x) = w^Tx = x^Tw $$ Bayesian Method Inference and Prediction Â¶reference https://courses.analyticsvidhya.com/courses/Fundamentals-of-Regression-Analysis?utm_source=blog&amp;utm_medium=introduction_to_regression https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/ Linear Regression by Sklearn Â¶OLS 1234567891011121314151617181920212223242526272829303132333435363738print(__doc__)import matplotlib.pyplot as pltimport numpy as np from sklearn import datasets, linear_modelfrom sklearn.metrics import mean_squared_error, r2_score # Load the diabetes datasetdiabetes_X, diabetes_Y = datasets.load_diabetes(return_X_y= True)# select one featurediabetes_X = diabetes_X[:, np.newaxis, 2]# Split the data set into training/testing setsX_train = diabetes_X[:-20]X_test = diabetes_X[-20:]Y_train = diabetes_Y[:-20]Y_test = diabetes_Y[-20:]# Create linear regression objectregr = linear_model.LinearRegression()# Train regression modelregr.fit(X_train, Y_train)# Predict Y_pred = regr.predict(X_test)# Evaluateprint('Mean squared error: %.2f'% mean_squared_error(Y_test, Y_pred))print('R2_score:%.2f'% r2_score(Y_test,Y_pred))# Plotplt.scatter(X_test,Y_test, color = 'black')plt.plot(X_test,Y_pred, color = 'blue',linewidth = 3)plt.show() Â¶Ridge &amp; Lasso 12345678910from sklearn import linear_modelreg = line_model.Ridge(alpha = .2)reg.fit([[0,0],[1,2],[3,8]],[0,1,2])reg.coef_reg.intercept_reg1 = line_model.RidgeCV(alphas = np.logspace(-6,6,13))reg1.fit([[0,0],[1,2],[3,8]],[0,1,2])reg.alpha_ 12345678910from sklearn import linear_modelreg = line_model.Lasso(alpha = .2)reg.fit([[0,0],[1,2],[3,8]],[0,1,2])reg.coef_reg.intercept_reg1 = line_model.LassoCV(alphas = np.logspace(-6,6,13))reg1.fit([[0,0],[1,2],[3,8]],[0,1,2])reg.alpha_ Â¶æ­£åˆ™ç³»æ•°é€‰æ‹© äº¤å‰éªŒè¯ LassoCVã€‚ LassoLarsCVåŸºäºLeast Angle Regression ç®—æ³• åæ ‡ä¸‹é™æ³• Â¶å¼¹æ€§å›å½’ Elastic-Net Â¶æœ€å°è§’å›å½’ Â¶LARS Lasso Â¶è´å¶æ–¯å²­å›å½’ 123456from sklearn import linear_modelX = [[0, 0], [1, 1], [2, 2], [3, 3]]Y = [0, 1, 2, 3]reg = linear_mode.BayesianRidge()reg.fit(X, Y) Â¶é€»è¾‘æ–¯ç‰¹å›å½’ 123from sklearn import linear_modelreg = linear_model.LogisticRegression() å‚æ•° penalty**{â€˜l1â€™, â€˜l2â€™, â€˜elasticnetâ€™, â€˜noneâ€™}, default=â€™l2â€™** tolfloat, default=1e-4 Tolerance for stopping criteria. Cfloat, default=1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. solver*{â€˜newton-cgâ€™, â€˜lbfgsâ€™, â€˜liblinearâ€™, â€˜sagâ€™, â€˜sagaâ€™}, default=â€™lbfgsâ€™* Algorithm to use in the optimization problem. max_iterint, default=100 n_jobsint, default=None]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>å›å½’åˆ†æ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[çŸ¥è¯†æ¸…å•]]></title>
    <url>%2F2020%2F06%2F19%2F%E7%9F%A5%E8%AF%86%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[ä¸»è¦æ˜¯åˆ—å‡ºå…³äºæ—¥å¸¸ä¸­é‡åˆ°çš„å¾ˆå¥½çš„èµ„æ–™ï¼Œè‡ªå·±ä¸æ¸…æ¥šçš„æ–‡ç« å’Œèµ„æ–™ã€‚ Â¶2020-8-24 5W2H 5W2Håˆ†åˆ«å¯¹åº”ç€7ä¸ªå…³é”®é—®å·ï¼š Whatï¼šä½•äº‹ï¼Ÿ Whoï¼šä½•äººï¼Ÿ Whenï¼šä½•æ—¶ï¼Ÿ Whereï¼šä½•åœ°ï¼Ÿ Whyï¼šä½•å› ï¼Ÿ Howï¼šæ€ä¹ˆåšï¼Ÿ How muchï¼šå¤šå°‘é’±ï¼Ÿ 5W2Hæ¢³ç†é”€å”®ä¸‹é™é—®é¢˜ æ–‡ç« æœ€å¼€å¤´ï¼Œå°Pè€æ¿æäº†ä¸€ä¸ªæå…¶æ¨¡ç³Šçš„é—®é¢˜ï¼š â€œæœ€è¿‘é”€å”®é¢ä¸ºä»€ä¹ˆä¸‹é™äº†ï¼Ÿâ€ å¦‚æœç”¨5W2Hæ³•ï¼Œåº”è¯¥æ€ä¹ˆç†æ¸…å¤´ç»ªå‘¢ï¼Ÿ å¾ˆç®€å•ï¼Œè·Ÿç€é—®å°±å®Œäº‹å„¿äº†ï¼ Whatï¼ˆä½•äº‹-é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿï¼‰ é—®é¢˜æ˜¯è€æ¿æŠ›å‡ºçš„é”€å”®é¢ä¸‹é™åŸå› åˆ†æï¼Œä½†è¿™ä¸ªéœ€æ±‚å¤ªè¿‡ç¬¼ç»Ÿï¼Œæˆ‘ä»¬éœ€è¦è¿›ä¸€æ­¥è¯¢é—®æ¥ç•Œå®šå’Œè§£æ„é—®é¢˜ã€‚ Whenï¼ˆä»€ä¹ˆæ—¶å€™ï¼Ÿï¼‰ æ˜¯ä»€ä¹ˆæ—¶é—´æ®µé”€å”®å¼€å§‹ä¸‹æ»‘ï¼Ÿä¸‹æ»‘æ˜¯ç¯æ¯”è¿˜æ˜¯åŒæ¯”ï¼Œäº¦æˆ–æ˜¯å’Œå¹³å‡ç›¸æ¯”ï¼Ÿä»è¶‹åŠ¿ä¸Šçœ‹ï¼Œæ˜¯æŒç»­æ€§ä¸‹æ»‘ï¼Œè¿˜æ˜¯æŸäº›æ—¶é—´èŠ‚ç‚¹çš„çªç„¶ä¸‹è·Œï¼Ÿ Whereï¼ˆä»€ä¹ˆåœ°æ–¹ï¼Ÿï¼‰ æ˜¯æ‰€æœ‰æ¸ é“çš„æ™®éä¸‹è·Œè¿˜æ˜¯æŸä¸ªé‡ç‚¹æ¸ é“çš„æŠ˜æˆŸï¼Ÿæ˜¯å…¨å›½å„åœ°æ™®éé”€å”®ä¸‹é™ï¼Œè¿˜æ˜¯æŸä¸ªåœ°åŒºé”€å”®ä¸‹é™çš„å‰å®³ï¼Ÿ Whoï¼ˆæ˜¯å“ªç¾¤äººï¼Ÿï¼‰ æ˜¯æ–°å®¢æˆ·è¿˜æ˜¯è€å®¢æˆ·çš„é”€å”®è´¡çŒ®ä¹åŠ›ï¼Ÿæ˜¯æ™®é€šå®¢æˆ·çš„å‡å°‘ï¼Œè¿˜æ˜¯å“ç‰Œå¿ è¯šå®¢æˆ·çš„æµå¤±ï¼Ÿ Whyï¼ˆä¸ºä»€ä¹ˆï¼Ÿï¼‰ å›ç­”å®Œä¸Šé¢4ä¸ªWï¼Œç»¼åˆèµ·æ¥åŸºæœ¬èƒ½å¤Ÿå›ç­”ä¸ºä»€ä¹ˆé”€å”®ä¸‹è·Œè¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯è¿™æ ·è¿˜ä¸å¤Ÿï¼Œæ•°æ®åˆ†ææ›´é‡è¦çš„æ˜¯æŒ‡å¯¼è¯¥æ€ä¹ˆåš Howï¼ˆæ€ä¹ˆåšï¼Ÿï¼‰ å¦‚æœæ˜¯æŸä¸ªæ¸ é“è€å®¢æµå¤±ä¸¥é‡ï¼Œåº”è¯¥å¿«é€Ÿåšå®¢æˆ·åŸå› å®šä½ï¼Œä»¥åŠç”¨CRMå…³æ€€æ¥æŒ½å›å®¢æˆ·ã€‚ å¦‚æœæ˜¯å„æ¸ é“ã€å…¨å›½æ€§æ™®éé”€å”®ä¸‹è·Œï¼Œå¸‚åœºä»½é¢è¢«å¯¹æ‰‹ä¾µèš€ï¼Œé‚£åº”è¯¥ç´§å¯†è§‚å¯Ÿå¸‚åœºï¼Œç´§ç›¯ç«å“åŠ¨ä½œã€‚ How muchï¼ˆé‡åŒ–åšå¤šå°‘ï¼Ÿï¼‰ ç»“åˆä¸Šä¸€æ­¥çš„è¡ŒåŠ¨ï¼Œå…·ä½“è¡¡é‡é€šè¿‡çŸ­ä¿¡æˆ–è€…å…¶ä»–æ–¹å¼è§¦è¾¾èŠ±è´¹å¤šå°‘ï¼Œéœ€è¦æŠ•å…¥å¤šå°‘æŠ˜æ‰£ï¼Œé¢„è®¡å”¤å›å¤šå°‘å®¢æˆ·ï¼Œæå‡å¤šå°‘é”€å”®é¢ï¼Œè¿™äº›éƒ½å¯ä»¥åŸºäºå†å²æ•°æ®é‡åŒ–ã€‚ æ€ä¹ˆæ ·ï¼Ÿ å¯¹äºä¸€ä¸ªæ¨¡ç³Šçš„é”€å”®ä¸‹è·Œé—®é¢˜ï¼Œé€šè¿‡è¿™7æ­¥çš„æ‹†è§£ï¼Œå¾ˆå¿«å°±æ‰“å¼€äº†åˆ†ææ€è·¯ã€‚ä¸è¿‡ï¼Œè¦å®Œå…¨ç²¾å‡†çš„å®šä½é—®é¢˜ï¼Œæ‰¾åˆ°æœ¬è´¨è§£å†³åŠæ³•ï¼Œè¿˜éœ€è¦è¿›ä¸€æ­¥çš„å®šä½ã€å‡è®¾å’ŒéªŒè¯ã€‚ 2020-6-29 Zæ£€æµ‹å’ŒTæ£€æµ‹ https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&amp;mid=2247485455&amp;idx=1&amp;sn=857066158bf8c2de38939f3037416035&amp;chksm=eb9321b9dce4a8afd68d764c295f8bcc69c62f2b1d000f3e1c5e61a7d9b6e2ec3de8df068174&amp;mpshare=1&amp;scene=24&amp;srcid=&amp;sharer_sharetime=1593403964973&amp;sharer_shareid=0e2d0ffe45c3a6dfb66aa422c3a1381d#rd 2020-6-28 è§†é¢‘ï¼š http://www.julyedu.com/video/play/58/405 2020-6-19 SQL ä¸­æ–‡: https://www.liaoxuefeng.com/wiki/1177760294764384 è‹±æ–‡ï¼š https://www.codecademy.com/courses/learn-sql/lessons/manipulation/exercises/sql è§†é¢‘ï¼š https://www.jikexueyuan.com/course/sql/ åŸºç¡€ https://study.163.com/course/courseMain.htm?courseId=215012&amp;trace_c_p_k2=f68f3d2867a343789ac2d3cfa92dd308 https://www.nowcoder.com/discuss/95812?type=2 https://www.cnblogs.com/zsh-blogs/category/1413021.html]]></content>
      <categories>
        <category>è§„åˆ’</category>
      </categories>
      <tags>
        <tag>æŠ€èƒ½</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data-Science]]></title>
    <url>%2F2020%2F06%2F11%2FData-Science%2F</url>
    <content type="text"><![CDATA[Course Tsinghua Dr. Yuan Data Mining: Theories and Algorithms for Tackling Big Data Tools Stata: https://www.stata.com/why-use-stata/ https://www.youtube.com/watch?v=AyXeh7iojuA BOOOOOOK https://www-users.cs.umn.edu/~kumar001/dmbook/index.php]]></content>
      <categories>
        <category>æ•°æ®ç§‘å­¦(Data Science)</category>
      </categories>
      <tags>
        <tag>Data Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¯†ç å­¦]]></title>
    <url>%2F2020%2F06%2F11%2F%E5%AF%86%E7%A0%81%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[ç§‘æ™® Day1 Â¶ç°ä»£ä¿¡æ¯å®‰å…¨çš„åŸºæœ¬è¦æ±‚ï¼š ä¿¡æ¯çš„ä¿å¯†æ€§ Confidentialityï¼šé˜²æ­¢ä¿¡æ¯æ³„æ¼ç»™æœªç»æˆæƒçš„äººï¼ˆåŠ å¯†è§£å¯†æŠ€æœ¯ï¼‰æœºå¯†æ€§ ä¿¡æ¯çš„å®Œæ•´æ€§ Integrityï¼šé˜²æ­¢ä¿¡æ¯è¢«æœªç»æˆæƒçš„ç¯¡æ”¹ï¼ˆæ¶ˆæ¯è®¤è¯ç ï¼Œæ•°å­—ç­¾åï¼‰ è®¤è¯æ€§ Authenticationï¼šä¿è¯ä¿¡æ¯æ¥è‡ªæ­£ç¡®çš„å‘é€è€…ï¼ˆæ¶ˆæ¯è®¤è¯ç ï¼Œæ•°å­—ç­¾åï¼‰è®¤ä¸º å…¶ä»– ä¸å¯å¦è®¤æ€§ Non-repudiationï¼šä¿è¯å‘é€è€…ä¸èƒ½å¦è®¤ä»–ä»¬å·²å‘é€çš„æ¶ˆæ¯ï¼ˆæ•°å­—ç­¾åï¼‰ ç¬¬ä¸€ç«  å¼•è¨€ æ¶‰åŠçš„çŸ¥è¯†ç‚¹åŒ…æ‹¬ä¿¡æ¯å®‰å…¨çš„è¦æ±‚ï¼ˆä¸»è¦å››ä¸ªæ–¹é¢ï¼‰ï¼Œå¯†ç å­¦åŸºæœ¬æ¦‚å¿µï¼Œå®‰å…¨çš„å®šä¹‰ï¼Œå¯†ç ç®—æ³•çš„è®¾è®¡è¦æ±‚ï¼Œå¤å…¸å¯†ç ï¼ˆæ›¿æ¢ï¼Œä»£æ›¿ï¼‰ oå¯†ç å­¦åŸºæœ¬æ¦‚å¿µ**,**å¦‚å¯†ç ç¼–ç å­¦ã€å¯†ç åˆ†æå­¦ã€æ˜æ–‡ã€å¯†æ–‡ã€åŠ å¯†ã€è§£å¯† oå¯¹ç§°å¯†ç ä½“åˆ¶å’Œéå¯¹ç§°å¯†ç ä½“åˆ¶ oå¤å…¸å¯†ç ä½“åˆ¶ï¼Œå¦‚ç½®æ¢å¯†ç ã€å•è¡¨ä»£æ¢å¯†ç ã€å¤šè¡¨ä»£æ¢å¯†ç ï¼ˆè¦ä¼šè®¡ç®—ï¼‰ ç°ä»£ä¿¡æ¯å®‰å…¨çš„åŸºæœ¬è¦æ±‚ï¼š ä¿¡æ¯çš„ä¿å¯†æ€§ Confidentialityï¼šé˜²æ­¢ä¿¡æ¯æ³„æ¼ç»™æœªç»æˆæƒçš„äººï¼ˆåŠ å¯†è§£å¯†æŠ€æœ¯ï¼‰ ä¿¡æ¯çš„å®Œæ•´æ€§ Integrityï¼šé˜²æ­¢ä¿¡æ¯è¢«æœªç»æˆæƒçš„ç¯¡æ”¹ï¼ˆæ¶ˆæ¯è®¤è¯ç ï¼Œæ•°å­—ç­¾åï¼‰ è®¤è¯æ€§ Authenticationï¼šä¿è¯ä¿¡æ¯æ¥è‡ªæ­£ç¡®çš„å‘é€è€…ï¼ˆæ¶ˆæ¯è®¤è¯ç ï¼Œæ•°å­—ç­¾åï¼‰ ä¸å¯å¦è®¤æ€§ Non-repudiationï¼šä¿è¯å‘é€è€…ä¸èƒ½å¦è®¤ä»–ä»¬å·²å‘é€çš„æ¶ˆæ¯ï¼ˆæ•°å­—ç­¾åï¼‰ http://yuqiangcoder.com/2019/10/07/%E5%AF%86%E7%A0%81%E5%AD%A6%E6%A6%82%E8%BF%B0.html Â¶å¯†ç å­¦ å°±æ˜¯è¦é€šè¿‡ç®—æ³•å’Œåè®®å®ç°ç›¸åº”çš„åŠŸèƒ½ å‡¯æ’’å¯†ç ï¼šç§»åŠ¨2ä½ï¼ŒH K æºæ’’å¯†ç  Â¶å®‰å…¨ pæ— æ¡ä»¶å®‰å…¨çš„(ä¸å¯ç ´è¯‘çš„)ï¼š pæ— è®ºæˆªè·å¤šå°‘å¯†æ–‡ï¼Œéƒ½æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯æ¥å”¯ä¸€ç¡®å®šæ˜æ–‡ï¼Œåˆ™è¯¥å¯†ç æ˜¯æ— æ¡ä»¶å®‰å…¨çš„ï¼Œå³å¯¹ç®—æ³•çš„ç ´è¯‘ä¸æ¯”çŒœæµ‹æœ‰ä¼˜åŠ¿ pè®¡ç®—ä¸Šå®‰å…¨çš„ï¼š pä½¿ç”¨æœ‰æ•ˆèµ„æºå¯¹ä¸€ä¸ªå¯†ç ç³»ç»Ÿè¿›è¡Œåˆ†æè€Œæœªèƒ½ç ´è¯‘ï¼Œåˆ™è¯¥å¯†ç æ˜¯å¼ºçš„æˆ–è®¡ç®—ä¸Šå®‰å…¨çš„ Â¶å¯†ç ç®—æ³•è¦æ±‚ å¯†ç ç®—æ³•åªè¦æ»¡è¶³ä»¥ä¸‹ä¸¤æ¡å‡†åˆ™ä¹‹ä¸€å°±è¡Œï¼š ï¼ˆ1ï¼‰ ç ´è¯‘å¯†æ–‡çš„ä»£ä»·è¶…è¿‡è¢«åŠ å¯†ä¿¡æ¯çš„ä»·å€¼ã€‚ ï¼ˆ2 ) ç ´è¯‘å¯†æ–‡æ‰€èŠ±çš„æ—¶é—´è¶…è¿‡ä¿¡æ¯çš„æœ‰ç”¨æœŸã€‚ æ»¡è¶³ä»¥ä¸Šä¸¤ä¸ªå‡†åˆ™çš„å¯†ç ç®—æ³•åœ¨å®é™…ä¸­æ˜¯å¯ç”¨çš„ã€‚ Â¶å•è¡¨ä»£æ›¿å¯†ç  å•è¡¨ä»£æ›¿å¯†ç å¯åˆ†ä¸º â€¢ åŠ æ³•å¯†ç  â€¢ ä¹˜æ³•å¯†ç  â€¢ ä»¿å°„å¯†ç  Â¶å¤å…¸å¯†ç  ç½®æ¢å¯†ç  å•è¡¨ä»£æ›¿å¯†ç ç®—æ³• å¤šè¡¨ä»£æ›¿å¯†ç ç®—æ³• ç¬¬äºŒç«  æµå¯†ç  ä¸€æ¬¡ä¸€å¯†ï¼Œæµå¯†ç ï¼Œå¯†é’¥æµä¸‰ä¸ªæ¦‚å¿µã€‚ oæµå¯†ç åŸºæœ¬æ¦‚å¿µã€ç‰¹ç‚¹ oçº¿æ€§åé¦ˆç§»ä½å¯„å­˜å™¨ oRC4 ä¸€æ¬¡ä¸€å¯†ï¼ˆç†æƒ³ï¼‰ â€¢ä¼˜ç‚¹ï¼š â€¢å¯†é’¥éšæœºäº§ç”Ÿï¼Œä»…ä½¿ç”¨ä¸€æ¬¡ â€¢æ— æ¡ä»¶å®‰å…¨ â€¢åŠ å¯†å’Œè§£å¯†ä¸ºåŠ æ³•è¿ç®—ï¼Œæ•ˆç‡è¾ƒé«˜ â€¢ç¼ºç‚¹ï¼š â€¢å¯†é’¥é•¿åº¦è‡³å°‘ä¸æ˜æ–‡é•¿åº¦ä¸€æ ·é•¿ï¼Œå¯†é’¥å…±äº«å›°éš¾ï¼Œä¸å¤ªå®ç”¨ Â¶æµå¯†ç  å¯†ç ä½“åˆ¶ åºåˆ—å¯†ç  â€¢æµå¯†ç çš„åŸºæœ¬æ€æƒ³ â€¢åˆ©ç”¨å¯†é’¥käº§ç”Ÿä¸€ä¸ªå¯†é’¥æµ â€¢å¯†é’¥æµ â€¢ç”±å¯†é’¥æµå‘ç”Ÿå™¨ f äº§ç”Ÿï¼š Ã˜å†…éƒ¨è®°å¿†å…ƒä»¶çš„çŠ¶æ€Ïƒiç‹¬ç«‹äºæ˜æ–‡å­—ç¬¦çš„å«åšåŒæ­¥æµå¯†ç ï¼Œå¦åˆ™å«åšè‡ªåŒæ­¥æµå¯†ç ã€‚ å¯†ç åˆ†æå­¦çš„ç›®æ ‡åœ¨äºç ´è¯‘ï¼ˆ BC ï¼‰ A. æ˜æ–‡ B. å¯†æ–‡ C. å¯†é’¥ D. ç®—æ³•ç»“æ„ Â¶ä¿å¯†é€šä¿¡ç³»ç»Ÿçš„å®‰å…¨å¨èƒ ä¿å¯†é€šä¿¡çš„å®‰å…¨å¨èƒï¼š è¢«åŠ¨æ”»å‡»ï¼šçªƒå¬ï¼Œå—…æ¢æµé‡åˆ†æç­‰ï¼Œä¸»è¦æ˜¯ç ´åæ¶ˆæ¯çš„æœºå¯†æ€§ï¼› ä¸»åŠ¨æ”»å‡»ï¼šä¸­æ–­ï¼Œç¯¡æ”¹ï¼Œå‡å†’ç­‰ã€‚ ä¸­æ–­ç ´åäº†ä¿¡æ¯çš„å¯ç”¨æ€§ ç¯¡æ”¹ç ´åäº†ä¿¡æ¯çš„å®Œæ•´æ€§ å‡å†’ç ´åäº†çœŸå®æ€§ï¼ˆè®¤è¯ï¼‰ æ‰€ä»¥ä¿å¯†é€šä¿¡ç³»ç»Ÿçš„å®‰å…¨éœ€æ±‚æœ‰ï¼š æœºå¯†æ€§â€”â€”é‡‡ç”¨åŠ å¯†æœºåˆ¶ å®Œæ•´æ€§â€”â€”é‡‡ç”¨å®Œæ•´æ€§éªŒè¯æœºåˆ¶ï¼Œå¦‚Hashå‡½æ•°ï¼Œæ¶ˆæ¯è®¤è¯ç  çœŸå®æ€§â€”â€”é‡‡ç”¨è®¤è¯æœºåˆ¶ï¼Œå¦‚æ•°å­—ç­¾åï¼Œè®¤è¯åè®® ä¸­æ–­â€”â€”ç”¨å¯†ç å­¦çš„æŠ€æœ¯æ²¡æœ‰å¤ªå¥½çš„åŠæ³•ï¼ˆè¿™æ˜¯æˆ‘ä¸ªäººçš„ç†è§£ï¼‰ Â¶å¤å…¸å¯†ç å­¦ ç½®æ¢å¯†ç ï¼šåˆç§°æ¢ä½å¯†ç ï¼ŒåŠ å¯†è¿‡ç¨‹ä¸­æ˜æ–‡çš„å­—æ¯ä¿æŒç›¸åŒï¼Œä½†æ˜¯é¡ºåºè¢«æ‰“ä¹±ã€‚åªè¦æŠŠä½ç½®æ¢å¤ï¼Œå°±èƒ½å¾—åˆ°æ˜æ–‡ã€‚ ä»£æ¢å¯†ç ï¼šæ˜æ–‡ä¸­çš„æ¯ä¸€ä¸ªå­—ç¬¦è¢«æ›¿æ¢æˆå¯†æ–‡ä¸­çš„å¦ä¸€ä¸ªå­—ç¬¦ã€‚æ¥æ”¶è€…å¯¹å¯†æ–‡åšåå‘æ›¿æ¢å°±å¯ä»¥æ¢å¤æ˜æ–‡ã€‚ å¤šåæˆ–åŒéŸ³ä»£æ›¿å¯†ç  å¤šå­—æ¯ä»£æ›¿å¯†ç  å¤šè¡¨ä»£æ›¿å¯†ç  æ€»ç»“å¤å…¸å¯†ç å­¦çš„ç‰¹ç‚¹ï¼šåŠ å¯†å¯¹è±¡ï¼›æ–¹æ³•ï¼›ä¿å¯†å†…å®¹ï¼›ç ´è§£ï¼› è®¡ç®—å¼ºåº¦å° å‡ºç°åœ¨ DES ä¹‹å‰ æ•°æ®å®‰å…¨åŸºäºç®—æ³•çš„ä¿å¯†ã€‚è¿™å’Œç°ä»£å¯†ç æœ‰å¾ˆå¤§çš„å·®è·ï¼Œåªè¦çŸ¥é“åŠ å¯†æ–¹æ³•ï¼Œå°±èƒ½è½»æ˜“çš„è·å–æ˜æ–‡ã€‚ç°ä»£çš„å¯†ç åŸºäºç§˜é’¥çš„åŠ å¯†ï¼Œç®—æ³•éƒ½æ˜¯å…¬å¼€çš„ï¼Œè€Œä¸”å…¬å¼€çš„å¯†ç ç®—æ³•å®‰å…¨æ€§æ›´é«˜ï¼Œèƒ½è¢«æ›´å¤šäººè¯„è®ºå’Œä½¿ç”¨ï¼ŒåŠ å¼ºæ¼æ´çš„ä¿®è¡¥ã€‚ ä»¥å­—æ¯è¡¨ä¸ºä¸»è¦åŠ å¯†å¯¹è±¡ã€‚å¤å…¸å¯†ç å¤§å¤šæ•°æ˜¯å¯¹æœ‰æ„ä¹‰çš„æ–‡å­—è¿›è¡ŒåŠ å¯†ï¼Œè€Œç°ä»£å¯†ç æ˜¯å¯¹æ¯”ç‰¹åºåˆ—è¿›è¡ŒåŠ å¯†ã€‚è¿™ä¹Ÿæ˜¯ç°ä»£å¯†ç å’Œå¤å…¸å¯†ç çš„åŒºåˆ«ï¼Œè€Œä¸”å¤å…¸å¯†ç çš„åˆ†ææ–¹æ³•ä¹Ÿæ˜¯ç”¨å­—æ¯é¢‘ç‡åˆ†æè¡¨æ¥ç ´è§£çš„ã€‚ æ›¿æ¢å’Œç½®æ¢æŠ€æœ¯ å¯†ç åˆ†ææ–¹æ³•åŸºäºå­—æ¯ä¸å­—æ¯ç»„åˆçš„é¢‘ç‡ç‰¹æ€§ä»¥åŠæ˜æ–‡çš„å¯è¯»æ€§ Â¶ç°ä»£å¯†ç å­¦ 1976ï¼šç”± Diffie å’Œ Hellman åœ¨ã€Š å¯†ç å­¦çš„æ–°æ–¹å‘ã€‹ï¼ˆã€ŠNew Directions in Cryptographyã€‹ï¼‰æå‡ºäº†å…¬é’¥å¯†ç å­¦ä½“åˆ¶çš„æ€æƒ³ 1977å¹´ï¼šç¾å›½å›½å®¶æ ‡å‡†å±€é¢å¸ƒæ•°æ®åŠ å¯†æ ‡å‡† DESï¼ˆData Encryption Standardï¼‰ 1978å¹´ï¼šç¬¬ä¸€ä¸ªå…¬é’¥ç®—æ³• RSA ç®—æ³•ï¼ˆç”± Ron Rivestã€Adi Shamir å’Œ Leonard Adleman çš„å§“æ°é¦–å­—æ¯ç»„æˆï¼‰ ç°ä»£å¯†ç å­¦ä¸»è¦æœ‰ä¸‰ä¸ªæ–¹å‘ï¼šç§é’¥å¯†ç ï¼ˆå¯¹ç§°å¯†ç ï¼‰ã€å…¬é’¥å¯†ç ï¼ˆéå¯¹ç§°å¯†ç ï¼‰ã€å®‰å…¨åè®®ã€‚ ç§é’¥å¯†ç ä¹Ÿç§°å¯¹ç§°å¯†ç ï¼Œæ˜¯å¯¹æ–‡å­—çš„åŠ å¯†è½¬æ¢æˆå¯¹æ¯”ç‰¹åºåˆ—çš„åŠ å¯†ï¼ˆç›¸å¯¹äºå¤å…¸å¯†ç ï¼‰ï¼Œç”¨åŒä¸€ä¸ªå¯†é’¥è¿›è¡ŒåŠ å¯†å’Œè§£å¯†æ“ä½œï¼Œè¿™ä¸ªå¯†é’¥å‘é€æ–¹å’Œæ¥æ”¶æ–¹éƒ½æ˜¯è¦ä¿å¯†çš„ï¼Œæ‰€ä»¥ç§°ä¸ºç§é’¥å¯†ç ã€‚å®ƒçš„ä¸¤ä¸ªåŸºæœ¬æ“ä½œå°±æ˜¯ä»£æ¢å’Œç½®æ¢å°±æ˜¯æ¥æºäºå¤å…¸å¯†ç å­¦çš„ã€‚ å¯¹ç§°å¯†ç æœ‰ä¸¤ä¸ªè®¾è®¡åŸåˆ™ï¼Œä¸€ä¸ªæ˜¯æ‰©æ•£ï¼ˆDiffusionï¼‰ï¼šæ˜æ–‡çš„ç»Ÿè®¡ç»“æ„è¢«æ‰©æ•£æ¶ˆå¤±åˆ°å¯†æ–‡çš„é•¿ç¨‹ç»Ÿè®¡ç‰¹æ€§ï¼Œä½¿å¾—æ˜æ–‡å’Œå¯†æ–‡ä¹‹é—´çš„ç»Ÿè®¡å…³ç³»å°½é‡å¤æ‚ã€‚ å¦ä¸€ä¸ªæ˜¯æ··ä¹±ï¼ˆconfusionï¼‰ï¼šä½¿å¾—å¯†æ–‡çš„ç»Ÿè®¡ç‰¹æ€§ä¸å¯†é’¥çš„å–å€¼ä¹‹é—´çš„å…³ç³»å°½é‡å¤æ‚ã€‚ å¯¹ç§°å¯†ç çš„ä»£è¡¨æœ‰ DES ç®—æ³•å’Œ AES ç®—æ³•ï¼Œ Â¶å…¬é’¥å¯†ç  DH å¯†é’¥äº¤æ¢åè®® RSA ç®—æ³•æ˜¯ç¬¬ä¸€ä¸ªå…¬é’¥å¯†ç ç®—æ³•ï¼Œä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªæ•°å­—ç­¾åç®—æ³•ã€‚ p q pi(n) =(p-1)(q-1);ä¸näº’è´¨çš„ä¹¦&lt;=n é€‰e ä¸pi(n)æœ€å¤§å…¬çº¦æ•°1ï¼Œäº’è´¨ï¼Œ æ‰¾dï¼Œe*d/pi(n)=1 (n,e)å…±ï¼ˆn,d)ç§é’¥ a^emod n =b b^d mod n=c æ ¹æ®ä»¥ä¸Šå¯†é’¥å¯¹çš„ç”Ÿæˆè¿‡ç¨‹ï¼š å¦‚æœæƒ³çŸ¥é“ d éœ€è¦çŸ¥é“æ¬§æ‹‰å‡½æ•° Ï†(n) å¦‚æœæƒ³çŸ¥é“æ¬§æ‹‰å‡½æ•° Ï†(n) éœ€è¦çŸ¥é“ P å’Œ Q è¦çŸ¥é“ P å’Œ Q éœ€è¦å¯¹ n è¿›è¡Œå› æ•°åˆ†è§£ã€‚ å¯¹äºæœ¬ä¾‹ä¸­çš„ 4757 ä½ å¯ä»¥è½»æ¾è¿›è¡Œå› æ•°åˆ†è§£ï¼Œä½†å¯¹äºå¤§æ•´æ•°çš„å› æ•°åˆ†è§£ï¼Œæ˜¯ä¸€ä»¶å¾ˆå›°éš¾çš„äº‹æƒ…ï¼Œç›®å‰é™¤äº†æš´åŠ›ç ´è§£ï¼Œè¿˜æ²¡æœ‰æ›´å¥½çš„åŠæ³•ï¼Œå¦‚æœä»¥ç›®å‰çš„è®¡ç®—é€Ÿåº¦ï¼Œç ´è§£éœ€è¦50å¹´ä»¥ä¸Šï¼Œåˆ™è¿™ä¸ªç®—æ³•å°±æ˜¯å®‰å…¨çš„ æ¤­åœ†æ›²çº¿åŠ å¯†ç®—æ³•ï¼Œç®€ç§°ECCï¼Œæ˜¯åŸºäºæ¤­åœ†æ›²çº¿æ•°å­¦ç†è®ºå®ç°çš„ä¸€ç§éå¯¹ç§°åŠ å¯†ç®—æ³•ã€‚ç›¸æ¯”RSAï¼ŒECCä¼˜åŠ¿æ˜¯å¯ä»¥ä½¿ç”¨æ›´çŸ­çš„å¯†é’¥ï¼Œæ¥å®ç°ä¸RSAç›¸å½“æˆ–æ›´é«˜çš„å®‰å…¨ï¼ŒRSAåŠ å¯†ç®—æ³•ä¹Ÿæ˜¯ä¸€ç§éå¯¹ç§°åŠ å¯†ç®—æ³• é‡åˆ Â¶å››ã€åŒä½™è¿ç®— åŒä½™å°±æ˜¯æœ‰ç›¸åŒçš„ä½™æ•°ï¼Œä¸¤ä¸ªæ•´æ•° aã€ bï¼Œè‹¥å®ƒä»¬é™¤ä»¥æ­£æ•´æ•° mæ‰€å¾—çš„ä½™æ•°ç›¸ç­‰ï¼Œåˆ™ç§° aï¼Œ bå¯¹äºæ¨¡måŒä½™ã€‚ ä¹˜æ³•é€†å…ƒï¼› Â¶å…­ã€ä¹˜æ³•é€†å…ƒ åœ¨æ¨¡7ä¹˜æ³•ä¸­ï¼š 1çš„é€†å…ƒä¸º1 (1*1)%7=1 2çš„é€†å…ƒä¸º4 (2*4)%7=1 3çš„é€†å…ƒä¸º5 (3*5)%7=1 4çš„é€†å…ƒä¸º2 (4*2)%7=1 5çš„é€†å…ƒä¸º3 (5*3)%7=1 6çš„é€†å…ƒä¸º6 (6*6)%7=1 https://zhuanlan.zhihu.com/p/101907402 ç¬¬ä¸‰ç« ç°ä»£å¯†ç  oåˆ†ç»„å¯†ç åŸºæœ¬æ¦‚å¿µã€ç‰¹ç‚¹ oFeistel ç½‘ç»œ oDESï¼Œå¯†é’¥é•¿åº¦ã€åˆ†ç»„é•¿åº¦ã€Sç›’ã€å¤šé‡DES oåˆ†ç»„å¯†ç çš„å››ç§è¿è¡Œæ¨¡å¼ oAESï¼Œå¯†é’¥é•¿åº¦ã€åˆ†ç»„é•¿åº¦ å¤ªéš¾è®°ä½äº†ï¼ŒåŸç†ä¹Ÿå¤ªéš¾äº† Day 2 ç°ä»£å¯†ç  Â¶ä¸€æ¬¡æ€§å¯†ç  Frank Miller åœ¨1882 å¹´æå‡ºäº†ä¸€æ¬¡æ€§å¯†ç ï¼ˆOne-time padï¼‰çš„æ¦‚å¿µâ€”â€”åŠ å¯†ï¼šå°†æ¶ˆæ¯å’Œç§é’¥è¿›è¡Œå¼‚æˆ–è¿ç®—å¾—åˆ°å¯†æ–‡ï¼›è§£å¯†ï¼šå°†å¯†é’¥å’Œå¯†æ–‡è¿›è¡Œå¼‚æˆ–è¿ç®—å¾—åˆ°åŸæ¶ˆæ¯ï¼Œè¿™ä¸ªè¿‡ç¨‹ç±»ä¼¼äºå‰é¢æåˆ°çš„ a âŠ• b âŠ• a = b ã€‚ä¸€æ¬¡æ€§å¯†ç çš„å®šä¹‰å¦‚ä¸‹æ‰€ç¤ºï¼š æ— æ¡ä»¶å®‰å…¨ å¯†é’¥éšæœºäº§ç”Ÿçš„ï¼Œåªèƒ½ç”¨ä¸€æ¬¡ å¼‚æˆ– 1+1 =0 ï¼Œ0+1 = 1 å…±äº«å¯†é’¥éš¾ Â¶æµå¯†ç ä½“åˆ¶ å¯†é’¥k,äº§ç”Ÿå¯†é’¥æµï¼ˆå‘ åŒæ­¥æµå¯†ç ï¼ˆçŠ¶æ€æ— å…‰ï¼‰ ä¸€.åŠ å¯†æ–¹æ³•çš„åˆ†ç±»ï¼š æŒ‰ç…§ä¸åŒçš„æ ‡å‡†æœ‰ä¸åŒçš„åˆ†ç±»æ ‡å‡†ï¼š 1.æŒ‰ç…§å¯†é’¥çš„ç‰¹å¾ä¸åŒï¼Œå¯ä»¥åˆ†ä¸ºå¯¹ç§°å¯†ç ä¸éå¯¹ç§°å¯†ç ã€‚ 2.æŒ‰ç…§åŠ å¯†æ–¹å¼çš„ä¸åŒï¼Œå¯ä»¥åˆ†ä¸ºæµå¯†ç å’Œåˆ†ç»„å¯†ç ã€‚ 3.éå¯¹ç§°å¯†ç å‡å±äºåˆ†ç»„å¯†ç ã€‚ Â¶1.æµå¯†ç ã€‚ åˆååºåˆ—å¯†ç ã€‚æ˜æ–‡ç§°ä¸ºæ˜æ–‡æµï¼Œä»¥åºåˆ—çš„æ–¹å¼è¡¨ç¤ºã€‚åŠ å¯†æ—¶å€™ï¼Œå…ˆç”±ç§å­å¯†é’¥ç”Ÿæˆä¸€ä¸ªå¯†é’¥æµã€‚ç„¶ååˆ©ç”¨åŠ å¯†ç®—æ³•æŠŠæ˜æ–‡æµå’Œå¯†é’¥æµè¿›è¡ŒåŠ å¯†ï¼Œäº§ç”Ÿå¯†æ–‡æµã€‚æµå¯†ç æ¯æ¬¡åªé’ˆå¯¹æ˜æ–‡æµä¸­çš„å•ä¸ªæ¯”ç‰¹ä½è¿›è¡ŒåŠ å¯†å˜æ¢ï¼ŒåŠ å¯†è¿‡ç¨‹æ‰€éœ€è¦çš„å¯†é’¥æµç”±ç§å­å¯†é’¥é€šè¿‡å¯†é’¥æµç”Ÿæˆå™¨äº§ç”Ÿã€‚æµå¯†ç çš„ä¸»è¦åŸç†æ˜¯é€šè¿‡éšæœºæ•°å‘ç”Ÿå™¨äº§ç”Ÿæ€§èƒ½ä¼˜è‰¯çš„ä¼ªéšæœºåºåˆ—ï¼Œä½¿ç”¨è¯¥åºåˆ—åŠ å¯†æ˜æ–‡æµï¼ˆæŒ‰æ¯”ç‰¹ä½åŠ å¯†ï¼‰ï¼Œå¾—åˆ°å¯†æ–‡æµã€‚ç”±äºæ¯ä¸€ä¸ªæ˜æ–‡éƒ½å¯¹åº”ä¸€ä¸ªéšæœºçš„åŠ å¯†å¯†é’¥ï¼Œæ‰€ä»¥æµå¯†ç åœ¨ç»å¯¹ç†æƒ³çš„æ¡ä»¶ä¸‹åº”è¯¥æ˜¯ç®—ä¸€ç§æ— æ¡ä»¶å®‰å…¨çš„ä¸€æ¬¡ä¸€å¯†å¯†ç ã€‚ æœºå¯†æµç¨‹ï¼š ç§å­å¯†ç -&gt;éšæœºæ•°å‘ç”Ÿå™¨-&gt;å¯†é’¥æµ æ˜æ–‡æµ-&gt;(é€šè¿‡å¯†é’¥æµ)-&gt;åŠ å¯†å˜æ¢-&gt;å¯†æ–‡æµ è®¾æ˜æ–‡æµä¸ºï¼šm=m1m2Â·Â·Â·Â·Â·miÂ·Â·Â·Â·Â·ï¼Œå¯†é’¥æµç”±å¯†é’¥æµå‘ç”Ÿå™¨fäº§ç”Ÿï¼šzi=fï¼ˆkï¼Œaiï¼‰ï¼ŒaiæŒ‡åŠ å¯†å™¨å­˜å‚¨å™¨åœ¨iæ—¶åˆ»çš„çŠ¶æ€ï¼Œfæ˜¯ç”±ç§å­å¯†é’¥kå’Œaiäº§ç”Ÿçš„å‡½æ•°ï¼Œè®¾æœ€ç»ˆçš„å¯†é’¥æµä¸ºk=k1k2Â·Â·Â·kiÂ·Â·Â·Â·Â·ï¼ŒåŠ å¯†ç»“æœä¸ºc=c1c2Â·Â·Â·Â·ciÂ·Â·Â·Â·Â·=Ek1ï¼ˆm1ï¼‰.ã€‚ã€‚ã€‚Ekiï¼ˆmiï¼‰ï¼Œè§£å¯†ç»“æœä¸ºm=Dk1ï¼ˆc1ï¼‰Dk2ï¼ˆc2ï¼‰Â·Â·Â·Dkiï¼ˆciï¼‰=m1m2Â·Â·Â·miï¼Œæ— è®ºåŠ å¯†è§£å¯†ï¼Œå…¶å…³é”®éƒ½æ˜¯å¯†é’¥æµã€‚ Â¶2.æµå¯†ç çš„åˆ†ç±» åˆ†ä¸ºåŒæ­¥æµå¯†ç å’Œè‡ªåŒæ­¥æµå¯†ç  3.æµå¯†ç çš„ç‰¹æ€§ï¼šæå¤§çš„å‘¨æœŸï¼Œè‰¯å¥½çš„ç»Ÿè®¡ç‰¹æ€§ï¼ŒæŠ—çº¿æ€§åˆ†æã€‚ 4.æµå¯†ç çš„å®‰å…¨æ€§å–å†³äºå¯†é’¥æµçš„å®‰å…¨æ€§ï¼Œè¦æ±‚å¯†é’¥æµåºåˆ—æœ‰è¾ƒå¥½çš„éšæœºæ€§ã€‚ 5.ä¸æ˜å¯†é’¥çš„äººå¦‚ä½•å¯¹æµå¯†ç è¿›è¡Œåˆ†æã€‚ è¿™ç§å¯†é’¥æµä¸€èˆ¬éƒ½æ˜¯å‘¨æœŸçš„ï¼Œåšåˆ°å®Œå…¨éšæœºæ˜¯å›°éš¾çš„ï¼Œè¿™æ ·ä¼ªéšæœºåºåˆ—ï¼Œç†è®ºä¸Šæ˜¯å¯ä»¥åˆ†æå‡ºæ¥çš„ã€‚ ä¸¾ä¸ªä¾‹å­ã€‚ æ•Œæ–¹æˆªè·äº†å¯†æ–‡ä¸²ï¼š101101011110010 æ˜æ–‡ä¸²ï¼š011001111111001 å¯†é’¥æµï¼š110100100001011 å¯ä»¥æ ¹æ®å‰10ä¸ªæ¯”ç‰¹å»ºç«‹å¦‚ä¸‹æ–¹ç¨‹ å¯†é’¥æµç”Ÿæˆå™¨ï¼š é«˜è¦æ±‚å…³é”® è¦æ±‚ï¼š æ¸¸ç¨‹ï¼šå‘¨æœŸ 0.1 å‘è©å‡½æ•° äº§ç”Ÿå¯†é’¥æµçš„è¦æ±‚ï¼Œæ–¹æ³•ã€è®¾è®¡ åé¦ˆç§»ä½å¯„å­˜å™¨ â€‹ ï¼šå¯„å­˜å™¨ â€‹ ï¼š è¿”å›å‡½æ•° åˆå§‹çŠ¶æ€ çº¿æ€§åé¦ˆç§»ä½å¯„å­˜å™¨ å¿« å‘¨æœŸâ€œ è¾“å‡ºå½¢çŠ¶ï¼šå‘è©å‡½æ•° ç®—æ³• RC4 Â¶æµå¯†ç æ˜¯ä¸€æ¬¡ä¸€å¯†å—ï¼Ÿä¸æ˜¯ RC4æ²¡æœ‰å®ç°çš„ Â¶m-åºåˆ— ä¸å¯çº¦ã€Š2^n-1 å……è¦ æœ¬åŸå¤šé¡¹å¼ åé¦ˆå‡½æ•°å½¢å¼ ä¼ªéšæœºæ€§ Â¶æ±‚ä½  12 Day 3 3_13 åˆ†ç»„å¯†ç  Â¶åº”ç”¨ Â¶è®¾è®¡ç»“æ„åŸç† Â¶å®‰å…¨æ€§åŸåˆ™ æ··æ·†åŸåˆ™ æ‰©æ•£åŸåˆ™ Â¶ç®—æ³•è¦æ±‚ åˆ†ç»„é•¿åº¦è¶³å¤Ÿå¤§ å¯†é’¥é‡è¶³å¤Ÿå¤§ Â¶DESç®—æ³• 56-64 IBMç¬¬ä¸€ä¸ªå•†ä¸š åé¢å‡ºç°äº†AES Â¶ç®—æ³•æ¡†å›¾ IP åˆå§‹ç½®æ¢ è®ºå‡½æ•° 16è®º åˆ†å·¦å³32bit å…¬å¼ï¼šå‡½æ•°ï¼ˆR,è½®å¯†é’¥ï¼‰ Sç›’ è¾“å…¥å…­ä½ï¼Œ8ä¸ªç›’å­ è¾“å‡º32bit step1; 32bit-48bit() é€‰æ‹©æ‰©å±•è¿ç®— E 8*4-ã€‹ä¸¤ç«¯ ç½®æ¢ Sç›’ 4*16 é€‰æ‹©å‹ç¼©è¿ç®— â€‹ è¾“å…¥è¾“å‡º â€‹ è¾“å…¥ï¼š6bit äºŒè¿›åˆ¶-ã€‹åè¿›åˆ¶ ç¡®å®šä½ç½® Pç›’ç½®æ¢ 32 -32 å¯†é’¥ç¼–æ’ ç½®æ¢-ã€‹ä¸¤ç»„-ã€‹å¾ªç¯å·¦ç§»ã€‹16è½®å¯†é’¥ æ€§è´¨ï¼šäº’è¡¥æ€§å’Œå¼±å¯†é’¥æ€§ 2DES 56+1 = 57 ä¸­é—´äººç›¸é‡å·¥å…· 3DES Â¶åˆ†ç»„å¯†ç çš„å·¥ä½œæ¨¡å¼ ä¸ºä»€ä¹ˆï¼Ÿåˆ†ç»„é•¿åº¦æ˜¯å›ºå®šï¼Œè€Œæ•°æ®é•¿åº¦å’Œæ ¼å¼æ˜¯ä¸åŒçš„ï¼Œ ç”µç æœ¬æ¨¡å¼ å¯†ç åˆ†ç»„é“¾æ¥æ¨¡å¼ â€‹ CBCåŠ å¯† å®Œæ•´æ€§ï¼ˆè®¤è¯ç ç”Ÿæˆï¼‰åŠ å¯†ï¼Œå¯¹æ¯” æ˜æ–‡æ ¡éªŒç -ã€‹CBC(M.r)&gt;å¯¹æ¯” è§£å†³ï¼šæ˜æ–‡ç»Ÿè®¡è§„å¾‹éšè— å·¥ä½œæ¨¡å¼ 2 æ•°æ®æ ¼å¼ï¼š â€‹ å­—èŠ‚ã€æ¯”ç‰¹ã€ç­‰ç­‰æ•…äº‹ Â¶åˆ†ç»„å¯†ç æ¦‚è¿° å…±äº«å¯†é’¥ IV Â¶æœ‰é™åŸŸçš„åŸºæœ¬æ¦‚å¿µ å•ä½å…ƒï¼šåŠ æ³• é€†å…ƒï¼šä¹˜æ³• Â¶AES å­—èŠ‚ä¸ºå¤„ç†å•å…ƒ 8bits åŠ æ³•ï¼šmod 2 å¤šé¡¹å¼é™¤æ³• 8 4 3 1 0 çš„æœ«å¤šé¡¹å¼å–æ¨¡ å¤šé¡¹å¼è¿ç®— 128 128ï¼Œ192ï¼Œ256 S-æŒ‰åˆ— å››ä¸ªåŸºæœ¬åšå‡º 10 12 14 s:16 å­—èŠ‚ä»£æ¢ She 16*16 Sé‡Œé¢æŸ¥è¡¨ä»£æ¢ äºŒè¿›åˆ¶ åå…­è¿›å±• æ±‚é€† æ··æ·†æ•ˆåº” ä¹±äº† è¡Œç§»ä½ â€‹ å¾ªç¯å·¦ç§» åˆ—æ··æ·† â€‹ æ¯ä¸€åˆ—çŸ©é˜µç°åœº â€‹ çœ‹æˆå¤šé¡¹å¼ â€‹ çŸ©é˜µé€‰æ‹© è½®å¯†é’¥åŠ  å¼‚æˆ–ï¼šå­å¯†é’¥ å±…ä½ â€‹ åˆå§‹å¯†é’¥ï¼Œ AES â€‹ å››ä¸ªä½-ã€‹ä¸€ä¸ªå­—èŠ‚-ã€‹16è¿›åˆ¶ ä¸€ä¸ªå­—èŠ‚=ã€‹ä¸¤ä¸ªåå…­è¿›æ•° å¯†é’¥æ‰©å±•ç®—æ³• é€†Sç›’ Day 4 ç°ä»£å¯†ç å­¦ 3-27 å…¬é’¥ï¼šå¯†é’¥ç®¡ç†ï¼Œ éå¯¹ç§°å¯†ç ä½“åˆ¶ å¯†é’¥å¯¹ pk sk åŠ å¯†ï¼šå…¬é’¥ ä¼˜åŠ¿ï¼š â€‹ å¯†é’¥åˆ†å‘ â€‹ å¯†é’¥ç®¡ç† ï¼š1 N-1 â€‹ å¼€æ”¾ç³»ç»Ÿ Â¶RSAåŠ å¯†ç®—æ³• æ•°å­¦çŸ¥è¯† â€‹ ç®—æ³• å¤§æ•°æ®åˆ†è§£ å¯†é’¥ç”Ÿæˆ æœ€å¤§å…¬å› å­å’Œä¹˜æ³•é€†å…ƒçš„è®¡ç®—æ–¹æ³•ã€‚ https://blog.csdn.net/boksic/article/details/7014386 https://blog.csdn.net/a745233700/article/details/102341542?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-5&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-5 https://blog.csdn.net/weixin_34138377/article/details/92199465?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-4&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-4 https://blog.csdn.net/weixin_41482303/article/details/85417302?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3 ç­¾å è¯ä¹¦ https://blog.csdn.net/weixin_34007879/article/details/85528967?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2]]></content>
      <categories>
        <category>ç§‘æ™®</category>
      </categories>
      <tags>
        <tag>æ—¥å¸¸</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[or]]></title>
    <url>%2F2019%2F12%2F01%2For%2F</url>
    <content type="text"><![CDATA[è¿ç­¹å­¦ ç›®çš„æ˜¯åœ¨å†³ç­–æ—¶ä¸ºç®¡ç†äººå‘˜æä¾›ç§‘å­¦ä¾æ®ã€‚ åˆ©ç”¨ç»Ÿè®¡å­¦ï¼Œæ•°å­¦æ¨¡å‹å’Œç®—æ³•ç­‰æ–¹æ³•ï¼Œå¯»æ‰¾å¤æ‚é—®é¢˜ä¸­çš„æœ€ä½³æˆ–è€…è¿‘ä¼¼æœ€ä½³çš„è§£ç­”ã€‚ è§£å†³é—®é¢˜çš„ä¼˜åŒ–ç®—æ³•ã€‚ Â¶æ¨¡å‹å»ºç«‹ å®é™…é—®é¢˜ å†³ç­–å˜é‡ å½±å“æ‰€è¦åˆ°è¾¾ç›®çš„çš„å› ç´ æ‰¾åˆ°å†³ç­–å˜é‡ ç›®æ ‡å‡½æ•° çº¦æŸæ¡ä»¶ Â¶çº¿æ€§è§„åˆ’ Â¶æ•´æ•°è§„åˆ’ 1ã€çº¯æ•´æ•°è§„åˆ’ï¼šæ‰€æœ‰å†³ç­–å˜é‡å‡è¦æ±‚ä¸ºæ•´æ•°çš„æ•´æ•°è§„åˆ’ 2ã€æ··åˆæ•´æ•°è§„åˆ’ï¼šéƒ¨åˆ†å†³ç­–å˜é‡å‡è¦æ±‚ä¸ºæ•´æ•°çš„æ•´æ•°è§„åˆ’ 3ã€çº¯0ï¼1æ•´æ•°è§„åˆ’ï¼šæ‰€æœ‰å†³ç­–å˜é‡å‡è¦æ±‚ä¸º0ï¼1çš„æ•´æ•°è§„åˆ’ 4ã€æ··åˆ0ï¼1è§„åˆ’ï¼šéƒ¨åˆ†å†³ç­–å˜é‡å‡è¦æ±‚ä¸º0ï¼1çš„æ•´æ•°è§„åˆ’ åˆ†æ”¯å®šç•Œæ³• : ç²¾ç¡®ç®—æ³•â€“åˆ†æ”¯å®šç•Œæ³•(Branch and Bound Algorithm, B&amp;B) è¿™å°±æ„å‘³ç€ï¼Œè¦ä¹ˆèŠ±é’±ä¹°ä»¥ä¸Šæ±‚è§£å™¨çš„ä½¿ç”¨æƒï¼Œè¦ä¹ˆå°±è‡ªå·±å†™B&amp;Bç®—æ³•çš„Codeï¼Œç„¶åå¿å—Cplex 1åˆ†é’Ÿå¯ä»¥æ±‚è§£çš„é—®é¢˜å´è¦èŠ±1å¤©æ—¶é—´çš„æ±‚è§£ã€‚ï¼ˆå¾ˆå¤šé—®é¢˜æ—¶é—´å°±æ˜¯é‡‘é’±ï¼Œä¾‹å¦‚èˆªç­å»¶è¯¯åå‰©ä½™èˆªç­é‡æ–°æ’ç­çš„é—®é¢˜ï¼Œé€šå¸¸éœ€è¦åœ¨10åˆ†é’Ÿå†…æ±‚è§£ï¼‰ æƒ³æ³•ï¼š é¦–å…ˆï¼Œå¯ä»¥ç¡®å®šçš„æ˜¯è¿™æ˜¯ä¸ªèˆªç­é‡æ–°æ’ç­çš„é—®é¢˜ï¼Œæ•°å­¦ä¸Šï¼Œèˆªç­å®‰æ’å±äºè¿ç­¹å­¦çš„é—®é¢˜ä¹‹ä¸€ï¼Œéœ€è¦åº”ç”¨å»ºç«‹ä¼˜åŒ–æ¨¡å‹è§£å†³ã€‚å»ºç«‹æœ€ä¼˜åŒ–é—®é¢˜ï¼Œæœ€é‡è¦çš„ä¸¤æ­¥æ˜¯æ¨¡å‹å»ºç«‹å’Œæ¨¡å‹æ±‚è§£ã€‚æ¨¡å‹çš„å»ºç«‹ï¼šéœ€è¦ç¡®å®šå†³ç­–å˜é‡ï¼ˆæ•´æ•°è§„åˆ’ï¼Œæ··åˆæ•´æ•°è§„åˆ’)ï¼Œç›®æ ‡å‡½æ•°ï¼ˆå¤šç›®æ ‡ï¼‰ï¼Œçº¦æŸæ¡ä»¶ã€‚ æ¨¡å‹çš„æ±‚è§£ï¼š åˆ†å±‚åºåˆ—æ³• ã€‚ Â¶è¯¾å ‚èƒŒæ™¯ èˆªç­çš„é‡æ’ç­é—®é¢˜ï¼Œæœ€ä¼˜åŒ–é—®é¢˜ï¼Œè¿ç­¹å­¦èŒƒç•´çš„é—®é¢˜ã€‚ Â¶çº¦æŸï¼š ç®—æ³•èƒ½å¤Ÿåœ¨æ»¡è¶³å¤šç§å®é™…çº¦æŸæ¡ä»¶çš„å‰æä¸‹ï¼Œå¯ä»¥å¯¹èˆªç­è®¡åˆ’è¿›è¡Œæ¢å¤ï¼Œå¹¶å¿«é€Ÿç»™å‡ºæœ€ä¼˜çš„èˆªç­è°ƒæ•´æ›¿æ¢æ–¹æ¡ˆ ï¼› èˆªç­è¿è¡Œä¸æœºç»„ç¼–æ’çš„å„ç±»çº¦æŸæ¡ä»¶ ï¼› æ ¹æ®èˆªç­è®¡åˆ’å¯¹æœºç»„æ’ç­è®¡åˆ’è¿›è¡Œè°ƒæ•´ï¼Œä½¿å¾—æœºç»„çš„èµ„è´¨ç­‰ä¸èˆªç­è®¡åˆ’å¯ä»¥åŒ¹é…ï¼Œ å·èˆªä¸€ä¸ªæœˆå†…çš„å…¨éƒ¨èˆªç­è®¡åˆ’ä¸æœºç»„æ’ç­è®¡åˆ’ å¤‡ç”¨é£æœº æœ‰é™ è°ƒæ•´åˆ†æœºé£è¡Œé¡ºåº èˆªç­å»¶è¿Ÿ ä¸èƒ½æå‰èµ·é£ ä¸èƒ½è¶…è¿‡å»¶è¯¯æ—¶é—´ èˆªç­å–æ¶ˆ å¦‚æœè¶…è¿‡äº†å»¶è¯¯æ—¶é—´ï¼Œç›®æ ‡å‡½æ•°å¢åŠ è°ƒæ•´æˆæœ¬ æ—…å®¢è½¬ç­¾ åªèƒ½ä¸€æ¬¡è½¬ï¼Œè¿˜æœ‰åº§ä½é™åˆ¶ èˆªç­ç›´é£ èˆªç­ç”±äºå¤©æ°”æˆ–æµæ§ç­‰åŸå› æ— æ³•é¡ºç•…è¿è¡Œæ—¶ï¼Œå°†è”ç¨‹èˆªç­ä¸­æ®µå–æ¶ˆç›´é£æœ€ç»ˆç›®çš„åœ°ï¼Œå¹¶å¦¥å–„å¤„ç½®æ—…å®¢æ˜¯èˆªç­è°ƒæ•´æ–¹æ³•ä¹‹ä¸€ã€‚ï¼ˆè”ç¨‹èˆªç­å®šä¹‰ä¸ºï¼Œå‰åæ®µè¡”æ¥å¹¶ä¸”èˆªç­å·ç›¸åŒçš„å¤šä¸ªèˆªç­ï¼‰ Â¶æœºç»„è°ƒæ•´ ï¼ˆä¸€ï¼‰ å¤‡ä»½æœºç»„ åœ¨èˆªç­è®¡åˆ’å‡ºç°æœºç»„å®åŠ›ç¼ºå£æ—¶ï¼Œå¯ä»¥åœ¨èˆªç­çš„å‡ºå‘åœ°å¯»æ‰¾ç©ºé—²æœºç»„ï¼Œå®‰æ’å…¶æ‰§è¡Œè¯¥èˆªç­ã€‚ ï¼ˆäºŒï¼‰ è°ƒæ¢æœºç»„ å°†å¤šä¸ªæœºç»„çš„èˆªç­è®¡åˆ’è¿›è¡Œè°ƒæ¢ã€‚ ï¼ˆä¸‰ï¼‰ æœºç»„æ‘†æ¸¡ å½“é‡åˆ°æœºç»„è®¡åˆ’ä¸è¡”æ¥ï¼ˆæœºç»„çš„ä¸Šä¸ªèˆªç­çš„ç›®çš„åœ°ä¸ä¸‹ä¸ªèˆªç­çš„å‡ºå‘åœ°ä¸ä¸€è‡´ï¼‰æ—¶ï¼Œå¯ä»¥é€šè¿‡æ‘†æ¸¡çš„æ–¹å¼ï¼Œé‡‡ç”¨é£æœºæˆ–æ˜¯å…¶ä»–äº¤é€šå·¥å…·åˆ°è¾¾ä¸‹ä¸ªèˆªç­çš„å‡ºå‘åœ°ã€‚ Â¶ç›®æ ‡ï¼š å°†èˆªç­è¿è¡Œæƒ…å†µå—åˆ°çš„å½±å“é™åˆ°æœ€ä½ ä»è€Œä½¿å¾—èˆªç­ä¸æœºç»„è®¡åˆ’å¾—åˆ°å¿«é€Ÿæ¢å¤ã€å‡å°‘èˆªç­å»¶è¯¯ã€æé«˜èˆªç­æ­£å¸¸ç‡ï¼Œä½¿æ—…å®¢æœ‰æ›´å¥½çš„å‡ºè¡Œä½“éªŒï¼Œå¹¶æå‡å…¬å¸çš„è¿è¡Œæ•ˆç‡ä¸ç»æµæ•ˆç›Šã€‚ èˆªå…¬å…¬å¸ï¼š æŸå¤±æœ€å° æ¸¸å®¢: èˆªç­å»¶æ—¶çŸ­]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux]]></title>
    <url>%2F2019%2F11%2F27%2Flinux%2F</url>
    <content type="text"><![CDATA[linuxå†…æ ¸ï¼Œlinuxå‘è¡Œç‰ˆï¼ˆå¸¦æ¡Œé¢ç¯å¢ƒï¼‰ï¼ŒæœåŠ¡å™¨çš„è®¿é—®æ–¹å¼ï¼ˆä¸‰ç§ï¼‰ï¼Œlinuxæ“ä½œç³»ç»Ÿçš„ç›¸å…³ä½¿ç”¨ï¼Œé€šè¿‡å‘½ä»¤è¡Œå’Œé”®ç›˜è¾“å…¥æå®šï¼ˆç”¨æˆ·æƒé™ï¼Œç”¨æˆ·åˆ†ç»„ï¼Œç”¨æˆ·ä¹‹é—´çš„å…³ç³»ï¼Œç”¨æˆ·æ“ä½œï¼›æ–‡ä»¶ç»“æ„ï¼Œæ–‡ä»¶ä½¿ç”¨ï¼Œæ–‡ä»¶æƒé™ï¼Œæ–‡ä»¶ç¼–è¾‘ï¼Œæ–‡ä»¶å‹ç¼©å’Œè§£å‹ï¼Œè¿™ç³»åˆ—æ“ä½œç±»æ¯”æ“ä½œç³»ç»Ÿï¼Œåªæ˜¯linuxç³»ç»Ÿé‡Œé¢ï¼Œéƒ½æ˜¯å‘½ä»¤è¡Œå®Œæˆï¼Œä¸æ˜¯å¯è§†åŒ–ç•Œé¢ç½¢äº†ï¼›å¸®åŠ©ï¼Œç»ˆæ­¢æ“ä½œï¼Œrootï¼Œsudo,è¶…çº§ç®¡ç†å‘˜ï¼Œç®¡ç†å‘˜ç»„ï¼Œæ™®é€šç”¨æˆ·ï¼‰windowsæ“ä½œç³»ç»Ÿå¯ä»¥å¹²çš„äº‹æƒ…ï¼Œåœ¨linuxæœåŠ¡å™¨é‡Œé¢ï¼Œéƒ½å¯ä»¥å¹²ï¼Œé€šè¿‡å‘½ä»¤è¡Œé…ç½®ï¼Œå®‰è£…ï¼Œå®Œæˆï¼ Linux Â¶Linux çš„ç§‘æ™® Linux æ˜¯ä¸€å¥—å…è´¹ä½¿ç”¨å’Œè‡ªç”±ä¼ æ’­çš„ç±» Unix æ“ä½œç³»ç»Ÿ ï¼Œæ”¯æŒå¤šç”¨æˆ·ï¼Œå¤šä»»åŠ¡ï¼Œæ”¯æŒå¤šçº¿ç¨‹å’Œå¯¹CPUçš„æ“ä½œç³»ç»Ÿã€‚ï¼Œå°±åƒä½ å¤šå°‘å·²ç»äº†è§£çš„ Windowsï¼ˆxpï¼Œ7ï¼Œ8ï¼‰å’Œ Mac OS ã€‚ æˆ–è®¸ä½ ä¹‹å‰ä¸çŸ¥é“ Linux ï¼Œè¦çŸ¥é“ï¼Œä½ ä¹‹å‰åœ¨ Windows ä½¿ç”¨ç™¾åº¦ã€è°·æ­Œï¼Œä¸Šæ·˜å®ï¼ŒèŠ QQ æ—¶ï¼Œæ”¯æ’‘è¿™äº›è½¯ä»¶å’ŒæœåŠ¡çš„ï¼Œæ˜¯åå°æˆåƒä¸Šä¸‡çš„ Linux æœåŠ¡å™¨ä¸»æœºï¼Œå®ƒä»¬æ—¶æ—¶åˆ»åˆ»éƒ½åœ¨å¿™ç¢Œåœ°è¿›è¡Œç€æ•°æ®å¤„ç†å’Œè¿ç®—ï¼Œå¯ä»¥è¯´ä¸–ç•Œä¸Šå¤§éƒ¨åˆ†è½¯ä»¶å’ŒæœåŠ¡éƒ½æ˜¯è¿è¡Œåœ¨ Linux ä¹‹ä¸Šçš„ã€‚ æ˜ç¡®ç›®çš„ï¼šä½ æ˜¯è¦ç”¨ Linux æ¥å¹²ä»€ä¹ˆï¼Œæ­å»ºæœåŠ¡å™¨ã€åšç¨‹åºå¼€å‘ã€æ—¥å¸¸åŠå…¬ï¼Œè¿˜æ˜¯å¨±ä¹æ¸¸æˆï¼› å…¼å…·å›¾å½¢ç•Œé¢æ“ä½œï¼ˆéœ€è¦ä½¿ç”¨å¸¦æœ‰æ¡Œé¢ç¯å¢ƒçš„å‘è¡Œç‰ˆï¼‰å’Œå®Œå…¨çš„å‘½ä»¤è¡Œæ“ä½œï¼Œå¯ä»¥åªç”¨é”®ç›˜å®Œæˆä¸€åˆ‡æ“ä½œï¼Œæ–°æ‰‹å…¥é—¨è¾ƒå›°éš¾ï¼Œéœ€è¦ä¸€äº›å­¦ä¹ å’ŒæŒ‡å¯¼ï¼ˆè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ï¼‰ï¼Œä¸€æ—¦ç†Ÿç»ƒä¹‹åæ•ˆç‡æé«˜ã€‚ ä¸€èˆ¬å‘½ä»¤è¡Œæ“ä½œï¼Œé€šè¿‡é”®ç›˜å®Œæˆ å› ä¸ºlinuxçš„å“²å­¦å°±æ˜¯ï¼šæ²¡æœ‰ç»“æœå°±æ˜¯æœ€å¥½çš„ç»“æœ å¦‚æœåªæ˜¯æ‰§è¡Œï¼Œæ‰§è¡Œå¤±è´¥ä¼šå‘Šè¯‰ä½ å“ªé‡Œé”™äº†ï¼Œå¦‚æœæ‰§è¡ŒæˆåŠŸé‚£ä¹ˆä¼šæ²¡æœ‰è¾“å‡ºï¼Œå› ä¸ºlinuxçš„å“²å­¦å°±æ˜¯ï¼šæ²¡æœ‰ç»“æœå°±æ˜¯æœ€å¥½çš„ç»“æœ Â¶Linuxçš„å‘è¡Œç‰ˆ Linuxå‘è¡Œç‰ˆ = linuxå†…æ ¸+åº”ç”¨è½¯ä»¶çš„æ‰“åŒ… çŸ¥åçš„å‘è¡Œç‰ˆï¼š ubuntuï¼Œredhat,centos Linuxç³»ç»Ÿ ç”¨æˆ·ç™»å½•ç³»ç»Ÿ ï¼ˆ1ï¼‰å‘½ä»¤è¡Œ ï¼ˆ2ï¼‰sshç™»å½• SSH ä¸º [Secure Shell](https://baike.baidu.com/item/Secure Shell) çš„ç¼©å†™ ï¼Œç”¨äºè¿œç¨‹ç™»é™†çš„åè®® è¿œç¨‹è¿æ¥å·¥å…·å®¢æˆ·ç«¯ï¼šxshell, putty, (3) å›¾å½¢ç•Œé¢ç™»å½• æ–‡ä»¶ç›®å½•ä»¥åŠæƒé™ Linux ä¸­åˆ›å»ºã€åˆ é™¤ç”¨æˆ·ï¼ŒåŠç”¨æˆ·ç»„ç­‰æ“ä½œã€‚ Linux ä¸­çš„æ–‡ä»¶æƒé™è®¾ç½®ã€‚ 1.2 å®éªŒçŸ¥è¯†ç‚¹ Linux ç”¨æˆ·ç®¡ç† Linux æƒé™ç®¡ç† ç”¨æˆ·ç®¡ç† é€šè¿‡ç¬¬ä¸€èŠ‚è¯¾ç¨‹çš„å­¦ä¹ ï¼Œä½ åº”è¯¥å·²ç»çŸ¥é“ï¼ŒLinux æ˜¯ä¸€ä¸ªå¯ä»¥å®ç°å¤šç”¨æˆ·ç™»å½•çš„æ“ä½œç³»ç»Ÿï¼Œæ¯”å¦‚â€œæé›·â€å’Œâ€œéŸ©æ¢…æ¢…â€éƒ½å¯ä»¥åŒæ—¶ç™»å½•åŒä¸€å°ä¸»æœºï¼Œä»–ä»¬å…±äº«ä¸€äº›ä¸»æœºçš„èµ„æºï¼Œä½†ä»–ä»¬ä¹Ÿåˆ†åˆ«æœ‰è‡ªå·±çš„ç”¨æˆ·ç©ºé—´ï¼Œç”¨äºå­˜æ”¾å„è‡ªçš„æ–‡ä»¶ã€‚ä½†å®é™…ä¸Šä»–ä»¬çš„æ–‡ä»¶éƒ½æ˜¯æ”¾åœ¨åŒä¸€ä¸ªç‰©ç†ç£ç›˜ä¸Šçš„ç”šè‡³åŒä¸€ä¸ªé€»è¾‘åˆ†åŒºæˆ–è€…ç›®å½•é‡Œï¼Œä½†æ˜¯ç”±äº Linux çš„ ç”¨æˆ·ç®¡ç†å’Œ æƒé™æœºåˆ¶ï¼Œä¸åŒç”¨æˆ·ä¸å¯ä»¥è½»æ˜“åœ°æŸ¥çœ‹ã€ä¿®æ”¹å½¼æ­¤çš„æ–‡ä»¶ã€‚ åœ¨ Linux ç³»ç»Ÿé‡Œï¼Œ root è´¦æˆ·æ‹¥æœ‰æ•´ä¸ªç³»ç»Ÿè‡³é«˜æ— ä¸Šçš„æƒåˆ©ï¼Œæ¯”å¦‚ æ–°å»º/æ·»åŠ  ç”¨æˆ·ã€‚ sudo adduser lilei åˆ›å»ºç”¨æˆ·ï¼ˆsudo ç»„ï¼‰ æˆ‘ä»¬ä¸€èˆ¬ç™»å½•ç³»ç»Ÿæ—¶éƒ½æ˜¯ä»¥æ™®é€šè´¦æˆ·çš„èº«ä»½ç™»å½•çš„ï¼Œè¦åˆ›å»ºç”¨æˆ·éœ€è¦ root æƒé™ï¼Œè¿™é‡Œå°±è¦ç”¨åˆ° sudo è¿™ä¸ªå‘½ä»¤äº†ã€‚ä¸è¿‡ä½¿ç”¨è¿™ä¸ªå‘½ä»¤æœ‰ä¸¤ä¸ªå¤§å‰æï¼Œä¸€æ˜¯ä½ è¦çŸ¥é“å½“å‰ç™»å½•ç”¨æˆ·çš„å¯†ç ï¼ŒäºŒæ˜¯å½“å‰ç”¨æˆ·å¿…é¡»åœ¨ sudo ç”¨æˆ·ç»„ã€‚ sudoå‘½ä»¤ï¼šè·å¾—rootæƒé™ ç”¨æˆ·ç»„ æŸ¥çœ‹ï¼š åœ¨ Linux é‡Œé¢æ¯ä¸ªç”¨æˆ·éƒ½æœ‰ä¸€ä¸ªå½’å±ï¼ˆç”¨æˆ·ç»„ï¼‰ï¼Œç”¨æˆ·ç»„ç®€å•åœ°ç†è§£å°±æ˜¯ä¸€ç»„ç”¨æˆ·çš„é›†åˆï¼Œå®ƒä»¬å…±äº«ä¸€äº›èµ„æºå’Œæƒé™ï¼ŒåŒæ—¶æ‹¥æœ‰ç§æœ‰èµ„æº ã€‚ groups shiyanlou åŠ å…¥sudoç”¨æˆ·ç»„ su ï¼šåˆ‡æ¢ç”¨æˆ·userï¼Œéœ€è¦è¾“å…¥ç›®æ ‡ç”¨æˆ·å’Œå¯†ç  sudo usermod -G sudo lilei æ–‡ä»¶æ‰€ä»¥è€… su -l lilei su chown ä¿®æ”¹æƒé™ `sudo ` å¯ä»¥ä»¥ç‰¹æƒçº§åˆ«è¿è¡Œ cmd å‘½ä»¤ï¼Œéœ€è¦å½“å‰ç”¨æˆ·å±äº sudo ç»„ï¼Œä¸”éœ€è¦è¾“å…¥å½“å‰ç”¨æˆ·çš„å¯†ç ã€‚ æ–‡æ¡£ç¼–è¾‘ vimç¼–è¾‘å™¨ i esc :wq linuxæ–‡ä»¶ç³»ç»Ÿä¸ç£ç›˜ç®¡ç† $ tree / pwd cd â€¦: ä¸Šä¸€çº§ç›®å½• â€¦/ /:æ ¹ç›®å½•ï¼šç»å¯¹è·¯å¾„ cd /home/shiyanlou touch test mkdir mydir cpï¼ˆcopyï¼‰å‘½ä»¤å¤åˆ¶ä¸€ä¸ªæ–‡ä»¶åˆ°æŒ‡å®šç›®å½• è¦æˆåŠŸå¤åˆ¶ç›®å½•éœ€è¦åŠ ä¸Š -r æˆ–è€… -R å‚æ•°ï¼Œè¡¨ç¤ºé€’å½’å¤åˆ¶ cd /home/shiyanlou mkdir family â€‹ cp -r father family rm test è·Ÿå¤åˆ¶ç›®å½•ä¸€æ ·ï¼Œè¦åˆ é™¤ä¸€ä¸ªç›®å½•ï¼Œä¹Ÿéœ€è¦åŠ ä¸Š `-r` æˆ– `-R` å‚æ•° mv æºç›®å½•æ–‡ä»¶ ç›®çš„ç›®å½• ï¼Œå¯ä»¥ç”¨æ¥é‡å‘½åæ–‡ä»¶ $ cd /home/shiyanlou/ ä½¿ç”¨é€šé…ç¬¦æ‰¹é‡åˆ›å»º 5 ä¸ªæ–‡ä»¶: $ touch file{1â€¦5}.txt æ‰¹é‡å°†è¿™ 5 ä¸ªåç¼€ä¸º .txt çš„æ–‡æœ¬æ–‡ä»¶é‡å‘½åä¸ºä»¥ .c ä¸ºåç¼€çš„æ–‡ä»¶: $ rename â€˜s/.txt/.c/â€™ *.txt æ‰¹é‡å°†è¿™ 5 ä¸ªæ–‡ä»¶ï¼Œæ–‡ä»¶åå’Œåç¼€æ”¹ä¸ºå¤§å†™: $ rename â€˜y/a-z/A-Z/â€™ *.c æ–‡ä»¶æ‰“åŒ…å’Œè§£å‹ç¼© zipï¼š æ‰“åŒ… ï¼šzip something.zip something ï¼ˆç›®å½•è¯·åŠ  -r å‚æ•°ï¼‰ è§£åŒ…ï¼šunzip something.zip æŒ‡å®šè·¯å¾„ï¼š-d å‚æ•° tarï¼š æ‰“åŒ…ï¼štar -cf something.tar something è§£åŒ…ï¼štar -xf something.tar æŒ‡å®šè·¯å¾„ï¼š-C å‚æ•°ã€ tar -cf shiyanlou.tar /home/shiyanlou/Desktop `-c` è¡¨ç¤ºåˆ›å»ºä¸€ä¸ª tar åŒ…æ–‡ä»¶ï¼Œ`-f` ç”¨äºæŒ‡å®šåˆ›å»ºçš„æ–‡ä»¶åï¼Œæ³¨æ„æ–‡ä»¶åå¿…é¡»ç´§è·Ÿåœ¨ `-f` å‚æ•°ä¹‹å tar -xf shiyanlou.tar -C tardir è§£åŒ…ä¸€ä¸ªæ–‡ä»¶ï¼ˆ`-x` å‚æ•°ï¼‰åˆ°æŒ‡å®šè·¯å¾„çš„**å·²å­˜åœ¨**ç›®å½•ï¼ˆ`-C` å‚æ•°ï¼‰ tar -xzf shiyanlou.tar.gz å‹ç¼©æ–‡ä»¶æ ¼å¼ å‚æ•° *.tar.gz -z *.tar.xz -J *tar.bz2 -j linuxå®‰è£…è½¯ä»¶ Â¶è¾“å…¥ï¼Œè¾“å‡º è¾“å…¥ï¼šè¾“å…¥å½“ç„¶å°±æ˜¯æ‰“å¼€ç»ˆç«¯ï¼Œç„¶åæŒ‰é”®ç›˜è¾“å…¥ï¼Œç„¶åæŒ‰å›è½¦ï¼Œè¾“å…¥æ ¼å¼ä¸€èˆ¬å°±æ˜¯è¿™ç±»çš„ è¾“å‡ºï¼š è¾“å‡ºä¼šè¿”å›ä½ æƒ³è¦çš„ç»“æœï¼Œæ¯”å¦‚ä½ è¦çœ‹ä»€ä¹ˆæ–‡ä»¶ï¼Œå°±ä¼šè¿”å›æ–‡ä»¶çš„å†…å®¹ã€‚ å¦‚æœåªæ˜¯æ‰§è¡Œï¼Œæ‰§è¡Œå¤±è´¥ä¼šå‘Šè¯‰ä½ å“ªé‡Œé”™äº†ï¼Œå¦‚æœæ‰§è¡ŒæˆåŠŸé‚£ä¹ˆä¼šæ²¡æœ‰è¾“å‡ºï¼Œå› ä¸ºlinuxçš„å“²å­¦å°±æ˜¯ï¼šæ²¡æœ‰ç»“æœå°±æ˜¯æœ€å¥½çš„ç»“æœ Tab: å‘½ä»¤è¡¥å…¨ Ctrl+C:å¼ºåˆ¶ç»ˆæ­¢ å­¦ä¼šä½¿ç”¨é€šé…ç¬¦ï¼šé€šé…ç¬¦ï¼š*,? å­¦ä¼šåœ¨å‘½ä»¤è¡Œä¸­è·å–å¸®åŠ©ï¼šmanå‘½ä»¤è°ƒç”¨æ‰‹å†Œé¡µï¼Œ åŒºæ®µ è¯´æ˜ 1 ä¸€èˆ¬å‘½ä»¤ 2 ç³»ç»Ÿè°ƒç”¨ 3 åº“å‡½æ•°ï¼Œæ¶µç›–äº†Cæ ‡å‡†å‡½æ•°åº“ 4 ç‰¹æ®Šæ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯/devä¸­çš„è®¾å¤‡ï¼‰å’Œé©±åŠ¨ç¨‹åº 5 æ–‡ä»¶æ ¼å¼å’Œçº¦å®š 6 æ¸¸æˆå’Œå±ä¿ 7 æ‚é¡¹ 8 ç³»ç»Ÿç®¡ç†å‘½ä»¤å’Œå®ˆæŠ¤è¿›ç¨‹ 1man 1 ls 1ls --help]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ¸…æ™°æœ‰æ•ˆçš„æ•°æ®åˆ†ææ€è·¯]]></title>
    <url>%2F2019%2F11%2F26%2F%E6%B8%85%E6%99%B0%E6%9C%89%E6%95%88%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[Â¶å¦‚ä½•åšæ•°æ®åˆ†ææ±‡æŠ¥ Â¶1. æè¿°æ•°æ®çš„è¡¨å¾ æè¿°æ€§ç»Ÿè®¡ï¼š å¹³å‡æ•° ä¸­ä½æ•° ä¼—æ•° å‡ ä½•å¹³å‡æ•° è°ƒå’Œå¹³å‡æ•° å¹³å‡å€¼ ä¸­é—´ä½ç½®çš„æ•° å‡ºç°æ¬¡æ•°æœ€å¤š æ–¹å·® æ ‡å‡†å·® åˆ†å¸ƒ å¾—åˆ°ç¬¬ä¸€ä»½æ•°æ®ç»“è®º Â¶2. å¯»æ‰¾å˜åŒ–ï¼Œæ·±å…¥è§‚å¯Ÿ å‘ç”Ÿå˜åŒ–çš„æŒ‡æ ‡ä¸€èˆ¬å°±æ˜¯æŒ‡æ ‡å…³è”çš„ä¸šåŠ¡ç¯å¢ƒå‘ç”Ÿäº†æŸç§å˜åŒ–ã€‚é€šè¿‡è§‚å¯Ÿå˜åŒ–é‡ï¼Œå¯»æ‰¾å¯èƒ½çš„ä¸šåŠ¡é—®é¢˜ç‚¹ã€‚ åŒæ¯” å¯¹æ¯”åŒæœŸçš„å˜åŒ– å¦‚ï¼šä¸Šå‘¨äº”å’Œå»å¹´è¿‡å¹´ ç¯æ¯” å¯¹æ¯”è¿ç»­å‘¨æœŸ å¦‚ï¼šä»Šå¤©å’Œæ˜¨å¤©ï¼›æœ¬æœˆå’Œä¸Šæœˆï¼› å¢é•¿ç‡ è¯„ä¼°ç´¯è®¡å‹æŒ‡æ ‡çš„æœ‰åŠ›å·¥å…· å¦‚ï¼šæ”¶å…¥ æ—¶é—´ä¸Šçš„å¯¹æ¯”ï¼Œä¹Ÿç§°ä¸ºçºµæ¯” ï¼šç¯æ¯”ï¼ŒåŒæ¯” åŒçº§å•ä½ä¹‹é—´çš„æ¯”è¾ƒï¼Œç®€ç§°æ¨ªæ¯” ï¼š ä¸åŒçœä»½ä¹‹é—´çš„åˆ†æ å¾—åˆ°ç¬¬äºŒä»½æ•°æ®ç»“è®ºï¼Œå¯ä»¥åˆ†æåˆ°é—®é¢˜æ‰€åœ¨ã€‚ Â¶3.å…¨é¢è¯„ä¼°ï¼Œå¤šç»´åˆ†æ å¤šç»´åˆ†æï¼š ç»´åº¦æ˜¯æè¿°æŒ‡æ ‡ ä¸åŒè§’åº¦ï¼Œé€šè¿‡å¤šç»´åˆ†æï¼Œæ¥å¯»æ±‚æŒ‡æ ‡çš„å˜åŒ–çš„å¯ä»¥çš„åŸå› ã€‚ å¹¿ä¹‰çš„å¤šç»´åˆ†æï¼Œä¸ä»…ä»…åŒ…æ‹¬ä»æŒ‡æ ‡çš„ä¸åŒç»´åº¦è¿›è¡Œåˆ†æï¼Œä¹ŸåŒ…å«æ‹†åˆ†ä¸ºå¤šä¸ªå­æŒ‡æ ‡è¿›è¡Œåˆ†æã€‚ æŒ‡æ ‡ä½“ç³»+ç»´åº¦ä½“ç³» åŸºç¡€/é€šç”¨ å¹´é¾„ã€æ€§åˆ«ã€å­¦å†ã€åœ°åŸŸã€æ‰‹æœºå‹å·ã€æ“ä½œç³»ç»Ÿ äº§å“ äº§å“ç±»å‹ã€å½’å±ä¸šåŠ¡ è¿è¥ å½’å±æ¸ é“ã€æŠ•æ”¾å‘¨æœŸã€æ´»åŠ¨ç±»å‹ è¥é”€ å¸‚åœºæ¨å¹¿ã€è¥é”€æ–¹å¼ã€è¥é”€ç›®çš„ å¦‚ç²½å­çš„ç»´åº¦ äº§å“ç»´åº¦ è‚‰ç²½ï¼Œå¤§æ£æ£•ï¼Œç³–æ€» æ¸ é“ç»´åº¦ çº¿ä¸Šï¼šè‡ªæœ‰APP,ç”µå•†æ¸ é“ï¼Œåˆä½œæ¸ é“ï¼›çº¿ä¸‹ï¼šåˆä½œé—¨åº—ï¼Œå¤§å±å¹¿å‘Š æ—¶é—´ç»´åº¦ æŠ•æ”¾å‘¨æœŸï¼›æŠ•æ”¾æ—¶æ®µ åœ°åŸŸç»´åº¦ ç›´è¾–å¸‚ï¼›çœä¼šï¼›äºŒä¸‰çº¿åŸå¸‚ å¹´é¾„ç»´åº¦ å¹´é¾„æ®µ çº¿ä¸Šå…¥å£ splash,banner,å¼¹çª—ï¼Œè§’æ ‡ è¾“å…¥ï¼šç¬¬ä¸‰ä»½ç»“è®ºï¼›å•æŒ‡æ ‡åˆ†æï¼šå¾—åˆ°åˆ†æåˆ°ä¸Šå‡ï¼Œä¸‹é™çš„åŸå› ã€‚ Â¶4. å¤šæŒ‡æ ‡äº¤å‰åˆ†æ ç»´åº¦åå·®ï¼š å¤§æ•°æ®æ¶‰åŠçš„ç»´åº¦å¾ˆå¤šå•ä¸€ç»´åº¦åˆ†æä¼šå‡ºç°åå·®ï¼Œå¤šä¸ªç»´åº¦ç»„åˆèµ·æ¥çš„æ—¶å€™å¯èƒ½å¾—åˆ°ç›¸åçš„ç»“è®ºã€‚ å¹¸å­˜è€…åå·®ï¼šæ ·æœ¬çš„ä¸¢å¤±é—®é¢˜ ç¬¬å››ä»½åˆ†æç»“è®ºï¼šåˆ†æå¾—åˆ°å‡ºç°çš„é—®é¢˜ï¼Ÿ Â¶5. é‡åŒ–è¯„ä¼°ï¼Œå¯»æ‰¾å½’å›  ç›¸å…³æ€§åˆ†æï¼š åœ¨ä¸šåŠ¡ä¸­ï¼Œé€šè¿‡æ˜¯ä¸ºäº†é‡åŒ–è¯„ä¼°å„ç§å› ç´ å¯¹äºæ ¸å¿ƒæŒ‡æ ‡çš„å½±å“ç¨‹åº¦ï¼Œå¯»æ‰¾å¯¹ä¸šåŠ¡å½±å“çš„åŸå› ã€‚ ç›¸å…³æ€§åˆ†æï¼š å•å› ç´ ç›¸å…³æ€§åˆ†æ å¤šå› ç´ çš„ç›¸å…³æ€§åˆ†æ ç¬¬äº”ä»½åˆ†æç»“è®ºï¼š æ‰¾åˆ°äº†æ ¸å¿ƒå½±å“å› ç´ äº† Â¶6. å›åˆ°æœªæ¥ã€è¶‹åŠ¿é¢„æµ‹ è¶‹åŠ¿é¢„æµ‹ï¼š é¢„æµ‹åˆ†ææ˜¯ä¸€ç§ç»Ÿè®¡æˆ–æ•°æ®æŒ–æ˜è§£å†³æ–¹æ¡ˆ æ—¶é—´åºåˆ—é¢„æµ‹ï¼šä¸€èˆ¬æ—¶é—´åºåˆ—é¢„æµ‹ï¼›å­£èŠ‚æ€§æ—¶é—´åºåˆ—é¢„æµ‹ï¼›å¤åˆæ—¶é—´åºåˆ—é¢„æµ‹ æ•°å­¦å±‚é¢æ˜¯ä¸¥è°¨çš„ ç”¨ä¸€äº›æ•°æ®é¢„æµ‹æ–¹æ³•å’Œç®—æ³•ï¼šæŒ‡æ•°å¹³æ»‘æ¨¡å‹ ä¸šåŠ¡å±‚é¢æ˜¯æ˜“å˜çš„ å®é™…ä¸šåŠ¡ç¯å¢ƒä¸­ï¼Œå½±å“æœªæ¥å‘å±•çš„è¿˜ä¼šæœ‰è¡Œä¸šç¯å¢ƒçš„çªå˜ï¼Œèµ„æºçš„çªå˜ï¼Œäº§å“å®¢ç¾¤çš„çªå˜ç­‰ï¼Œäººä¸ºçš„å¹²æ‰°è¾ƒå¤§ã€‚ä¸šåŠ¡å±‚é¢çš„è¶‹åŠ¿é¢„æµ‹æ˜¯ä¸ç¨³å®šçš„ï¼Œä¸”æ˜“å˜çš„ å¾—åˆ°ç¬¬äº”ä»½ç»“è®ºï¼šæœªæ¥æ•ˆæœ Â¶7. åˆ†æçš„å®ç›¸ï¼Œè½åœ°ä¸šåŠ¡ åˆ†æçš„ç»“è®ºå’Œæ•°æ®é€»è¾‘ä¸ä¸šåŠ¡æ–¹â€”ç¡®è®¤ï¼Œæ•°æ®åˆ†æä¸€å®šè¦é—­ç¯ï¼Œå³ä»ä¸šåŠ¡ä¸­æ¥ï¼Œåˆ°ä¸šåŠ¡ä¸­å»ã€‚ Â¶æŒ‡æ ‡å’Œç»´çš„æ¦‚å¿µ æŒ‡æ ‡ â€‹ æŒ‡æ ‡:è¡¡é‡äº‹ç‰©å‘å±•ç¨‹åº¦çš„å•ä½å’Œæ–¹æ³•ï¼Œä¹Ÿå«åº¦é‡ã€‚å¦‚ï¼šäººå£æ•°ï¼ŒGDP, æ”¶å…¥ï¼Œç”¨æˆ·æ•°ï¼Œåˆ©æ¶¦åˆ©ï¼Œç•™å­˜ç‡ï¼Œè¦†ç›–ç‡ç­‰ã€‚ â€‹ æŒ‡æ ‡åˆ†ä¸ºï¼šç»å¯¹æ•°æŒ‡æ ‡å’Œç›¸å¯¹æ•°æŒ‡æ ‡ã€‚ç»å¯¹æŒ‡æ ‡ï¼šåæ˜ äº†è§„æ¨¡å¤§å°ï¼›ç›¸å¯¹æŒ‡æ ‡ï¼šåæ˜ äº†è´¨é‡å¥½åçš„æŒ‡æ ‡ã€‚ ç»´åº¦ äº‹ç‰©æˆ–è€…ç°è±¡çš„æŸç§ç‰¹å¾ï¼Œå¦‚æ€§åˆ«ï¼Œåœ°åŒºï¼Œæ—¶é—´ã€‚ â€‹ åˆ†ä¸ºå®šé‡ç»´åº¦å’Œå®šæ€§ç»´åº¦ã€‚å®šæ€§ï¼šå­—ç¬¦å‹æ•°æ®ï¼›å®šé‡ï¼šæ•°å€¼å‹ã€‚ åªæœ‰é€šè¿‡äº‹ç‰©å‘å±•çš„æ•°é‡ã€è´¨é‡ä¸¤å¤§æ–¹é¢ï¼Œä»æ¨ªæ¯”ã€çºµæ¯”è§’åº¦è¿›è¡Œå…¨æ–¹ä½çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬æ‰èƒ½å¤Ÿå…¨é¢çš„äº†è§£äº‹ç‰©å‘å±•çš„å¥½å é€šä¿—ä¸¾ä¸ªä¾‹å­ï¼š2019å¹´å„ä¸ªçœçº§çš„ç»æµå‘å±•çŠ¶å†µï¼šGDPæ€»é‡ï¼šæŒ‡æ ‡ï¼›çœä»½ï¼ŒäºŒä¸‰çº¿åŸå¸‚ï¼šç»´åº¦ï¼› â€‹ æ€»ç»“ï¼š æ•°æ®åˆ†æçš„å…¸å‹è¿‡ç¨‹ï¼›æŒ‡æ ‡æ‹†åˆ†ï¼Œç»´åº¦å¯¹æ¯”ï¼› äº§å“ï¼ˆProductï¼‰ï¼Œæ˜¯ç”¨æ¥æ»¡è¶³äººä»¬éœ€æ±‚å’Œæ¬²æœ›çš„ç‰©ä½“æˆ–æ— å½¢çš„è½½ä½“ã€‚äº§å“çš„å®ä½“ç§°ä¸ºä¸€èˆ¬äº§å“ã€‚äº§å“åŒ…å«äº†äº§å“çš„æ ¸å¿ƒåˆ©ç›Šï¼ˆå‘æ¶ˆè´¹è€…æä¾›çš„åŸºæœ¬æ•ˆç”¨å’Œåˆ©ç›Šï¼‰ 1. è½¯ä»¶ï¼Œé€šè®¯ï¼Œæ‰‹æœºï¼Œç§‘æŠ€äº§å“ å¸‚åœºæ˜¯æŒ‡ä¸€ç§è´§ç‰©æˆ–åŠ³åŠ¡çš„æ½œåœ¨è´­ä¹°è€…çš„é›†åˆéœ€æ±‚ã€‚ åœ¨å¸‚åœºè¥é”€ç»„åˆä¸­ï¼Œ 4P åˆ†åˆ«æ˜¯äº§å“( product) ã€ä»·æ ¼( price) ã€åœ°ç‚¹( place) ã€ä¿ƒé”€( promotion) è¥é”€æ˜¯åˆ›é€ ã€æ²Ÿé€šä¸ä¼ é€ä»·å€¼ç»™é¡¾å®¢ï¼ŒåŠç»è¥é¡¾å®¢å…³ç³»ä»¥ä¾¿è®©ç»„ç»‡ä¸å…¶åˆ©ç›Šå…³ç³»äººï¼ˆstakeholderï¼‰å—ç›Šçš„ä¸€ç§ç»„ç»‡åŠŸèƒ½ä¸ç¨‹åºã€‚ é€šä¿—åœ°è®²ï¼Œå°±æ˜¯é€šè¿‡å®£ä¼ ã€æ¨å¹¿ï¼Œè¿›è€Œä¿ƒè¿›äº§å“æˆ–æœåŠ¡çš„é”€å”®ã€‚ äº’è”ç½‘äº§å“å…¬å¸ä¸‰ä¸ªä¸šåŠ¡éƒ¨åˆ†ï¼šäº§å“ï¼ŒæŠ€æœ¯ï¼Œè¿è¥ äº§å“ï¼šæŠŠä¸œè¥¿æƒ³å‡ºæ¥ æŠ€æœ¯ï¼šæŠŠä¸œè¥¿åšå‡ºæ¥ è¿è¥ï¼šæŠŠä¸œè¥¿ç”¨èµ·æ¥ ä»å­—é¢ä¸Šçœ‹ï¼Œè¿ï¼Œæ˜¯è®©äº§å“ç»´æŒè¿è½¬ï¼›è¥ï¼Œæ˜¯è®©äº§å“è¿è½¬å¾—æ›´å¥½ï¼Œå°±æ˜¯è¦å¯¹ç”¨æˆ·ç¾¤ä½“è¿›è¡Œæœ‰ç›®çš„åœ°ç»„ç»‡å’Œç®¡ç†ï¼Œå¢åŠ ç”¨æˆ·æ•°é‡ã€ç”¨æˆ·ç²˜æ€§ã€ç”¨æˆ·è´¡çŒ®å’Œç”¨æˆ·å¿ è¯šåº¦ï¼Œè¿™ä¹Ÿå°±æ¶‰åŠåˆ°è¿è¥å·¥ä½œçš„ä¸‰ä¸ªé‡è¦æ–¹é¢ï¼šæ‹‰æ–°ã€ç•™å­˜ã€ä¿ƒæ´»ã€‚ ç†è§£é—®é¢˜â€“&gt; è®¾è®¡è§£å†³æ–¹æ¡ˆâ€“&gt; è¿­ä»£æ–¹æ¡ˆï¼Œç›´åˆ°é—®é¢˜è§£å†³ Â¶æ•°æ®åˆ†æå¸ˆçš„æŠ€èƒ½ä¹‹è·¯ week 01: Excelå­¦ä¹ æŒæ¡ week 02: æ•°æ®å¯è§†åŒ– week 03ï¼š åˆ†ææ€ç»´çš„è®­ç»ƒ week 04: æ•°æ®åº“å­¦ä¹  week 05: ç»Ÿè®¡çŸ¥è¯†å­¦ä¹  week 06: ä¸šåŠ¡å­¦ä¹ ï¼ˆç”¨æˆ·è¡Œä¸ºï¼Œäº§å“ï¼Œè¿è¥ï¼‰ week 07ï¼š Python/Rå­¦ä¹  Â¶æ•°æ®åˆ†æåº”æœ‰çš„é€»è¾‘æ€ç»´åŠåˆ†ææ–¹æ³• æå‡ºé—®é¢˜âŸåˆ†æé—®é¢˜âŸæå‡ºå‡è®¾âŸéªŒè¯å‡è®¾âŸè¾“å‡ºç»“è®º Â¶01 ç›®æ ‡æ€ç»´ åœ¨é™ˆè¿°é—®é¢˜æ—¶æ‰€ä½¿ç”¨çš„KWICæ–¹æ³•ï¼Œå…¶å®ä¹Ÿæ˜¯é€»è¾‘è¦ç´ çš„å»¶ä¼¸ï¼š 1ï¼‰Kï¼ˆKEYï¼‰ï¼šæ ¸å¿ƒè§‚ç‚¹ 2ï¼‰Wï¼ˆWidenï¼‰ï¼šæ‰©å±•æ ¸å¿ƒè§‚ç‚¹åŒ…å«çš„å†…å®¹ 3ï¼‰Iï¼ˆIllustrateï¼‰ï¼šä¸¾ä¾‹è¯´æ˜ä½è¯è§‚ç‚¹ 4ï¼‰Cï¼ˆConcludeï¼‰ï¼šæ€»ç»“ Â¶02 ç»“æ„åŒ–æ€ç»´ ç»“æ„åŒ–æ€ç»´èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬å°†æ— åºã€æ•£ä¹±çš„ä¿¡æ¯è¿›è¡Œèšç„¦ã€å½’çº³ã€åˆ†ç±»ã€‚ Â¶03 æ¨ç†æ€ç»´ ç¡®è®¤è®ºç‚¹ï¼Œç»“æ„åŒ–è®ºæ®ï¼Œä¸‹ä¸€æ­¥æ˜¯è®ºè¯ã€‚åœ¨è®ºè¯ä¸­è¿ç”¨æ¨ç†æ€ç»´èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬è¿…é€Ÿæ‰¾åˆ°é—®é¢˜çš„å¼‚åŒç‚¹ï¼Œä»è€Œå‘ç°å®ƒä»¬çš„è§„å¾‹ã€‚ å½’çº³æ³•ï¼ŒæŒ‡ä»ç‰¹æ®Šï¼ˆéƒ¨åˆ†æ ·æœ¬ï¼‰åˆ°ä¸€èˆ¬ï¼ˆå…¨é‡æ ·æœ¬ï¼‰çš„è¿‡ç¨‹ï¼Œé€šä¿—çš„è¯´æ˜¯ä»ä¸ªåˆ«çš„ç»éªŒå½’çº³å‡ºæ™®éè§„å¾‹çš„æ–¹æ³•ã€‚ è¿™å®è´¨ä¸Šæ˜¯ä»¥åæ¦‚å…¨çš„æ–¹æ³•ï¼Œä¸€æ—¦æœ‰ä¸€ä¸ªç”¨æˆ·ä¸æ»¡è¶³è¿™ä¸ªå‰æï¼Œè¿™ä¸ªç»“è®ºå°±æ— æ³•æˆç«‹ã€‚ åœ¨è¾“å‡ºç»“è®ºä¹‹å‰éœ€è¦åˆ¤æ–­æ ·æœ¬æ˜¯å¦è¶³å¤Ÿæœ‰ä»£è¡¨æ€§ï¼Œåˆ¤æ–­æ˜¯å¿…ç„¶äº‹ä»¶è¿˜æ˜¯éšæœºäº‹ä»¶ã€‚ 3-2ã€æ¼”ç»æ³• æ¼”ç»æ³•åˆ™ä¸å½’çº³æ³•ç›¸å,æ˜¯ä»æ—¢æœ‰ç»è¯å®çš„æ™®éæ€§ç»“è®ºï¼Œæ¨å¯¼å‡ºä¸ªåˆ«æ€§ç»“è®ºçš„ä¸€ç§æ–¹æ³•ï¼Œå¸¸è§çš„è¡¨ç°å½¢å¼æ˜¯é€»è¾‘ä¸‰æ®µè®ºã€‚ é€»è¾‘ä¸‰æ®µè®ºçš„æ ¼å¼ä¸ºï¼šå¤§å‰æã€å°å‰æã€ç»“è®ºã€‚ 3-3ã€å› æœå…³ç³»åˆ†ææ³• æšä¸¾å®Œæ¯•åï¼Œè¾©è¯æ—¶æé—®3ä¸ªé—®é¢˜ï¼š 1ï¼‰åŸå› æ˜¯å¦çœŸå®ï¼Ÿ 2ï¼‰ç»“æœæ˜¯å¦çœŸå® 3ï¼‰è¿™ä¸ªåŸå› ä¸€å®šä¼šå¼•èµ·è¿™ä¸ªç»“æœå—ï¼Ÿæ˜¯å¦æœ‰å…¶ä»–çš„åŸå› ï¼Ÿ Â¶æ•°æ®åˆ†æçš„æ–¹æ³• Â¶01 æ•°æ®åˆ†æå‰çš„å‡†å¤‡ 1-1ã€åˆ†æ¸…æ¥šç›®æ ‡å’ŒæŒ‡æ ‡ æ•°æ®åˆ†æï¼Œèƒ½å¸®åŠ©æˆ‘ä»¬äº†è§£ä¸šåŠ¡è¿è¡ŒçŠ¶å†µï¼Œå¹¶ä»ä¸­å‘ç°é—®é¢˜ã€ä¼˜åŒ–é—®é¢˜ã€‚å…¶æ¬¡ï¼Œè¿˜èƒ½å¤Ÿå¸®åŠ©æ´å¯Ÿä¸‹ä¸€ä¸ªå¢é•¿ç‚¹ã€‚ **æ•°æ®åˆ†æçš„æ„ä¹‰ï¼Œå¾€å¾€åœ¨æ•°æ®äº§ç”Ÿä¹‹å‰ã€‚**æˆ‘ä»¬åº”å›´ç»•äº§å“ç›®æ ‡ï¼Œè¿›è¡Œäº§å“è®¾è®¡ä»¥åŠè¿è¥ç­–åˆ’ã€‚ ç›®æ ‡æ˜¯ç»“æœï¼Œè€ŒæŒ‡æ ‡æ˜¯å¯¹ç»“æœåˆ†æ‹†çš„å…·ä½“è¦æ±‚ï¼Œæ˜¯å¯¹ç›®æ ‡çš„è¡¡é‡ã€‚ å‡è®¾æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æå‡å¹´åº¦æˆäº¤é‡‘é¢ï¼Œé‚£è¡¡é‡è¿™ä¸ªç›®æ ‡çš„æ–¹æ³•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ æ ¹æ®è¡¡é‡çš„æ–¹æ³•æˆ‘ä»¬æ‰èƒ½å®šå‘çš„è®¾ç½®è°ƒæ•´äº§å“è®¾è®¡åŠè¿è¥ç­–ç•¥ã€‚å¦‚æœç¼ºå°‘å¯è¡¡é‡ç›®æ ‡çš„å•ä½å’Œæ–¹æ³•ï¼Œç›®æ ‡ä¼šéš¾ä»¥è¾¾æˆã€‚ è€Œå›´ç»•ç›®æ ‡è®¾ç½®æ•°æ®çš„é‡‡é›†æ–¹æ¡ˆï¼Œå¯ä»¥å¤§å¤§èŠ‚çœæ•°æ®è¿‡æ»¤å’Œæ¸…æ´—çš„æ—¶é—´ã€‚ ç”šè‡³äºåœ¨æ˜ç¡®æŒ‡æ ‡åå†æœ€å¼€å§‹å°±è®¾ç½®å¥½åˆ†ææ¨¡å‹ï¼Œé€šè¿‡ç›‘æµ‹æ¨¡å‹ä¸­çš„æ•°æ®æƒ…å†µæ›´åŠæ—¶çš„å‘ç°é—®é¢˜ï¼Œåšå‡ºæ›´é«˜è´¨ã€é«˜æ•ˆçš„å†³ç­–ã€‚ 1-2ã€è¾¨åˆ«æŒ‡æ ‡çš„ç›®çš„ ç»“æœæŒ‡æ ‡ç”¨äºè¡¡é‡ç›®æ ‡ï¼Œè¿‡ç¨‹æŒ‡æ ‡ç”¨äºä½“ç°å¦‚ä½•å®Œæˆã€‚è§‚å¯ŸæŒ‡æ ‡åˆ™æŒ‡çš„å—å½±å“æŒ‡æ ‡ï¼Œå…¶æ˜¯å¦ä¼šå—åˆ°è‡ªå˜é‡ï¼ˆç»“æœæŒ‡æ ‡ï¼‰çš„å½±å“ï¼Œå¯¼è‡´ä¸Šå‡æˆ–ä¸‹é™ã€‚ åœ¨ä¸Šå›¾ä¸­ï¼ŒåŸºäºæˆäº¤è®¢å•æ•°ï¼Œè®¾ç½®è¿‡ç¨‹æŒ‡æ ‡ä¸ºè®¢å•å¹³å‡é‡‘é¢åŠå•†å“åˆ†å¸ƒèƒ½å¸®åŠ©æˆ‘ä»¬äº†è§£å®Œæˆçš„æ–¹å¼ã€‚ 1-3ã€ç¡®è®¤åˆ†æç±»å‹ åœ¨å®Œæˆç›®æ ‡å’ŒæŒ‡æ ‡åï¼Œä¸‹ä¸€æ­¥å°±æ˜¯åº”ç”¨ç»“æ„åŒ–æ€ç»´è¿›è¡Œæ‹†è§£å’Œå»¶ä¼¸ã€‚ æ‹†è§£å‡ºçš„æŒ‡æ ‡ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿæ ¹æ®ç›®çš„æˆ‘ä»¬æ‰èƒ½æœ‰å€¾å‘æ€§çš„åˆ†æã€‚ 1ï¼‰æè¿°æ€§åˆ†æ è¡¨ç°å½¢å¼ï¼šæ•°æ®æŠ¥è¡¨ æ•°æ®æŠ¥è¡¨èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬æè¿°äº‹ä»¶å‘å±•çš„æƒ…å†µï¼Œä½†å¾ˆéš¾è§£é‡ŠæŸç§ç»“æœå‘ç”Ÿçš„åŸå› å’Œæœªæ¥å¯èƒ½çš„è¶‹åŠ¿ã€‚ å®ƒæ›´åå‘ç»“æœæ€§çš„æè¿°ï¼Œæ­¤å‰çš„ç»“æœå¯¹æ­¤åæ˜¯ä¸å…·å¤‡å¤ªå¤šå‚è€ƒæ„ä¹‰çš„ã€‚ 2ï¼‰æµ‹æ€§åˆ†æ è¡¨ç°å½¢å¼ï¼šç”¨æˆ·ç›¸ä¼¼åº¦åŠç‰©å“ç›¸ä¼¼åº¦è®¡ç®—ã€ç”¨æˆ·è´­ä¹°é¥±å’Œåº¦ã€ç”¨æˆ·æˆäº¤å½±å“å› å­ é¢„æµ‹æ€§åˆ†æå¯ä»¥ç†è§£ä¸ºå¯¹ç»“æœå’Œå˜é‡çš„å…³ç³»è¿›è¡Œé¢„æµ‹çš„è¿‡ç¨‹ï¼ŒåŒ…å«ç›¸ä¼¼åº¦ã€ç›¸å…³æ€§åˆ†æã€å›å½’åˆ†æç­‰ã€‚ ç›¸ä¼¼åº¦å¤šç”¨äºæ¨èç®—æ³•ï¼Œé€šè¿‡è®¡ç®—ç”¨æˆ·çš„ç›¸ä¼¼åº¦å’Œå•†å“ç›¸ä¼¼åº¦ä»è€Œæ¨èç»™ç”¨æˆ·ã€‚è€Œç›¸å…³åˆ†æç”¨äºé¢„æµ‹å˜é‡çš„å…³è”æ€§ï¼Œå¦‚ç”¨æˆ·çš„æˆäº¤ä¼šå—ä»€ä¹ˆå› ç´ å½±å“ã€‚ 3ï¼‰å®è¯æ€§åˆ†æåŠè§„èŒƒæ€§åˆ†æ è¡¨ç°å½¢å¼ï¼šA/Bå®éªŒ å®è¯æ€§åˆ†æï¼ŒæŒ‡æ˜¯ä»€ä¹ˆï¼Œåå‘äºå®¢è§‚ï¼›è§„èŒƒæ€§åˆ†ææŒ‡åº”å½“åšä»€ä¹ˆï¼Œåå‘äºä¸»è§‚ã€‚ åœ¨å®é™…ä½¿ç”¨è¿‡ç¨‹ï¼Œä¸Šè¿°çš„4ç§åˆ†æç±»å‹å¸¸å¸¸ä¼šè¢«æ··åˆä½¿ç”¨ï¼Œæ··åˆä½¿ç”¨æ—¶åº”æ˜ç¡®ä¸åŒç±»å‹æˆ‘ä»¬åº”é‡‡å–çš„åˆ†æç»´åº¦ã€‚ æ•°æ®åˆ†ææ˜¯æœ‰é¡ºæ‰¿å…³ç³»çš„ï¼Œå…ˆé‡‡é›†äº‹å®ï¼Œå†æ ¹æ®äº‹å®æˆ–è€…é¢„æµ‹ï¼Œæå‡ºæˆ‘ä»¬çš„å‡è®¾ã€‚é€æ­¥ç°åº¦åœ°éªŒè¯å‡è®¾ï¼Œæœ€ç»ˆæ‰è¾“å‡ºæˆ‘ä»¬çš„ç»“è®ºã€‚ ä¸èƒ½å°†ä¸»è§‚çŒœæµ‹å¼ºåŠ äºäº‹å®ä¹‹ä¸Šï¼Œå·²ç»å‘ç”Ÿçš„ç»“æœå¹¶ä¸ä¸€å®šæ˜¯æœªæ¥çš„ç»“æœ 02 æ•°æ®åˆ†æå¦‚ä½•å¸¦æ¥é•¿æœŸä»·å€¼ ä¸ºäº†ä½¿æœ‰ç”¨åŠŸæ›´å¤šï¼Œä¸‹æ–‡å°†ä»ç”¨æˆ·å’Œæ”¶ç›Š2ä¸ªç»´åº¦åˆ†äº«æ•°æ®å¦‚ä½•ä¸ºæˆ‘ä»¬æ²‰æ·€é•¿æœŸä»·å€¼ã€‚ 2-1ã€äº†è§£æˆ‘ä»¬çš„ç”¨æˆ· 1ï¼‰åŸºç¡€ä¿¡æ¯ åŸºç¡€ä¿¡æ¯ï¼ŒæŒ‡ç”¨æˆ·æœ¬èº«çš„å±æ€§ã€‚ èº«ä»½ç‰¹å¾ï¼Œå¯ä»¥ä»è‡ªç„¶å±æ€§ã€ç¤¾ä¼šå±æ€§å‘ä¸‹ç»†åˆ†ï¼ŒåŒ…å«ç”¨æˆ·çš„æ€§åˆ«ã€å¹´é¾„ã€èŒä¸šã€æ•™è‚²ç­‰ã€‚ æ¸ é“å±æ€§ï¼ŒæŒ‡ç”¨æˆ·çš„æ³¨å†Œæ—¶é—´ã€æ³¨å†Œå¹³å°ã€æ³¨å†Œæ¥æºç­‰ã€‚ 2ï¼‰å†³ç­–ç±»å‹ **å†³ç­–ç±»å‹ï¼Œä¸»è¦åˆ†ä¸ºå†³ç­–å‘¨æœŸã€å“ç±»åå¥½ã€ä¿ƒé”€åå¥½ã€å¯¹è±¡åå¥½ï¼Œ**è¿™æ˜¯ç”¨æˆ·åˆ†æä¸­å¸¸å¸¸è¢«å¿½ç•¥çš„ä¸€æ–¹é¢ã€‚ å†³ç­–å‘¨æœŸä¸­çš„é¦–æ¬¡è®¿é—®ï¼ŒæŒ‡çš„é¦–æ¬¡è§¦åŠè¯¥å•†å“çš„æ—¶é—´ã€‚ç»“åˆæ¬¡æ•°ã€æ—¶é•¿ä»¥åŠæˆäº¤æ—¶é—´ï¼Œä»è€Œäº†è§£ç”¨æˆ·çš„å†³ç­–å‘¨æœŸã€‚ å“ç±»åå¥½ï¼Œç»“åˆå“ç‰Œå’Œå†å²æˆäº¤å•æ•°ï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬è·æ‚‰å“ç‰Œã€ä»·æ ¼ç»¼åˆå¯¹ç”¨æˆ·çš„å½±å“ã€‚ è€Œæˆäº¤å“ç±»ã€å•†å“ã€å•æ•°åˆ™æ˜¯å¸®åŠ©æˆ‘ä»¬ç†è§£å…¶å“ç±»è´­ä¹°æ·±åº¦åŠè·¯å¾„ï¼Œç”¨äºè¿›è¡Œå…³è”æ¨èå’Œè¯„åˆ¤ç”¨æˆ·çš„ä»·å€¼ã€‚ ä¿ƒé”€åå¥½ï¼Œç»“åˆå“ç±»å’ŒæŠ˜æ‰£é‡‘é¢äº†è§£ç”¨æˆ·çš„æ•æ„Ÿåº¦ï¼Œèƒ½æ›´å¥½çš„æé«˜å…¶è½¬åŒ–ç‡ã€‚å¯¹è±¡åå¥½ï¼ŒåŒæ ·æ˜¯äº†è§£è´­ä¹°æ·±åº¦åŠè·¯å¾„ï¼Œä¸è¿‡ç»´åº¦ä¸åŒã€‚ åœ¨ç”¨æˆ·å±‚é¢çš„åˆ†æï¼Œæ­¤å‰æ¥è§¦çš„ä¸€äº›æœ‹å‹éƒ½éå¸¸çƒ­è¡·äºä½¿ç”¨RFMæ¨¡å‹ï¼Œåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ä¹Ÿåº”â€œå› åœ°åˆ¶å®œâ€ã€‚ ** ** 3ï¼‰è´­ä¹°è·¯å¾„ å“ç±»æ·±åº¦ã€å¯¹è±¡æ·±åº¦æ˜¯å½±å“å†³ç­–ç±»å‹çš„å› å­ï¼Œå½“å®ƒä»¬åœ¨è´­ä¹°è·¯å¾„æ—¶åˆ™èšç„¦äºæ¬¡åºã€‚ æ ¹æ®æ¬¡åºï¼Œåˆ¶å®šè¿è¥çš„å‘åŠ›ç‚¹ï¼Œå†éµå¾ªç”¨æˆ·çš„è´­ä¹°è·¯å¾„åˆ¶å®šè½¬åŒ–è·¯å¾„ã€‚ åœ¨ç”¨æˆ·åˆ†å¸ƒç›¸å¯¹ç¨³å®šçš„å‰æä¸‹ï¼Œåº”é¡ºä»ç”¨æˆ·çš„è´­ä¹°è§„å¾‹è€Œéå€¾åŠ›äºå¦ä¸€æ¡ä¸»çº¿ã€‚ ä¸€ä¸“å¤šå¼ºçš„å‰ææ˜¯ä¸“ï¼Œåªæœ‰èšç„¦ä¼˜åŠ¿å“ç±»æˆ–ä¸»é¢˜å»ºç«‹äº†ä¼˜åŠ¿ï¼Œæ‰èƒ½ä¸ºå…¶ä»–çš„æ–¹å‘ä¾›åº”ç‚®å¼¹ã€‚ 4ï¼‰å¢é•¿è§‚å¯Ÿ å‰é¢è§£å†³çš„é—®é¢˜æ˜¯ï¼šä»–æ˜¯è°ï¼Œä¹°ä»€ä¹ˆä»¥åŠæ€ä¹ˆä¹°ã€‚æœ€åä¸€ç‚¹ï¼Œåˆ™æ˜¯å¢é•¿è§‚å¯Ÿã€‚ è´­ä¹°è·¯å¾„èšç„¦äºæ¬¡åºï¼Œå¢é•¿è§‚å¯Ÿèšç„¦äºæ·±åº¦ã€‚è´­ä¹°çš„æ¬¡åºæ˜¯è¿è¥çš„ä¸»çº¿ï¼Œè´­ä¹°çš„æ·±åº¦ç”¨äºç²¾ç»†åŒ–è¿è¥ã€‚ äº†è§£ç”¨æˆ·åœ¨å“ç±»å’Œå¯¹è±¡çš„è´­ä¹°æ·±åº¦ï¼Œå†è¾…ä»¥ARPUä¸LTVçš„æ¯”å¯¹ï¼Œä»ç”¨æˆ·çš„å‰©ä½™æ½œåŠ›å¯»æ‰¾å¹³å°å¢é•¿ç‚¹çš„æ–¹å¼ã€‚ 2-2ã€å»ºç«‹ä½ çš„ç”¨æˆ·æ¨¡å‹ å½“æ—¶æˆ‘æŠŠå¹³å°ç”¨æˆ·çš„åœ°åŸŸå¹´é¾„ã€æ€§åˆ«ç­‰åˆ†å¸ƒä»‹ç»äº†ä¸€ç•ªã€‚ç´§æ¥ç€ä»–æé—®ï¼šâ€œæ ¹æ®è¿™æ ·çš„ç”»åƒä½ èƒ½å¤Ÿåšä»€ä¹ˆå‘¢ï¼Ÿâ€ åŸºäºå¯¹ç”¨æˆ·çš„è®¤è¯†å»ºç«‹æ¨¡å‹ï¼Œä»¥ä¸Šä¸€å°èŠ‚çš„å†³ç­–æ¨¡å‹ä¸ºä¾‹ã€‚ å°†å†³ç­–ç±»å‹ã€å“ç±»åå¥½ã€å¯¹è±¡åå¥½ã€ä¿ƒé”€åå¥½4ä¸ªå› å­çš„å…³è”ï¼Œå¹¶è¾…ä»¥ç”¨æˆ·çš„åŸºç¡€ä¿¡æ¯è¿›è¡Œç»„åˆã€‚ å¦‚ï¼šâ€œç²¾æ‰“ç»†ç®—ã€ä¸“æ³¨å¤§ç‰Œã€ç–¼çˆ±å­©å­çš„æ¯äº²â€ã€‚ è¿™æ ·ä¸€æ¥å†°å†·çš„æ•°æ®ä¹Ÿè¢«èµ‹äºˆäº†æƒ…æ„ŸåŒ–çš„è¡¨è¾¾ï¼Œæ— è®ºæ˜¯äº§å“è®¾è®¡ã€äº¤äº’è®¾è®¡ã€äº§å“è¿è¥éƒ½ä¼šå˜å¾—å®¹æ˜“çš„å¤šã€‚ å»ºç«‹èµ·ç”¨æˆ·æ¨¡å‹ï¼Œæ‰èƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œæƒ…æ„ŸåŒ–è®¾è®¡ã€ç²¾ç»†åŒ–è¿è¥ã€‚ https://mp.weixin.qq.com/s/eWYiHNJ57aXtqygitnwVqw]]></content>
      <categories>
        <category>æ•°æ®åˆ†æ</category>
      </categories>
      <tags>
        <tag>æ•°æ®åˆ†æ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apriori]]></title>
    <url>%2F2019%2F11%2F25%2FApriori%2F</url>
    <content type="text"><![CDATA[Apriori ç®—æ³•æ˜¯ä¸€ç§æŒ–æ˜å…³è”è§„åˆ™çš„é¢‘ç¹é¡¹é›†çš„ç®—æ³•ã€‚ Â¶å¼•è¨€ å¯¹äºç‰¹å¾æ„æˆçš„é›†åˆ$A$, å¦‚æœåˆ—å‡ºéç©ºé›†åˆæœ‰$a^{|A|}-1$ç§ï¼Œå¤ªææ€–äº†ã€‚ Apriorç®—æ³•ï¼šæ ¸å¿ƒæƒ³æ³•æ˜¯ L_1æ˜¯é¢‘ç¹çš„ï¼Œåˆ™å…¶å­é›†ä¹Ÿæ˜¯é¢‘ç¹çš„ã€‚ L_1æ˜¯éé¢‘ç¹çš„ï¼Œåˆ™å…¶è¶…é›†æ˜¯éé¢‘ç¹çš„ è¿™æ ·çš„åŒ–ï¼Œå°±å¤§å¤§å‡å°äº†æœç´¢ç©ºé—´äº†ã€‚ Apriorç®—æ³•çš„è¿‡ç¨‹ï¼š $C_i$ï¼šè¡¨ç¤ºæ•°æ®é›†ç”Ÿæˆå€™é€‰é¡¹é›† $L_i$:è¡¨ç¤ºç”Ÿæˆçš„é¢‘ç¹é¡¹é›† $C_{k-1}$äº§ç”Ÿ$L_k$ Â¶æ”¯æŒåº¦ $$ support({A,B}) = num{AUB}/W = P(A \ bing \ B) $$ W:æ€»çš„è®°å½•ï¼Œ Â¶ç½®ä¿¡åº¦ $$ Confidence(A-&gt;B) = support({A,B})/support(B) = P(B/A) $$ æ³¨æ„ï¼šsupport(B)å’ŒConfidence(A-&gt;B)çš„å½±å“ï¼Œ Â¶åºåˆ—æ¨¡å‹ è€ƒè™‘æ—¶é—´ï¼Œå¦‚å‘¨ä¸€ä¹°ä¸€å †å¯¹è±¡ï¼Œå‘¨äºŒä¹°ä¸€å †ä¸œè¥¿ $$ t= {t_1,t_2,â€¦,t_n}\ s = {s_1,s_2,â€¦,s_n} $$ &lt;{s1},{s_2}&gt;æ˜¯æ­£ç¡®çš„ &lt;{s1,s2}}&gt;æ˜¯é”™è¯¯çš„è¡¨è¾¾]]></content>
      <categories>
        <category>æ•°æ®æŒ–æ˜</category>
      </categories>
      <tags>
        <tag>å…³è”è§„åˆ™</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ç®€å•çš„æ•°æ®æ¢ç´¢]]></title>
    <url>%2F2019%2F11%2F20%2F%E7%AE%80%E5%8D%95%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[ç®€å•çš„æ¢ç´¢æ•°æ®çš„æ–¹æ³• æ€»ç»“ä¸€äº›ç®€å•çš„æ•°æ®åˆ†ææ–¹æ³•ï¼Œä»¥åŠå¸¸ç”¨çš„python åº“ Pandasé‡Œé¢ç›¸åº”çš„å‡½æ•°ã€‚ Â¶ç»Ÿè®¡æ±‡æ€» Â¶å•ä¸ªç‰¹å¾ 1decrible() # ç»™å‡ºæ ·æœ¬çš„åŸºæœ¬ç»Ÿè®¡é‡ é¢‘ç‡ ä¼—æ•° ç™¾åˆ†ä½æ•° ä½ç½®åº¦é‡ å‡å€¼å’Œæ–¹å·® æ•£å¸ƒåº¦é‡ï¼š æå·®å’Œæ–¹å·®]]></content>
      <categories>
        <category>æ•°æ®æŒ–æ˜</category>
        <category>æ•°æ®åˆ†æ</category>
      </categories>
      <tags>
        <tag>æ•°æ®æ¢ç´¢</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F11%2F15%2Ftest%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Python Basics]]></title>
    <url>%2F2019%2F05%2F28%2FPython-basic%2F</url>
    <content type="text"><![CDATA[é‡æ–°å­¦ä¹  å¼€å§‹å¾ˆä¹±çš„å­¦ä¹ Pythonï¼Œç°åœ¨æƒ³ç³»ç»Ÿå­¦ä¹ åŸºç¡€ï¼ŒçœŸæ­£äº†è§£pythonic,]]></content>
      <categories>
        <category>ç¼–ç¨‹è¯­è¨€</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Networks]]></title>
    <url>%2F2019%2F05%2F12%2FDeel%20Learning%20ai_Convolutional%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[C4 : Convolutional Neural Networks(å·ç§¯ç¥ç»ç½‘ç»œ) Â¶W1 :Convolutional Neural Networks(å·ç§¯ç¥ç»ç½‘ç»œ) Â¶L1: Computer Vision Image classification Object detection Neural Style Transfer Problem : input big ç¥ç»ç½‘ç»œç»“æ„å¤æ‚ï¼Œæ•°æ®é‡ç›¸å¯¹è¾ƒå°‘ï¼Œå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆï¼› æ‰€éœ€å†…å­˜å’Œè®¡ç®—é‡å·¨å¤§ã€‚ Â¶L2: Edge detection example æˆ‘ä»¬ä¹‹å‰æåˆ°è¿‡ï¼Œç¥ç»ç½‘ç»œç”±æµ…å±‚åˆ°æ·±å±‚ï¼Œåˆ†åˆ«å¯ä»¥æ£€æµ‹å‡ºå›¾ç‰‡çš„è¾¹ç¼˜ç‰¹å¾ã€å±€éƒ¨ç‰¹å¾ï¼ˆä¾‹å¦‚çœ¼ç›ã€é¼»å­ç­‰ï¼‰ï¼Œåˆ°æœ€åé¢çš„ä¸€å±‚å°±å¯ä»¥æ ¹æ®å‰é¢æ£€æµ‹çš„ç‰¹å¾æ¥è¯†åˆ«æ•´ä½“é¢éƒ¨è½®å»“ã€‚è¿™äº›å·¥ä½œéƒ½æ˜¯ä¾æ‰˜å·ç§¯ç¥ç»ç½‘ç»œæ¥å®ç°çš„ã€‚ **å·ç§¯è¿ç®—ï¼ˆConvolutional Operationï¼‰**æ˜¯å·ç§¯ç¥ç»ç½‘ç»œæœ€åŸºæœ¬çš„ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬ä»¥è¾¹ç¼˜æ£€æµ‹ä¸ºä¾‹ï¼Œæ¥è§£é‡Šå·ç§¯æ˜¯æ€æ ·è¿ç®—çš„ã€‚ å¸¸è§çš„è¾¹ç¼˜æ£€æµ‹ å‚ç›´è¾¹ç¼˜ï¼ˆVertical Edges) å’Œ æ°´å¹³è¾¹ç¼˜ï¼ˆhorizontal Edges) ![](Deel Learning ai_Convolutional Neural Networks\Different-edges.png) è¿™å¼ å›¾çš„æ æ†å°±å¯¹åº”å‚ç›´çº¿ï¼Œæ æ†çš„æ°´å¹³çº¿æ˜¯æ°´å¹³è¾¹ç¼˜ã€‚ é‚£ä¹ˆå›¾ç‰‡æ˜¯æ€ä¹ˆæ£€æµ‹è¾¹ç¼˜çš„å‘¢ï¼Ÿ è¿‡æ»¤å™¨ï¼šfilter åœ¨æ•°å­¦ä¸­â€œâ€å°±æ˜¯å·ç§¯çš„æ ‡å‡†æ ‡å¿—ï¼Œä½†æ˜¯åœ¨Pythonä¸­ï¼Œè¿™ä¸ªæ ‡è¯†å¸¸å¸¸è¢«ç”¨æ¥è¡¨ç¤ºä¹˜æ³•æˆ–è€…å…ƒç´ ä¹˜æ³•ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\example_1.png) Output; 4 by 4 ![](Deel Learning ai_Convolutional Neural Networks\example_1â€”â€”2.png) å…·ä½“è¿ç®—ï¼š 1ï¼‰ ä¸ºäº†è®¡ç®—ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œåœ¨4Ã—4å·¦ä¸Šè§’çš„é‚£ä¸ªå…ƒç´ ï¼Œä½¿ç”¨3Ã—3çš„è¿‡æ»¤å™¨ï¼Œå°†å…¶è¦†ç›–åœ¨è¾“å…¥å›¾åƒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ç„¶åè¿›è¡Œå…ƒç´ ä¹˜æ³•ï¼ˆelement-wise productsï¼‰è¿ç®— ![](Deel Learning ai_Convolutional Neural Networks\example_1_3png.png) 2ï¼‰ä¸ºäº†å¼„æ˜ç™½ç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä»€ä¹ˆï¼Œä½ è¦æŠŠè“è‰²çš„æ–¹å—ï¼Œå‘å³ç§»åŠ¨ä¸€æ­¥ï¼Œåƒè¿™æ ·ï¼ŒæŠŠè¿™äº›ç»¿è‰²çš„æ ‡è®°å»æ‰ï¼š ![](Deel Learning ai_Convolutional Neural Networks\example_1_4png.png) 6Ã—6çŸ©é˜µå’Œ3Ã—3çŸ©é˜µè¿›è¡Œå·ç§¯è¿ç®—å¾—åˆ°4Ã—4çŸ©é˜µã€‚è¿™äº›å›¾ç‰‡å’Œè¿‡æ»¤å™¨æ˜¯ä¸åŒç»´åº¦çš„çŸ©é˜µï¼Œä½†å·¦è¾¹çŸ©é˜µå®¹æ˜“è¢«ç†è§£ä¸ºä¸€å¼ å›¾ç‰‡ï¼Œä¸­é—´çš„è¿™ä¸ªè¢«ç†è§£ä¸ºè¿‡æ»¤å™¨ï¼Œå³è¾¹çš„å›¾ç‰‡æˆ‘ä»¬å¯ä»¥ç†è§£ä¸ºå¦ä¸€å¼ å›¾ç‰‡ã€‚è¿™ä¸ªå°±æ˜¯å‚ç›´è¾¹ç¼˜æ£€æµ‹å™¨ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Convolutional-operation.jpg) ä¸¾ä¾‹è¯´æ˜ï¼š Vertical edge detection ![](Deel Learning ai_Convolutional Neural Networks\example_1_5.png) è¿™é‡Œåœ¨ç»“æœå¯èƒ½æœ‰ç‚¹ä¸å¯¹å¤´ï¼Œæ£€æµ‹åˆ°çš„è¾¹ç¼˜å¤ªç²—äº†ï¼Œä¸»è¦æ˜¯å›¾ç‰‡å¤ªå°äº†ï¼Œ å·ç§¯æ“ä½œAPI åœ¨ Python ä¸­ï¼Œå·ç§¯ç”¨conv_forward()è¡¨ç¤ºï¼› åœ¨ Tensorflow ä¸­ï¼Œå·ç§¯ç”¨tf.nn.conv2d()è¡¨ç¤ºï¼› åœ¨ keras ä¸­ï¼Œå·ç§¯ç”¨Conv2D()è¡¨ç¤ºã€‚ Â¶L3: Edge Detection Example é¢œè‰²ç”±æš—åˆ°äº®ï¼Œè¿˜æ˜¯äº®åˆ°æš— ![](Deel Learning ai_Convolutional Neural Networks\example_2_1.png) ![](Deel Learning ai_Convolutional Neural Networks\example_2_2.png) è¿™ç§æ»¤æ³¢å™¨å¯ä»¥åŒºåˆ†æ˜æš—å˜åŒ–ï¼Œå–ç»å¯¹å€¼æ²¡æœ‰åŒºåˆ«äº† æ°´å¹³è¾¹ç¼˜ ä¸Šè¾¹ç›¸å¯¹è¾ƒäº®ï¼Œè€Œä¸‹æ–¹ç›¸å¯¹è¾ƒæš— ![](Deel Learning ai_Convolutional Neural Networks\example_2_3.png) å¤æ‚æ —å­ ![](Deel Learning ai_Convolutional Neural Networks\example_2_4.png) è¿™å—åŒºåŸŸå·¦è¾¹ä¸¤åˆ—æ˜¯æ­£è¾¹ï¼Œå³è¾¹ä¸€åˆ—æ˜¯è´Ÿè¾¹ï¼Œæ­£è¾¹å’Œè´Ÿè¾¹çš„å€¼åŠ åœ¨ä¸€èµ·å¾—åˆ°äº†ä¸€ä¸ªä¸­é—´å€¼ã€‚ä½†å‡å¦‚è¿™ä¸ªä¸€ä¸ªéå¸¸å¤§çš„1000Ã—1000çš„ç±»ä¼¼è¿™æ ·æ£‹ç›˜é£æ ¼çš„å¤§å›¾ï¼Œå°±ä¸ä¼šå‡ºç°è¿™äº›äº®åº¦ä¸º10çš„è¿‡æ¸¡å¸¦äº†ï¼Œå› ä¸ºå›¾ç‰‡å°ºå¯¸å¾ˆå¤§ï¼Œè¿™äº›ä¸­é—´å€¼å°±ä¼šå˜å¾—éå¸¸å°ã€‚ filter ![](Deel Learning ai_Convolutional Neural Networks\example_2_5.png) sobelè¿‡æ»¤å™¨ï¼Œä¼˜ç‚¹åœ¨äºå¢åŠ äº†ä¸­é—´ä¸€è¡Œå…ƒç´ çš„æƒé‡ï¼Œè¿™ä½¿å¾—ç»“æœçš„é²æ£’æ€§ä¼šæ›´é«˜ä¸€äº›ã€‚ charrè¿‡æ»¤å™¨ï¼Œå®ƒæœ‰ç€å’Œä¹‹å‰å®Œå…¨ä¸åŒçš„ç‰¹æ€§ï¼Œå®é™…ä¸Šä¹Ÿæ˜¯ä¸€ç§å‚ç›´è¾¹ç¼˜æ£€æµ‹ï¼Œå¦‚æœä½ å°†å…¶ç¿»è½¬90åº¦ï¼Œä½ å°±èƒ½å¾—åˆ°å¯¹åº”æ°´å¹³è¾¹ç¼˜æ£€æµ‹ã€‚ å­¦ä¹ çš„å…¶ä¸­ä¸€ä»¶äº‹å°±æ˜¯å½“ä½ çœŸæ­£æƒ³å»æ£€æµ‹å‡ºå¤æ‚å›¾åƒçš„è¾¹ç¼˜ï¼Œä½ ä¸ä¸€å®šè¦å»ä½¿ç”¨é‚£äº›ç ”ç©¶è€…ä»¬æ‰€é€‰æ‹©çš„è¿™ä¹ä¸ªæ•°å­—ï¼Œä½†ä½ å¯ä»¥ä»ä¸­è·ç›ŠåŒªæµ…ã€‚æŠŠè¿™çŸ©é˜µä¸­çš„9ä¸ªæ•°å­—å½“æˆ9ä¸ªå‚æ•°ï¼Œå¹¶ä¸”åœ¨ä¹‹åä½ å¯ä»¥å­¦ä¹ ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•ï¼Œå…¶ç›®æ ‡å°±æ˜¯å»ç†è§£è¿™9ä¸ªå‚æ•°ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\example_2_6.png) è¿™æ ·å¯èƒ½å¾—åˆ°ä¸€ä¸ªå‡ºè‰²çš„è¾¹ç¼˜æ£€æµ‹ ç›¸æ¯”è¿™ç§å•çº¯çš„å‚ç›´è¾¹ç¼˜å’Œæ°´å¹³è¾¹ç¼˜ï¼Œå®ƒå¯ä»¥æ£€æµ‹å‡º45Â°æˆ–70Â°æˆ–73Â°ï¼Œç”šè‡³æ˜¯ä»»ä½•è§’åº¦çš„è¾¹ç¼˜ã€‚æ‰€ä»¥å°†çŸ©é˜µçš„æ‰€æœ‰æ•°å­—éƒ½è®¾ç½®ä¸ºå‚æ•°ï¼Œé€šè¿‡æ•°æ®åé¦ˆï¼Œè®©ç¥ç»ç½‘ç»œè‡ªåŠ¨å»å­¦ä¹ å®ƒä»¬ï¼Œæˆ‘ä»¬ä¼šå‘ç°ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ ä¸€äº›ä½çº§çš„ç‰¹å¾ï¼Œä¾‹å¦‚è¿™äº›è¾¹ç¼˜çš„ç‰¹å¾ã€‚ ä¸ç®¡æ˜¯å‚ç›´çš„è¾¹ç¼˜ï¼Œæ°´å¹³çš„è¾¹ç¼˜ï¼Œè¿˜æœ‰å…¶ä»–å¥‡æ€ªè§’åº¦çš„è¾¹ç¼˜ï¼Œç”šè‡³æ˜¯å…¶å®ƒçš„è¿åå­—éƒ½æ²¡æœ‰çš„è¿‡æ»¤å™¨ã€‚ Â¶Padding æŒ‰ç…§æˆ‘ä»¬ä¸Šé¢è®²çš„å›¾ç‰‡å·ç§¯ï¼Œå¦‚æœåŸå§‹å›¾ç‰‡å°ºå¯¸ä¸º$n x n$ï¼Œfilterå°ºå¯¸ä¸º$f x f$ï¼Œåˆ™å·ç§¯åçš„å›¾ç‰‡å°ºå¯¸ä¸º$(n-f+1) x (n-f+1)$ï¼Œæ³¨æ„fä¸€èˆ¬ä¸ºå¥‡æ•°ã€‚è¿™æ ·ä¼šå¸¦æ¥ä¸¤ä¸ªé—®é¢˜ï¼š å·ç§¯è¿ç®—åï¼Œè¾“å‡ºå›¾ç‰‡å°ºå¯¸ç¼©å° åŸå§‹å›¾ç‰‡è¾¹ç¼˜ä¿¡æ¯å¯¹è¾“å‡ºè´¡çŒ®å¾—å°‘ï¼Œè¾“å‡ºå›¾ç‰‡ä¸¢å¤±è¾¹ç¼˜ä¿¡æ¯ è¾¹ç¼˜åƒç´ ç‚¹åªè¢«ä¸€ä¸ªè¾“å‡ºæ‰€è§¦ç¢°æˆ–è€…ä½¿ç”¨ï¼Œ ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¯ä»¥åœ¨è¿›è¡Œå·ç§¯æ“ä½œå‰ï¼Œå¯¹åŸå§‹å›¾ç‰‡åœ¨è¾¹ç•Œä¸Šè¿›è¡Œå¡«å……ï¼ˆPaddingï¼‰ï¼Œä»¥å¢åŠ çŸ©é˜µçš„å¤§å°ã€‚é€šå¸¸å°† 0 ä½œä¸ºå¡«å……å€¼ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Padding.jpg) ç»è¿‡paddingä¹‹åï¼Œå¡«å……p,åŸå§‹å›¾ç‰‡å°ºå¯¸ä¸º$(n+2p) x (n+2p)$ï¼Œfilterå°ºå¯¸ä¸º$f x f$ï¼Œåˆ™å·ç§¯åçš„å›¾ç‰‡å°ºå¯¸ä¸º$(n+2p-f+1) x (n+2p-f+1)$ã€‚è‹¥è¦ä¿è¯å·ç§¯å‰åå›¾ç‰‡å°ºå¯¸ä¸å˜ï¼Œåˆ™påº”æ»¡è¶³ï¼š$ p=(f-1)/2$,fé€šå¸¸æ˜¯å¥‡æ•°ï¼Œå¦‚æœæ˜¯å¶æ•°ï¼Œé€ æˆä¸å¯¹ç§°å¡«å……ï¼Œç¬¬äºŒä¸ªåŸå› æ˜¯å½“ä½ æœ‰ä¸€ä¸ªå¥‡æ•°ç»´è¿‡æ»¤å™¨ï¼Œæ¯”å¦‚3Ã—3æˆ–è€…5Ã—5çš„ï¼Œå®ƒå°±æœ‰ä¸€ä¸ªä¸­å¿ƒç‚¹ã€‚æœ‰æ—¶åœ¨è®¡ç®—æœºè§†è§‰é‡Œï¼Œå¦‚æœæœ‰ä¸€ä¸ªä¸­å¿ƒåƒç´ ç‚¹ä¼šæ›´æ–¹ä¾¿ï¼Œä¾¿äºæŒ‡å‡ºè¿‡æ»¤å™¨çš„ä½ç½® p=0,Valid convolution p=((f-1))/2,Same convolution Â¶L05: Strided convolutionï¼ˆå·ç§¯æ­¥é•¿ï¼‰ Strideè¡¨ç¤ºfilteråœ¨åŸå›¾ç‰‡ä¸­æ°´å¹³æ–¹å‘å’Œå‚ç›´æ–¹å‘æ¯æ¬¡çš„æ­¥è¿›é•¿åº¦ã€‚ä¹‹å‰æˆ‘ä»¬é»˜è®¤stride=1ã€‚è‹¥stride=2ï¼Œåˆ™è¡¨ç¤ºfilteræ¯æ¬¡æ­¥è¿›é•¿åº¦ä¸º2ï¼Œå³éš”ä¸€ç‚¹ç§»åŠ¨ä¸€æ¬¡ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Stride.jpg) æˆ‘ä»¬ç”¨sè¡¨ç¤ºstrideé•¿åº¦ï¼Œpè¡¨ç¤ºpaddingé•¿åº¦ï¼Œå¦‚æœåŸå§‹å›¾ç‰‡å°ºå¯¸ä¸ºn x nï¼Œfilterå°ºå¯¸ä¸ºf x fï¼Œåˆ™å·ç§¯åçš„å›¾ç‰‡å°ºå¯¸ä¸ºï¼š $$ \left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor X\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor $$ å‘ä¸‹å–æ•´ ç›®å‰ä¸ºæ­¢æˆ‘ä»¬å­¦ä¹ çš„â€œå·ç§¯â€å®é™…ä¸Šè¢«ç§°ä¸ºäº’ç›¸å…³ï¼ˆcross-correlationï¼‰ï¼Œè€Œéæ•°å­¦æ„ä¹‰ä¸Šçš„å·ç§¯ã€‚çœŸæ­£çš„å·ç§¯æ“ä½œåœ¨åšå…ƒç´ ä¹˜ç§¯æ±‚å’Œä¹‹å‰ï¼Œè¦å°†æ»¤æ³¢å™¨æ²¿æ°´å¹³å’Œå‚ç›´è½´ç¿»è½¬ï¼ˆç›¸å½“äºæ—‹è½¬ 180 åº¦ï¼‰ã€‚å› ä¸ºè¿™ç§ç¿»è½¬å¯¹ä¸€èˆ¬ä¸ºæ°´å¹³æˆ–å‚ç›´å¯¹ç§°çš„æ»¤æ³¢å™¨å½±å“ä¸å¤§ï¼ŒæŒ‰ç…§æœºå™¨å­¦ä¹ çš„æƒ¯ä¾‹ï¼Œæˆ‘ä»¬é€šå¸¸ä¸è¿›è¡Œç¿»è½¬æ“ä½œï¼Œåœ¨ç®€åŒ–ä»£ç çš„åŒæ—¶ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚ ![](Deel Learning ai_Convolutional Neural Networks\example_2_6.png) äº’ç›¸å…³ï¼šè¿‡æ»¤å™¨æ²¿æ°´å¹³å’Œå‚ç›´è½´ç¿»è½¬ï¼Œå…ƒç´ ç›¸ä¹˜æ¥è®¡ç®—ï¼Œè¿™äº›è§†é¢‘ä¸­å®šä¹‰å·ç§¯è¿ç®—æ—¶ï¼Œæˆ‘ä»¬è·³è¿‡äº†è¿™ä¸ªé•œåƒæ“ä½œã€‚ï¼ˆä¸è¿›è¡Œç¿»è½¬æ“ä½œï¼‰å«åšå·ç§¯æ“ä½œ Â¶L06: Convolution over volumes(ä¸‰ç»´å·ç§¯) å·ç§¯è¿ç®— ![](Deel Learning ai_Convolutional Neural Networks\Convolutions-on-RGB-image.png) è¿‡ç¨‹æ˜¯å°†æ¯ä¸ªå•é€šé“ï¼ˆRï¼ŒGï¼ŒBï¼‰ä¸å¯¹åº”çš„filterè¿›è¡Œå·ç§¯è¿ç®—æ±‚å’Œï¼Œç„¶åå†å°†3é€šé“çš„å’Œç›¸åŠ ï¼Œå¾—åˆ°è¾“å‡ºå›¾ç‰‡çš„ä¸€ä¸ªåƒç´ å€¼ã€‚ ä¸åŒé€šé“çš„æ»¤æ³¢ç®—å­å¯ä»¥ä¸ç›¸åŒã€‚ä¾‹å¦‚Ré€šé“filterå®ç°å‚ç›´è¾¹ç¼˜æ£€æµ‹ï¼ŒGå’ŒBé€šé“ä¸è¿›è¡Œè¾¹ç¼˜æ£€æµ‹ï¼Œå…¨éƒ¨ç½®é›¶ï¼Œæˆ–è€…å°†Rï¼ŒGï¼ŒBä¸‰é€šé“filterå…¨éƒ¨è®¾ç½®ä¸ºæ°´å¹³è¾¹ç¼˜æ£€æµ‹ã€‚ ä¸ºäº†è¿›è¡Œå¤šä¸ªå·ç§¯è¿ç®—ï¼Œå®ç°æ›´å¤šè¾¹ç¼˜æ£€æµ‹ï¼Œå¯ä»¥å¢åŠ æ›´å¤šçš„æ»¤æ³¢å™¨ç»„ã€‚ä¾‹å¦‚è®¾ç½®ç¬¬ä¸€ä¸ªæ»¤æ³¢å™¨ç»„å®ç°å‚ç›´è¾¹ç¼˜æ£€æµ‹ï¼Œç¬¬äºŒä¸ªæ»¤æ³¢å™¨ç»„å®ç°æ°´å¹³è¾¹ç¼˜æ£€æµ‹ã€‚è¿™æ ·ï¼Œä¸åŒæ»¤æ³¢å™¨ç»„å·ç§¯å¾—åˆ°ä¸åŒçš„è¾“å‡ºï¼Œä¸ªæ•°ç”±æ»¤æ³¢å™¨ç»„å†³å®šã€‚ ![](Deel Learning ai_Convolutional Neural Networks\example_2_8.png) ä¸ºäº†è¿›è¡Œå¤šä¸ªå·ç§¯è¿ç®—ï¼Œå®ç°æ›´å¤šè¾¹ç¼˜æ£€æµ‹ï¼Œå¯ä»¥å¢åŠ æ›´å¤šçš„æ»¤æ³¢å™¨ç»„ã€‚ä¾‹å¦‚è®¾ç½®ç¬¬ä¸€ä¸ªæ»¤æ³¢å™¨ç»„å®ç°å‚ç›´è¾¹ç¼˜æ£€æµ‹ï¼Œç¬¬äºŒä¸ªæ»¤æ³¢å™¨ç»„å®ç°æ°´å¹³è¾¹ç¼˜æ£€æµ‹ã€‚è¿™æ ·ï¼Œä¸åŒæ»¤æ³¢å™¨ç»„å·ç§¯å¾—åˆ°ä¸åŒçš„è¾“å‡ºï¼Œä¸ªæ•°ç”±æ»¤æ³¢å™¨ç»„å†³å®šã€‚ ![](Deel Learning ai_Convolutional Neural Networks\example_2_9.png) è‹¥è¾“å…¥å›¾ç‰‡çš„å°ºå¯¸ä¸ºn x n x ncï¼Œnc: é€šé“æ•°ç›®ï¼Œfilterå°ºå¯¸ä¸ºf x f x ncï¼Œåˆ™å·ç§¯åçš„å›¾ç‰‡å°ºå¯¸ä¸º(n-f+1) x (n-f+1) x ncâ€²ã€‚å…¶ä¸­ï¼Œncä¸ºå›¾ç‰‡é€šé“æ•°ç›®ï¼Œncâ€²ä¸ºæ»¤æ³¢å™¨ç»„ä¸ªæ•°ã€‚ Â¶L7 : One layer of a convolution network (å•å±‚ç¥ç»ç½‘ç»œ) ![](Deel Learning ai_Convolutional Neural Networks\example_2_10.png) ![](Deel Learning ai_Convolutional Neural Networks\example_2_11.png) ![](Deel Learning ai_Convolutional Neural Networks\example_2_12.png) CNNå•å±‚çš„æ‰€ä»¥æ ‡è®°ç¬¦å·ï¼Œè®¾å±‚æ•°$l$, $$ \begin{array}{l}{f^{[l]}=\text { filter size }} \ {p^{[l]}=\text { padding }} \ {g^{[l]}=\text { stride }} \ {n_{c}^{[l]}=\text { number of filters }}\end{array} $$ ![](Deel Learning ai_Convolutional Neural Networks\example_2_13.png) $$ \begin{array}{c}{n_{H}{[l]}=\left\lfloor\frac{n_{H}{[l-1]}+2 p{[l]}-f{[l]}}{s^{[l]}}+1\right\rfloor} \ { n_{W}{[l]}=\left\lfloor\frac{n_{W}{[l-1]}+2 p{[l]}-f{[l]}}{s^{[l]}}+1\right\rfloor}\end{array} $$ å¦‚æœ$m$ä¸ªæ ·æœ¬ï¼Œè¿›è¡Œå‘é‡åŒ–è¿ç®—ï¼Œç›¸åº”çš„è¾“å‡ºç»´åº¦ï¼Œä¸º $$ \mathrm{m} \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]} $$ Â¶L8 : A simple convolution network exampleï¼ˆç®€å•å·ç§¯ç½‘ç»œç¤ºä¾‹ï¼‰ ![](Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-33.jpg) ä¸€èˆ¬è€Œè¨€ï¼Œå›¾ç‰‡çš„height $n^{[l]}_{H}$å’Œwidth $n^{[l]}_W$éšç€å±‚æ•°çš„å¢åŠ é€æ¸é™ä½ï¼Œä½†channel $n^{[l]}_C$é€æ¸å¢åŠ ã€‚ CNNæœ‰ä¸‰ç§ç±»å‹çš„layerï¼š Convolutionå±‚ï¼ˆCONVï¼‰ Poolingå±‚ï¼ˆPOOLï¼‰ Fully connectedå±‚ï¼ˆFCï¼‰ Â¶L9: Pooling layers(æ± åŒ–å±‚) å·ç§¯ç¥ç»ç½‘ç»œé™¤äº†å·ç§¯å±‚ï¼Œè¿˜æœ‰æ± åŒ–å±‚æ¥ç¼©å‡æ¨¡å‹çš„å¤§å°ï¼Œæé«˜è¿ç®—é€Ÿåº¦å’Œé²æ£’æ€§ æ± çš„ç±»å‹æœ‰max pooling(æœ€å¤§æ± åŒ–) ![](Deel Learning ai_Convolutional Neural Networks\example_2_14.png) ![](Deel Learning ai_Convolutional Neural Networks\example_2_15.png) è¿™é‡Œæ­¥å¹…æ˜¯s=2ï¼Œfilter = 2*2æ˜¯æœ€å¤§æ± åŒ–çš„è¶…å‚æ•°,å¦‚æœæ˜¯ä¸‰ç»´ï¼Œåˆ™å•ç‹¬åœ¨æ¯ä¸ªé€šé“æ‰§è¡Œæœ€å¤§æ± åŒ–æ“ä½œ å…³äºmax poolingçš„ç›´è§‰è§£é‡Šï¼š å…ƒç´ è¾ƒå¤§çš„å€¼ï¼Œå¯èƒ½æ˜¯å·ç§¯è¿‡ç¨‹ä¸­æå–åˆ°çš„æŸäº›ç‰¹å¾ï¼ˆæ¯”å¦‚è¾¹ç•Œï¼‰ï¼Œè€Œmax poolingåˆ™åœ¨å‹ç¼©äº†çŸ©é˜µå¤§å°çš„æƒ…å†µä¸‹ï¼Œä¿ç•™æ¯ä¸ªåˆ†åŒºå†…æœ€å¤§çš„è¾“å‡ºï¼Œå³ä¿ç•™äº†æå–çš„ç‰¹å¾ã€‚ä½†ç†è®ºä¸Šè¿˜æ²¡æœ‰è¯æ˜max poolingçš„åŸç†ï¼Œmax poolingåº”ç”¨çš„åŸå› æ˜¯åœ¨å®è·µä¸­æ•ˆæœå¾ˆå¥½ã€‚ Pooling layer: Average pooling ![](Deel Learning ai_Convolutional Neural Networks\example_2_16.png) ä½†æ˜¯æœ€å¤§æ± åŒ–æ›´å¥½ç”¨ summary : è¾“å…¥$n_Hn_Wn_C$,å¦‚æœæ²¡æœ‰padding,è¾“å‡º$(n_h-f)/s+1*(n_w-f)/s+1*n_c$ ![](Deel Learning ai_Convolutional Neural Networks\example_2_17.png) Â¶L10: Convolutional neural network example (å·ç§¯ç¥ç»ç½‘ç»œå®ä¾‹) åšä¸€ä¸ªè¯†åˆ«æ•°å­—çš„CNNç½‘ç»œ LeNet-5æ¶æ„å¦‚ä¸‹ï¼š ![](Deel Learning ai_Convolutional Neural Networks\CNN.jpg) é€šå¸¸Conv Layerå’ŒPooling Layeråˆåœ¨ä¸€èµ·ç®—ä¸€ä¸ªlayerï¼Œå› ä¸ºpooling layerå¹¶æ²¡æœ‰å‚æ•°è®­ç»ƒ å¸¸è§çš„ç»“æ„ï¼šConv ==&gt; Pool ==&gt; Conv ==&gt; Pool ==&gt; FC ==&gt; FC ==&gt; softmax æœ€ç»ˆè¿˜ä¼šç”¨FCå±‚ï¼ˆå…¨è¿æ¥å±‚ï¼‰ï¼Œä¸ä¸€èˆ¬NNçš„å¤„ç†ä¸€æ ·ï¼›å¹¶åœ¨è¾“å‡ºå±‚ï¼Œåº”ç”¨softmaxå¾—åˆ°10ä¸ªæ•°å­—çš„æ¦‚ç‡ã€‚ åœ¨æ•´ä¸ªç½‘ç»œä¸­ï¼ŒHeightå’ŒWidthæ˜¯é€æ¸é€’å‡çš„ï¼Œä½†channelå’Œfilteræ˜¯é€’å¢çš„ã€‚ å…³äºCNNå¦‚ä½•é€‰æ‹©è¶…å‚ï¼šå¯ä»¥å‚è€ƒè®ºæ–‡çš„ç»éªŒã€‚ ![](Deel Learning ai_Convolutional Neural Networks\example_2_18.png) Activation shape Activation Size #parameters Input: (32, 32, 3) 3072 0 CONV1(f=5, s=1) (28, 28, 6) 4704 156 (=556+6) POOL1 (14, 14, 6) 1176 0 CONV2(f=5, s=1) (10, 10, 16) 1600 416 (=5516+16) POOL2 (5, 5, 16) 400 0 FC3 (120, 1) 120 48120 (=120*400+120) FC4 (84, 1) 84 10164 (=84*120+84) Softmax (10, 1) 10 850 (=10*84+10) Â¶L11 Why convolution å‚æ•°å…±äº«ï¼ˆparameter sharing) å¦‚æœç”¨FCçš„è¯ï¼Œå‚æ•°çˆ†ç‚¸å•Šï¼å¦‚æœconv layer å°±éœ€è¦filteræ£€æµ‹å™¨ï¼Œè¿™ä¸ªå‚æ•°å°±å°‘äº†ï¼Œè¿˜å‚æ•°å…±äº« ![](Deel Learning ai_Convolutional Neural Networks\example_2_19.png) ç¨€ç–è¿æ¥(sparsity of connection) è¾“å‡ºä¸­çš„æ¯ä¸ªå•å…ƒä»…å’Œè¾“å…¥çš„ä¸€ä¸ªå°åˆ†åŒºç›¸å…³ï¼Œæ¯”å¦‚è¾“å‡ºçš„å·¦ä¸Šè§’çš„åƒç´ ä»…ä»…ç”±è¾“å…¥å·¦ä¸Šè§’çš„9ä¸ªåƒç´ å†³å®šï¼ˆå‡è®¾filterå¤§å°æ˜¯3*3ï¼‰ï¼Œè€Œå…¶ä»–è¾“å…¥éƒ½ä¸ä¼šå½±å“ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\example_2_20.png) Â¶summary 1. å·ç§¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬æ„é€ å’Œè®¡ç®—è¿‡ç¨‹ 2. å¦‚ä½•æ•´åˆè¿™äº›æ¨¡å‹ 3. å“ªäº›è¶…å‚æ•° 4. ä¸ºä»€ä¹ˆä½¿ç”¨å·ç§¯ Â¶W2 : Deep convolutional models: case studies(æ·±åº¦å·ç§¯ç½‘ç»œï¼šå®ä¾‹æ¢ç©¶) Â¶L1 : Why look at case studies?(ä¸ºä»€ä¹ˆè¦è¿›è¡Œå®ä¾‹æ¢ç©¶ï¼Ÿ) æœ¬æ–‡å°†ä¸»è¦ä»‹ç»å‡ ä¸ªå…¸å‹çš„CNNæ¡ˆä¾‹ã€‚é€šè¿‡å¯¹å…·ä½“CNNæ¨¡å‹åŠæ¡ˆä¾‹çš„ç ”ç©¶ï¼Œæ¥å¸®åŠ©æˆ‘ä»¬ç†è§£çŸ¥è¯†å¹¶è®­ç»ƒå®é™…çš„æ¨¡å‹ã€‚ å…¸å‹çš„CNNæ¨¡å‹åŒ…æ‹¬ï¼š LeNet-5 AlexNet VGG è¿˜ä¼šä»‹ç»Residual Networkï¼ˆResNetï¼‰ã€‚å…¶ç‰¹ç‚¹æ˜¯å¯ä»¥æ„å»ºå¾ˆæ·±å¾ˆæ·±çš„ç¥ç»ç½‘ç»œï¼ˆç›®å‰æœ€æ·±çš„å¥½åƒæœ‰152å±‚ï¼‰ã€‚è¿˜ä¼šä»‹ç»Inception Neural Network Â¶L2 : Classic networks(ç»å…¸ç½‘ç»œ) Â¶1. LeNet-5 LeNet-5æ˜¯é’ˆå¯¹ç°åº¦å›¾ç‰‡è®­ç»ƒçš„ï¼Œä½¿ç”¨6ä¸ª5Ã—5çš„è¿‡æ»¤å™¨ï¼Œæ­¥å¹…ä¸º1ã€‚ç”±äºä½¿ç”¨äº†6ä¸ªè¿‡æ»¤å™¨ï¼Œæ­¥å¹…ä¸º1ï¼Œpaddingä¸º0ï¼Œè¾“å‡ºç»“æœä¸º28Ã—28Ã—6ï¼Œå›¾åƒå°ºå¯¸ä»32Ã—32ç¼©å°åˆ°28Ã—28ã€‚ç„¶åè¿›è¡Œæ± åŒ–æ“ä½œï¼Œåœ¨è¿™ç¯‡è®ºæ–‡å†™æˆçš„é‚£ä¸ªå¹´ä»£ï¼Œäººä»¬æ›´å–œæ¬¢ä½¿ç”¨å¹³å‡æ± åŒ–ï¼Œè€Œç°åœ¨æˆ‘ä»¬å¯èƒ½ç”¨æœ€å¤§æ± åŒ–æ›´å¤šä¸€äº›ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œå¹³å‡æ± åŒ–ï¼Œè¿‡æ»¤å™¨çš„å®½åº¦ä¸º2ï¼Œæ­¥å¹…ä¸º2ï¼Œå›¾åƒçš„å°ºå¯¸ï¼Œé«˜åº¦å’Œå®½åº¦éƒ½ç¼©å°äº†2å€ï¼Œè¾“å‡ºç»“æœæ˜¯ä¸€ä¸ª14Ã—14Ã—6çš„å›¾åƒã€‚æˆ‘è§‰å¾—è¿™å¼ å›¾ç‰‡åº”è¯¥ä¸æ˜¯å®Œå…¨æŒ‰ç…§æ¯”ä¾‹ç»˜åˆ¶çš„ï¼Œå¦‚æœä¸¥æ ¼æŒ‰ç…§æ¯”ä¾‹ç»˜åˆ¶ï¼Œæ–°å›¾åƒçš„å°ºå¯¸åº”è¯¥åˆšå¥½æ˜¯åŸå›¾åƒçš„ä¸€åŠã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-34.jpg) è¯¥LeNetæ¨¡å‹æ€»å…±åŒ…å«äº†å¤§çº¦6ä¸‡ä¸ªå‚æ•°ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå½“æ—¶Yann LeCunæå‡ºçš„LeNet-5æ¨¡å‹æ± åŒ–å±‚ä½¿ç”¨çš„æ˜¯average poolï¼Œè€Œä¸”å„å±‚æ¿€æ´»å‡½æ•°ä¸€èˆ¬æ˜¯Sigmoidå’Œtanhã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦ï¼Œåšå‡ºæ”¹è¿›ï¼Œä½¿ç”¨max poolå’Œæ¿€æ´»å‡½æ•°ReLUã€‚ Â¶1. AlexNet AlexNetæ¨¡å‹æ˜¯ç”±Alex Krizhevskyã€Ilya Sutskeverå’ŒGeoffrey Hintonå…±åŒæå‡ºçš„ï¼Œå…¶ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š ![](Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-35.jpg) AlexNeté¦–å…ˆç”¨ä¸€å¼ 227Ã—227Ã—3çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼Œå®é™…ä¸ŠåŸæ–‡ä¸­ä½¿ç”¨çš„å›¾åƒæ˜¯224Ã—224Ã—3ï¼Œä½†æ˜¯å¦‚æœä½ å°è¯•å»æ¨å¯¼ä¸€ä¸‹ï¼Œä½ ä¼šå‘ç°227Ã—227è¿™ä¸ªå°ºå¯¸æ›´å¥½ä¸€äº›ã€‚ç¬¬ä¸€å±‚æˆ‘ä»¬ä½¿ç”¨96ä¸ª11Ã—11çš„è¿‡æ»¤å™¨ï¼Œæ­¥å¹…ä¸º4ï¼Œç”±äºæ­¥å¹…æ˜¯4ï¼Œå› æ­¤å°ºå¯¸ç¼©å°åˆ°55Ã—55ï¼Œç¼©å°äº†4å€å·¦å³ã€‚ç„¶åç”¨ä¸€ä¸ª3Ã—3çš„è¿‡æ»¤å™¨æ„å»ºæœ€å¤§æ± åŒ–å±‚,f=3ï¼Œæ­¥å¹…ä¸º2ï¼Œå·ç§¯å±‚å°ºå¯¸ç¼©å°ä¸º27Ã—27Ã—96ã€‚æ¥ç€å†æ‰§è¡Œä¸€ä¸ª5Ã—5çš„å·ç§¯ï¼Œpaddingä¹‹åï¼Œè¾“å‡ºæ˜¯27Ã—27Ã—276ã€‚ç„¶åå†æ¬¡è¿›è¡Œæœ€å¤§æ± åŒ–ï¼Œå°ºå¯¸ç¼©å°åˆ°13Ã—13ã€‚å†æ‰§è¡Œä¸€æ¬¡sameå·ç§¯ï¼Œç›¸åŒçš„paddingï¼Œå¾—åˆ°çš„ç»“æœæ˜¯13Ã—13Ã—384ï¼Œ384ä¸ªè¿‡æ»¤å™¨ã€‚å†åšä¸€æ¬¡sameå·ç§¯ï¼Œå°±åƒè¿™æ ·ã€‚å†åšä¸€æ¬¡åŒæ ·çš„æ“ä½œï¼Œæœ€åå†è¿›è¡Œä¸€æ¬¡æœ€å¤§æ± åŒ–ï¼Œå°ºå¯¸ç¼©å°åˆ°6Ã—6Ã—256ã€‚6Ã—6Ã—256ç­‰äº9216ï¼Œå°†å…¶å±•å¼€ä¸º9216ä¸ªå•å…ƒï¼Œç„¶åæ˜¯ä¸€äº›å…¨è¿æ¥å±‚ã€‚æœ€åä½¿ç”¨softmaxå‡½æ•°è¾“å‡ºè¯†åˆ«çš„ç»“æœï¼Œçœ‹å®ƒç©¶ç«Ÿæ˜¯1000ä¸ªå¯èƒ½çš„å¯¹è±¡ä¸­çš„å“ªä¸€ä¸ªã€‚ å®é™…ä¸Šï¼Œè¿™ç§ç¥ç»ç½‘ç»œä¸LeNetæœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ï¼Œä¸è¿‡AlexNetè¦å¤§å¾—å¤šã€‚æ­£å¦‚å‰é¢è®²åˆ°çš„LeNetæˆ–LeNet-5å¤§çº¦æœ‰6ä¸‡ä¸ªå‚æ•°ï¼Œè€ŒAlexNetåŒ…å«çº¦6000ä¸‡ä¸ªå‚æ•°ã€‚å½“ç”¨äºè®­ç»ƒå›¾åƒå’Œæ•°æ®é›†æ—¶ï¼ŒAlexNetèƒ½å¤Ÿå¤„ç†éå¸¸ç›¸ä¼¼çš„åŸºæœ¬æ„é€ æ¨¡å—ï¼Œè¿™äº›æ¨¡å—å¾€å¾€åŒ…å«ç€å¤§é‡çš„éšè—å•å…ƒæˆ–æ•°æ®ï¼Œè¿™ä¸€ç‚¹AlexNetè¡¨ç°å‡ºè‰²ã€‚AlexNetæ¯”LeNetè¡¨ç°æ›´ä¸ºå‡ºè‰²çš„å¦ä¸€ä¸ªåŸå› æ˜¯å®ƒä½¿ç”¨äº†ReLuæ¿€æ´»å‡½æ•°ã€‚åŸä½œè€…è¿˜æåˆ°äº†ä¸€ç§ä¼˜åŒ–æŠ€å·§ï¼Œå«åšLocal Response Normalization(LRN)ã€‚ è€Œåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒLRNçš„æ•ˆæœå¹¶ä¸çªå‡ºã€‚ Â¶3. VGG-16 ![](Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-36.jpg) é¦–å…ˆç”¨3Ã—3ï¼Œæ­¥å¹…ä¸º1çš„è¿‡æ»¤å™¨æ„å»ºå·ç§¯å±‚ï¼Œpaddingå‚æ•°ä¸ºsameå·ç§¯ä¸­çš„å‚æ•°ã€‚ç„¶åç”¨ä¸€ä¸ª2Ã—2ï¼Œæ­¥å¹…ä¸º2çš„è¿‡æ»¤å™¨æ„å»ºæœ€å¤§æ± åŒ–å±‚ã€‚å› æ­¤VGGç½‘ç»œçš„ä¸€å¤§ä¼˜ç‚¹æ˜¯å®ƒç¡®å®ç®€åŒ–äº†ç¥ç»ç½‘ç»œç»“æ„ï¼Œä¸‹é¢æˆ‘ä»¬å…·ä½“è®²è®²è¿™ç§ç½‘ç»œç»“æ„ã€‚ æ•°å­—16ï¼Œå°±æ˜¯æŒ‡åœ¨è¿™ä¸ªç½‘ç»œä¸­åŒ…å«16ä¸ªå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚ã€‚æ€»å…±åŒ…å«çº¦1.38äº¿ä¸ªå‚æ•° Â¶L3 : Residual Networks (ResNets)(æ®‹å·®ç½‘ç»œ(ResNets)) æˆ‘ä»¬çŸ¥é“ï¼Œå¦‚æœç¥ç»ç½‘ç»œå±‚æ•°è¶Šå¤šï¼Œç½‘ç»œè¶Šæ·±ï¼Œæºäºæ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„å½±å“ï¼Œæ•´ä¸ªæ¨¡å‹éš¾ä»¥è®­ç»ƒæˆåŠŸã€‚è§£å†³çš„æ–¹æ³•ä¹‹ä¸€æ˜¯äººä¸ºåœ°è®©ç¥ç»ç½‘ç»œæŸäº›å±‚è·³è¿‡ä¸‹ä¸€å±‚ç¥ç»å…ƒçš„è¿æ¥ï¼Œéš”å±‚ç›¸è¿ï¼Œå¼±åŒ–æ¯å±‚ä¹‹é—´çš„å¼ºè”ç³»ã€‚è¿™ç§ç¥ç»ç½‘ç»œè¢«ç§°ä¸ºResidual Networks(ResNets)ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\example_2_21.png) ![](Deel Learning ai_Convolutional Neural Networks\Residual-Network.jpg) ![](Deel Learning ai_Convolutional Neural Networks\ResNet-Training-Error.jpg) Â¶L4: Why ResNets work?(æ®‹å·®ç½‘ç»œä¸ºä»€ä¹ˆæœ‰ç”¨ï¼Ÿ) ![](Deel Learning ai_Convolutional Neural Networks\example_2_22.png) å› æ­¤ï¼Œè¿™ä¸¤å±‚é¢å¤–çš„æ®‹å·®å—ä¸ä¼šé™ä½ç½‘ç»œæ€§èƒ½ã€‚è€Œå¦‚æœæ²¡æœ‰å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±æ—¶ï¼Œè®­ç»ƒå¾—åˆ°çš„éçº¿æ€§å…³ç³»ä¼šä½¿å¾—è¡¨ç°æ•ˆæœè¿›ä¸€æ­¥æé«˜ã€‚ æ³¨æ„ï¼Œå¦‚æœ$ a[l]$ä¸ $a[l+2]$çš„ç»´åº¦ä¸åŒï¼Œéœ€è¦å¼•å…¥çŸ©é˜µ $W_s$ä¸ $a_{[l]}$ç›¸ä¹˜ï¼Œä½¿å¾—äºŒè€…çš„ç»´åº¦ç›¸åŒ¹é…ã€‚å‚æ•°çŸ©é˜µ $W_s$æ—¢å¯ä»¥é€šè¿‡æ¨¡å‹è®­ç»ƒå¾—åˆ°ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå›ºå®šå€¼ï¼Œä»…ä½¿ $a[l]$æˆªæ–­æˆ–è€…è¡¥é›¶ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Xnip2018-07-04_08-28-37.jpg) Â¶L5 : Network in Network and 1Ã—1 convolutions(ç½‘ç»œä¸­çš„ç½‘ç»œä»¥åŠ 1Ã—1 å·ç§¯) ä½œç”¨ å‡è®¾è¿™æ˜¯ä¸€ä¸ª28Ã—28Ã—192çš„è¾“å…¥å±‚ï¼Œä½ å¯ä»¥ä½¿ç”¨æ± åŒ–å±‚å‹ç¼©å®ƒçš„é«˜åº¦å’Œå®½åº¦ï¼Œè¿™ä¸ªè¿‡ç¨‹æˆ‘ä»¬å¾ˆæ¸…æ¥šã€‚ä½†å¦‚æœé€šé“æ•°é‡å¾ˆå¤§ï¼Œè¯¥å¦‚ä½•æŠŠå®ƒå‹ç¼©ä¸º28Ã—28Ã—32ç»´åº¦çš„å±‚å‘¢ï¼Ÿä½ å¯ä»¥ç”¨32ä¸ªå¤§å°ä¸º1Ã—1çš„è¿‡æ»¤å™¨ï¼Œä¸¥æ ¼æ¥è®²æ¯ä¸ªè¿‡æ»¤å™¨å¤§å°éƒ½æ˜¯1Ã—1Ã—192ç»´ï¼Œå› ä¸ºè¿‡æ»¤å™¨ä¸­é€šé“æ•°é‡å¿…é¡»ä¸è¾“å…¥å±‚ä¸­é€šé“çš„æ•°é‡ä¿æŒä¸€è‡´ã€‚ä½†æ˜¯ä½ ä½¿ç”¨äº†32ä¸ªè¿‡æ»¤å™¨ï¼Œè¾“å‡ºå±‚ä¸º28Ã—28Ã—32ï¼Œè¿™å°±æ˜¯å‹ç¼©é€šé“æ•°ï¼ˆ$n_c$ï¼‰çš„æ–¹æ³•ï¼Œå¯¹äºæ± åŒ–å±‚æˆ‘åªæ˜¯å‹ç¼©äº†è¿™äº›å±‚çš„é«˜åº¦å’Œå®½åº¦ ![](Deel Learning ai_Convolutional Neural Networks\example_2_23.png) doing something pretty non-trivial å®ƒç»™ç¥ç»ç½‘ç»œæ·»åŠ äº†ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œä»è€Œå‡å°‘æˆ–ä¿æŒè¾“å…¥å±‚ä¸­çš„é€šé“æ•°é‡ä¸å˜ï¼Œå½“ç„¶å¦‚æœä½ æ„¿æ„ï¼Œä¹Ÿå¯ä»¥å¢åŠ é€šé“æ•°é‡ã€‚ Â¶L6 : Inception network motivation(è°·æ­Œ Inception ç½‘ç»œç®€ä»‹) ![](Deel Learning ai_Convolutional Neural Networks\99f8fc7dbe7cd0726f5271aae11b9872.png) æœ‰äº†è¿™æ ·çš„Inceptionæ¨¡å—ï¼Œä½ å°±å¯ä»¥è¾“å…¥æŸä¸ªé‡ï¼Œå› ä¸ºå®ƒç´¯åŠ äº†æ‰€æœ‰æ•°å­—ï¼Œè¿™é‡Œçš„æœ€ç»ˆè¾“å‡ºä¸º32+32+128+64=256ã€‚æœ‰äº†è¿™æ ·çš„Inceptionæ¨¡å—ï¼Œä½ å°±å¯ä»¥è¾“å…¥æŸä¸ªé‡ï¼Œå› ä¸ºå®ƒç´¯åŠ äº†æ‰€æœ‰æ•°å­—ï¼Œè¿™é‡Œçš„æœ€ç»ˆè¾“å‡ºä¸º32+32+128+64=256ã€‚Inception ç½‘ç»œé€‰ç”¨ä¸åŒå°ºå¯¸çš„æ»¤æ³¢å™¨è¿›è¡Œ Same å·ç§¯ï¼Œå¹¶å°†å·ç§¯å’Œæ± åŒ–å¾—åˆ°çš„è¾“å‡ºç»„åˆæ‹¼æ¥èµ·æ¥ï¼Œæœ€ç»ˆè®©ç½‘ç»œè‡ªå·±å»å­¦ä¹ éœ€è¦çš„å‚æ•°å’Œé‡‡ç”¨çš„æ»¤æ³¢å™¨ç»„åˆã€‚ 1x1 çš„å·ç§¯å±‚é€šå¸¸è¢«ç§°ä½œç“¶é¢ˆå±‚ï¼ˆBottleneck layerï¼‰ è®¡ç®—é‡ä¸º 28x28x32x5x5x192 = 1.2äº¿ ![](Deel Learning ai_Convolutional Neural Networks\The-problem-of-computational-cost.png) ![](Deel Learning ai_Convolutional Neural Networks\Using-1x1-convolution.png) 28x28x192x16 + 28x28x32x5x5x15 = 1.24 åƒä¸‡ï¼Œå‡å°‘äº†çº¦ 90%ã€‚ Â¶L7 : Inception network(Inception ç½‘ç»œ) ![](Deel Learning ai_Convolutional Neural Networks\example_2_24.png) ![](Deel Learning ai_Convolutional Neural Networks\example_2_25.png) Â¶L8 : Using open-source implementations( ä½¿ç”¨å¼€æºçš„å®ç°æ–¹æ¡ˆ) å¼€æºé¡¹ç›® Â¶L9 ï¼š Transfer Learningï¼ˆè¿ç§»å­¦ä¹ ï¼‰ å¦‚æœä½ ä¸‹è½½åˆ«äººå·²ç»è®­ç»ƒå¥½ç½‘ç»œç»“æ„çš„æƒé‡ï¼Œä½ é€šå¸¸èƒ½å¤Ÿè¿›å±•çš„ç›¸å½“å¿«ï¼Œç”¨è¿™ä¸ªä½œä¸ºé¢„è®­ç»ƒï¼Œç„¶åè½¬æ¢åˆ°ä½ æ„Ÿå…´è¶£çš„ä»»åŠ¡ä¸Šã€‚ åªæœ‰å¾ˆå°æ•°æ®é›†ï¼š å¯ä»¥ä½ åªéœ€è¦è®­ç»ƒsoftmaxå±‚çš„æƒé‡ï¼ŒæŠŠå‰é¢è¿™äº›å±‚çš„æƒé‡éƒ½å†»ç»“ã€‚ ç¨å¾®æ›´å¤§çš„æ•°æ®é›†ï¼š ä½ åº”è¯¥å†»ç»“æ›´å°‘çš„å±‚ï¼Œæ¯”å¦‚åªæŠŠè¿™äº›å±‚å†»ç»“ï¼Œç„¶åè®­ç»ƒåé¢çš„å±‚ã€‚å¦‚æœä½ çš„è¾“å‡ºå±‚çš„ç±»åˆ«ä¸åŒï¼Œé‚£ä¹ˆä½ éœ€è¦æ„å»ºè‡ªå·±çš„è¾“å‡ºå•å…ƒï¼›æˆ–è€…ä½ å¯ä»¥ç›´æ¥å»æ‰è¿™å‡ å±‚ï¼Œæ¢æˆä½ è‡ªå·±çš„éšè—å•å…ƒå’Œä½ è‡ªå·±çš„softmaxè¾“å‡ºå±‚ï¼Œè¿™äº›æ–¹æ³•å€¼å¾—ä¸€è¯•ã€‚ å¤§é‡æ•°æ®ï¼š ä½ å¯ä»¥ç”¨ä¸‹è½½çš„æƒé‡åªä½œä¸ºåˆå§‹åŒ–ï¼Œç”¨å®ƒä»¬æ¥ä»£æ›¿éšæœºåˆå§‹åŒ–ï¼Œæ¥ç€ä½ å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™è®­ç»ƒï¼Œæ›´æ–°ç½‘ç»œæ‰€æœ‰å±‚çš„æ‰€æœ‰æƒé‡ã€‚ Â¶L10 ï¼š Data augmentationï¼ˆæ•°æ®å¢å¼ºï¼‰ æ•°æ®é‡è¿œè¿œä¸å¤Ÿ Mirroring ![](Deel Learning ai_Convolutional Neural Networks\Mirroring.png) Random Cropping ![](Deel Learning ai_Convolutional Neural Networks\Mirroring_1.png) å½©è‰²è½¬æ¢color shifting r,g,bæ•°æ®æ”¹å˜ ![](Deel Learning ai_Convolutional Neural Networks\Mirroring_2.png) é™¤äº†éšæ„æ”¹å˜RGBé€šé“æ•°å€¼å¤–ï¼Œè¿˜å¯ä»¥æ›´æœ‰é’ˆå¯¹æ€§åœ°å¯¹å›¾ç‰‡çš„RGBé€šé“è¿›è¡ŒPCA color augmentationï¼Œä¹Ÿå°±æ˜¯å¯¹å›¾ç‰‡é¢œè‰²è¿›è¡Œä¸»æˆåˆ†åˆ†æï¼Œå¯¹ä¸»è¦çš„é€šé“é¢œè‰²è¿›è¡Œå¢åŠ æˆ–å‡å°‘ï¼Œå¯ä»¥é‡‡ç”¨é«˜æ–¯æ‰°åŠ¨åšæ³•ã€‚è¿™æ ·ä¹Ÿèƒ½å¢åŠ æœ‰æ•ˆçš„æ ·æœ¬æ•°é‡ã€‚å…·ä½“çš„PCA color augmentationåšæ³•å¯ä»¥æŸ¥é˜…AlexNetçš„ç›¸å…³è®ºæ–‡ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Mirroring_3.png) å¸¸ç”¨çš„å®ç°æ•°æ®æ‰©å……çš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ä¸ªçº¿ç¨‹æˆ–è€…æ˜¯å¤šçº¿ç¨‹ï¼Œè¿™äº›å¯ä»¥ç”¨æ¥åŠ è½½æ•°æ®ï¼Œå®ç°å˜å½¢å¤±çœŸï¼Œç„¶åä¼ ç»™å…¶ä»–çš„çº¿ç¨‹æˆ–è€…å…¶ä»–è¿›ç¨‹ï¼Œæ¥è®­ç»ƒè¿™ä¸ªï¼ˆç¼–å·2ï¼‰å’Œè¿™ä¸ªï¼ˆç¼–å·1ï¼‰ï¼Œå¯ä»¥å¹¶è¡Œå®ç°ã€‚ Â¶L11ï¼šThe state of computer vision(è®¡ç®—æœºè§†è§‰ç°çŠ¶) ç¥ç»ç½‘ç»œéœ€è¦æ•°æ®ï¼Œä¸åŒçš„ç½‘ç»œæ¨¡å‹æ‰€éœ€çš„æ•°æ®é‡æ˜¯ä¸åŒçš„ã€‚Object dectionï¼ŒImage recognitionï¼ŒSpeech recognitionæ‰€éœ€çš„æ•°æ®é‡ä¾æ¬¡å¢åŠ ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœdataè¾ƒå°‘ï¼Œé‚£ä¹ˆå°±éœ€è¦æ›´å¤šçš„hand-engineeringï¼Œå¯¹å·²æœ‰dataè¿›è¡Œå¤„ç†ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Mirroring_4.png) hand-engineeringæ˜¯ä¸€é¡¹éå¸¸é‡è¦ä¹Ÿæ¯”è¾ƒå›°éš¾çš„å·¥ä½œã€‚å¾ˆå¤šæ—¶å€™ï¼Œhand-engineeringå¯¹æ¨¡å‹è®­ç»ƒæ•ˆæœå½±å“å¾ˆå¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®é‡ä¸å¤šçš„æƒ…å†µä¸‹ã€‚ å½“ä½ æœ‰å°‘é‡çš„æ•°æ®æ—¶ï¼Œæœ‰ä¸€ä»¶äº‹å¯¹ä½ å¾ˆæœ‰å¸®åŠ©ï¼Œé‚£å°±æ˜¯è¿ç§»å­¦ä¹ ã€‚åœ¨åˆ«äººåšå¥½çš„åŸºç¡€ä¸Šç ”ç©¶ æå‡æ€§èƒ½ ![](Deel Learning ai_Convolutional Neural Networks\Mirroring_5.png)* ç”±äºè®¡ç®—æœºè§†è§‰é—®é¢˜å»ºç«‹åœ¨å°æ•°æ®é›†ä¹‹ä¸Šï¼Œå…¶ä»–äººå·²ç»å®Œæˆäº†å¤§é‡çš„ç½‘ç»œæ¶æ„çš„æ‰‹å·¥å·¥ç¨‹ã€‚ä¸€ä¸ªç¥ç»ç½‘ç»œåœ¨æŸä¸ªè®¡ç®—æœºè§†è§‰é—®é¢˜ä¸Šå¾ˆæœ‰æ•ˆï¼Œä½†ä»¤äººæƒŠè®¶çš„æ˜¯å®ƒé€šå¸¸ä¹Ÿä¼šè§£å†³å…¶ä»–è®¡ç®—æœºè§†è§‰é—®é¢˜ã€‚ æ‰€ä»¥ï¼Œè¦æƒ³å»ºç«‹ä¸€ä¸ªå®ç”¨çš„ç³»ç»Ÿï¼Œä½ æœ€å¥½å…ˆä»å…¶ä»–äººçš„ç¥ç»ç½‘ç»œæ¶æ„å…¥æ‰‹ã€‚å¦‚æœå¯èƒ½çš„è¯ï¼Œä½ å¯ä»¥ä½¿ç”¨å¼€æºçš„ä¸€äº›åº”ç”¨ï¼Œå› ä¸ºå¼€æ”¾çš„æºç å®ç°å¯èƒ½å·²ç»æ‰¾åˆ°äº†æ‰€æœ‰ç¹ççš„ç»†èŠ‚ï¼Œæ¯”å¦‚å­¦ä¹ ç‡è¡°å‡æ–¹å¼æˆ–è€…è¶…å‚æ•°ã€‚ Â¶summary 1. CNNçš„å¸¸è§ç½‘ç»œç»“æ„ é‡ç‚¹è¯´äº†ä¸€äº›æ®‹å·®ç½‘ç»œ 2.æ•°æ®å¢åŠ çš„æ–¹æ³• 3. å¤šç”¨å¼€æºæ¡†æ¶ï¼Œä¸ç”¨ä»å¤´å¼€å§‹è®­ç»ƒ W3 Object detection(ç›®æ ‡æ£€æµ‹) Â¶L1 :Object localization(ç›®æ ‡å®šä½) ç›®æ ‡å®šä½å’Œç›®æ ‡æ£€æµ‹ ![](Deel Learning ai_Convolutional Neural Networks\Detering_1.png) æ¨¡å‹ ![](Deel Learning ai_Convolutional Neural Networks\Detering_2.png) è¾“å…¥è¿˜åŒ…æ‹¬ä½ç½®ä¿¡æ¯ ![](Deel Learning ai_Convolutional Neural Networks\Detering_3.png) æŸå¤±å‡½æ•° æƒ…å†µä¸€ï¼šæ£€æµ‹åˆ°äº† ![](Deel Learning ai_Convolutional Neural Networks\Detering_4.png) æƒ…å†µäºŒï¼š ![](Deel Learning ai_Convolutional Neural Networks\Detering_5.png) Â¶L2: Landmark detection(ç‰¹å¾ç‚¹æ£€æµ‹) ![](Deel Learning ai_Convolutional Neural Networks\Detering_6.png) è¯¥ç½‘ç»œæ¨¡å‹å…±æ£€æµ‹äººè„¸ä¸Š64å¤„ç‰¹å¾ç‚¹ï¼ŒåŠ ä¸Šæ˜¯å¦ä¸ºfaceçš„æ ‡å¿—ä½ï¼Œè¾“å‡ºlabelå…±æœ‰64x2+1=129ä¸ªå€¼ã€‚é€šè¿‡æ£€æµ‹äººè„¸ç‰¹å¾ç‚¹å¯ä»¥è¿›è¡Œæƒ…ç»ªåˆ†ç±»ä¸åˆ¤æ–­ï¼Œæˆ–è€…åº”ç”¨äºARé¢†åŸŸç­‰ç­‰ã€‚ é™¤äº†äººè„¸ç‰¹å¾ç‚¹æ£€æµ‹ä¹‹å¤–ï¼Œè¿˜å¯ä»¥æ£€æµ‹äººä½“å§¿åŠ¿åŠ¨ä½œï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š ![](Deel Learning ai_Convolutional Neural Networks\Detering_7.png) Â¶L3 :Object detection(ç›®æ ‡æ£€æµ‹) å­¦è¿‡äº†å¯¹è±¡å®šä½å’Œç‰¹å¾ç‚¹æ£€æµ‹ï¼Œä»Šå¤©æˆ‘ä»¬æ¥æ„å»ºä¸€ä¸ªå¯¹è±¡æ£€æµ‹ç®—æ³•ã€‚è¿™èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•é€šè¿‡å·ç§¯ç½‘ç»œè¿›è¡Œå¯¹è±¡æ£€æµ‹ï¼Œé‡‡ç”¨çš„æ˜¯åŸºäºæ»‘åŠ¨çª—å£çš„ç›®æ ‡æ£€æµ‹ç®—æ³•ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_8.png) è®­ç»ƒå®Œè¿™ä¸ªå·ç§¯ç½‘ç»œï¼Œå°±å¯ä»¥ç”¨å®ƒæ¥å®ç°æ»‘åŠ¨çª—å£ç›®æ ‡æ£€æµ‹ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ã€‚ é€‰å®šç‰¹å®šå¤§å°çš„çª—å£ï¼Œçª—å£åœˆå®šè¾“å…¥å·ç§¯ç¥ç»ç½‘ç»œï¼Œå·ç§¯ç¥ç»ç½‘ç»œå¼€å§‹é¢„æµ‹ã€‚ é‡å¤ä¸Šè¿°æ“ä½œï¼Œä¸è¿‡è¿™æ¬¡æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªæ›´å¤§çš„çª—å£ï¼Œæˆªå–æ›´å¤§çš„åŒºåŸŸï¼Œå¹¶è¾“å…¥ç»™å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†ï¼Œä½ å¯ä»¥æ ¹æ®å·ç§¯ç½‘ç»œå¯¹è¾“å…¥å¤§å°è°ƒæ•´è¿™ä¸ªåŒºåŸŸï¼Œç„¶åè¾“å…¥ç»™å·ç§¯ç½‘ç»œï¼Œè¾“å‡º0æˆ–![](Deel Learning ai_Convolutional Neural Networks\Detering_10.png) ![](Deel Learning ai_Convolutional Neural Networks\Detering_11.png) å¦‚æœä½ è¿™æ ·åšï¼Œä¸è®ºæ±½è½¦åœ¨å›¾ç‰‡çš„ä»€ä¹ˆä½ç½®ï¼Œæ€»æœ‰ä¸€ä¸ªçª—å£å¯ä»¥æ£€æµ‹åˆ°å®ƒã€‚ è¿™ç§ç®—æ³•å«ä½œæ»‘åŠ¨çª—å£ç›®æ ‡æ£€æµ‹ï¼Œå› ä¸ºæˆ‘ä»¬ä»¥æŸä¸ªæ­¥å¹…æ»‘åŠ¨è¿™äº›æ–¹æ¡†çª—å£éå†æ•´å¼ å›¾ç‰‡ï¼Œå¯¹è¿™äº›æ–¹å½¢åŒºåŸŸè¿›è¡Œåˆ†ç±»ï¼Œåˆ¤æ–­é‡Œé¢æœ‰æ²¡æœ‰æ±½è½¦ã€‚ æ»‘åŠ¨çª—ç®—æ³•çš„ä¼˜ç‚¹æ˜¯åŸç†ç®€å•ï¼Œä¸”ä¸éœ€è¦äººä¸ºé€‰å®šç›®æ ‡åŒºåŸŸï¼ˆæ£€æµ‹å‡ºç›®æ ‡çš„æ»‘åŠ¨çª—å³ä¸ºç›®æ ‡åŒºåŸŸï¼‰ã€‚ä½†æ˜¯å…¶ç¼ºç‚¹ä¹Ÿå¾ˆæ˜æ˜¾ï¼Œé¦–å…ˆæ»‘åŠ¨çª—çš„å¤§å°å’Œæ­¥è¿›é•¿åº¦éƒ½éœ€è¦äººä¸ºç›´è§‚è®¾å®šã€‚æ»‘åŠ¨çª—è¿‡å°æˆ–è¿‡å¤§ï¼Œæ­¥è¿›é•¿åº¦è¿‡å¤§å‡ä¼šé™ä½ç›®æ ‡æ£€æµ‹æ­£ç¡®ç‡ã€‚è€Œä¸”ï¼Œæ¯æ¬¡æ»‘åŠ¨çª—åŒºåŸŸéƒ½è¦è¿›è¡Œä¸€æ¬¡CNNç½‘ç»œè®¡ç®—ï¼Œå¦‚æœæ»‘åŠ¨çª—å’Œæ­¥è¿›é•¿åº¦è¾ƒå°ï¼Œæ•´ä¸ªç›®æ ‡æ£€æµ‹çš„ç®—æ³•è¿è¡Œæ—¶é—´ä¼šå¾ˆé•¿ã€‚æ‰€ä»¥ï¼Œæ»‘åŠ¨çª—ç®—æ³•è™½ç„¶ç®€å•ï¼Œä½†æ˜¯æ€§èƒ½ä¸ä½³ï¼Œä¸å¤Ÿå¿«ï¼Œä¸å¤Ÿçµæ´»ã€‚ Â¶L 4 : Convolutional implementation of sliding windows(æ»‘åŠ¨çª—å£çš„å·ç§¯å®ç°) å…¨è¿æ¥å±‚è½¬åŒ–ä¸ºå·ç§¯å±‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_12.png) å•ä¸ªçª—å£åŒºåŸŸå·ç§¯ç½‘ç»œç»“æ„å»ºç«‹å®Œæ¯•ä¹‹åï¼Œå¯¹äºå¾…æ£€æµ‹å›¾ç‰‡ï¼Œå³å¯ä½¿ç”¨è¯¥ç½‘ç»œå‚æ•°å’Œç»“æ„è¿›è¡Œè¿ç®—ã€‚ä¾‹å¦‚16 x 16 x 3çš„å›¾ç‰‡ï¼Œæ­¥è¿›é•¿åº¦ä¸º2ï¼ŒCNNç½‘ç»œå¾—åˆ°çš„è¾“å‡ºå±‚ä¸º2 x 2 x 4ã€‚å…¶ä¸­ï¼Œ2 x 2è¡¨ç¤ºå…±æœ‰4ä¸ªçª—å£ç»“æœã€‚å¯¹äºæ›´å¤æ‚çš„28 x 28 x3çš„å›¾ç‰‡ï¼ŒCNNç½‘ç»œå¾—åˆ°çš„è¾“å‡ºå±‚ä¸º8 x 8 x 4ï¼Œå…±64ä¸ªçª—å£ç»“æœã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_13.png) ä¹‹å‰çš„æ»‘åŠ¨çª—ç®—æ³•éœ€è¦åå¤è¿›è¡ŒCNNæ­£å‘è®¡ç®—ï¼Œä¾‹å¦‚16 x 16 x 3çš„å›¾ç‰‡éœ€è¿›è¡Œ4æ¬¡ï¼Œ28 x 28 x3çš„å›¾ç‰‡éœ€è¿›è¡Œ64æ¬¡ã€‚è€Œåˆ©ç”¨å·ç§¯æ“ä½œä»£æ›¿æ»‘åŠ¨çª—ç®—æ³•ï¼Œåˆ™ä¸ç®¡åŸå§‹å›¾ç‰‡æœ‰å¤šå¤§ï¼Œåªéœ€è¦è¿›è¡Œä¸€æ¬¡CNNæ­£å‘è®¡ç®—ï¼Œå› ä¸ºå…¶ä¸­å…±äº«äº†å¾ˆå¤šé‡å¤è®¡ç®—éƒ¨åˆ†ï¼Œè¿™å¤§å¤§èŠ‚çº¦äº†è¿ç®—æˆæœ¬ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œçª—å£æ­¥è¿›é•¿åº¦ä¸é€‰æ‹©çš„MAX POOLå¤§å°æœ‰å…³ã€‚å¦‚æœéœ€è¦æ­¥è¿›é•¿åº¦ä¸º4ï¼Œåªéœ€è®¾ç½®MAX POOLä¸º4 x 4å³å¯ã€‚ Â¶L5 ï¼š Bounding box predictionsï¼ˆBounding Boxé¢„æµ‹ï¼‰ ![](Deel Learning ai_Convolutional Neural Networks\Detering_14.png) YOLOï¼ˆYou Only Look Onceï¼‰ç®—æ³•å¯ä»¥è§£å†³è¿™ç±»é—®é¢˜ï¼Œç”Ÿæˆæ›´åŠ å‡†ç¡®çš„ç›®æ ‡åŒºåŸŸï¼ˆå¦‚ä¸Šå›¾çº¢è‰²çª—å£ï¼‰ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_16.png) å¦‚æœç›®æ ‡ä¸­å¿ƒåæ ‡(bx,by)ä¸åœ¨å½“å‰ç½‘æ ¼å†…ï¼Œåˆ™å½“å‰ç½‘æ ¼Pc=0ï¼›ç›¸åï¼Œåˆ™å½“å‰ç½‘æ ¼Pc=1ï¼ˆå³åªçœ‹ä¸­å¿ƒåæ ‡æ˜¯å¦åœ¨å½“å‰ç½‘æ ¼å†…ï¼‰ã€‚åˆ¤æ–­æœ‰ç›®æ ‡çš„ç½‘æ ¼ä¸­ï¼Œbx,by,bh,bwé™å®šäº†ç›®æ ‡åŒºåŸŸã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“å‰ç½‘æ ¼å·¦ä¸Šè§’åæ ‡è®¾å®šä¸º(0, 0)ï¼Œå³ä¸‹è§’åæ ‡è®¾å®šä¸º(1, 1)ï¼Œ(bx,by)èŒƒå›´é™å®šåœ¨[0,1]ä¹‹é—´ï¼Œä½†æ˜¯bh,bwå¯ä»¥å¤§äº1ã€‚å› ä¸ºç›®æ ‡å¯èƒ½è¶…å‡ºè¯¥ç½‘æ ¼ï¼Œæ¨ªè·¨å¤šä¸ªåŒºåŸŸï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ç›®æ ‡å å‡ ä¸ªç½‘æ ¼æ²¡æœ‰å…³ç³»ï¼Œç›®æ ‡ä¸­å¿ƒåæ ‡å¿…ç„¶åœ¨ä¸€ä¸ªç½‘æ ¼ä¹‹å†…ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_15.png) Â¶L6 ï¼šIntersection over unionï¼ˆäº¤å¹¶æ¯”) ![](Deel Learning ai_Convolutional Neural Networks\Detering_17.png) ä¸€èˆ¬çº¦å®šï¼Œåœ¨è®¡ç®—æœºæ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå¦‚æœlou&gt;=0.5ï¼Œå°±è¯´æ£€æµ‹æ­£ç¡®ï¼Œå¦‚æœé¢„æµ‹å™¨å’Œå®é™…è¾¹ç•Œæ¡†å®Œç¾é‡å ï¼ŒloUå°±æ˜¯1ï¼Œå› ä¸ºäº¤é›†å°±ç­‰äºå¹¶é›†ã€‚ä½†ä¸€èˆ¬æ¥è¯´åªè¦lou&gt;=0.5ï¼Œé‚£ä¹ˆç»“æœæ˜¯å¯ä»¥æ¥å—çš„ï¼Œçœ‹èµ·æ¥è¿˜å¯ä»¥ã€‚ä¸€èˆ¬çº¦å®šï¼Œ0.5æ˜¯é˜ˆå€¼ï¼Œç”¨æ¥åˆ¤æ–­é¢„æµ‹çš„è¾¹ç•Œæ¡†æ˜¯å¦æ­£ç¡®ã€‚ä¸€èˆ¬æ˜¯è¿™ä¹ˆçº¦å®šï¼Œä½†å¦‚æœä½ å¸Œæœ›æ›´ä¸¥æ ¼ä¸€ç‚¹ï¼Œä½ å¯ä»¥å°†loUå®šå¾—æ›´é«˜ï¼Œæ¯”å¦‚è¯´å¤§äº0.6æˆ–è€…æ›´å¤§çš„æ•°å­—ï¼Œä½†loUè¶Šé«˜ï¼Œè¾¹ç•Œæ¡†è¶Šç²¾ç¡®ã€‚ Â¶L7: Non-max suppression(éæå¤§å€¼æŠ‘åˆ¶) åˆ°ç›®å‰ä¸ºæ­¢ä½ ä»¬å­¦åˆ°çš„å¯¹è±¡æ£€æµ‹ä¸­çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œä½ çš„ç®—æ³•å¯èƒ½å¯¹åŒä¸€ä¸ªå¯¹è±¡åšå‡ºå¤šæ¬¡æ£€æµ‹ï¼Œæ‰€ä»¥ç®—æ³•ä¸æ˜¯å¯¹æŸä¸ªå¯¹è±¡æ£€æµ‹å‡ºä¸€æ¬¡ï¼Œè€Œæ˜¯æ£€æµ‹å‡ºå¤šæ¬¡ã€‚éæå¤§å€¼æŠ‘åˆ¶è¿™ä¸ªæ–¹æ³•å¯ä»¥ç¡®ä¿ä½ çš„ç®—æ³•å¯¹æ¯ä¸ªå¯¹è±¡åªæ£€æµ‹ä¸€æ¬¡ï¼Œæˆ‘ä»¬è®²ä¸€ä¸ªä¾‹å­ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_18.png) å‡è®¾ä½ éœ€è¦åœ¨è¿™å¼ å›¾ç‰‡é‡Œæ£€æµ‹è¡Œäººå’Œæ±½è½¦ï¼Œä½ å¯èƒ½ä¼šåœ¨ä¸Šé¢æ”¾ä¸ª19Ã—19ç½‘æ ¼ï¼Œç†è®ºä¸Šè¿™è¾†è½¦åªæœ‰ä¸€ä¸ªä¸­ç‚¹ï¼Œæ‰€ä»¥å®ƒåº”è¯¥åªè¢«åˆ†é…åˆ°ä¸€ä¸ªæ ¼å­é‡Œï¼Œå·¦è¾¹çš„è½¦å­ä¹Ÿåªæœ‰ä¸€ä¸ªä¸­ç‚¹ï¼Œæ‰€ä»¥ç†è®ºä¸Šåº”è¯¥åªæœ‰ä¸€ä¸ªæ ¼å­åšå‡ºæœ‰è½¦çš„é¢„æµ‹ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_19.png) å®é™…æƒ…å†µæ˜¯æ ¼å­1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6éƒ½è®¤ä¸ºé‡Œé¢æœ‰è½¦ã€‚å› ä¸ºä½ è¦åœ¨361ä¸ªæ ¼å­ä¸Šéƒ½è¿è¡Œä¸€æ¬¡å›¾åƒæ£€æµ‹å’Œå®šä½ç®—æ³•ï¼Œé‚£ä¹ˆå¯èƒ½å¾ˆå¤šæ ¼å­éƒ½ä¼šä¸¾æ‰‹è¯´æˆ‘çš„pc,æˆ‘è¿™ä¸ªæ ¼å­é‡Œæœ‰è½¦çš„æ¦‚ç‡å¾ˆé«˜ï¼Œè€Œä¸æ˜¯361ä¸ªæ ¼å­ä¸­ä»…æœ‰ä¸¤ä¸ªæ ¼å­ä¼šæŠ¥å‘Šå®ƒä»¬æ£€æµ‹å‡ºä¸€ä¸ªå¯¹è±¡ã€‚ éæœ€å¤§å€¼æŠ‘åˆ¶ï¼ˆNon-max Suppressionï¼‰åšæ³•å¾ˆç®€å•ï¼Œå›¾ç¤ºæ¯ä¸ªç½‘æ ¼çš„Pcå€¼å¯ä»¥æ±‚å‡ºï¼ŒPcå€¼åæ˜ äº†è¯¥ç½‘æ ¼åŒ…å«ç›®æ ‡ä¸­å¿ƒåæ ‡çš„å¯ä¿¡åº¦ã€‚é¦–å…ˆé€‰å–Pcæœ€å¤§å€¼å¯¹åº”çš„ç½‘æ ¼å’ŒåŒºåŸŸï¼Œç„¶åè®¡ç®—è¯¥åŒºåŸŸä¸æ‰€æœ‰å…¶å®ƒåŒºåŸŸçš„IoUï¼Œå‰”é™¤æ‰IoUå¤§äºé˜ˆå€¼ï¼ˆä¾‹å¦‚0.5ï¼‰çš„æ‰€æœ‰ç½‘æ ¼åŠåŒºåŸŸã€‚è¿™æ ·å°±èƒ½ä¿è¯åŒä¸€ç›®æ ‡åªæœ‰ä¸€ä¸ªç½‘æ ¼ä¸ä¹‹å¯¹åº”ï¼Œä¸”è¯¥ç½‘æ ¼Pcæœ€å¤§ï¼Œæœ€å¯ä¿¡ã€‚æ¥ç€ï¼Œå†ä»å‰©ä¸‹çš„ç½‘æ ¼ä¸­é€‰å–Pcæœ€å¤§çš„ç½‘æ ¼ï¼Œé‡å¤ä¸Šä¸€æ­¥çš„æ“ä½œã€‚æœ€åï¼Œå°±èƒ½ä½¿å¾—æ¯ä¸ªç›®æ ‡éƒ½ä»…ç”±ä¸€ä¸ªç½‘æ ¼å’ŒåŒºåŸŸå¯¹åº”ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š ![](Deel Learning ai_Convolutional Neural Networks\Detering_20.png) æ€»ç»“ä¸€ä¸‹éæœ€å¤§å€¼æŠ‘åˆ¶ç®—æ³•çš„æµç¨‹ï¼š å‰”é™¤Pcå€¼å°äºæŸé˜ˆå€¼ï¼ˆä¾‹å¦‚0.6ï¼‰çš„æ‰€æœ‰ç½‘æ ¼ï¼› é€‰å–Pcå€¼æœ€å¤§çš„ç½‘æ ¼ï¼Œåˆ©ç”¨IoUï¼Œæ‘’å¼ƒä¸è¯¥ç½‘æ ¼äº¤å è¾ƒå¤§çš„ç½‘æ ¼ï¼› å¯¹å‰©ä¸‹çš„ç½‘æ ¼ï¼Œé‡å¤æ­¥éª¤2ã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå¯¹è±¡æ£€æµ‹ä¸­å­˜åœ¨çš„ä¸€ä¸ªé—®é¢˜æ˜¯æ¯ä¸ªæ ¼å­åªèƒ½æ£€æµ‹å‡ºä¸€ä¸ªå¯¹è±¡ï¼Œå¦‚æœä½ æƒ³è®©ä¸€ä¸ªæ ¼å­æ£€æµ‹å‡ºå¤šä¸ªå¯¹è±¡ï¼Œä½ å¯ä»¥è¿™ä¹ˆåšï¼Œå°±æ˜¯ä½¿ç”¨anchor boxè¿™ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªä¾‹å­å¼€å§‹è®²å§ã€‚æ–¹æ³•æ˜¯ä½¿ç”¨ä¸åŒå½¢çŠ¶çš„Anchor Boxesã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_21.png) è¿™å°±æ˜¯anchor boxçš„æ¦‚å¿µï¼Œæˆ‘ä»¬å»ºç«‹anchor boxè¿™ä¸ªæ¦‚å¿µï¼Œæ˜¯ä¸ºäº†å¤„ç†ä¸¤ä¸ªå¯¹è±¡å‡ºç°åœ¨åŒä¸€ä¸ªæ ¼å­çš„æƒ…å†µï¼Œå®è·µä¸­è¿™ç§æƒ…å†µå¾ˆå°‘å‘ç”Ÿ Â¶L9 : YOLO ç®—æ³•ï¼ˆPutting it together: YOLO algorithmï¼‰ ![](Deel Learning ai_Convolutional Neural Networks\Detering_22.png) ![](Deel Learning ai_Convolutional Neural Networks\Detering_23.png) ![](Deel Learning ai_Convolutional Neural Networks\Detering_24.png) ![](Deel Learning ai_Convolutional Neural Networks\Detering_25.png) ![](Deel Learning ai_Convolutional Neural Networks\Detering_26.png) è¿™å°±æ˜¯YOLOå¯¹è±¡æ£€æµ‹ç®—æ³•ï¼Œè¿™å®é™…ä¸Šæ˜¯æœ€æœ‰æ•ˆçš„å¯¹è±¡æ£€æµ‹ç®—æ³•ä¹‹ä¸€ï¼ŒåŒ…å«äº†æ•´ä¸ªè®¡ç®—æœºè§†è§‰å¯¹è±¡æ£€æµ‹é¢†åŸŸæ–‡çŒ®ä¸­å¾ˆå¤šæœ€ç²¾å¦™çš„æ€è·¯ Â¶Region proposals (Optional)ï¼ˆå€™é€‰åŒºåŸŸï¼ˆé€‰ä¿®ï¼‰ï¼‰ ä¹‹å‰ä»‹ç»çš„æ»‘åŠ¨çª—ç®—æ³•ä¼šå¯¹åŸå§‹å›¾ç‰‡çš„æ¯ä¸ªåŒºåŸŸéƒ½è¿›è¡Œæ‰«æï¼Œå³ä½¿æ˜¯ä¸€äº›ç©ºç™½çš„æˆ–æ˜æ˜¾æ²¡æœ‰ç›®æ ‡çš„åŒºåŸŸï¼Œä¾‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚è¿™æ ·ä¼šé™ä½ç®—æ³•è¿è¡Œæ•ˆç‡ï¼Œè€—è´¹æ—¶é—´ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_27.png) ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå°½é‡é¿å…å¯¹æ— ç”¨åŒºåŸŸçš„æ‰«æï¼Œå¯ä»¥ä½¿ç”¨Region Proposalsçš„æ–¹æ³•ã€‚å…·ä½“åšæ³•æ˜¯å…ˆå¯¹åŸå§‹å›¾ç‰‡è¿›è¡Œåˆ†å‰²ç®—æ³•å¤„ç†ï¼Œç„¶åæ”¯é˜Ÿåˆ†å‰²åçš„å›¾ç‰‡ä¸­çš„å—è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚ ![](Deel Learning ai_Convolutional Neural Networks\Detering_28.png) ![](Deel Learning ai_Convolutional Neural Networks\Detering_29.png) Region Proposalså…±æœ‰ä¸‰ç§æ–¹æ³•ï¼š R-CNN: æ»‘åŠ¨çª—çš„å½¢å¼ï¼Œä¸€æ¬¡åªå¯¹å•ä¸ªåŒºåŸŸå—è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œè¿ç®—é€Ÿåº¦æ…¢ã€‚ Fast R-CNN: åˆ©ç”¨å·ç§¯å®ç°æ»‘åŠ¨çª—ç®—æ³•ï¼Œç±»ä¼¼ç¬¬4èŠ‚åšæ³•ã€‚ Faster R-CNN: åˆ©ç”¨å·ç§¯å¯¹å›¾ç‰‡è¿›è¡Œåˆ†å‰²ï¼Œè¿›ä¸€æ­¥æé«˜è¿è¡Œé€Ÿåº¦ã€‚ Â¶W4ï¼šSpecial applications: Face recognition &amp;Neural style transfer( ç‰¹æ®Šåº”ç”¨ï¼šäººè„¸è¯†åˆ«å’Œç¥ç»é£æ ¼è½¬æ¢) Â¶C1 ï¼š What is face recognition? é¦–å…ˆç®€å•ä»‹ç»ä¸€ä¸‹äººè„¸éªŒè¯ï¼ˆface verificationï¼‰å’Œäººè„¸è¯†åˆ«ï¼ˆface recognitionï¼‰çš„åŒºåˆ«ã€‚ äººè„¸éªŒè¯ï¼šè¾“å…¥ä¸€å¼ äººè„¸å›¾ç‰‡ï¼ŒéªŒè¯è¾“å‡ºä¸æ¨¡æ¿æ˜¯å¦ä¸ºåŒä¸€äººï¼Œå³ä¸€å¯¹ä¸€é—®é¢˜ã€‚ äººè„¸è¯†åˆ«ï¼šè¾“å…¥ä¸€å¼ äººè„¸å›¾ç‰‡ï¼ŒéªŒè¯è¾“å‡ºæ˜¯å¦ä¸ºKä¸ªæ¨¡æ¿ä¸­çš„æŸä¸€ä¸ªï¼Œå³ä¸€å¯¹å¤šé—®é¢˜ã€‚ Â¶L2 ï¼š One-shot learning One-shot learningå°±æ˜¯è¯´æ•°æ®åº“ä¸­æ¯ä¸ªäººçš„è®­ç»ƒæ ·æœ¬åªåŒ…å«ä¸€å¼ ç…§ç‰‡ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªCNNæ¨¡å‹æ¥è¿›è¡Œäººè„¸è¯†åˆ«ã€‚è‹¥æ•°æ®åº“æœ‰Kä¸ªäººï¼Œåˆ™CNNæ¨¡å‹è¾“å‡ºsoftmaxå±‚å°±æ˜¯Kç»´çš„ã€‚ ä½†æ˜¯One-shot learningçš„æ€§èƒ½å¹¶ä¸å¥½ï¼Œå…¶åŒ…å«äº†ä¸¤ä¸ªç¼ºç‚¹ï¼š æ¯ä¸ªäººåªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œè®­ç»ƒæ ·æœ¬å°‘ï¼Œæ„å»ºçš„CNNç½‘ç»œä¸å¤Ÿå¥å£®ã€‚ è‹¥æ•°æ®åº“å¢åŠ å¦ä¸€ä¸ªäººï¼Œè¾“å‡ºå±‚softmaxçš„ç»´åº¦å°±è¦å‘ç”Ÿå˜åŒ–ï¼Œç›¸å½“äºè¦é‡æ–°æ„å»ºCNNç½‘ç»œï¼Œä½¿æ¨¡å‹è®¡ç®—é‡å¤§å¤§å¢åŠ ï¼Œä¸å¤Ÿçµæ´»ã€‚ ä¸ºäº†è§£å†³One-shot learningçš„é—®é¢˜ï¼Œæˆ‘ä»¬å…ˆæ¥ä»‹ç»ç›¸ä¼¼å‡½æ•°ï¼ˆsimilarity functionï¼‰ã€‚ç›¸ä¼¼å‡½æ•°è¡¨ç¤ºä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œç”¨d(img1,img2)æ¥è¡¨ç¤ºã€‚è‹¥d(img1,img2)è¾ƒå°ï¼Œåˆ™è¡¨ç¤ºä¸¤å¼ å›¾ç‰‡ç›¸ä¼¼ï¼›è‹¥d(img1,img2)è¾ƒå¤§ï¼Œåˆ™è¡¨ç¤ºä¸¤å¼ å›¾ç‰‡ä¸æ˜¯åŒä¸€ä¸ªäººã€‚ç›¸ä¼¼å‡½æ•°å¯ä»¥åœ¨äººè„¸éªŒè¯ä¸­ä½¿ç”¨ï¼š d(img1,img2)â‰¤Ï„ : ä¸€æ · d(img1,img2)&gt;Ï„ : ä¸ä¸€æ · ![](Deel Learning ai_Convolutional Neural Networks\congtion_1.png) ç°åœ¨ä½ å·²ç»çŸ¥é“å‡½æ•°dæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œé€šè¿‡è¾“å…¥ä¸¤å¼ ç…§ç‰‡ï¼Œå®ƒå°†è®©ä½ èƒ½å¤Ÿè§£å†³ä¸€æ¬¡å­¦ä¹ é—®é¢˜ã€‚é‚£ä¹ˆï¼Œä¸‹èŠ‚è§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†ä¼šå­¦ä¹ å¦‚ä½•è®­ç»ƒä½ çš„ç¥ç»ç½‘ç»œå­¦ä¼šè¿™ä¸ªå‡½æ•°ã€‚ Â¶L3: Siamese network æœ€åä¸€å±‚å»æ‰softmaxå•å…ƒåšåˆ†ç±» ![](Deel Learning ai_Convolutional Neural Networks\congtion_2.png) ![](Deel Learning ai_Convolutional Neural Networks\congtion_3.png) å¦‚æœä½ è¦æ¯”è¾ƒä¸¤ä¸ªå›¾ç‰‡çš„è¯ï¼Œä¾‹å¦‚è¿™é‡Œçš„ç¬¬ä¸€å¼ ï¼ˆç¼–å·1ï¼‰å’Œç¬¬äºŒå¼ å›¾ç‰‡ï¼ˆç¼–å·2ï¼‰ï¼Œä½ è¦åšçš„å°±æ˜¯æŠŠç¬¬äºŒå¼ å›¾ç‰‡å–‚ç»™æœ‰åŒæ ·å‚æ•°çš„åŒæ ·çš„ç¥ç»ç½‘ç»œï¼Œç„¶åå¾—åˆ°ä¸€ä¸ªä¸åŒçš„128ç»´çš„å‘é‡ï¼ˆç¼–å·3ï¼‰ï¼Œè¿™ä¸ªå‘é‡ä»£è¡¨æˆ–è€…ç¼–ç ç¬¬äºŒä¸ªå›¾ç‰‡ï¼Œæˆ‘è¦æŠŠç¬¬äºŒå¼ å›¾ç‰‡çš„ç¼–ç å«åš$f(x{(2)})$ã€‚è¿™é‡Œæˆ‘ç”¨$x{(1)}$å’Œ$x^{(2)}$ä»…ä»…ä»£è¡¨ä¸¤ä¸ªè¾“å…¥å›¾ç‰‡, $$ d(x{(1)},x{(2)})=||f(x{(1)}-f(x{(2)}||^2 $$ ä¸åŒçš„å›¾ç‰‡çš„CNNç½‘ç»œç»“æ„å’Œå‚æ•°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œç›®æ ‡å°±æ˜¯åˆ©ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œè°ƒæ•´ç½‘ç»œå‚æ•°]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Deep Learning Specialization]]></title>
    <url>%2F2019%2F05%2F05%2FDeep%20Learning%20ai_Deep%20Learning%20Specialization%2F</url>
    <content type="text"><![CDATA[C3 Improving Model Performance Â¶W1 ML Strategy(1) Â¶L01 Improving Model Performance éœ€è¦æé«˜è®­ç»ƒç»“æœçš„è¡¨ç°ï¼Œè¡¨ç°å¾—æ›´å¥½çš„æªæ–½ Machine Learning Strategy ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-24_21-03-13.jpg) Â¶L2 : Orthogonalization(æ­£äº¤åŒ–) æ‰€è°“æ­£äº¤ï¼Œå°±æ˜¯ä½ çš„æ“æ§æ•ˆæœå°½é‡åªå½±å“ä¸€ä¸ªæ–¹é¢ã€‚æ¯”å¦‚ä»¥è€å¼ç”µè§†æœºä¸ºä¾‹ï¼Œè°ƒèŠ‚å›¾åƒçš„å¤§å°ã€å·¦å³åç§»ã€ä¸Šä¸‹åç§»ã€‚è€Œä¸æ˜¯ä¸€ä¸ªæŒ‰é’®å¯ä»¥åŒæ—¶è°ƒèŠ‚å›¾åƒå¤§å°å’Œå·¦å³åç§»ï¼Œé‚£æ ·ä¼šå¾ˆéš¾æ“ä½œã€‚ å…·ä½“åˆ°supervised learningï¼Œæœ‰ä»¥ä¸‹4ä¸ªå‡è®¾æ˜¯æ­£äº¤çš„ï¼Ÿ Fit training set well in cost function If it doesnâ€™t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help. Fit development set well on cost function If it doesnâ€™t fit well, regularization or using bigger training set might help. Fit test set well on cost function If it doesnâ€™t fit well, the use of a bigger development set might help Performs well in real world If it doesnâ€™t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing. åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°æ¬ ä½³ï¼Œéœ€è¦åˆ‡æ¢åˆ°å¥½çš„ä¼˜åŒ–ç®—æ³• åœ¨éªŒè¯é›†ä¸Šè¡¨ç°ä¸å¥½ï¼Œä¸€ç»„æ­£åˆ™åŒ–æŒ‰é’® åœ¨æµ‹è¯•é›†è¡¨ç°ä¸å¥½ï¼Œéœ€è¦æ›´å¥½çš„éªŒè¯é›† åœ¨ç”¨æˆ·ä½“éªŒä¸å¥½ï¼Œéœ€è¦æ”¹å˜æµ‹è¯•é›†å¤§å°æˆ–è€…æˆæœ¬å‡½æ•° Â¶L3 Single number evaluation metric(å•ä¸€æ•°å­—è¯„ä¼°æŒ‡æ ‡) Â¶classification ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-25_20-42-10.jpg) Â¶Precesion ï¼ˆæŸ¥å‡†ç‡ï¼‰ ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-25_20-47-46.jpg) Â¶recallï¼ˆæŸ¥å…¨ç‡ï¼‰ ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-25_20-48-11.jpg) $$ F 1=\frac{2}{\frac{1}{P}+\frac{1}{R}}=\frac{2 P R}{P+R} $$ Â¶L4 Satisficing and optimizing metrics(æ»¡è¶³å’Œä¼˜åŒ–æŒ‡æ ‡) å¦‚æœæˆ‘ä»¬è¿˜æƒ³è¦å°†åˆ†ç±»å™¨çš„è¿è¡Œæ—¶é—´ä¹Ÿçº³å…¥è€ƒè™‘èŒƒå›´ï¼Œå°†å…¶å’Œç²¾ç¡®ç‡ã€å¬å›ç‡ç»„åˆæˆä¸€ä¸ªå•å€¼è¯„ä»·æŒ‡æ ‡æ˜¾ç„¶ä¸é‚£ä¹ˆåˆé€‚ã€‚è¿™æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†æŸäº›æŒ‡æ ‡ä½œä¸ºä¼˜åŒ–æŒ‡æ ‡ï¼ˆOptimizing Matricï¼‰ï¼Œå¯»æ±‚å®ƒä»¬çš„æœ€ä¼˜å€¼ï¼›è€Œå°†æŸäº›æŒ‡æ ‡ä½œä¸ºæ»¡è¶³æŒ‡æ ‡ï¼ˆSatisficing Matricï¼‰ï¼Œåªè¦åœ¨ä¸€å®šé˜ˆå€¼ä»¥å†…å³å¯ã€‚ åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå‡†ç¡®ç‡å°±æ˜¯ä¸€ä¸ªä¼˜åŒ–æŒ‡æ ‡ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦åˆ†ç±»å™¨å°½å¯èƒ½åšåˆ°æ­£ç¡®åˆ†ç±»ï¼›è€Œè¿è¡Œæ—¶é—´å°±æ˜¯ä¸€ä¸ªæ»¡è¶³æŒ‡æ ‡ï¼Œå¦‚æœä½ æƒ³è¦åˆ†ç±»å™¨çš„è¿è¡Œæ—¶é—´ä¸å¤šäºæŸä¸ªé˜ˆå€¼ï¼Œé‚£æœ€ç»ˆé€‰æ‹©çš„åˆ†ç±»å™¨å°±åº”è¯¥æ˜¯ä»¥è¿™ä¸ªé˜ˆå€¼ä¸ºç•Œé‡Œé¢å‡†ç¡®ç‡æœ€é«˜çš„é‚£ä¸ªã€‚ å¦‚æ­¤ï¼Œaccuracyå°±å˜æˆäº†optimizing metricï¼Œè€Œrunning timeåˆ™æ˜¯satisfying metricï¼Œstatisfying metricåªè¦è¾¾åˆ°æ ‡å‡†å³å¯ï¼Œè€Œoptimizing metricåˆ™è¿½æ±‚æ›´å¥½ã€‚ä¸€èˆ¬çš„ï¼Œé€‰æ‹©ä¸€é¡¹metricä½œä¸ºoptimizing metricï¼Œå…¶ä»–çš„åˆ™è®¾ç½®ä¸ºsatisfying metricï¼š ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-09-36.jpg) Â¶L 5: Train/dev/test distributions(è®­ç»ƒ/å¼€å‘/æµ‹è¯•é›†åˆ’åˆ†) å¼€å‘ï¼ˆdevï¼‰é›†ä¹Ÿå«åšå¼€å‘é›†ï¼ˆdevelopment setï¼‰ï¼Œæœ‰æ—¶ç§°ä¸ºä¿ç•™äº¤å‰éªŒè¯é›†ï¼ˆhold out cross validation setï¼‰ã€‚ å¦‚ä½•è®¾ç½®Train/dev/testé›†ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå½±å“äº†æœºå™¨å­¦ä¹ çš„é€Ÿåº¦ã€‚ Train/dev/testçš„åŒºåˆ« Workflow in machine learning is that you try a lot of ideas, train up different models on the training set, and then use the dev set to evaluate the different ideas and pick one. And, keep innovating to improve dev set performance until, finally, you have one class that youâ€™re happy with that you then evaluate on your test set. ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-09-37.jpg) å¼€å‘é›†åˆå’Œå¼€å‘é›†åˆæ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œå¦‚æœæ˜¯ä¸åŒåˆ†å¸ƒï¼Œç›¸å½“äºé¶å¿ƒç§»åŠ¨äº† Â¶L 6: Size of dev and test sets(å¼€å‘é›†å’Œæµ‹è¯•é›†çš„å¤§å°) ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-29-51.jpg) ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-25.jpg) Â¶L7 : When to change dev/test sets and metrics(ä»€ä¹ˆæ—¶å€™è¯¥æ”¹å˜å¼€å‘/æµ‹è¯•é›†å’ŒæŒ‡æ ‡) å¦‚æœå‘ç°è®¾å®šç›®æ ‡å’Œå®é™…æœŸæœ›ä¸ç¬¦ï¼Œé‚£å°±è°ƒæ•´ç›®æ ‡ã€‚ ä¸¾ä¸ªä¾‹å­ Aå¯èƒ½æŠŠä¸€äº›è‰²æƒ…ç…§ç‰‡ä¹Ÿåˆ†ç±»æˆçŒ«äº†ï¼Œå› æ­¤æ”¹å˜ä¼˜åŒ–æŒ‡æ ‡ ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-26.jpg) ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-27.jpg) æˆ‘æƒ³ä½ å¤„ç†æœºå™¨å­¦ä¹ é—®é¢˜æ—¶ï¼Œåº”è¯¥æŠŠå®ƒåˆ‡åˆ†æˆç‹¬ç«‹çš„æ­¥éª¤ã€‚ä¸€æ­¥æ˜¯å¼„æ¸…æ¥šå¦‚ä½•å®šä¹‰ä¸€ä¸ªæŒ‡æ ‡æ¥è¡¡é‡ä½ æƒ³åšçš„äº‹æƒ…çš„è¡¨ç°ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥åˆ†å¼€è€ƒè™‘å¦‚ä½•æ”¹å–„ç³»ç»Ÿåœ¨è¿™ä¸ªæŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚ä½ ä»¬è¦æŠŠæœºå™¨å­¦ä¹ ä»»åŠ¡çœ‹æˆä¸¤ä¸ªç‹¬ç«‹çš„æ­¥éª¤ï¼Œç”¨ç›®æ ‡è¿™ä¸ªæ¯”å–»ï¼Œç¬¬ä¸€æ­¥å°±æ˜¯è®¾å®šç›®æ ‡ã€‚æ‰€ä»¥è¦å®šä¹‰ä½ è¦ç„å‡†çš„ç›®æ ‡ï¼Œè¿™æ˜¯å®Œå…¨ç‹¬ç«‹çš„ä¸€æ­¥ï¼Œè¿™æ˜¯ä½ å¯ä»¥è°ƒèŠ‚çš„ä¸€ä¸ªæ—‹é’®ã€‚å¦‚ä½•è®¾ç«‹ç›®æ ‡æ˜¯ä¸€ä¸ªå®Œå…¨ç‹¬ç«‹çš„é—®é¢˜ï¼ŒæŠŠå®ƒçœ‹æˆæ˜¯ä¸€ä¸ªå•ç‹¬çš„æ—‹é’®ï¼Œå¯ä»¥è°ƒè¯•ç®—æ³•è¡¨ç°çš„æ—‹é’®ï¼Œå¦‚ä½•ç²¾ç¡®ç„å‡†ï¼Œå¦‚ä½•å‘½ä¸­ç›®æ ‡ï¼Œå®šä¹‰æŒ‡æ ‡æ˜¯ç¬¬ä¸€æ­¥ã€‚ åç¬¬äºŒæ­¥è¦åšåˆ«çš„äº‹æƒ…ï¼Œåœ¨é€¼è¿‘ç›®æ ‡çš„æ—¶å€™ï¼Œä¹Ÿè®¸ä½ çš„å­¦ä¹ ç®—æ³•é’ˆå¯¹æŸä¸ªé•¿è¿™æ ·çš„æˆæœ¬å‡½æ•°ä¼˜åŒ–ï¼Œ$J=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)$ä½ è¦æœ€å°åŒ–è®­ç»ƒé›†ä¸Šçš„æŸå¤±ã€‚ä½ å¯ä»¥åšçš„å…¶ä¸­ä¸€ä»¶äº‹æ˜¯ï¼Œä¿®æ”¹è¿™ä¸ªï¼Œä¸ºäº†å¼•å…¥è¿™äº›æƒé‡ï¼Œä¹Ÿè®¸æœ€åéœ€è¦ä¿®æ”¹è¿™ä¸ªå½’ä¸€åŒ–å¸¸æ•°ï¼Œ$J=\frac{1}{\sum w^{(i)}} \sum_{i=1}^{m} w^{(i)} L\left(\hat{y}^{(i)}, y^{(i)}\right)$ å†æ¬¡ï¼Œå¦‚ä½•å®šä¹‰Jå¹¶ä¸é‡è¦ï¼Œå…³é”®åœ¨äºæ­£äº¤åŒ–çš„æ€è·¯ï¼ŒæŠŠè®¾ç«‹ç›®æ ‡å®šä¸ºç¬¬ä¸€æ­¥ï¼Œç„¶åç„å‡†å’Œå°„å‡»ç›®æ ‡æ˜¯ç‹¬ç«‹çš„ç¬¬äºŒæ­¥ã€‚æ¢ç§è¯´æ³•ï¼Œæˆ‘é¼“åŠ±ä½ ä»¬å°†å®šä¹‰æŒ‡æ ‡çœ‹æˆä¸€æ­¥ï¼Œç„¶ååœ¨å®šä¹‰äº†æŒ‡æ ‡ä¹‹åï¼Œä½ æ‰èƒ½æƒ³å¦‚ä½•ä¼˜åŒ–ç³»ç»Ÿæ¥æé«˜è¿™ä¸ªæŒ‡æ ‡è¯„åˆ†ã€‚æ¯”å¦‚æ”¹å˜ä½ ç¥ç»ç½‘ç»œè¦ä¼˜åŒ–çš„æˆæœ¬å‡½æ•°Jã€‚ Â¶L8 : Why human-level performance?(ä¸ºä»€ä¹ˆæ˜¯äººçš„è¡¨ç°ï¼Ÿ) ![](Deep Learning ai_Deep Learning Specialization\Bayes-Optimal-Error.png) ä¸Šå›¾å±•ç¤ºäº†éšç€æ—¶é—´çš„æ¨è¿›ï¼Œæœºå™¨å­¦ä¹ ç³»ç»Ÿå’Œäººçš„è¡¨ç°æ°´å¹³çš„å˜åŒ–ã€‚ä¸€èˆ¬çš„ï¼Œå½“æœºå™¨å­¦ä¹ è¶…è¿‡äººçš„è¡¨ç°æ°´å¹³åï¼Œå®ƒçš„è¿›æ­¥é€Ÿåº¦é€æ¸å˜å¾—ç¼“æ…¢ï¼Œæœ€ç»ˆæ€§èƒ½æ— æ³•è¶…è¿‡æŸä¸ªç†è®ºä¸Šé™ï¼Œè¿™ä¸ªä¸Šé™è¢«ç§°ä¸ºè´å¶æ–¯æœ€ä¼˜è¯¯å·®ï¼ˆBayes Optimal Errorï¼‰ã€‚ ä¹Ÿå› æ­¤ï¼Œåªè¦å»ºç«‹çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°è¿˜æ²¡è¾¾åˆ°äººç±»çš„è¡¨ç°æ°´å¹³æ—¶ï¼Œå°±å¯ä»¥é€šè¿‡å„ç§æ‰‹æ®µæ¥æå‡å®ƒã€‚ä¾‹å¦‚é‡‡ç”¨äººå·¥æ ‡è®°è¿‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡äººå·¥è¯¯å·®åˆ†æäº†è§£ä¸ºä»€ä¹ˆäººèƒ½å¤Ÿæ­£ç¡®è¯†åˆ«ï¼Œæˆ–è€…æ˜¯è¿›è¡Œåå·®ã€æ–¹å·®åˆ†æã€‚ å½“æ¨¡å‹çš„è¡¨ç°è¶…è¿‡äººç±»åï¼Œè¿™äº›æ‰‹æ®µèµ·çš„ä½œç”¨å°±å¾®ä¹å…¶å¾®äº†ã€‚ ![](Deep Learning ai_Deep Learning Specialization\e1ef954731399bb4fbf18f2fb99b863a.png) Â¶L9 : Avoidable bias(å¯é¿å…åå·®) training error æˆ‘ä»¬ç»å¸¸ä½¿ç”¨çŒ«åˆ†ç±»å™¨æ¥åšä¾‹å­ï¼Œæ¯”å¦‚äººç±»å…·æœ‰è¿‘ä¹å®Œç¾çš„å‡†ç¡®åº¦ï¼Œæ‰€ä»¥äººç±»æ°´å¹³çš„é”™è¯¯æ˜¯1%ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæ‚¨çš„å­¦ä¹ ç®—æ³•è¾¾åˆ°8%çš„è®­ç»ƒé”™è¯¯ç‡å’Œ10%çš„å¼€å‘é”™è¯¯ç‡ï¼Œé‚£ä¹ˆä½ ä¹Ÿè®¸æƒ³åœ¨è®­ç»ƒé›†ä¸Šå¾—åˆ°æ›´å¥½çš„ç»“æœã€‚æ‰€ä»¥äº‹å®ä¸Šï¼Œä½ çš„ç®—æ³•åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°å’Œäººç±»æ°´å¹³çš„è¡¨ç°æœ‰å¾ˆå¤§å·®è·çš„è¯ï¼Œè¯´æ˜ä½ çš„ç®—æ³•å¯¹è®­ç»ƒé›†çš„æ‹Ÿåˆå¹¶ä¸å¥½ã€‚æ‰€ä»¥ä»å‡å°‘åå·®å’Œæ–¹å·®çš„å·¥å…·è¿™ä¸ªè§’åº¦çœ‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä¼šæŠŠé‡ç‚¹æ”¾åœ¨å‡å°‘åå·®ä¸Šã€‚ä½ éœ€è¦åšçš„æ˜¯ï¼Œæ¯”å¦‚è¯´è®­ç»ƒæ›´å¤§çš„ç¥ç»ç½‘ç»œï¼Œæˆ–è€…è·‘ä¹…ä¸€ç‚¹æ¢¯åº¦ä¸‹é™ï¼Œå°±è¯•è¯•èƒ½ä¸èƒ½åœ¨è®­ç»ƒé›†ä¸Šåšå¾—æ›´å¥½ã€‚ ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-28.jpg) dev error ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-29.jpg) è´å¶æ–¯é”™è¯¯ç‡æˆ–è€…å¯¹è´å¶æ–¯é”™è¯¯ç‡çš„ä¼°è®¡å’Œè®­ç»ƒé”™è¯¯ç‡ä¹‹é—´çš„å·®å€¼ç§°ä¸ºå¯é¿å…åå·® Â¶L 10: Understanding human-level performance(ç†è§£äººçš„è¡¨ç°) è¿˜è®°å¾—ä¸Šä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬ç”¨è¿‡è¿™ä¸ªè¯â€œäººç±»æ°´å¹³é”™è¯¯ç‡â€ç”¨æ¥ä¼°è®¡è´å¶æ–¯è¯¯å·®ï¼Œé‚£å°±æ˜¯ç†è®ºæœ€ä½çš„é”™è¯¯ç‡ï¼Œä»»ä½•å‡½æ•°ä¸ç®¡æ˜¯ç°åœ¨è¿˜æ˜¯å°†æ¥ï¼Œèƒ½å¤Ÿåˆ°è¾¾çš„æœ€ä½å€¼ Â¶L11 : Surpassing human- level performance(è¶…è¿‡äººçš„è¡¨ç°) ç°åœ¨ï¼Œæœºå™¨å­¦ä¹ æœ‰å¾ˆå¤šé—®é¢˜å·²ç»å¯ä»¥å¤§å¤§è¶…è¶Šäººç±»æ°´å¹³äº†ã€‚ ![](Deep Learning ai_Deep Learning Specialization\de2eb0ddc7918f6e9213871e07b8fa56.png) Â¶L12 : Improving your model performance(æ”¹å–„ä½ çš„æ¨¡å‹çš„è¡¨ç°) ä½ ä»¬å­¦è¿‡æ­£äº¤åŒ–ï¼Œå¦‚ä½•è®¾ç«‹å¼€å‘é›†å’Œæµ‹è¯•é›†ï¼Œç”¨äººç±»æ°´å¹³é”™è¯¯ç‡æ¥ä¼°è®¡è´å¶æ–¯é”™è¯¯ç‡ä»¥åŠå¦‚ä½•ä¼°è®¡å¯é¿å…åå·®å’Œæ–¹å·®ã€‚æˆ‘ä»¬ç°åœ¨æŠŠå®ƒä»¬å…¨éƒ¨ç»„åˆèµ·æ¥å†™æˆä¸€å¥—æŒ‡å¯¼æ–¹é’ˆï¼Œå¦‚ä½•æé«˜å­¦ä¹ ç®—æ³•æ€§èƒ½çš„æŒ‡å¯¼æ–¹é’ˆã€‚ ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-39.jpg) method ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-49.jpg) Â¶summary è¿™ä¸€å‘¨çš„å†…å®¹ä¸»è¦æ˜¯æ”¹å–„æ¨¡å‹çš„è¡¨ç°ï¼Œä¸»è¦æ˜¯æŒ‰ç…§æ­£äº¤åŒ–ï¼Œä½¿å¾—æ›´å¥½çš„æ»¡è¶³ 1. è¯„ä»·æŒ‡æ ‡ 2. æ•°æ®é›†çš„åˆ’åˆ† 3. äººçš„è¡¨ç°çš„é‡è¦æ€§ 4. å½“å‡ºç°è¡¨ç°ä¸å¥½çš„æ—¶å€™ï¼Œå¦‚ä½•æ”¹å–„å‘¢ï¼Œæœ‰å“ªäº›æ–¹æ³•å‘¢ï¼Ÿ Â¶W2 ML Strategy(2) Â¶C 1: Carrying out error analysis(è¿›è¡Œè¯¯å·®åˆ†æ) Â¶1. simple analysis ![](Deep Learning ai_Deep Learning Specialization\e1ef954731399bb4fbf18f2fb99b863.png) é€šè¿‡è§‚å¯Ÿå‘ç°ç®—æ³•åˆ†ç±»å‡ºé”™çš„ä¾‹å­ï¼Œæ˜¯æŠŠç‹—åˆ†æˆçŒ«ï¼Œæé«˜å‡†ç¡®ç‡çš„æ–¹æ³•å°±æ˜¯å¦‚ä½•é’ˆå¯¹ç‹—çš„å›¾ç‰‡ä¼˜åŒ–ç®—æ³•ã€‚ä½ å¯ä»¥é’ˆå¯¹ç‹—ï¼Œæ”¶é›†æ›´å¤šçš„ç‹—å›¾ï¼Œæˆ–è€…è®¾è®¡ä¸€äº›åªå¤„ç†ç‹—çš„ç®—æ³•åŠŸèƒ½ä¹‹ç±»çš„ï¼Œä¸ºäº†è®©ä½ çš„çŒ«åˆ†ç±»å™¨åœ¨ç‹—å›¾ä¸Šåšçš„æ›´å¥½ï¼Œè®©ç®—æ³•ä¸å†å°†ç‹—åˆ†ç±»æˆçŒ«ã€‚ç°åœ¨è€ƒè™‘çš„æ˜¯åº”è¯¥ä¸åº”è¯¥è¿™ä¹ˆå»åšå‘¢ï¼Ÿç»Ÿè®¡ä¸€ä¸‹dev seté‡Œé¢å¤šå°‘æ˜¯é”™è¯¯æ ‡è®°æ˜¯ç‹—çš„ä¸ªæ•°ï¼Œåˆ†æå‡ºå¯ä»¥æ”¹å–„çš„ç®—æ³•çš„ä¸Šé™ã€‚ Â¶mutiply analysis ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-51.jpg) ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-50.jpg) Â¶C2 : Cleaning up Incorrectly labeled data(æ¸…é™¤æ ‡æ³¨é”™è¯¯çš„æ•°æ®) Â¶incorrct label Â¶traning set DL algorithms are quite robust to random errors in the traning set so long as your errors or your labeled example to once those errors are not too far from random . Â¶distribution é¦–å…ˆï¼Œæˆ‘é¼“åŠ±ä½ ä¸ç®¡ç”¨ä»€ä¹ˆä¿®æ­£æ‰‹æ®µï¼Œéƒ½è¦åŒæ—¶ä½œç”¨åˆ°å¼€å‘é›†å’Œæµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡ä¸ºä»€ä¹ˆï¼Œå¼€å‘å’Œæµ‹è¯•é›†å¿…é¡»æ¥è‡ªç›¸åŒçš„åˆ†å¸ƒã€‚å¼€å‘é›†ç¡®å®šäº†ä½ çš„ç›®æ ‡ï¼Œå½“ä½ å‡»ä¸­ç›®æ ‡åï¼Œä½ å¸Œæœ›ç®—æ³•èƒ½å¤Ÿæ¨å¹¿åˆ°æµ‹è¯•é›†ä¸Šï¼Œè¿™æ ·ä½ çš„å›¢é˜Ÿèƒ½å¤Ÿæ›´é«˜æ•ˆçš„åœ¨æ¥è‡ªåŒä¸€åˆ†å¸ƒçš„å¼€å‘é›†å’Œæµ‹è¯•é›†ä¸Šè¿­ä»£ã€‚å¦‚æœä½ æ‰“ç®—ä¿®æ­£å¼€å‘é›†ä¸Šçš„éƒ¨åˆ†æ•°æ®ï¼Œé‚£ä¹ˆæœ€å¥½ä¹Ÿå¯¹æµ‹è¯•é›†åšåŒæ ·çš„ä¿®æ­£ä»¥ç¡®ä¿å®ƒä»¬ç»§ç»­æ¥è‡ªç›¸åŒçš„åˆ†å¸ƒã€‚æ‰€ä»¥æˆ‘ä»¬é›‡ä½£äº†ä¸€ä¸ªäººæ¥ä»”ç»†æ£€æŸ¥è¿™äº›æ ‡ç­¾ï¼Œä½†å¿…é¡»åŒæ—¶æ£€æŸ¥å¼€å‘é›†å’Œæµ‹è¯•é›†ã€‚ ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-52.jpg) ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-53.jpg) Â¶suggestion æœ€åæˆ‘è®²å‡ ä¸ªå»ºè®®ï¼š é¦–å…ˆï¼Œæ·±åº¦å­¦ä¹ ç ”ç©¶äººå‘˜æœ‰æ—¶ä¼šå–œæ¬¢è¿™æ ·è¯´ï¼šâ€œæˆ‘åªæ˜¯æŠŠæ•°æ®æä¾›ç»™ç®—æ³•ï¼Œæˆ‘è®­ç»ƒè¿‡äº†ï¼Œæ•ˆæœæ‹”ç¾¤â€ã€‚è¿™è¯è¯´å‡ºäº†å¾ˆå¤šæ·±åº¦å­¦ä¹ é”™è¯¯çš„çœŸç›¸ï¼Œæ›´å¤šæ—¶å€™ï¼Œæˆ‘ä»¬æŠŠæ•°æ®å–‚ç»™ç®—æ³•ï¼Œç„¶åè®­ç»ƒå®ƒï¼Œå¹¶å‡å°‘äººå·¥å¹²é¢„ï¼Œå‡å°‘ä½¿ç”¨äººç±»çš„è§è§£ã€‚ä½†æˆ‘è®¤ä¸ºï¼Œåœ¨æ„é€ å®é™…ç³»ç»Ÿæ—¶ï¼Œé€šå¸¸éœ€è¦æ›´å¤šçš„äººå·¥é”™è¯¯åˆ†æï¼Œæ›´å¤šçš„äººç±»è§è§£æ¥æ¶æ„è¿™äº›ç³»ç»Ÿï¼Œå°½ç®¡æ·±åº¦å­¦ä¹ çš„ç ”ç©¶äººå‘˜ä¸æ„¿æ„æ‰¿è®¤è¿™ç‚¹ã€‚ å…¶æ¬¡ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œæˆ‘çœ‹ä¸€äº›å·¥ç¨‹å¸ˆå’Œç ”ç©¶äººå‘˜ä¸æ„¿æ„äº²è‡ªå»çœ‹è¿™äº›æ ·æœ¬ï¼Œä¹Ÿè®¸åšè¿™äº›äº‹æƒ…å¾ˆæ— èŠï¼Œåä¸‹æ¥çœ‹100æˆ–å‡ ç™¾ä¸ªæ ·æœ¬æ¥ç»Ÿè®¡é”™è¯¯æ•°é‡ï¼Œä½†æˆ‘ç»å¸¸äº²è‡ªè¿™ä¹ˆåšã€‚å½“æˆ‘å¸¦é¢†ä¸€ä¸ªæœºå™¨å­¦ä¹ å›¢é˜Ÿæ—¶ï¼Œæˆ‘æƒ³çŸ¥é“å®ƒæ‰€çŠ¯çš„é”™è¯¯ï¼Œæˆ‘ä¼šäº²è‡ªå»çœ‹çœ‹è¿™äº›æ•°æ®ï¼Œå°è¯•å’Œä¸€éƒ¨åˆ†é”™è¯¯ä½œæ–—äº‰ã€‚æˆ‘æƒ³å°±å› ä¸ºèŠ±äº†è¿™å‡ åˆ†é’Ÿï¼Œæˆ–è€…å‡ ä¸ªå°æ—¶å»äº²è‡ªç»Ÿè®¡æ•°æ®ï¼ŒçœŸçš„å¯ä»¥å¸®ä½ æ‰¾åˆ°éœ€è¦ä¼˜å…ˆå¤„ç†çš„ä»»åŠ¡ï¼Œæˆ‘å‘ç°èŠ±æ—¶é—´äº²è‡ªæ£€æŸ¥æ•°æ®éå¸¸å€¼å¾—ï¼Œæ‰€ä»¥æˆ‘å¼ºçƒˆå»ºè®®ä½ ä»¬è¿™æ ·åšï¼Œå¦‚æœä½ åœ¨æ­å»ºä½ çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„è¯ï¼Œç„¶åä½ æƒ³ç¡®å®šåº”è¯¥ä¼˜å…ˆå°è¯•å“ªäº›æƒ³æ³•ï¼Œæˆ–è€…å“ªäº›æ–¹å‘ã€‚ è¿™å°±æ˜¯é”™è¯¯åˆ†æè¿‡ç¨‹ï¼Œåœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘æƒ³åˆ†äº«ä¸€ä¸‹é”™è¯¯åˆ†ææ˜¯å¦‚ä½•åœ¨å¯åŠ¨æ–°çš„æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­å‘æŒ¥ä½œç”¨çš„ã€‚ Â¶C3: Build your first system quickly, then iterate(å¿«é€Ÿæ­å»ºä½ çš„ç¬¬ä¸€ä¸ªç³»ç»Ÿï¼Œå¹¶è¿›è¡Œè¿­ä»£) Â¶1. iteration I recommend that you first quickly set up a definition and metrics so this is really you know deciding where to place your target and you get it wrong you can always move it later we just set up a target somewhere and then I recommend you build an inital machine learning system quickly find the traning set train it and see start to see and understand how well your are doing against your Devon chess setting evaluation metric when you build your initial system you then be able to use bias variance analysis we should talk about earlier as well as error analysis whick we talked about just in last several videos to prioritize the next step in particular if error analysis causes you to realize that a lot of the errors are from the spearker being very far from the mirophone which causes special challenges speech recognitin then that would give you a good reason to focus on techniques to address this it called fast used speech recognition which basically means handling when the speaker is very far from microphone along the value of building this inital system it can be a quick and diry implementation you know do not overthink it but all the value of the inital system is having some learning system having some tranin system allows you lok at bias and variance to do error analysis look at some mistakes to figure out all the different directins you could go in. æˆ‘é¼“åŠ±ä½ ä»¬æ­å»ºå¿«é€Ÿè€Œç²—ç³™çš„å®ç°ï¼Œç„¶åç”¨å®ƒåšåå·®/æ–¹å·®åˆ†æï¼Œç”¨å®ƒåšé”™è¯¯åˆ†æï¼Œç„¶åç”¨åˆ†æç»“æœç¡®å®šä¸‹ä¸€æ­¥ä¼˜å…ˆè¦åšçš„æ–¹å‘ã€‚ Â¶C4 : Training and testing on different distributions(ä½¿ç”¨æ¥è‡ªä¸åŒåˆ†å¸ƒçš„æ•°æ®ï¼Œè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•) this is resulted in many teams sometimes taking one of the days you can find and just shoving it into the training set . Cat app example ![](Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-54.jpg) å‡è®¾ä½ åœ¨å¼€å‘ä¸€ä¸ªæ‰‹æœºåº”ç”¨ï¼Œç”¨æˆ·ä¼šä¸Šä¼ ä»–ä»¬ç”¨æ‰‹æœºæ‹æ‘„çš„ç…§ç‰‡ï¼Œä½ æƒ³è¯†åˆ«ç”¨æˆ·ä»åº”ç”¨ä¸­ä¸Šä¼ çš„å›¾ç‰‡æ˜¯ä¸æ˜¯çŒ«ã€‚ç°åœ¨ä½ æœ‰ä¸¤ä¸ªæ•°æ®æ¥æºï¼Œä¸€ä¸ªæ˜¯ä½ çœŸæ­£å…³å¿ƒçš„æ•°æ®åˆ†å¸ƒï¼Œæ¥è‡ªåº”ç”¨ä¸Šä¼ çš„æ•°æ®ï¼Œæ¯”å¦‚å³è¾¹çš„åº”ç”¨ï¼Œè¿™äº›ç…§ç‰‡ä¸€èˆ¬æ›´ä¸šä½™ï¼Œå–æ™¯ä¸å¤ªå¥½ï¼Œæœ‰äº›ç”šè‡³å¾ˆæ¨¡ç³Šï¼Œå› ä¸ºå®ƒä»¬éƒ½æ˜¯ä¸šä½™ç”¨æˆ·æ‹çš„ã€‚å¦ä¸€ä¸ªæ•°æ®æ¥æºå°±æ˜¯ä½ å¯ä»¥ç”¨çˆ¬è™«ç¨‹åºæŒ–æ˜ç½‘é¡µç›´æ¥ä¸‹è½½ï¼Œå°±è¿™ä¸ªæ ·æœ¬è€Œè¨€ï¼Œå¯ä»¥ä¸‹è½½å¾ˆå¤šå–æ™¯ä¸“ä¸šã€é«˜åˆ†è¾¨ç‡ã€æ‹æ‘„ä¸“ä¸šçš„çŒ«å›¾ç‰‡ã€‚å¦‚æœä½ çš„åº”ç”¨ç”¨æˆ·æ•°è¿˜ä¸å¤šï¼Œä¹Ÿè®¸ä½ åªæ”¶é›†åˆ°10,000å¼ ç”¨æˆ·ä¸Šä¼ çš„ç…§ç‰‡ï¼Œä½†é€šè¿‡çˆ¬è™«æŒ–æ˜ç½‘é¡µï¼Œä½ å¯ä»¥ä¸‹è½½åˆ°æµ·é‡çŒ«å›¾ï¼Œä¹Ÿè®¸ä½ ä»äº’è”ç½‘ä¸Šä¸‹è½½äº†è¶…è¿‡20ä¸‡å¼ çŒ«å›¾ã€‚è€Œä½ çœŸæ­£å…³å¿ƒçš„ç®—æ³•è¡¨ç°æ˜¯ä½ çš„æœ€ç»ˆç³»ç»Ÿå¤„ç†æ¥è‡ªåº”ç”¨ç¨‹åºçš„è¿™ä¸ªå›¾ç‰‡åˆ†å¸ƒæ—¶æ•ˆæœå¥½ä¸å¥½ï¼Œå› ä¸ºæœ€åä½ çš„ç”¨æˆ·ä¼šä¸Šä¼ ç±»ä¼¼å³è¾¹è¿™äº›å›¾ç‰‡ï¼Œä½ çš„åˆ†ç±»å™¨å¿…é¡»åœ¨è¿™ä¸ªä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚ç°åœ¨ä½ å°±é™·å…¥å›°å¢ƒäº†ï¼Œå› ä¸ºä½ æœ‰ä¸€ä¸ªç›¸å¯¹å°çš„æ•°æ®é›†ï¼Œåªæœ‰10,000ä¸ªæ ·æœ¬æ¥è‡ªé‚£ä¸ªåˆ†å¸ƒï¼Œè€Œä½ è¿˜æœ‰ä¸€ä¸ªå¤§å¾—å¤šçš„æ•°æ®é›†æ¥è‡ªå¦ä¸€ä¸ªåˆ†å¸ƒï¼Œå›¾ç‰‡çš„å¤–è§‚å’Œä½ çœŸæ­£æƒ³è¦å¤„ç†çš„å¹¶ä¸ä¸€æ ·ã€‚ä½†ä½ åˆä¸æƒ³ç›´æ¥ç”¨è¿™10,000å¼ å›¾ç‰‡ï¼Œå› ä¸ºè¿™æ ·ä½ çš„è®­ç»ƒé›†å°±å¤ªå°äº†ï¼Œä½¿ç”¨è¿™20ä¸‡å¼ å›¾ç‰‡ä¼¼ä¹æœ‰å¸®åŠ©ã€‚ä½†æ˜¯ï¼Œå›°å¢ƒåœ¨äºï¼Œè¿™20ä¸‡å¼ å›¾ç‰‡å¹¶ä¸å®Œå…¨æ¥è‡ªä½ æƒ³è¦çš„åˆ†å¸ƒï¼Œé‚£ä¹ˆä½ å¯ä»¥æ€ä¹ˆåšå‘¢ï¼Ÿ æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„æ˜¯æ¥è‡ªæ‰‹æœºæ‰‹æœºæ”¶é›†çš„æ•°æ®ï¼Œè€Œä¸æ˜¯æ¥è‡ªç½‘é¡µã€‚æ–¹æ³•ä¸€ï¼Œéšæœºåˆ†é…è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼Œè¿™æ ·çš„åæœå°±æ˜¯èŠ±äº†å¤§é‡æ—¶é—´åœ¨å®é™…ä¸å…³å¿ƒçš„æ•°æ®åˆ†å¸ƒå»ä¼˜åŒ–ã€‚ è®­ç»ƒé›†20ä¸‡å¼ ç½‘ç»œï¼Œ5000æ‰‹æœºï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†å„2500ï¼Œè¿™æ ·å¯ä»¥ä¿è¯éªŒè¯é›†å’Œæµ‹è¯•é›†æ›´æ¥è¿‘å®é™…åº”ç”¨åœºæ™¯ï¼Œæˆ‘ä»¬è¯•è¯•æ­å»ºä¸€ä¸ªå­¦ä¹ ç³»ç»Ÿï¼Œè®©ç³»ç»Ÿåœ¨å¤„ç†æ‰‹æœºä¸Šä¼ å›¾ç‰‡åˆ†å¸ƒæ—¶æ•ˆæœè‰¯å¥½ã€‚ç¼ºç‚¹åœ¨äºï¼Œå½“ç„¶äº†ï¼Œç°åœ¨ä½ çš„è®­ç»ƒé›†åˆ†å¸ƒå’Œä½ çš„å¼€å‘é›†ã€æµ‹è¯•é›†åˆ†å¸ƒå¹¶ä¸ä¸€æ ·ã€‚ä½†äº‹å®è¯æ˜ï¼Œè¿™æ ·æŠŠæ•°æ®åˆ†æˆè®­ç»ƒã€å¼€å‘å’Œæµ‹è¯•é›†ï¼Œåœ¨é•¿æœŸèƒ½ç»™ä½ å¸¦æ¥æ›´å¥½çš„ç³»ç»Ÿæ€§èƒ½ã€‚æˆ‘ä»¬ä»¥åä¼šè®¨è®ºä¸€äº›ç‰¹æ®Šçš„æŠ€å·§ï¼Œå¯ä»¥å¤„ç† è®­ç»ƒé›†çš„åˆ†å¸ƒå’Œå¼€å‘é›†å’Œæµ‹è¯•é›†åˆ†å¸ƒä¸ä¸€æ ·çš„æƒ…å†µã€‚ Â¶C5: Bias and Variance with mismatched data distributionsï¼ˆæ•°æ®åˆ†å¸ƒä¸åŒ¹é…æ—¶ï¼Œåå·®ä¸æ–¹å·®çš„åˆ†æï¼‰ é¦–å…ˆç®—æ³•åªçœ‹è¿‡è®­ç»ƒé›†æ•°æ®ï¼Œæ²¡çœ‹è¿‡å¼€å‘é›†æ•°æ®ã€‚ç¬¬äºŒï¼Œå¼€å‘é›†æ•°æ®æ¥è‡ªä¸åŒçš„åˆ†å¸ƒã€‚å¾ˆéš¾ç¡®è®¤è¿™å¢åŠ çš„9%è¯¯å·®ç‡æœ‰å¤šå°‘æ˜¯å› ä¸ºç®—æ³•æ²¡çœ‹åˆ°å¼€å‘é›†ä¸­çš„æ•°æ®å¯¼è‡´çš„ï¼Œè¿™ä¹ˆè¯„ä¼°å‘¢ï¼Ÿåˆ°åº•å“ªä¸ªå½±å“å…ƒç´ æ›´å¤§ï¼Œ è¯„ä¼°æ–¹æ³•ï¼Œè®­ç»ƒé›†çš„åˆ†å¸ƒæŒ–å‡ºï¼Œtraning-dev set : Same distributation as traning set ,but not used for training. ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†è®­ç»ƒé›†é”™è¯¯ç‡ã€è®­ç»ƒ-éªŒè¯é›†é”™è¯¯ç‡ï¼Œä»¥åŠéªŒè¯é›†é”™è¯¯ç‡ã€‚å…¶ä¸­ï¼Œè®­ç»ƒé›†é”™è¯¯ç‡å’Œè®­ç»ƒ-éªŒè¯é›†é”™è¯¯ç‡çš„å·®å€¼åæ˜ äº†æ–¹å·®ï¼›è€Œè®­ç»ƒ-éªŒè¯é›†é”™è¯¯ç‡å’ŒéªŒè¯é›†é”™è¯¯ç‡çš„å·®å€¼åæ˜ äº†æ ·æœ¬åˆ†å¸ƒä¸ä¸€è‡´çš„é—®é¢˜ï¼Œä»è€Œè¯´æ˜æ¨¡å‹æ“…é•¿å¤„ç†çš„æ•°æ®å’Œæˆ‘ä»¬å…³å¿ƒçš„æ•°æ®æ¥è‡ªä¸åŒçš„åˆ†å¸ƒï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º**æ•°æ®ä¸åŒ¹é…ï¼ˆData Mismatchï¼‰**é—®é¢˜ã€‚ ![](K:\MyBlog\hexo\source_posts\Deep Learning ai_Deep Learning Specialization\Analysis-With-Data-Mismatch.png) Â¶C6: Addressing data mismatchï¼ˆå¤„ç†æ•°æ®ä¸åŒ¹é…é—®é¢˜ï¼‰ I![](K:\MyBlog\hexo\source_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-55.jpg) Data: Artifical data synthesis æ‰€ä»¥ï¼Œæ€»è€Œè¨€ä¹‹ï¼Œå¦‚æœä½ è®¤ä¸ºå­˜åœ¨æ•°æ®ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘å»ºè®®ä½ åšé”™è¯¯åˆ†æï¼Œæˆ–è€…çœ‹çœ‹è®­ç»ƒé›†ï¼Œæˆ–è€…çœ‹çœ‹å¼€å‘é›†ï¼Œè¯•å›¾æ‰¾å‡ºï¼Œè¯•å›¾äº†è§£è¿™ä¸¤ä¸ªæ•°æ®åˆ†å¸ƒåˆ°åº•æœ‰ä»€ä¹ˆä¸åŒï¼Œç„¶åçœ‹çœ‹æ˜¯å¦æœ‰åŠæ³•æ”¶é›†æ›´å¤šçœ‹èµ·æ¥åƒå¼€å‘é›†çš„æ•°æ®ä½œè®­ç»ƒã€‚ Â¶C7: Transfer learningï¼ˆè¿ç§»å­¦ä¹ ï¼‰ **è¿ç§»å­¦ä¹ ï¼ˆTranfer Learningï¼‰**æ˜¯é€šè¿‡å°†å·²è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€éƒ¨åˆ†ç½‘ç»œç»“æ„åº”ç”¨åˆ°å¦ä¸€æ¨¡å‹ï¼Œå°†ä¸€ä¸ªç¥ç»ç½‘ç»œä»æŸä¸ªä»»åŠ¡ä¸­å­¦åˆ°çš„çŸ¥è¯†å’Œç»éªŒè¿ç”¨åˆ°å¦ä¸€ä¸ªä»»åŠ¡ä¸­ï¼Œä»¥æ˜¾è‘—æé«˜å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½ã€‚ ä¾‹å¦‚ï¼Œæˆ‘ä»¬å°†ä¸ºçŒ«è¯†åˆ«å™¨æ„å»ºçš„ç¥ç»ç½‘ç»œè¿ç§»åº”ç”¨åˆ°æ”¾å°„ç§‘è¯Šæ–­ä¸­ã€‚å› ä¸ºçŒ«è¯†åˆ«å™¨çš„ç¥ç»ç½‘ç»œå·²ç»å­¦ä¹ åˆ°äº†æœ‰å…³å›¾åƒçš„ç»“æ„å’Œæ€§è´¨ç­‰æ–¹é¢çš„çŸ¥è¯†ï¼Œæ‰€ä»¥åªè¦å…ˆåˆ é™¤ç¥ç»ç½‘ç»œä¸­åŸæœ‰çš„è¾“å‡ºå±‚ï¼ŒåŠ å…¥æ–°çš„è¾“å‡ºå±‚å¹¶éšæœºåˆå§‹åŒ–æƒé‡ç³»æ•°ï¼ˆ$W[L]$ã€$b[L]$ï¼‰ï¼Œéšåç”¨æ–°çš„è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒï¼Œå°±å®Œæˆäº†ä»¥ä¸Šçš„è¿ç§»å­¦ä¹ ã€‚ å¦‚æœæ–°çš„æ•°æ®é›†å¾ˆå°ï¼Œå¯èƒ½åªéœ€è¦é‡æ–°è®­ç»ƒè¾“å‡ºå±‚å‰çš„æœ€åä¸€å±‚çš„æƒé‡ï¼Œå³$W[L]$$ã€b[L]$ï¼Œå¹¶ä¿æŒå…¶ä»–å‚æ•°ä¸å˜ï¼›è€Œå¦‚æœæœ‰è¶³å¤Ÿå¤šçš„æ•°æ®ï¼Œå¯ä»¥åªä¿ç•™ç½‘ç»œç»“æ„ï¼Œé‡æ–°è®­ç»ƒç¥ç»ç½‘ç»œä¸­æ‰€æœ‰å±‚çš„ç³»æ•°ã€‚è¿™æ—¶åˆå§‹æƒé‡ç”±ä¹‹å‰çš„æ¨¡å‹è®­ç»ƒå¾—åˆ°ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºé¢„è®­ç»ƒï¼ˆPre-Trainingï¼‰ï¼Œä¹‹åçš„æƒé‡æ›´æ–°è¿‡ç¨‹ç§°ä¸ºå¾®è°ƒï¼ˆFine-Tuningï¼‰ã€‚ ![](K:\MyBlog\hexo\source_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-56.jpg) ![](K:\MyBlog\hexo\source_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-57.jpg) åœ¨ä¸‹è¿°åœºåˆè¿›è¡Œè¿ç§»å­¦ä¹ æ˜¯æœ‰æ„ä¹‰çš„ï¼š ä¸¤ä¸ªä»»åŠ¡æœ‰åŒæ ·çš„è¾“å…¥ï¼ˆæ¯”å¦‚éƒ½æ˜¯å›¾åƒæˆ–è€…éƒ½æ˜¯éŸ³é¢‘ï¼‰ï¼› æ‹¥æœ‰æ›´å¤šæ•°æ®çš„ä»»åŠ¡è¿ç§»åˆ°æ•°æ®è¾ƒå°‘çš„ä»»åŠ¡ï¼› æŸä¸€ä»»åŠ¡çš„ä½å±‚æ¬¡ç‰¹å¾ï¼ˆåº•å±‚ç¥ç»ç½‘ç»œçš„æŸäº›åŠŸèƒ½ï¼‰å¯¹å¦ä¸€ä¸ªä»»åŠ¡çš„å­¦ä¹ æœ‰å¸®åŠ©ã€‚ ![](K:\MyBlog\hexo\source_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-58.jpg) Â¶C8; Multi-task learning ï¼ˆå¤šä»»åŠ¡å­¦ä¹ ï¼‰ For example, autonomous driving example,check cars,stop signs,trfffic lights ,è¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªå‘é‡ï¼Œ ![](K:\MyBlog\hexo\source_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-59.jpg) ![](K:\MyBlog\hexo\source_posts\Deep Learning ai_Deep Learning Specialization\Xnip2018-06-26_08-32-60.jpg) Â¶C9 : What is end-to-end deep learning?(ä»€ä¹ˆæ˜¯ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ ) åœ¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†å—æ¨¡å‹ä¸­ï¼Œæ¯ä¸€ä¸ªæ¨¡å—å¤„ç†ä¸€ç§è¾“å…¥ï¼Œç„¶åå…¶è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªæ¨¡å—çš„è¾“å…¥ï¼Œæ„æˆä¸€æ¡æµæ°´çº¿ã€‚è€Œ**ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ ï¼ˆEnd-to-end Deep Learningï¼‰**åªç”¨ä¸€ä¸ªå•ä¸€çš„ç¥ç»ç½‘ç»œæ¨¡å‹æ¥å®ç°æ‰€æœ‰çš„åŠŸèƒ½ã€‚å®ƒå°†æ‰€æœ‰æ¨¡å—æ··åˆåœ¨ä¸€èµ·ï¼Œåªå…³å¿ƒè¾“å…¥å’Œè¾“å‡ºã€‚ ![](K:\MyBlog\hexo\source_posts\Deep Learning ai_Deep Learning Specialization\End-to-end-Deep-Learning.png) Â¶ä¼˜ç‚¹ä¸ç¼ºç‚¹ åº”ç”¨ç«¯åˆ°ç«¯å­¦ä¹ çš„ä¼˜ç‚¹ï¼š åªè¦æœ‰è¶³å¤Ÿå¤šçš„æ•°æ®ï¼Œå‰©ä¸‹çš„å…¨éƒ¨äº¤ç»™ä¸€ä¸ªè¶³å¤Ÿå¤§çš„ç¥ç»ç½‘ç»œã€‚æ¯”èµ·ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†å—æ¨¡å‹ï¼Œå¯èƒ½æ›´èƒ½æ•è·æ•°æ®ä¸­çš„ä»»ä½•ç»Ÿè®¡ä¿¡æ¯ï¼Œè€Œä¸éœ€è¦ç”¨äººç±»å›ºæœ‰çš„è®¤çŸ¥ï¼ˆæˆ–è€…è¯´ï¼Œæˆè§ï¼‰æ¥è¿›è¡Œåˆ†æï¼› æ‰€éœ€æ‰‹å·¥è®¾è®¡çš„ç»„ä»¶æ›´å°‘ï¼Œç®€åŒ–è®¾è®¡å·¥ä½œæµç¨‹ï¼› ç¼ºç‚¹ï¼š éœ€è¦å¤§é‡çš„æ•°æ®ï¼› æ’é™¤äº†å¯èƒ½æœ‰ç”¨çš„äººå·¥è®¾è®¡ç»„ä»¶ï¼› æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œå†³å®šä¸€ä¸ªé—®é¢˜æ˜¯å¦åº”ç”¨ç«¯åˆ°ç«¯å­¦ä¹ çš„å…³é”®ç‚¹æ˜¯ï¼šæ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ”¯æŒèƒ½å¤Ÿç›´æ¥å­¦ä¹ ä» x æ˜ å°„åˆ° y å¹¶ä¸”è¶³å¤Ÿå¤æ‚çš„å‡½æ•°ï¼Ÿ Â¶Whether to use end-to-end learning?(æ˜¯å¦è¦ä½¿ç”¨ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ ?) Pros: â€‹ let the data speak : x-&gt;y â€‹ less hand-designing of components needed Cons: â€‹ May need large amount of data â€‹ excludes potentially useful hand-designed components Key question: Do you hava sufficient data to learn a function of the complexity needed to map x to y? å¦‚æœä½ æƒ³ä½¿ç”¨æœºå™¨å­¦ä¹ æˆ–è€…æ·±åº¦å­¦ä¹ æ¥å­¦ä¹ æŸäº›å•ç‹¬çš„ç»„ä»¶ï¼Œé‚£ä¹ˆå½“ä½ åº”ç”¨ç›‘ç£å­¦ä¹ æ—¶ï¼Œä½ åº”è¯¥ä»”ç»†é€‰æ‹©è¦å­¦ä¹ çš„xåˆ°yæ˜ å°„ç±»å‹ï¼Œè¿™å–å†³äºé‚£äº›ä»»åŠ¡ä½ å¯ä»¥æ”¶é›†æ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè°ˆè®ºçº¯ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ–¹æ³•æ˜¯å¾ˆæ¿€åŠ¨äººå¿ƒçš„ï¼Œä½ è¾“å…¥å›¾åƒï¼Œç›´æ¥å¾—å‡ºæ–¹å‘ç›˜è½¬è§’ï¼Œä½†æ˜¯å°±ç›®å‰èƒ½æ”¶é›†åˆ°çš„æ•°æ®è€Œè¨€ï¼Œè¿˜æœ‰æˆ‘ä»¬ä»Šå¤©èƒ½å¤Ÿç”¨ç¥ç»ç½‘ç»œå­¦ä¹ çš„æ•°æ®ç±»å‹è€Œè¨€ï¼Œè¿™å®é™…ä¸Šä¸æ˜¯æœ€æœ‰å¸Œæœ›çš„æ–¹æ³•ï¼Œæˆ–è€…è¯´è¿™ä¸ªæ–¹æ³•å¹¶ä¸æ˜¯å›¢é˜Ÿæƒ³å‡ºçš„æœ€å¥½ç”¨çš„æ–¹æ³•ã€‚è€Œæˆ‘è®¤ä¸ºè¿™ç§çº¯ç²¹çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå…¶å®å‰æ™¯ä¸å¦‚è¿™æ ·æ›´å¤æ‚çš„å¤šæ­¥æ–¹æ³•ã€‚å› ä¸ºç›®å‰èƒ½æ”¶é›†åˆ°çš„æ•°æ®ï¼Œè¿˜æœ‰æˆ‘ä»¬ç°åœ¨è®­ç»ƒç¥ç»ç½‘ç»œçš„èƒ½åŠ›æ˜¯æœ‰å±€é™çš„ã€‚ Summary å­¦ä¹ å¦‚ä½•é€šè¿‡ä¸€äº›æ‰‹æ®µæé«˜æ¨¡å‹çš„è¡¨ç°ï¼Œé¦–å…ˆäº†è§£æ¨¡å‹çš„æ€§èƒ½çš„ä½“ç°ï¼Œbiasã€varianceã€è´å¶æ–¯è¯¯å·®ã€‚ä»¥åŠå¦‚ä½•ä¸€æ­¥æ­¥çš„æ”¹å–„æ€§èƒ½ã€‚å…·ä½“è§£å†³äº†å¦‚ä¸‹é—®é¢˜ï¼Œ1. æ•°æ®çš„åˆ’åˆ† 2. äººçš„è¡¨ç°ä¸æœºå™¨æ€§èƒ½çš„å…³ç³»ã€åå·®ã€æ–¹å·® 3. è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ†å¸ƒé—®é¢˜ï¼Œå½“æ•°æ®æ ·æœ¬å¯¹äºè§£å†³é—®é¢˜ä¸è¶³çš„æ—¶å€™çš„è§£å†³åŠæ³•ï¼Œ4. è¿ç§»å­¦ä¹  5. ç«¯åˆ°ç«¯çš„å­¦ä¹  6. å¤šä»»åŠ¡å­¦ä¹ ã€‚6. åœ¨æ€§èƒ½ä¸å¥½çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨çš„åˆ†æè¯¯å·®ï¼Œå¯¹æµ‹è¯•é›†é”™è¯¯æ ·ä¾‹åšç»Ÿè®¡ç­‰ç­‰ï¼Œ]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[aiai_]]></title>
    <url>%2F2019%2F04%2F17%2FImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning%2C%20Regularization%20and%20Optimization%2F</url>
    <content type="text"><![CDATA[C2W1 Â¶L01 : Train/Dev/Test Sets Â¶1. process åº”ç”¨å‹æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªé«˜åº¦è¿­ä»£çš„è¿‡ç¨‹ï¼Œé€šå¸¸åœ¨é¡¹ç›®å¯åŠ¨æ—¶ï¼Œæˆ‘ä»¬ä¼šå…ˆæœ‰ä¸€ä¸ªåˆæ­¥æƒ³æ³•ï¼Œæ¯”å¦‚æ„å»ºä¸€ä¸ªå«æœ‰ç‰¹å®šå±‚æ•°ï¼Œéšè—å•å…ƒæ•°é‡æˆ–æ•°æ®é›†ä¸ªæ•°ç­‰ç­‰çš„ç¥ç»ç½‘ç»œï¼Œç„¶åç¼–ç ï¼Œå¹¶å°è¯•è¿è¡Œè¿™äº›ä»£ç ï¼Œé€šè¿‡è¿è¡Œå’Œæµ‹è¯•å¾—åˆ°è¯¥ç¥ç»ç½‘ç»œæˆ–è¿™äº›é…ç½®ä¿¡æ¯çš„è¿è¡Œç»“æœï¼Œä½ å¯èƒ½ä¼šæ ¹æ®è¾“å‡ºç»“æœé‡æ–°å®Œå–„è‡ªå·±çš„æƒ³æ³•ï¼Œæ”¹å˜ç­–ç•¥ï¼Œæˆ–è€…ä¸ºäº†æ‰¾åˆ°æ›´å¥½çš„ç¥ç»ç½‘ç»œä¸æ–­è¿­ä»£æ›´æ–°è‡ªå·±çš„æ–¹æ¡ˆã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_2.png) Â¶2. data split è®­ç»ƒé›†ï¼ˆtrain setï¼‰ï¼šç”¨è®­ç»ƒé›†å¯¹ç®—æ³•æˆ–æ¨¡å‹è¿›è¡Œè®­ç»ƒè¿‡ç¨‹ï¼› éªŒè¯é›†ï¼ˆdevelopment setï¼‰ï¼šåˆ©ç”¨éªŒè¯é›†ï¼ˆåˆç§°ä¸ºç®€å•äº¤å‰éªŒè¯é›†ï¼Œhold-out cross validation setï¼‰è¿›è¡Œäº¤å‰éªŒè¯ï¼Œé€‰æ‹©å‡ºæœ€å¥½çš„æ¨¡å‹æˆ–è€…éªŒè¯ä¸åŒç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ æµ‹è¯•é›†ï¼ˆtest setï¼‰ï¼šæœ€ååˆ©ç”¨æµ‹è¯•é›†å¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œè·å–æ¨¡å‹è¿è¡Œçš„æ— åä¼°è®¡ï¼ˆå¯¹å­¦ä¹ æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼‰ã€‚ å‡è®¾è¿™æ˜¯è®­ç»ƒæ•°æ®ï¼Œæˆ‘ç”¨ä¸€ä¸ªé•¿æ–¹å½¢è¡¨ç¤ºï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°†è¿™äº›æ•°æ®åˆ’åˆ†æˆå‡ éƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†ä½œä¸ºè®­ç»ƒé›†ï¼Œä¸€éƒ¨åˆ†ä½œä¸ºç®€å•äº¤å‰éªŒè¯é›†ï¼Œæœ‰æ—¶ä¹Ÿç§°ä¹‹ä¸ºéªŒè¯é›†ï¼Œæ–¹ä¾¿èµ·è§ï¼Œæˆ‘å°±å«å®ƒéªŒè¯é›†ï¼ˆdev setï¼‰ï¼Œå…¶å®éƒ½æ˜¯åŒä¸€ä¸ªæ¦‚å¿µï¼Œæœ€åä¸€éƒ¨åˆ†åˆ™ä½œä¸ºæµ‹è¯•é›†ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_3.png) åœ¨æœºå™¨å­¦ä¹ å‘å±•çš„å°æ•°æ®é‡æ—¶ä»£ï¼Œå¦‚ 100ã€1000ã€10000 çš„æ•°æ®é‡å¤§å°ï¼Œå¯ä»¥å°†æ•°æ®é›†æŒ‰ç…§ä»¥ä¸‹æ¯”ä¾‹è¿›è¡Œåˆ’åˆ†ï¼š æ— éªŒè¯é›†çš„æƒ…å†µï¼š70% / 30%ï¼› æœ‰éªŒè¯é›†çš„æƒ…å†µï¼š60% / 20% / 20%ï¼› åœ¨å¦‚ä»Šçš„å¤§æ•°æ®æ—¶ä»£ï¼Œå¯¹äºä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®é›†çš„è§„æ¨¡å¯èƒ½æ˜¯ç™¾ä¸‡çº§åˆ«çš„ï¼Œæ‰€ä»¥éªŒè¯é›†å’Œæµ‹è¯•é›†æ‰€å çš„æ¯”é‡ä¼šè¶‹å‘äºå˜å¾—æ›´å°ã€‚ éªŒè¯é›†çš„ç›®çš„æ˜¯ä¸ºäº†éªŒè¯ä¸åŒçš„ç®—æ³•å“ªç§æ›´åŠ æœ‰æ•ˆï¼Œæ‰€ä»¥éªŒè¯é›†åªè¦è¶³å¤Ÿå¤§åˆ°èƒ½å¤ŸéªŒè¯å¤§çº¦ 2-10 ç§ç®—æ³•å“ªç§æ›´å¥½ï¼Œè€Œä¸éœ€è¦ä½¿ç”¨ 20% çš„æ•°æ®ä½œä¸ºéªŒè¯é›†ã€‚å¦‚ç™¾ä¸‡æ•°æ®ä¸­æŠ½å– 1 ä¸‡çš„æ•°æ®ä½œä¸ºéªŒè¯é›†å°±å¯ä»¥äº†ã€‚ æµ‹è¯•é›†çš„ä¸»è¦ç›®çš„æ˜¯è¯„ä¼°æ¨¡å‹çš„æ•ˆæœï¼Œå¦‚åœ¨å•ä¸ªåˆ†ç±»å™¨ä¸­ï¼Œå¾€å¾€åœ¨ç™¾ä¸‡çº§åˆ«çš„æ•°æ®ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©å…¶ä¸­ 1000 æ¡æ•°æ®è¶³ä»¥è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ•ˆæœã€‚ 100 ä¸‡æ•°æ®é‡ï¼š98% / 1% / 1%ï¼› è¶…ç™¾ä¸‡æ•°æ®é‡ï¼š99.5% / 0.25% / 0.25%ï¼ˆæˆ–è€…99.5% / 0.4% / 0.1%ï¼‰ Â¶3. å»ºè®® éªŒè¯é›†è¦å’Œè®­ç»ƒé›†æ¥è‡ªäºåŒä¸€ä¸ªåˆ†å¸ƒï¼ˆæ•°æ®æ¥æºä¸€è‡´ï¼‰ï¼Œå¯ä»¥ä½¿å¾—æœºå™¨å­¦ä¹ ç®—æ³•å˜å¾—æ›´å¿«å¹¶è·å¾—æ›´å¥½çš„æ•ˆæœã€‚ å¦‚æœä¸éœ€è¦ç”¨æ— åä¼°è®¡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œåˆ™å¯ä»¥ä¸éœ€è¦æµ‹è¯•é›†ã€‚å¦‚æœåªæœ‰éªŒè¯é›†ï¼Œæ²¡æœ‰æµ‹è¯•é›†ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒï¼Œå°è¯•ä¸åŒçš„æ¨¡å‹æ¡†æ¶ï¼Œåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°è¿™äº›æ¨¡å‹ï¼Œç„¶åè¿­ä»£å¹¶é€‰å‡ºé€‚ç”¨çš„æ¨¡å‹ã€‚å› ä¸ºéªŒè¯é›†ä¸­å·²ç»æ¶µç›–æµ‹è¯•é›†æ•°æ®ï¼Œå…¶ä¸å†æä¾›æ— åæ€§èƒ½è¯„ä¼°ã€‚å½“ç„¶ï¼Œå¦‚æœä½ ä¸éœ€è¦æ— åä¼°è®¡ï¼Œé‚£å°±å†å¥½ä¸è¿‡äº†ã€‚ Â¶L02 : Bias/Variance **â€œåå·®-æ–¹å·®åˆ†è§£â€ï¼ˆbias-variance decompositionï¼‰**æ˜¯è§£é‡Šå­¦ä¹ ç®—æ³•æ³›åŒ–æ€§èƒ½çš„ä¸€ç§é‡è¦å·¥å…·ã€‚ æ³›åŒ–è¯¯å·®å¯åˆ†è§£ä¸ºåå·®ã€æ–¹å·®ä¸å™ªå£°ä¹‹å’Œï¼š åå·®ï¼šåº¦é‡äº†å­¦ä¹ ç®—æ³•çš„æœŸæœ›é¢„æµ‹ä¸çœŸå®ç»“æœçš„åç¦»ç¨‹åº¦ï¼Œå³åˆ»ç”»äº†å­¦ä¹ ç®—æ³•æœ¬èº«çš„æ‹Ÿåˆèƒ½åŠ›ï¼› æ–¹å·®ï¼šåº¦é‡äº†åŒæ ·å¤§å°çš„è®­ç»ƒé›†çš„å˜åŠ¨æ‰€å¯¼è‡´çš„å­¦ä¹ æ€§èƒ½çš„å˜åŒ–ï¼Œå³åˆ»ç”»äº†æ•°æ®æ‰°åŠ¨æ‰€é€ æˆçš„å½±å“ï¼› å™ªå£°ï¼šè¡¨è¾¾äº†åœ¨å½“å‰ä»»åŠ¡ä¸Šä»»ä½•å­¦ä¹ ç®—æ³•æ‰€èƒ½å¤Ÿè¾¾åˆ°çš„æœŸæœ›æ³›åŒ–è¯¯å·®çš„ä¸‹ç•Œï¼Œå³åˆ»ç”»äº†å­¦ä¹ é—®é¢˜æœ¬èº«çš„éš¾åº¦ã€‚ high bias ,underfitting high variance, overfitting just right Â¶1. example ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_5.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_6.png) Your algorithms ever on the training set and dev set you can try to diganose whether has problems high barriers or high variances or both or neither. Â¶L03 Basic Recipe for Machine learning Â¶1. METHOD ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_8.png) Training a bigger network almost never hurts. And the main cost of training a neural network thatâ€™s too big is just computational time, so long as youâ€™re regularizing. ä»Šå¤©æˆ‘ä»¬è®²äº†å¦‚ä½•é€šè¿‡ç»„ç»‡æœºå™¨å­¦ä¹ æ¥è¯Šæ–­åå·®å’Œæ–¹å·®çš„åŸºæœ¬æ–¹æ³•ï¼Œç„¶åé€‰æ‹©è§£å†³é—®é¢˜çš„æ­£ç¡®æ“ä½œï¼Œå¸Œæœ›å¤§å®¶æœ‰æ‰€äº†è§£å’Œè®¤è¯†ã€‚æˆ‘åœ¨è¯¾ä¸Šä¸æ­¢ä¸€æ¬¡æåˆ°äº†æ­£åˆ™åŒ–ï¼Œå®ƒæ˜¯ä¸€ç§éå¸¸å®ç”¨çš„å‡å°‘æ–¹å·®çš„æ–¹æ³•ï¼Œæ­£åˆ™åŒ–æ—¶ä¼šå‡ºç°åå·®æ–¹å·®æƒè¡¡é—®é¢˜ï¼Œåå·®å¯èƒ½ç•¥æœ‰å¢åŠ ï¼Œå¦‚æœç½‘ç»œè¶³å¤Ÿå¤§ï¼Œå¢å¹…é€šå¸¸ä¸ä¼šå¤ªé«˜ï¼Œæˆ‘ä»¬ä¸‹èŠ‚è¯¾å†ç»†è®²ï¼Œä»¥ä¾¿å¤§å®¶æ›´å¥½ç†è§£å¦‚ä½•å®ç°ç¥ç»ç½‘ç»œçš„æ­£åˆ™åŒ–ã€‚ ç¬¬ä¸€ç‚¹ï¼Œé«˜åå·®å’Œé«˜æ–¹å·®æ˜¯ä¸¤ç§ä¸åŒçš„æƒ…å†µï¼Œæˆ‘ä»¬åç»­è¦å°è¯•çš„æ–¹æ³•ä¹Ÿå¯èƒ½å®Œå…¨ä¸åŒ åªè¦æ­£åˆ™é€‚åº¦ï¼Œé€šå¸¸æ„å»ºä¸€ä¸ªæ›´å¤§çš„ç½‘ç»œä¾¿å¯ä»¥ï¼Œåœ¨ä¸å½±å“æ–¹å·®çš„åŒæ—¶å‡å°‘åå·®ï¼Œè€Œé‡‡ç”¨æ›´å¤šæ•°æ®é€šå¸¸å¯ä»¥åœ¨ä¸è¿‡å¤šå½±å“åå·®çš„åŒæ—¶å‡å°‘æ–¹å·®ã€‚è¿™ä¸¤æ­¥å®é™…è¦åšçš„å·¥ä½œæ˜¯ï¼šè®­ç»ƒç½‘ç»œï¼Œé€‰æ‹©ç½‘ç»œæˆ–è€…å‡†å¤‡æ›´å¤šæ•°æ®ï¼Œç°åœ¨æˆ‘ä»¬æœ‰å·¥å…·å¯ä»¥åšåˆ°åœ¨å‡å°‘åå·®æˆ–æ–¹å·®çš„åŒæ—¶ï¼Œä¸å¯¹å¦ä¸€æ–¹äº§ç”Ÿè¿‡å¤šä¸è‰¯å½±å“ã€‚ Â¶L04 Â¶1. over fitting Â¶regularization L2 regularization L1 regularizaion: w will be sparse L1 æ­£åˆ™åŒ–æœ€åå¾—åˆ° w å‘é‡ä¸­å°†å­˜åœ¨å¤§é‡çš„ 0 ä¸ºä»€ä¹ˆåªæ­£åˆ™åŒ–å‚æ•°wï¼Ÿä¸ºä»€ä¹ˆä¸å†åŠ ä¸Šå‚æ•°b å‘¢ï¼Ÿä½ å¯ä»¥è¿™ä¹ˆåšï¼Œåªæ˜¯æˆ‘ä¹ æƒ¯çœç•¥ä¸å†™ï¼Œå› ä¸ºé€šå¸¸wæ˜¯ä¸€ä¸ªé«˜ç»´å‚æ•°çŸ¢é‡ï¼Œwå·²ç»å¯ä»¥è¡¨è¾¾é«˜åå·®é—®é¢˜ï¼Œå¯èƒ½wåŒ…å«æœ‰å¾ˆå¤šå‚æ•°ï¼Œæˆ‘ä»¬ä¸å¯èƒ½æ‹Ÿåˆæ‰€æœ‰å‚æ•°ï¼Œè€Œåªæ˜¯bå•ä¸ªæ•°å­—ï¼Œæ‰€ä»¥wå‡ ä¹æ¶µç›–æ‰€æœ‰å‚æ•°ï¼Œè€Œä¸æ˜¯ï¼Œå¦‚æœåŠ äº†å‚æ•°bï¼Œå…¶å®ä¹Ÿæ²¡å¤ªå¤§å½±å“ï¼Œå› ä¸ºbåªæ˜¯ä¼—å¤šå‚æ•°ä¸­çš„ä¸€ä¸ªï¼Œæ‰€ä»¥æˆ‘é€šå¸¸çœç•¥ä¸è®¡ï¼Œå¦‚æœä½ æƒ³åŠ ä¸Šè¿™ä¸ªå‚æ•°ï¼Œå®Œå…¨æ²¡é—®é¢˜ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_9.png) 2.![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_10.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_11.png)çŸ©é˜µèŒƒæ•°è¢«ç§°ä½œâ€œå¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°â€ï¼Œç”¨ä¸‹æ ‡æ ‡æ³¨F åå‘ä¼ æ’­æ—¶ï¼Œå¡«ä¸Šæ­£åˆ™åŒ–çš„ä¸€é¡¹ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_12.png) å› æ­¤L2æ­£åˆ™åŒ–ä¹Ÿè¢«ç§°ä¸ºâ€œæƒé‡è¡°å‡â€ã€‚ to get more training data Â¶L05 :Why Regularization Reduces Overfitting æˆ‘ä»¬æ·»åŠ æ­£åˆ™é¡¹ï¼Œå®ƒå¯ä»¥é¿å…æ•°æ®æƒå€¼çŸ©é˜µè¿‡å¤§ï¼Œè¿™å°±æ˜¯å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°ï¼Œä¸ºä»€ä¹ˆå‹ç¼©èŒƒæ•°ï¼Œæˆ–è€…å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°æˆ–è€…å‚æ•°å¯ä»¥å‡å°‘è¿‡æ‹Ÿåˆï¼Ÿæˆ‘ä»¬å°è¯•æ¶ˆé™¤æˆ–è‡³å°‘å‡å°‘è®¸å¤šéšè—å•å…ƒçš„å½±å“ï¼Œæœ€ç»ˆè¿™ä¸ªç½‘ç»œä¼šå˜å¾—æ›´ç®€å•.Regularizationå…¶å®æ˜¯è®©å‡½æ•°å˜å¾—ç®€åŒ–ã€‚ ç›´è§‚ä¸Šç†è§£å°±æ˜¯å¦‚æœæ­£åˆ™åŒ–è®¾ç½®å¾—è¶³å¤Ÿå¤§ï¼Œæƒé‡çŸ©é˜µè¢«è®¾ç½®ä¸ºæ¥è¿‘äº0çš„å€¼ï¼Œç›´è§‚ç†è§£å°±æ˜¯æŠŠå¤šéšè—å•å…ƒçš„æƒé‡è®¾ä¸º0ï¼Œäºæ˜¯åŸºæœ¬ä¸Šæ¶ˆé™¤äº†è¿™äº›éšè—å•å…ƒçš„è®¸å¤šå½±å“ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œè¿™ä¸ªè¢«å¤§å¤§ç®€åŒ–äº†çš„ç¥ç»ç½‘ç»œä¼šå˜æˆä¸€ä¸ªå¾ˆå°çš„ç½‘ç»œï¼Œå°åˆ°å¦‚åŒä¸€ä¸ªé€»è¾‘å›å½’å•å…ƒï¼Œå¯æ˜¯æ·±åº¦å´å¾ˆå¤§ï¼Œå®ƒä¼šä½¿è¿™ä¸ªç½‘ç»œä»è¿‡åº¦æ‹Ÿåˆçš„çŠ¶æ€æ›´æ¥è¿‘å·¦å›¾çš„é«˜åå·®çŠ¶æ€ã€‚ æ€»ç»“ä¸€ä¸‹ï¼Œå¦‚æœæ­£åˆ™åŒ–å‚æ•°å˜å¾—å¾ˆå¤§ï¼Œwå‚æ•°å¾ˆå°ï¼Œzä¹Ÿä¼šç›¸å¯¹å˜å°ï¼Œæ­¤æ—¶å¿½ç•¥çš„bå½±å“ï¼Œzä¼šç›¸å¯¹å˜å°ï¼Œå®é™…ä¸Šï¼Œzçš„å–å€¼èŒƒå›´å¾ˆå°ï¼Œè¿™ä¸ªæ¿€æ´»å‡½æ•°tanhï¼Œä¹Ÿå°±æ˜¯æ›²çº¿å‡½æ•°ä¼šç›¸å¯¹å‘ˆçº¿æ€§ï¼Œæ•´ä¸ªç¥ç»ç½‘ç»œä¼šè®¡ç®—ç¦»çº¿æ€§å‡½æ•°è¿‘çš„å€¼ï¼Œè¿™ä¸ªçº¿æ€§å‡½æ•°éå¸¸ç®€å•ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªæå¤æ‚çš„é«˜åº¦éçº¿æ€§å‡½æ•°ï¼Œä¸ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆã€‚ L2 regularizationçš„ä¸è¶³ï¼šè¦é€šè¿‡ä¸æ–­çš„é€‰ç”¨ä¸åŒçš„Î»è¿›è¡Œæµ‹è¯•ï¼Œè®¡ç®—é‡åŠ å¤§äº†ã€‚ Â¶L06 : Dropout Regularization Â¶1. å·¥ä½œåŸç† ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_15.png) å¦‚æœä¸Šé¢è¿™å¹…å›¾å­˜åœ¨over fittingã€‚å¤åˆ¶è¿™ä¸ªç¥ç»ç½‘ç»œï¼Œdropoutä¼šéå†ç½‘ç»œçš„æ¯ä¸€å±‚ã€‚å‡è®¾ç½‘ç»œä¸­çš„æ¯ä¸€å±‚ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½ä»¥æŠ›ç¡¬å¸çš„æ–¹å¼è®¾ç½®æ¦‚ç‡ï¼Œæ¯ä¸ªèŠ‚ç‚¹å¾—ä»¥ä¿ç•™å’Œæ¶ˆé™¤çš„æ¦‚ç‡éƒ½æ˜¯0.5ï¼Œè®¾ç½®å®ŒèŠ‚ç‚¹æ¦‚ç‡ï¼Œæˆ‘ä»¬ä¼šæ¶ˆé™¤ä¸€äº›èŠ‚ç‚¹ï¼Œç„¶ååˆ é™¤æ‰ä»è¯¥èŠ‚ç‚¹è¿›å‡ºçš„è¿çº¿ï¼Œæœ€åå¾—åˆ°ä¸€ä¸ªèŠ‚ç‚¹æ›´å°‘ï¼Œè§„æ¨¡æ›´å°çš„ç½‘ç»œï¼Œç„¶åç”¨backpropæ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_13.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_14.png) æˆ‘ä»¬é’ˆå¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬è®­ç»ƒè§„æ¨¡æå°çš„ç½‘ç»œï¼Œæœ€åä½ å¯èƒ½ä¼šè®¤è¯†åˆ°ä¸ºä»€ä¹ˆè¦æ­£åˆ™åŒ–ç½‘ç»œï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒæå°çš„ç½‘ç»œã€‚ Â¶2. inverted dropoutï¼ˆåå‘éšæœºå¤±æ´»ï¼‰ å¯¹ç¬¬L 1234keep_prob = 0.8 # è®¾ç½®ç¥ç»å…ƒä¿ç•™æ¦‚ç‡dl = np.random.rand(al.shape[0], al.shape[1]) &lt; keep_probal = np.multiply(al, dl)al /= keep_prob æœ€åä¸€æ­¥al /= keep_probæ˜¯å› ä¸º a[l]a[l]ä¸­çš„ä¸€éƒ¨åˆ†å…ƒç´ å¤±æ´»ï¼ˆç›¸å½“äºè¢«å½’é›¶ï¼‰ï¼Œä¸ºäº†åœ¨ä¸‹ä¸€å±‚è®¡ç®—æ—¶ä¸å½±å“ $Z[l+1]=W[l+1]a[l]+b[l+1]$çš„æœŸæœ›å€¼ï¼Œå› æ­¤é™¤ä»¥ä¸€ä¸ªkeep_probã€‚ä¸¾ä¾‹è§£é‡Šæˆ‘ä»¬å‡è®¾ç¬¬ä¸‰éšè—å±‚ä¸Šæœ‰50ä¸ªå•å…ƒæˆ–50ä¸ªç¥ç»å…ƒï¼Œåœ¨ä¸€ç»´ä¸Šæ˜¯50ï¼Œæˆ‘ä»¬é€šè¿‡å› å­åˆ†è§£å°†å®ƒæ‹†åˆ†æˆç»´çš„ï¼Œä¿ç•™å’Œåˆ é™¤å®ƒä»¬çš„æ¦‚ç‡åˆ†åˆ«ä¸º80%å’Œ20%ï¼Œè¿™æ„å‘³ç€æœ€åè¢«åˆ é™¤æˆ–å½’é›¶çš„å•å…ƒå¹³å‡æœ‰10ï¼ˆ50Ã—20%=10ï¼‰ä¸ªï¼Œç°åœ¨æˆ‘ä»¬çœ‹ä¸‹$z{[4]}$ï¼Œï¼Œæˆ‘ä»¬çš„é¢„æœŸæ˜¯$z{[4]}=w{[4]}a{[3]}$ï¼Œ$a{[3]}$å‡å°‘20%ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸­æœ‰$a{[3]}$20%çš„å…ƒç´ è¢«å½’é›¶ï¼Œä¸ºäº†ä¸å½±å“çš„$a{[4]}$æœŸæœ›å€¼ï¼Œæˆ‘ä»¬éœ€è¦ç”¨$w{[4]}a{[3]}/keep_prob$ï¼Œå®ƒå°†ä¼šä¿®æ­£æˆ–å¼¥è¡¥æˆ‘ä»¬æ‰€éœ€çš„é‚£20%ï¼Œ$a{[3]}$çš„æœŸæœ›å€¼ä¸ä¼šå˜ï¼Œåˆ’çº¿éƒ¨åˆ†å°±æ˜¯æ‰€è°“çš„dropoutæ–¹æ³•ã€‚ Â¶L07 : Understanding Dropout ç›´è§‚ä¸Šç†è§£ï¼šä¸è¦ä¾èµ–äºä»»ä½•ä¸€ä¸ªç‰¹å¾ï¼Œå› ä¸ºè¯¥å•å…ƒçš„è¾“å…¥å¯èƒ½éšæ—¶è¢«æ¸…é™¤ï¼Œå› æ­¤è¯¥å•å…ƒé€šè¿‡è¿™ç§æ–¹å¼ä¼ æ’­ä¸‹å»ï¼Œå¹¶ä¸ºå•å…ƒçš„å››ä¸ªè¾“å…¥å¢åŠ ä¸€ç‚¹æƒé‡ï¼Œé€šè¿‡ä¼ æ’­æ‰€æœ‰æƒé‡ï¼Œdropoutå°†äº§ç”Ÿæ”¶ç¼©æƒé‡çš„å¹³æ–¹èŒƒæ•°çš„æ•ˆæœï¼Œå’Œä¹‹å‰è®²çš„L2æ­£åˆ™åŒ–ç±»ä¼¼ï¼›å®æ–½dropoutçš„ç»“æœå®å®ƒä¼šå‹ç¼©æƒé‡ï¼Œå¹¶å®Œæˆä¸€äº›é¢„é˜²è¿‡æ‹Ÿåˆçš„å¤–å±‚æ­£åˆ™åŒ–ï¼›L2å¯¹ä¸åŒæƒé‡çš„è¡°å‡æ˜¯ä¸åŒçš„ï¼Œå®ƒå–å†³äºæ¿€æ´»å‡½æ•°å€å¢çš„å¤§å°ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_16.png) è®¡ç®—è§†è§‰ä¸­çš„è¾“å…¥é‡éå¸¸å¤§ï¼Œè¾“å…¥å¤ªå¤šåƒç´ ï¼Œä»¥è‡³äºæ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ‰€ä»¥dropoutåœ¨è®¡ç®—æœºè§†è§‰ä¸­åº”ç”¨å¾—æ¯”è¾ƒé¢‘ç¹ï¼Œæœ‰äº›è®¡ç®—æœºè§†è§‰ç ”ç©¶äººå‘˜éå¸¸å–œæ¬¢ç”¨å®ƒï¼Œå‡ ä¹æˆäº†é»˜è®¤çš„é€‰æ‹©ï¼Œä½†è¦ç‰¢è®°ä¸€ç‚¹ï¼Œdropoutæ˜¯ä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒæœ‰åŠ©äºé¢„é˜²è¿‡æ‹Ÿåˆï¼Œå› æ­¤é™¤éç®—æ³•è¿‡æ‹Ÿåˆï¼Œä¸ç„¶æˆ‘æ˜¯ä¸ä¼šä½¿ç”¨dropoutçš„ï¼Œæ‰€ä»¥å®ƒåœ¨å…¶å®ƒé¢†åŸŸåº”ç”¨å¾—æ¯”è¾ƒå°‘ï¼Œä¸»è¦å­˜åœ¨äºè®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ‰€ä»¥ä¸€ç›´å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œè¿™å°±æ˜¯æœ‰äº›è®¡ç®—æœºè§†è§‰ç ”ç©¶äººå‘˜å¦‚æ­¤é’Ÿæƒ…äºdropoutå‡½æ•°çš„åŸå› ã€‚ç›´è§‚ä¸Šæˆ‘è®¤ä¸ºä¸èƒ½æ¦‚æ‹¬å…¶å®ƒå­¦ç§‘ã€‚dropoutå°†äº§ç”Ÿæ”¶ç¼©æƒé‡çš„å¹³æ–¹èŒƒæ•°çš„æ•ˆæœã€‚å½“ç„¶ï¼Œä¸åŒçš„å±‚ï¼Œå€¼å¯ä»¥è®¾ç½®æˆä¸åŒï¼Œå¦‚æœä½ è§‰å¾—æŸä¸€å±‚å®¹æ˜“è¿‡æ‹Ÿåˆï¼ŒæŠŠå€¼è®¾ç½®å°ä¸€ç‚¹ã€‚ dropout çš„ä¸€å¤§ç¼ºç‚¹æ˜¯æˆæœ¬å‡½æ•°æ— æ³•è¢«æ˜ç¡®å®šä¹‰ã€‚å› ä¸ºæ¯æ¬¡è¿­ä»£éƒ½ä¼šéšæœºæ¶ˆé™¤ä¸€äº›ç¥ç»å…ƒç»“ç‚¹çš„å½±å“ï¼Œå› æ­¤æ— æ³•ç¡®ä¿æˆæœ¬å‡½æ•°å•è°ƒé€’å‡ã€‚å› æ­¤ï¼Œä½¿ç”¨ dropout æ—¶ï¼Œå…ˆå°†keep_probå…¨éƒ¨è®¾ç½®ä¸º 1.0 åè¿è¡Œä»£ç ï¼Œç¡®ä¿ $J(w,b)$å‡½æ•°å•è°ƒé€’å‡ï¼Œå†æ‰“å¼€ dropoutã€‚ Â¶L08 : Other Regularization Methods æ•°æ®æ‰©å¢ï¼ˆData Augmentationï¼‰ï¼šé€šè¿‡å›¾ç‰‡çš„ä¸€äº›å˜æ¢ï¼ˆç¿»è½¬ï¼Œå±€éƒ¨æ”¾å¤§ååˆ‡å‰²ç­‰ï¼‰ï¼Œå¾—åˆ°æ›´å¤šçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_17.png) æ—©åœæ­¢æ³•ï¼ˆEarly Stoppingï¼‰ï¼šå°†è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡Œæ¢¯åº¦ä¸‹é™æ—¶çš„æˆæœ¬å˜åŒ–æ›²çº¿ç”»åœ¨åŒä¸€ä¸ªåæ ‡è½´å†…ï¼Œå½“è®­ç»ƒé›†è¯¯å·®é™ä½ä½†éªŒè¯é›†è¯¯å·®å‡é«˜ï¼Œä¸¤è€…å¼€å§‹å‘ç”Ÿè¾ƒå¤§åå·®æ—¶åŠæ—¶åœæ­¢è¿­ä»£ï¼Œå¹¶è¿”å›å…·æœ‰æœ€å°éªŒè¯é›†è¯¯å·®çš„è¿æ¥æƒå’Œé˜ˆå€¼ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆã€‚è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯æ— æ³•åŒæ—¶è¾¾æˆåå·®å’Œæ–¹å·®çš„æœ€ä¼˜ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_18.png) ä½†å¯¹æˆ‘æ¥è¯´early stoppingçš„ä¸»è¦ç¼ºç‚¹å°±æ˜¯ä½ ä¸èƒ½ç‹¬ç«‹åœ°å¤„ç†è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œå› ä¸ºææ—©åœæ­¢æ¢¯åº¦ä¸‹é™ï¼Œä¹Ÿå°±æ˜¯åœæ­¢äº†ä¼˜åŒ–ä»£ä»·å‡½æ•°ï¼Œå› ä¸ºç°åœ¨ä½ ä¸å†å°è¯•é™ä½ä»£ä»·å‡½æ•°ï¼Œæ‰€ä»¥ä»£ä»·å‡½æ•°çš„å€¼å¯èƒ½ä¸å¤Ÿå°ï¼ŒåŒæ—¶ä½ åˆå¸Œæœ›ä¸å‡ºç°è¿‡æ‹Ÿåˆï¼Œä½ æ²¡æœ‰é‡‡å–ä¸åŒçš„æ–¹å¼æ¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œè€Œæ˜¯ç”¨ä¸€ç§æ–¹æ³•åŒæ—¶è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼Œè¿™æ ·åšçš„ç»“æœæ˜¯æˆ‘è¦è€ƒè™‘çš„ä¸œè¥¿å˜å¾—æ›´å¤æ‚ã€‚ Early stoppingçš„ä¼˜ç‚¹æ˜¯ï¼Œåªè¿è¡Œä¸€æ¬¡æ¢¯åº¦ä¸‹é™ï¼Œä½ å¯ä»¥æ‰¾å‡ºçš„wè¾ƒå°å€¼ï¼Œä¸­é—´å€¼å’Œè¾ƒå¤§å€¼ï¼Œè€Œæ— éœ€å°è¯•æ­£åˆ™åŒ–è¶…çº§å‚æ•°çš„å¾ˆå¤šå€¼ã€‚ Â¶L09 ï¼š Normalizing inputs é›¶å‡å€¼ $u=\frac{1}{m}\sum x^{(i)}$,$x-u$ å½’ä¸€åŒ–æ–¹å·®ï¼› $\delta2=\frac{1}{m}(x{(i)})^2$,æ¯ä¸ªç‰¹å¾çš„æ–¹å·®ï¼Œæ¯ä¸ªç‰¹å¾æ•°æ®é™¤ä»¥å®ƒï¼Œå°±å½’ä¸€åŒ–æ–¹å·®äº† ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_19.png) Â¶why ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_20.png) åœ¨ä¸ä½¿ç”¨æ ‡å‡†åŒ–çš„æˆæœ¬å‡½æ•°ä¸­ï¼Œå¦‚æœè®¾ç½®ä¸€ä¸ªè¾ƒå°çš„å­¦ä¹ ç‡ï¼Œå¯èƒ½éœ€è¦å¾ˆå¤šæ¬¡è¿­ä»£æ‰èƒ½åˆ°è¾¾å…¨å±€æœ€ä¼˜è§£ï¼›è€Œå¦‚æœä½¿ç”¨äº†æ ‡å‡†åŒ–ï¼Œé‚£ä¹ˆæ— è®ºä»å“ªä¸ªä½ç½®å¼€å§‹è¿­ä»£ï¼Œéƒ½èƒ½ä»¥ç›¸å¯¹è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚ Â¶L10 : Vanishing /Exploding Gradients è®­ç»ƒç¥ç»ç½‘ç»œï¼Œå°¤å…¶æ˜¯æ·±åº¦ç¥ç»æ‰€é¢ä¸´çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸ï¼Œä¹Ÿå°±æ˜¯ä½ è®­ç»ƒç¥ç»ç½‘ç»œçš„æ—¶å€™ï¼Œå¯¼æ•°æˆ–å¡åº¦æœ‰æ—¶ä¼šå˜å¾—éå¸¸å¤§ï¼Œæˆ–è€…éå¸¸å°ï¼Œç”šè‡³äºä»¥æŒ‡æ•°æ–¹å¼å˜å°ï¼Œè¿™åŠ å¤§äº†è®­ç»ƒçš„éš¾åº¦ã€‚ åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œæ¿€æ´»å‡½æ•°å°†ä»¥æŒ‡æ•°çº§é€’å‡ï¼Œè™½ç„¶æˆ‘åªæ˜¯è®¨è®ºäº†æ¿€æ´»å‡½æ•°ä»¥ä¸ç›¸å…³çš„æŒ‡æ•°çº§æ•°å¢é•¿æˆ–ä¸‹é™ï¼Œå®ƒä¹Ÿé€‚ç”¨äºä¸å±‚æ•°ç›¸å…³çš„å¯¼æ•°æˆ–æ¢¯åº¦å‡½æ•°ï¼Œä¹Ÿæ˜¯å‘ˆæŒ‡æ•°çº§å¢é•¿æˆ–å‘ˆæŒ‡æ•°é€’å‡ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_21.png) å‡å®š g(z)=z,b[l]=0g(z)=z,b[l]=0ï¼Œå¯¹äºç›®æ ‡è¾“å‡ºæœ‰ï¼š $y^=W[L]W[Lâˆ’1]â€¦W[2]W[1]X$ å¯¹äº$ W[l]$çš„å€¼å¤§äº 1 çš„æƒ…å†µï¼Œæ¿€æ´»å‡½æ•°çš„å€¼å°†ä»¥æŒ‡æ•°çº§é€’å¢ï¼› å¯¹äº $W[l]$çš„å€¼å°äº 1 çš„æƒ…å†µï¼Œæ¿€æ´»å‡½æ•°çš„å€¼å°†ä»¥æŒ‡æ•°çº§é€’å‡ã€‚ å¯¹äºå¯¼æ•°åŒç†ã€‚å› æ­¤ï¼Œåœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œæ ¹æ®ä¸åŒæƒ…å†µæ¢¯åº¦å‡½æ•°ä¼šä»¥æŒ‡æ•°çº§é€’å¢æˆ–é€’å‡ï¼Œå¯¼è‡´è®­ç»ƒå¯¼æ•°éš¾åº¦ä¸Šå‡ï¼Œæ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ­¥é•¿ä¼šå˜å¾—éå¸¸å°ï¼Œéœ€è¦è®­ç»ƒçš„æ—¶é—´å°†ä¼šéå¸¸é•¿ã€‚ Â¶L11 : Weight initialization in a deep network ä¸ºäº†é¢„é˜²å€¼zè¿‡å¤§æˆ–è¿‡å°ï¼Œä½ å¯ä»¥çœ‹åˆ°nè¶Šå¤§ï¼Œä½ å¸Œæœ›wè¶Šå°ï¼Œå› ä¸ºzæ˜¯wx+bçš„å’Œ,æœ€åˆç†çš„æ–¹æ³•$w_i=1/n$ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_22.png) å› æ­¤ï¼Œå®é™…ä¸Šï¼Œä½ è¦åšçš„å°±æ˜¯è®¾ç½®æŸå±‚æƒé‡çŸ©é˜µ $w^{[l]}=n p . random. randn (shape) * np.sqrt \left(\frac{1}{n^{[l-1]}}\right)$ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_23.png) å½“å¤šä¸ªèŠ‚ç‚¹æ—¶ï¼Œä¹Ÿä¸€æ ·çš„çœ‹ï¼Œä½¿å¾—è¿™ä¸ªèŠ‚ç‚¹$z^{L}$ä¸è¦å¤ªå¤§ï¼Œå•ç‹¬çœ‹æ¯ä¸ªèŠ‚ç‚¹æ—¢å¯ä»¥ relu : var(w(i)) = 2/n or $\frac{2}{n{[l-1]}*n{[l]}}$ tanh: var(w(i)) = 1/n é€šè¿‡è®¾ç½®åˆå§‹åŒ–åŒ–æƒé‡çŸ©é˜µï¼Œä½¿å¾—ä¸ä¼šå¢é•¿å¤ªå¿«æˆ–è€…å¤ªæ…¢ Â¶L12 ï¼š Numerical Approximations of Gradients å•è¾¹è¯¯å·® $f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0} \frac{f(\theta+\varepsilon)-(\theta)}{\varepsilon}$ è¯¯å·®$O(\varepsilon)$ åŒè¾¹è¯¯å·® $f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0}=\frac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2 \varepsilon}$ $O\left(\varepsilon^{2}\right)$ Â¶L 13 Gradient Checking æ¢¯åº¦æ£€éªŒå¸®æˆ‘ä»¬èŠ‚çœäº†å¾ˆå¤šæ—¶é—´ï¼Œä¹Ÿå¤šæ¬¡å¸®æˆ‘å‘ç°backpropå®æ–½è¿‡ç¨‹ä¸­çš„bugï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åˆ©ç”¨å®ƒæ¥è°ƒè¯•æˆ–æ£€éªŒbackpropçš„å®æ–½æ˜¯å¦æ­£ç¡®ã€‚ é¦–å…ˆè¦åšçš„å°±æ˜¯ï¼ŒæŠŠæ‰€æœ‰å‚æ•°è½¬æ¢æˆä¸€ä¸ªå·¨å¤§çš„å‘é‡æ•°æ®ï¼Œä½ è¦åšçš„å°±æ˜¯æŠŠçŸ©é˜µwè½¬æ¢æˆä¸€ä¸ªå‘é‡ï¼ŒæŠŠæ‰€æœ‰çŸ©é˜µwè½¬æ¢æˆå‘é‡ä¹‹åï¼Œåšè¿æ¥è¿ç®—ï¼Œå¾—åˆ°ä¸€ä¸ªå·¨å‹å‘é‡$\theta$ï¼Œè¯¥å‘é‡è¡¨ç¤ºä¸ºå‚æ•°$\theta$ï¼Œä»£ä»·å‡½æ•°Jæ˜¯æ‰€æœ‰Wå’Œbçš„å‡½æ•°ï¼Œç°åœ¨ä½ å¾—åˆ°äº†ä¸€ä¸ªçš„ä»£ä»·å‡½æ•°ï¼ˆå³ï¼‰ã€‚æ¥ç€ï¼Œä½ å¾—åˆ°ä¸å’Œé¡ºåºç›¸åŒçš„æ•°æ®ï¼Œä½ åŒæ ·å¯ä»¥æŠŠ$dW{[l]}$,å’Œ$db{[l]}$ è½¬æ¢æˆä¸€ä¸ªæ–°çš„å‘é‡ï¼Œç”¨å®ƒä»¬æ¥åˆå§‹åŒ–å¤§å‘é‡$d\theta$ï¼Œå®ƒä¸$\theta$å…·æœ‰ç›¸åŒç»´åº¦ã€‚ æ¢¯åº¦çš„é€¼è¿‘å€¼ $$ d \theta_{\text { approx }}[i]=\frac{J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}+\varepsilon, \ldots\right)-J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}-\varepsilon, \ldots\right)}{2 \varepsilon} $$ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_24.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_25.png) ç°åœ¨ä½ å·²ç»äº†è§£äº†æ¢¯åº¦æ£€éªŒçš„å·¥ä½œåŸç†ï¼Œå®ƒå¸®åŠ©æˆ‘åœ¨ç¥ç»ç½‘ç»œå®æ–½ä¸­å‘ç°äº†å¾ˆå¤šbugï¼Œå¸Œæœ›å®ƒå¯¹ä½ ä¹Ÿæœ‰æ‰€å¸®åŠ©ã€‚ L 14 : Gradient Checking Implementation notes ä¸è¦åœ¨è®­ç»ƒä¸­ä½¿ç”¨æ¢¯åº¦æ£€éªŒï¼Œå®ƒåªç”¨äºè°ƒè¯•ï¼ˆdebugï¼‰ã€‚ä½¿ç”¨å®Œæ¯•å…³é—­æ¢¯åº¦æ£€éªŒçš„åŠŸèƒ½ï¼›å¤ªæ…¢äº† å¦‚æœç®—æ³•çš„æ¢¯åº¦æ£€éªŒå¤±è´¥ï¼Œè¦æ£€æŸ¥æ‰€æœ‰é¡¹ï¼Œå¹¶è¯•ç€æ‰¾å‡º bugï¼Œå³ç¡®å®šå“ªä¸ª dÎ¸approx[i] ä¸ dÎ¸ çš„å€¼ç›¸å·®æ¯”è¾ƒå¤§ï¼› å½“æˆæœ¬å‡½æ•°åŒ…å«æ­£åˆ™é¡¹æ—¶ï¼Œä¹Ÿéœ€è¦å¸¦ä¸Šæ­£åˆ™é¡¹è¿›è¡Œæ£€éªŒï¼› æ¢¯åº¦æ£€éªŒä¸èƒ½ä¸ dropout åŒæ—¶ä½¿ç”¨ã€‚å› ä¸ºæ¯æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œdropout ä¼šéšæœºæ¶ˆé™¤éšè—å±‚å•å…ƒçš„ä¸åŒå­é›†ï¼Œéš¾ä»¥è®¡ç®— dropout åœ¨æ¢¯åº¦ä¸‹é™ä¸Šçš„æˆæœ¬å‡½æ•° Jã€‚å»ºè®®å…³é—­ dropoutï¼Œç”¨æ¢¯åº¦æ£€éªŒè¿›è¡ŒåŒé‡æ£€æŸ¥ï¼Œç¡®å®šåœ¨æ²¡æœ‰ dropout çš„æƒ…å†µä¸‹ç®—æ³•æ­£ç¡®ï¼Œç„¶åæ‰“å¼€ dropoutï¼› ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week1_25.png) Â¶Summary å›é¡¾è¿™ä¸€å‘¨ï¼Œæˆ‘ä»¬è®²äº†å¦‚ä½•é…ç½®è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œå¦‚ä½•åˆ†æåå·®å’Œæ–¹å·®ï¼Œå¦‚ä½•å¤„ç†é«˜åå·®æˆ–é«˜æ–¹å·®ä»¥åŠé«˜åå·®å’Œé«˜æ–¹å·®å¹¶å­˜çš„é—®é¢˜ï¼Œå¦‚ä½•åœ¨ç¥ç»ç½‘ç»œä¸­åº”ç”¨ä¸åŒå½¢å¼çš„æ­£åˆ™åŒ–ï¼Œå¦‚æ­£åˆ™åŒ–å’Œdropoutï¼Œè¿˜æœ‰åŠ å¿«ç¥ç»ç½‘ç»œè®­ç»ƒé€Ÿåº¦çš„æŠ€å·§ï¼Œæœ€åæ˜¯æ¢¯åº¦æ£€éªŒã€‚ C2W2 :Optimization Algorithm Â¶L 01 : Mini Batch Gradient Descent Vectorization Mini batch not entire training set bady training set iï¼Œ$x^$ mini batch training set ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_1.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_2.png) mini batch gradient descent ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_3.png) Â¶L 02 : Understanding Mini-Batch Gradient Decent ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_4.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_6.png) å·¦å›¾ï¼Œéšç€iterations increased, it should decrease .if it ever goes up on iteration,something is wrong. å³å›¾ : itâ€™s as if on every iteration youâ€™re training on a different training set or really training on a different mini batch. It should trend downwards, but itâ€™s also going to be a little bit noisier.So if you plot J{t}, as youâ€™re training mini batch in descent it may be over multiple epochs,you might expect to see a curve like this. Â¶Choosing your mini-batch size ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_5.png) Â¶1. ä¼˜ç¼ºç‚¹ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_7.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_8.png) é€šè¿‡å‡å°å­¦ä¹ ç‡ï¼Œå™ªå£°ä¼šè¢«æ”¹å–„æˆ–æœ‰æ‰€å‡å°ï¼Œä½†éšæœºæ¢¯åº¦ä¸‹é™æ³•çš„ä¸€å¤§ç¼ºç‚¹æ˜¯ï¼Œä½ ä¼šå¤±å»æ‰€æœ‰å‘é‡åŒ–å¸¦ç»™ä½ çš„åŠ é€Ÿï¼Œå› ä¸ºä¸€æ¬¡æ€§åªå¤„ç†äº†ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè¿™æ ·æ•ˆç‡è¿‡äºä½ä¸‹ï¼Œæ‰€ä»¥å®è·µä¸­æœ€å¥½é€‰æ‹©ä¸å¤§ä¸å°çš„mini-batchå°ºå¯¸ï¼Œå®é™…ä¸Šå­¦ä¹ ç‡è¾¾åˆ°æœ€å¿«ã€‚ä½ ä¼šå‘ç°ä¸¤ä¸ªå¥½å¤„ï¼Œä¸€æ–¹é¢ï¼Œä½ å¾—åˆ°äº†å¤§é‡å‘é‡åŒ–ï¼Œä¸Šä¸ªè§†é¢‘ä¸­æˆ‘ä»¬ç”¨è¿‡çš„ä¾‹å­ä¸­ï¼Œå¦‚æœmini-batchå¤§å°ä¸º1000ä¸ªæ ·æœ¬ï¼Œä½ å°±å¯ä»¥å¯¹1000ä¸ªæ ·æœ¬å‘é‡åŒ–ï¼Œæ¯”ä½ ä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ ·æœ¬å¿«å¾—å¤šã€‚å¦ä¸€æ–¹é¢ï¼Œä½ ä¸éœ€è¦ç­‰å¾…æ•´ä¸ªè®­ç»ƒé›†è¢«å¤„ç†å®Œå°±å¯ä»¥å¼€å§‹è¿›è¡Œåç»­å·¥ä½œï¼Œå†ç”¨ä¸€ä¸‹ä¸Šä¸ªè§†é¢‘çš„æ•°å­—ï¼Œæ¯æ¬¡è®­ç»ƒé›†å…è®¸æˆ‘ä»¬é‡‡å–5000ä¸ªæ¢¯åº¦ä¸‹é™æ­¥éª¤ï¼Œæ‰€ä»¥å®é™…ä¸Šä¸€äº›ä½äºä¸­é—´çš„mini-batchå¤§å°æ•ˆæœæœ€å¥½ã€‚ ä½¿ç”¨batchæ¢¯åº¦ä¸‹é™æ³•æ—¶ï¼Œæ¯æ¬¡è¿­ä»£ä½ éƒ½éœ€è¦å†éæ•´ä¸ªè®­ç»ƒé›†ï¼Œå¯ä»¥é¢„æœŸæ¯æ¬¡è¿­ä»£æˆæœ¬éƒ½ä¼šä¸‹é™ï¼Œæ‰€ä»¥å¦‚æœæˆæœ¬å‡½æ•°æ˜¯è¿­ä»£æ¬¡æ•°çš„ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒåº”è¯¥ä¼šéšç€æ¯æ¬¡è¿­ä»£è€Œå‡å°‘ï¼Œå¦‚æœåœ¨æŸæ¬¡è¿­ä»£ä¸­å¢åŠ äº†ï¼Œé‚£è‚¯å®šå‡ºäº†é—®é¢˜ï¼Œä¹Ÿè®¸ä½ çš„å­¦ä¹ ç‡å¤ªå¤§ã€‚ åœ¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ä¸­ï¼Œä»æŸä¸€ç‚¹å¼€å§‹ï¼Œæˆ‘ä»¬é‡æ–°é€‰å–ä¸€ä¸ªèµ·å§‹ç‚¹ï¼Œæ¯æ¬¡è¿­ä»£ï¼Œä½ åªå¯¹ä¸€ä¸ªæ ·æœ¬è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œå¤§éƒ¨åˆ†æ—¶å€™ä½ å‘ç€å…¨å±€æœ€å°å€¼é è¿‘ï¼Œæœ‰æ—¶å€™ä½ ä¼šè¿œç¦»æœ€å°å€¼ï¼Œå› ä¸ºé‚£ä¸ªæ ·æœ¬æ°å¥½ç»™ä½ æŒ‡çš„æ–¹å‘ä¸å¯¹ï¼Œå› æ­¤éšæœºæ¢¯åº¦ä¸‹é™æ³•æ˜¯æœ‰å¾ˆå¤šå™ªå£°çš„ï¼Œå¹³å‡æ¥çœ‹ï¼Œå®ƒæœ€ç»ˆä¼šé è¿‘æœ€å°å€¼ï¼Œä¸è¿‡æœ‰æ—¶å€™ä¹Ÿä¼šæ–¹å‘é”™è¯¯ï¼Œå› ä¸ºéšæœºæ¢¯åº¦ä¸‹é™æ³•æ°¸è¿œä¸ä¼šæ”¶æ•›ï¼Œè€Œæ˜¯ä¼šä¸€ç›´åœ¨æœ€å°å€¼é™„è¿‘æ³¢åŠ¨ï¼Œä½†å®ƒå¹¶ä¸ä¼šåœ¨è¾¾åˆ°æœ€å°å€¼å¹¶åœç•™åœ¨æ­¤ã€‚ ç”¨mini-batchæ¢¯åº¦ä¸‹é™æ³•ï¼Œæˆ‘ä»¬ä»è¿™é‡Œå¼€å§‹ï¼Œä¸€æ¬¡è¿­ä»£è¿™æ ·åšï¼Œä¸¤æ¬¡ï¼Œä¸‰æ¬¡ï¼Œå››æ¬¡ï¼Œå®ƒä¸ä¼šæ€»æœå‘æœ€å°å€¼é è¿‘ï¼Œä½†å®ƒæ¯”éšæœºæ¢¯åº¦ä¸‹é™è¦æ›´æŒç»­åœ°é è¿‘æœ€å°å€¼çš„æ–¹å‘ï¼Œå®ƒä¹Ÿä¸ä¸€å®šåœ¨å¾ˆå°çš„èŒƒå›´å†…æ”¶æ•›æˆ–è€…æ³¢åŠ¨ï¼Œå¦‚æœå‡ºç°è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æ…¢æ…¢å‡å°‘å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬åœ¨ä¸‹ä¸ªè§†é¢‘ä¼šè®²åˆ°å­¦ä¹ ç‡è¡°å‡ï¼Œä¹Ÿå°±æ˜¯å¦‚ä½•å‡å°å­¦ä¹ ç‡ã€‚ batch : too long,too time éšæœºï¼š lose speeding ,å™ªå£°å¤§ mini-batchå’Œstochasticéƒ½å­˜åœ¨å™ªå£°é—®é¢˜ï¼Œä¸”åœ¨local optimaé™„è¿‘ä¼šå¾˜å¾Šã€‚ä½†è®¾ç½®åˆé€‚å¤§å°çš„mini-batch sizeï¼Œå™ªå£°å’Œå¾˜å¾Šé—®é¢˜å¯æ¥å—çš„èŒƒå›´å†…ã€‚ size=1,åˆå«éšæœºæ¢¯åº¦ä¸‹é™æ³• stochastic gradient descent Â¶how å¦‚ä½•é€‰æ‹©mini-batch sizeï¼ˆè¿™æ˜¯ä¸€ä¸ªhyperparameterï¼‰ï¼š å°æ•°æ®é‡ï¼Œæ¯”å¦‚æ€»çš„æ ·æœ¬åªæœ‰å‡ åƒä¸ªï¼Œå®Œå…¨å¯ä»¥ç›´æ¥ç”¨batch gradient descent å¤§æ•°é‡ï¼Œmini-batch sizeå€¾å‘äºé€‰æ‹©2^nä¸ªï¼Œæ¯”å¦‚64, 128, 256ç­‰ mini-batch ä¸CPU/GPU memoryçš„å†…å­˜å®¹é‡ã€‚ In practice of course the mini batch size is another hyper parameter that you might do a quick search over to try to figure out which one is most sufficient of reducing the cost function j. æŒ‰ç…§ä¸Šé¢çš„æ–¹æ³• Â¶L 03: Exponentially Weighted Averages In order to understand those algorithms, you need to be able they use something called exponentially weighted averages. Also called exponentially weighted moving averages in statistics. Â¶1. æŒ‡æ•°åŠ æƒå¹³å‡æ•°ï¼ˆExponentially weighted averagesï¼‰ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_9.png) $\theta _i$è¡¨ç¤ºæ¯ä¸€æ—¥çš„æ¸©åº¦å€¼ï¼Œè“è‰²çš„ç‚¹ï¼Œ$v_t$è¡¨ç¤ºåŠ æƒå¹³å‡åçš„,çº¢è‰² æƒå¹³å‡æ–¹æ³•æ˜¯ï¼šæ¯å¤©çš„æ¸©åº¦å€¼åŠ æƒå€¼$vt$è®¾ç½®ä¸ºå‰ä¸€å¤©çš„æ¸©åº¦åŠ æƒå€¼$vtâˆ’1$å’Œå½“å¤©çš„æ¸©åº¦å®é™…å€¼$Î¸t$åšåŠ æƒå¹³å‡ï¼š $$ v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t} $$ ç”±äºä»¥åæˆ‘ä»¬è¦è€ƒè™‘çš„åŸå› ï¼Œåœ¨è®¡ç®—æ—¶å¯è§†$v_T$å¤§æ¦‚æ˜¯$\frac{1}{(1-\beta)}$çš„æ¯æ—¥æ¸©åº¦çš„åŠ æƒå¹³å‡ï¼Œ å¦‚æœæ˜¯$\beta$=0.9ï¼Œè¿™æ˜¯åå¤©çš„å¹³å‡å€¼ï¼Œçº¢è‰² å¦‚æœ$\beta$=0.98,æ˜¯50å¤©çš„ç»“æœï¼Œç»¿è‰² å¦‚æœ$beta$=0.5,æ˜¯2dayçš„ç»“æœï¼Œé»„è‰² ç”±äºä»…å¹³å‡äº†ä¸¤å¤©çš„æ¸©åº¦ï¼Œå¹³å‡çš„æ•°æ®å¤ªå°‘ï¼Œæ‰€ä»¥å¾—åˆ°çš„æ›²çº¿æœ‰æ›´å¤šçš„å™ªå£°ï¼Œæœ‰å¯èƒ½å‡ºç°å¼‚å¸¸å€¼ï¼Œä½†æ˜¯è¿™ä¸ªæ›²çº¿èƒ½å¤Ÿæ›´å¿«é€‚åº”æ¸©åº¦å˜åŒ–ã€‚ å½“ $\beta$è¾ƒå¤§æ—¶ï¼ŒæŒ‡æ•°åŠ æƒå¹³å‡å€¼é€‚åº”åœ°æ›´ç¼“æ…¢ä¸€äº›ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_10.png) $ Â¶L 04 : Understanding Exponentially Weighted Averages å‡å¦‚Î²=0.9ï¼Œæ¯ä¸ªvçš„è®¡ç®—å¦‚ä¸‹ï¼š $$ \begin{aligned} v_{100} &amp;=0.9 v_{99}+0.1 \theta_{100} \ v_{99} &amp;=0.9 v_{98}+0.1 \theta_{99} \ v_{98} &amp;=0.9 v_{97}+0.1 \theta_{98} \end{aligned} $$ é€’æ¨å¯å¾—ï¼š $$ v_{100}=0.1 \theta_{100}+0.1 * 0.9 \theta_{99}+0.1 *(0.9)^{2} \theta_{98}+\ldots $$ æŒ‡æ•°çš„è¡°å‡è§„å¾‹ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_11.png) ä¸€èˆ¬çš„ $$ v_{t}=\sum_{i=1}^{t}(1-\beta) \beta^{t-i} \theta_{i} $$ æ— ç©·çº§æ•°æ±‚å’Œï¼š $$ \sum_{t=1}^{n}(1-\beta) \beta^{t}=1 $$ å› æ­¤å¯ä»¥è¿‘ä¼¼çš„è®¤ä¸ºæ‰€æœ‰é¡¹çš„ç³»æ•°ä¹‹å’Œæ­£å¥½ä¸º100%ã€‚ å³ï¼Œ$vt$æ˜¯å¯¹tæ—¥ä¹‹å‰æ‰€æœ‰çš„å®é™…æ¸©åº¦çš„åŠ æƒå¹³å‡,æƒé‡æ˜¯æŒ‡æ•°é€’å‡çš„ã€‚ åå¤©åï¼Œæ›²çº¿é«˜åº¦ä¸‹é™åˆ°äº†1/3,èµ‹äºˆæƒé‡$\beta^{t-i}$ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_12.png) $$ 0.9^{10}=0.35=1/e $$ ä¸€èˆ¬è®¤ä¸ºï¼Œ$v_t$è¿‘ä¼¼å‰$\frac{1}{1-\beta}$çš„åŠ æƒå¹³å‡å€¼ Â¶L05 : Bias correction in exponentially weighted averages æŒ‡æ•°åŠ æƒå¹³å‡çš„åå·®ä¿®æ­£ ç”±äºè®¡ç®—$v1$çš„æ—¶å€™ï¼Œå¹¶æ²¡æœ‰å†å²å€¼åšåŠ æƒï¼Œè¿™ä¸ªæ—¶å€™ä»¤å…¶å‰ä¸€ä¸ªåŠ æƒå€¼$v0=0$ï¼Œåˆ™ä¼šå¯¼è‡´$v_1$è¿œå°äº$\theta_1$,ä¾æ¬¡ç±»æ¨ï¼Œåœ¨é è¿‘å‰é¢çš„å€¼ä¼šå‡ºç°æ˜¾è‘—çš„å°äºå®é™…å€¼çš„æƒ…å†µ å› æ­¤åšä¸€ä¸ªä¿®æ­£ $$ v_{t}=\frac{\beta v_{t-1}+(1-\beta) \theta_{t}}{1-\beta^{t}} $$ ä½ ä¼šå‘ç°éšç€$\beta^t$å¢åŠ ï¼Œæ¥è¿‘äº0ï¼Œæ‰€ä»¥å½“tå¾ˆå¤§çš„æ—¶å€™ï¼Œåå·®ä¿®æ­£å‡ ä¹æ²¡æœ‰ä½œç”¨ï¼Œå› æ­¤å½“tè¾ƒå¤§çš„æ—¶å€™ï¼Œç´«çº¿åŸºæœ¬å’Œç»¿çº¿é‡åˆäº†ã€‚ä¸è¿‡åœ¨å¼€å§‹å­¦ä¹ é˜¶æ®µï¼Œä½ æ‰å¼€å§‹é¢„æµ‹çƒ­èº«ç»ƒä¹ ï¼Œåå·®ä¿®æ­£å¯ä»¥å¸®åŠ©ä½ æ›´å¥½é¢„æµ‹æ¸©åº¦ï¼Œåå·®ä¿®æ­£å¯ä»¥å¸®åŠ©ä½ ä½¿ç»“æœä»ç´«çº¿å˜æˆç»¿çº¿ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_13.png) å› ä¸ºåœ¨Machine Learningä¸­çœ‹é‡çš„æ˜¯å¾ˆå¤šæ¬¡è¿­ä»£åçš„ç»“æœï¼ŒåˆæœŸçš„åå·®å½±å“å¹¶ä¸å¤§ã€‚ Â¶L 06 : Gradient Descent With Momentum åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•ï¼Œè¿è¡Œé€Ÿåº¦å‡ ä¹æ€»æ˜¯å¿«äºæ ‡å‡†çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_14.png) å½“æ…¢æ…¢ä¸‹é™åˆ°æœ€å°å€¼ï¼Œä¸Šä¸‹æ³¢åŠ¨çš„æ¢¯åº¦ä¸‹é™æ³•çš„é€Ÿåº¦å‡ç¼“ï¼Œæ— æ³•ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼Œ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_15.png) åœ¨çºµè½´ä¸Šï¼Œå¸Œæœ›å­¦æ ¡æ…¢ä¸€ç‚¹ï¼Œä¸éœ€è¦æ‘†åŠ¨ï¼Œæ¨ªç€ä¸Šï¼ŒåŠ å¿«å­¦æ ¡ï¼ŒåŸºäºæ­¤å°±æœ‰äº†Gradient descent with momentumã€‚ $$ \begin{array}{c}{v_{d W} :=\beta v_{d W}+(1-\beta) d W} \ {v_{d b} :=\beta v_{d b}+(1-\beta) d b} \ {w=w-\alpha v_{d W}} \ {b=b-\alpha v_{d b}}\end{array} $$ è¿™æ ·ï¼Œå¯ä»¥è®©gradientæ›´å¹³æ»‘ å¯¹äºä¸Šå›¾å‚ç›´æ–¹å‘ï¼ŒåŸæ¥æ˜¯ä¼šä¸Šä¸‹éœ‡è¡ï¼Œä½†å¼•å…¥äº†exponentially weighted averageï¼Œç›¸å½“äºå¯¹å‰é¢çš„éœ‡è¡è¿›è¡Œäº†å¹³å‡ï¼Œç»“æœå°±æ˜¯ä¸Šä¸‹éœ‡è¡äº’ç›¸æŠµæ¶ˆäº†ã€‚è€Œæ°´å¹³æ–¹å‘éƒ½æ˜¯å‘å³æ²¡æœ‰éœ‡è¡ï¼Œå› æ­¤å¹³å‡åè¿˜æ˜¯å‘å³ã€‚æœ€ç»ˆå¯¼è‡´å‘ˆç°ä¸Šå›¾çº¢è‰²çš„ä¸‹é™è·¯çº¿ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_18.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_16.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_17.png) Â¶L 07 : RMSprop RMSprop (Root Mean Square Propagationï¼Œå‡æ–¹æ ¹ä¼ é€’)ï¼Œä¸momentumä¸€æ ·ï¼Œä¹Ÿæ˜¯é™ä½æ¢¯åº¦çš„æŠ–åŠ¨ã€‚è€Œæ˜¯å¹³æŠ‘ä¸åŒå¤§å°æ¢¯åº¦çš„æ›´æ–°é€Ÿç‡ã€‚å®é™…ä¸Š ä½œç”¨åœ¨Î±ä¸Šçš„ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_19.png) å›å¿†ä¸€ä¸‹æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ï¼Œå¦‚æœä½ æ‰§è¡Œæ¢¯åº¦ä¸‹é™ï¼Œè™½ç„¶æ¨ªè½´æ–¹å‘æ­£åœ¨æ¨è¿›ï¼Œä½†çºµè½´æ–¹å‘ä¼šæœ‰å¤§å¹…åº¦æ‘†åŠ¨ï¼Œä¸ºäº†åˆ†æè¿™ä¸ªä¾‹å­ï¼Œå‡è®¾bçºµè½´ä»£è¡¨å‚æ•°ï¼Œæ¨ªè½´ä»£è¡¨å‚æ•°Wï¼Œå¯èƒ½æœ‰w1ï¼Œæˆ–è€…w2å…¶å®ƒé‡è¦çš„å‚æ•°ï¼Œä¸ºäº†ä¾¿äºç†è§£ï¼Œè¢«ç§°ä¸ºbå’Œwã€‚ æˆ‘ä»¬å¸Œæœ›å­¦ä¹ é€Ÿåº¦å¿«ï¼Œè€Œåœ¨å‚ç›´æ–¹å‘ï¼Œä¹Ÿå°±æ˜¯ä¾‹å­ä¸­çš„æ–¹å‘ï¼Œæˆ‘ä»¬å¸Œæœ›å‡ç¼“çºµè½´ä¸Šçš„æ‘†åŠ¨ï¼Œæ‰€ä»¥æœ‰äº†$S_{d W} $å’Œ$ S_{d b}$ï¼Œæˆ‘ä»¬å¸Œæœ›$S_{d W} $ä¼šç›¸å¯¹è¾ƒå°ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦é™¤ä»¥ä¸€ä¸ªè¾ƒå°çš„æ•°ï¼Œè€Œå¸Œæœ›$ S_{d b}$åˆè¾ƒå¤§ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬è¦é™¤ä»¥è¾ƒå¤§çš„æ•°å­—ï¼Œè¿™æ ·å°±å¯ä»¥å‡ç¼“çºµè½´ä¸Šçš„å˜åŒ–ã€‚ è¿™äº›å¾®åˆ†ï¼Œå‚ç›´æ–¹å‘çš„è¦æ¯”æ°´å¹³æ–¹å‘çš„å¤§å¾—å¤šï¼Œæ‰€ä»¥æ–œç‡åœ¨æ–¹å‘ç‰¹åˆ«å¤§ï¼Œæ‰€ä»¥è¿™äº›å¾®åˆ†ä¸­ï¼Œdbè¾ƒå¤§ï¼Œdwè¾ƒå°ï¼Œå› ä¸ºå‡½æ•°çš„å€¾æ–œç¨‹åº¦ï¼Œåœ¨çºµè½´ä¸Šï¼Œä¹Ÿå°±æ˜¯bæ–¹å‘ä¸Šè¦å¤§äºåœ¨æ¨ªè½´ä¸Šï¼Œä¹Ÿå°±æ˜¯æ–¹å‘ä¸ŠWã€‚dbçš„å¹³æ–¹è¾ƒå¤§ï¼Œæ‰€ä»¥$Sdb$ä¹Ÿä¼šè¾ƒå¤§ï¼Œè€Œç›¸æ¯”ä¹‹ä¸‹ï¼Œdwä¼šå°ä¸€äº›ï¼Œäº¦æˆ–dwå¹³æ–¹ä¼šå°ä¸€äº›ï¼Œå› æ­¤$Sdw$ä¼šå°ä¸€äº›ï¼Œç»“æœå°±æ˜¯çºµè½´ä¸Šçš„æ›´æ–°è¦è¢«ä¸€ä¸ªè¾ƒå¤§çš„æ•°ç›¸é™¤ï¼Œå°±èƒ½æ¶ˆé™¤æ‘†åŠ¨ï¼Œè€Œæ°´å¹³æ–¹å‘çš„æ›´æ–°åˆ™è¢«è¾ƒå°çš„æ•°ç›¸é™¤ã€‚ RMSpropçš„å½±å“å°±æ˜¯ä½ çš„æ›´æ–°æœ€åä¼šå˜æˆè¿™æ ·ï¼ˆç»¿è‰²çº¿ï¼‰ï¼Œçºµè½´æ–¹å‘ä¸Šæ‘†åŠ¨è¾ƒå°ï¼Œè€Œæ¨ªè½´æ–¹å‘ç»§ç»­æ¨è¿›ã€‚è¿˜æœ‰ä¸ªå½±å“å°±æ˜¯ï¼Œä½ å¯ä»¥ç”¨ä¸€ä¸ªæ›´å¤§å­¦ä¹ ç‡ï¼Œç„¶ååŠ å¿«å­¦ä¹ ï¼Œè€Œæ— é¡»åœ¨çºµè½´ä¸Šå‚ç›´æ–¹å‘åç¦»ã€‚ å®é™…ä¸­dwæ˜¯ä¸€ä¸ªé«˜ç»´åº¦çš„å‚æ•°å‘é‡ï¼Œdbä¹Ÿæ˜¯ä¸€ä¸ªé«˜ç»´åº¦å‚æ•°å‘é‡ï¼Œä½†æ˜¯ä½ çš„ç›´è§‰æ˜¯ï¼Œåœ¨ä½ è¦æ¶ˆé™¤æ‘†åŠ¨çš„ç»´åº¦ä¸­ï¼Œæœ€ç»ˆä½ è¦è®¡ç®—ä¸€ä¸ªæ›´å¤§çš„å’Œå€¼ï¼Œè¿™ä¸ªå¹³æ–¹å’Œå¾®åˆ†çš„åŠ æƒå¹³å‡å€¼ï¼Œæ‰€ä»¥ä½ æœ€åå»æ‰äº†é‚£äº›æœ‰æ‘†åŠ¨çš„æ–¹å‘ã€‚æ‰€ä»¥è¿™å°±æ˜¯RMSpropï¼Œå…¨ç§°æ˜¯å‡æ–¹æ ¹ï¼Œå› ä¸ºä½ å°†å¾®åˆ†è¿›è¡Œå¹³æ–¹ï¼Œç„¶åæœ€åä½¿ç”¨å¹³æ–¹æ ¹ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_20.png) è§£é‡Šå¹³æ–¹ï¼š å‚ç›´æ–¹å‘ï¼Œæ¯”è¾ƒé™¡ï¼Œæ¢¯åº¦æ¯”è¾ƒå¤§ï¼Œä½†æˆ‘ä»¬åˆå¸Œæœ›å®ƒä¸‹é™çš„æ…¢ã€‚å› æ­¤å¯¹æ¢¯åº¦é™¤ä»¥ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œæ‰€ä»¥ç”¨æ¢¯åº¦çš„å¹³æ–¹çš„å¹³å‡æ¥è¡¨ç¤ºã€‚è®©ä¸åŒçš„å‚æ•°æ‹¥æœ‰ä¸åŒçš„learning rateã€‚ ä»æŸç§è§’åº¦çœ‹ï¼ŒRMSpropä¼šæ ¹æ®å½“å‰çš„æ¢¯åº¦è‡ªåŠ¨è°ƒæ•´å‚æ•°çš„learning rateï¼Œæ¢¯åº¦å¤§é™ä½learning rateï¼Œæ¢¯åº¦å°çš„æ—¶å€™æé«˜learning rateï¼Œä»è€Œä¸€æ–¹é¢é¿å…äº†éœ‡è¡ï¼Œå¦ä¸€æ–¹é¢é¿å…åœ¨å¹³å¦çš„åœ°æ–¹å¾˜å¾Šå¤ªä¹…ã€‚ ä¸ºäº†é¿å…å‡ºç°åˆ†æ¯ä¸º0 $$ \begin{array}{c}{s_{d w}=\beta s_{d w}+(1-\beta)(d w)^{2}} \ {s_{d b}=\beta s_{d b}+(1-\beta)(d b)^{2}} \ {w :=w-\alpha \frac{d w}{\sqrt{s_{d w}+\varepsilon}}} \ {b :=b-\alpha \frac{d b}{\sqrt{s_{d b}+\varepsilon}}}\end{array} $$ $\varepsilon$å–$10^{-8}$ä¸é”™çš„é€‰æ‹©. è¡¥å……ï¼š RMSPropç®—æ³•å¯¹æ¢¯åº¦è®¡ç®—äº†å¾®åˆ†å¹³æ–¹åŠ æƒå¹³å‡æ•°ã€‚è¿™ç§åšæ³•æœ‰åˆ©äºæ¶ˆé™¤äº†æ‘†åŠ¨å¹…åº¦å¤§çš„æ–¹å‘ï¼Œç”¨æ¥ä¿®æ­£æ‘†åŠ¨å¹…åº¦ï¼Œä½¿å¾—å„ä¸ªç»´åº¦çš„æ‘†åŠ¨å¹…åº¦éƒ½è¾ƒå°ã€‚å¦ä¸€æ–¹é¢ä¹Ÿä½¿å¾—ç½‘ç»œå‡½æ•°æ”¶æ•›æ›´å¿« Â¶L 08 Adam optimization algorithm Adamï¼ˆAdaptive Moment Estimationï¼Œè‡ªé€‚åº”çŸ©ä¼°è®¡ï¼‰å°±æ˜¯momentumå’ŒRMSpropçš„ç»“åˆã€‚momentumè´Ÿè´£å¹³æ»‘æ¢¯åº¦ï¼Œè€ŒRMSpropè´Ÿè´£è°ƒè§£learning rateã€‚ Â¶1. Adam a. å¼•å…¥çš„å˜é‡æœ‰ï¼š $v$ : è®¡ç®—åŒmomentumç®—æ³•ï¼Œå°†æ¢¯åº¦è¿›è¡ŒæŒ‡æ•°åŠ æƒå¹³å‡ $s$: è®¡ç®—åŒRMSpropï¼Œå°†æ¢¯åº¦çš„å¹³æ–¹è¿›è¡ŒæŒ‡æ•°åŠ æƒå¹³å‡ $Î²1$ : è®¡ç®—vvçš„åŠ æƒå‚æ•° $Î²2$ : è®¡ç®—ssçš„åŠ æƒå‚æ•° b. åœ¨è¿­ä»£å‰ï¼Œåˆå§‹åŒ–å‚æ•°vå’Œs $$ v_{d W}=0, s_{d W}=0, v_{d b}=0, s_{d b}=0 $$ c. å¯¹ç¬¬tæ¬¡æ¢¯åº¦ä¸‹é™çš„è¿­ä»£ a. é¦–å…ˆè®¡ç®—dwå’Œdbçš„vå’Œs $$ \begin{array}{c}{v_{d W}=\beta_{1} v_{d W}+\left(1-\beta_{1}\right) d W} \ {s_{d W}=\beta_{2} s_{d W}+\left(1-\beta_{2}\right)(d W)^{2}} \ {v_{d b}=\beta_{1} v_{d b}+\left(1-\beta_{1}\right) d b} \ {s_{d b}=\beta_{2} s_{d b}+\left(1-\beta_{2}\right)(d b)^{2}}\end{array} $$ d. ä¿®æ­£ $$ v_{d W}^{\text {corrected}}=\frac{v_{d W}}{1-\left(\beta_{1}\right)^{t}}\ \begin{aligned} s_{d W}^{\text {corrected}} &amp;=\frac{s_{d W}}{1-\left(\beta_{2}\right)^{t}} \ v_{d b}^{\text {corrected}} &amp;=\frac{v_{d b}}{1-\left(\beta_{1}\right)^{t}} \ s_{d b}^{\text {corrected}} &amp;=\frac{s_{d b}}{1-\left(\beta_{2}\right)^{t}} \end{aligned} $$ e. æœ€åæ›´æ–°å‚æ•°Wå’Œb $$ W=W-\alpha \frac{v_{d W}^{\text {corrected}}}{\sqrt{s_{d W}^{\text { corrected }}+\varepsilon}}\ b=b-\alpha \frac{v_{d b}^{\text {corrected}}}{\sqrt{s_{d b}^{\text { corrected }}+\varepsilon}} $$ è¶…å‚çš„é€‰æ‹©ï¼š Î±ï¼šéœ€è¦è°ƒä¼˜ Î²1: é€šå¸¸é€‰æ‹©ä¸º0.9 Î²2: é€šå¸¸é€‰æ‹©ä¸º0.999 Îµ: ä¸€èˆ¬ä¸éœ€è¦è°ƒä¼˜ï¼Œé€‰æ‹©ä¸€ä¸ªå°æ•°ï¼Œæ¯”å¦‚10âˆ’8 ä½ å¯ä»¥å°è¯•ä¸€ç³»åˆ—å€¼Î±ï¼Œç„¶åçœ‹å“ªä¸ªæœ‰æ•ˆ Â¶L09 : Learning Rate Decay why ä¸ºä»€ä¹ˆè¦åšlearning rate decayï¼Ÿ è¾ƒå¤§çš„learning rateè™½ç„¶åœ¨ç®—æ³•å¼€å§‹é˜¶æ®µä¼šåŠ å¿«æ”¶æ•›é€Ÿåº¦ï¼Œä½†åœ¨æ”¶æ•›æ¥è¿‘åˆ°ä¼˜åŒ–ç‚¹çš„æ—¶å€™ï¼Œç®—æ³•ä¼šåœ¨ä¼˜åŒ–ç‚¹é™„è¿‘éœ‡è¡ï¼Œå¦‚ä¸‹å›¾ï¼š ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_22.png) 2.å¦‚ä½•åšlearning rate decayï¼Ÿ æ€è·¯å¾ˆç®€å•ï¼Œå°±æ˜¯å¼•å…¥ä¸€ä¸ªå‡½æ•°ï¼Œè®©Î±éšç€è¿­ä»£ï¼ˆæ¯”å¦‚min-batchçš„epochï¼‰é€’å‡ã€‚ä¸ºæ­¤å¯ä»¥é‡‡ç”¨çš„decayå‡½æ•°æœ‰ï¼š å€’æ•°ï¼š $$ \alpha :=\frac{1}{1+\text { decay rate * epoch num}} \alpha $$ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_23.png) Â¶L 10: The Problem of local Optima äº‹å®ä¸Šï¼Œå¦‚æœä½ è¦åˆ›å»ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œé€šå¸¸æ¢¯åº¦ä¸ºé›¶çš„ç‚¹å¹¶ä¸æ˜¯è¿™ä¸ªå›¾ä¸­çš„å±€éƒ¨æœ€ä¼˜ç‚¹ï¼Œå®é™…ä¸Šæˆæœ¬å‡½æ•°çš„é›¶æ¢¯åº¦ç‚¹ï¼Œé€šå¸¸æ˜¯éç‚¹ã€‚ ä½†æ˜¯ä¸€ä¸ªå…·æœ‰é«˜ç»´åº¦ç©ºé—´çš„å‡½æ•°ï¼Œå¦‚æœæ¢¯åº¦ä¸º0ï¼Œé‚£ä¹ˆåœ¨æ¯ä¸ªæ–¹å‘ï¼Œå®ƒå¯èƒ½æ˜¯å‡¸å‡½æ•°ï¼Œä¹Ÿå¯èƒ½æ˜¯å‡¹å‡½æ•°ã€‚å¦‚æœä½ åœ¨2ä¸‡ç»´ç©ºé—´ä¸­ï¼Œé‚£ä¹ˆæƒ³è¦å¾—åˆ°å±€éƒ¨æœ€ä¼˜ï¼Œæ‰€æœ‰çš„2ä¸‡ä¸ªæ–¹å‘éƒ½éœ€è¦æ˜¯è¿™æ ·ï¼Œä½†å‘ç”Ÿçš„æœºç‡ä¹Ÿè®¸å¾ˆå°ï¼Œä¹Ÿè®¸æ˜¯$2^{-20000}$ï¼Œå› æ­¤æ›´æœ‰å¯èƒ½é‡åˆ°æœ‰äº›æ–¹å‘çš„æ›²çº¿ä¼šè¿™æ ·å‘ä¸Šå¼¯æ›²ï¼Œå¦ä¸€äº›æ–¹å‘æ›²çº¿å‘ä¸‹å¼¯ï¼Œè€Œä¸æ˜¯æ‰€æœ‰çš„éƒ½å‘ä¸Šå¼¯æ›²ï¼Œå› æ­¤åœ¨é«˜ç»´åº¦ç©ºé—´ï¼Œä½ æ›´å¯èƒ½ç¢°åˆ°éç‚¹ã€‚æ‰€æœ‰ï¼Œæ‹…å¿ƒæ”¶æ•›åˆ°local optimaï¼ŒçœŸæ˜¯äººä»¬æƒ³å¤šäº†ï¼Œå®é™…ä¸Šå¹¶æ²¡æœ‰æƒ³è±¡çš„é‚£ä¹ˆå¤šlocal optimaã€‚åœ¨é«˜ç»´ç©ºé—´ï¼Œå‡ ä¹ä¸å¤ªå¯èƒ½è¢«å›°åœ¨ä¸€ä¸ªlocal optimaï¼Œè¿™æ˜¯ä¸€ä¸ªå¥½æ¶ˆæ¯ã€‚ å› æ­¤ï¼Œåœ¨é«˜ç»´ç©ºé—´é‡åˆ°çš„é—®é¢˜æ˜¯é«˜åŸé—®é¢˜ï¼ˆProblem of plateausï¼‰ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_27.png) Adamç®—æ³•å¯ä»¥åŠ é€Ÿå­¦ä¹  W3 Hyperparameter tuning Â¶L01 Tuning process åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ¥è§¦åˆ°çš„hyperparameteræœ‰ï¼š learning rate: Î±Î± momentum å‚æ•°: Î²Î² Adamå‚æ•°: Î²1Î²1å’Œ Î²2Î²2ä»¥åŠÎµÎµ ç¥ç»ç½‘ç»œå±‚æ•°: L ç¥ç»ç½‘ç»œéšè—å±‚neuronæ•°ï¼šn[l]n[l] learning rate decayå‚æ•° min-batch size è¿™äº›hyperparameteré‡è¦æ€§æ’åºï¼š æœ€é‡è¦çš„ï¼š learning rate: Î±Î± æ¯”è¾ƒé‡è¦çš„ï¼š momentum å‚æ•°: Î²Î² ç¥ç»ç½‘ç»œå±‚æ•°: L ç¥ç»ç½‘ç»œéšè—å±‚neuronæ•°ï¼šn[l]n[l] æ¬¡é‡è¦çš„ï¼š ç¥ç»ç½‘ç»œéšè—å±‚neuronæ•° learning rate decayå‚æ•° åŸºæœ¬ä¸éœ€è°ƒæ•´çš„ Î²1Î²1å’Œ Î²2Î²2ä»¥åŠÎµ Â¶1. Try random values : Donâ€™t use a grid ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_28.png) why: ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾è¶…å‚æ•°1æ˜¯ï¼ˆå­¦ä¹ é€Ÿç‡ï¼‰ï¼Œå–ä¸€ä¸ªæç«¯çš„ä¾‹å­ï¼Œå‡è®¾è¶…å‚æ•°2æ˜¯Adamç®—æ³•ä¸­ï¼Œåˆ†æ¯ä¸­çš„$\varepsilon$ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œaçš„å–å€¼å¾ˆé‡è¦ï¼Œè€Œ$\varepsilon$å–å€¼åˆ™æ— å…³ç´§è¦ã€‚å¦‚æœä½ åœ¨ç½‘æ ¼ä¸­å–ç‚¹ï¼Œæ¥ç€ï¼Œä½ è¯•éªŒäº†açš„5ä¸ªå–å€¼ï¼Œé‚£ä½ ä¼šå‘ç°ï¼Œæ— è®º$\varepsilon$å–ä½•å€¼ï¼Œç»“æœåŸºæœ¬ä¸Šéƒ½æ˜¯ä¸€æ ·çš„ã€‚æ‰€ä»¥ï¼Œä½ çŸ¥é“å…±æœ‰25ç§æ¨¡å‹ï¼Œä½†è¿›è¡Œè¯•éªŒçš„å€¼åªæœ‰5ä¸ªï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯å¾ˆé‡è¦çš„ã€‚ å¯¹æ¯”è€Œè¨€ï¼Œå¦‚æœä½ éšæœºå–å€¼ï¼Œä½ ä¼šè¯•éªŒ25ä¸ªç‹¬ç«‹çš„a,$\varepsilon$ï¼Œä¼¼ä¹ä½ æ›´æœ‰å¯èƒ½å‘ç°æ•ˆæœåšå¥½çš„é‚£ä¸ªã€‚ Â¶2. ç”±ç²—ç³™åˆ°ç²¾ç»†çš„ç­–ç•¥ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_29.png) Â¶L 02: Using an Appropriate Scale to pick hyperparameters $a$å–å€¼0.0001,1,å¦‚æœä½ ç”»ä¸€æ¡ä»0.0001åˆ°1çš„æ•°è½´ï¼Œæ²¿å…¶éšæœºå‡åŒ€å–å€¼ï¼Œé‚£90%çš„æ•°å€¼å°†ä¼šè½åœ¨0.1åˆ°1ä¹‹é—´ï¼Œç»“æœå°±æ˜¯ï¼Œåœ¨0.1åˆ°1ä¹‹é—´ï¼Œåº”ç”¨äº†90%çš„èµ„æºï¼Œè€Œåœ¨0.0001åˆ°0.1ä¹‹é—´ï¼Œåªæœ‰10%çš„æœç´¢èµ„æºï¼Œè¿™çœ‹ä¸Šå»ä¸å¤ªå¯¹ã€‚ åŒæ—¶åœ¨èŒƒå›´å†…æœç´¢ï¼Œä¹Ÿä¸æ˜¯å‡åŒ€åˆ†å¸ƒï¼ˆuniformly randomï¼‰çš„ï¼Œé€šå¸¸æœ‰è¿™ä¸ªå‚æ•°çš„scaleï¼Œæ¯”å¦‚å¯¹æ•°scaleã€‚ åè€Œï¼Œç”¨å¯¹æ•°æ ‡å°ºæœç´¢è¶…å‚æ•°çš„æ–¹å¼ä¼šæ›´åˆç†ï¼Œå› æ­¤è¿™é‡Œä¸ä½¿ç”¨çº¿æ€§è½´ï¼Œåˆ†åˆ«ä¾æ¬¡å–0.0001ï¼Œ0.001ï¼Œ0.01ï¼Œ0.1ï¼Œ1ï¼Œåœ¨å¯¹æ•°è½´ä¸Šå‡åŒ€éšæœºå–ç‚¹ï¼Œè¿™æ ·ï¼Œåœ¨0.0001åˆ°0.001ä¹‹é—´ï¼Œå°±ä¼šæœ‰æ›´å¤šçš„æœç´¢èµ„æºå¯ç”¨ï¼Œè¿˜æœ‰åœ¨0.001åˆ°0.01ä¹‹é—´ç­‰ç­‰ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_32.png) Â¶L 03 : Hyperparameter tuning i practice ä¸åŒçš„ç®—æ³•å’Œåœºæ™¯ï¼Œå¯¹è¶…å‚çš„scaleæ•æ„Ÿæ€§å¯èƒ½ä¸ä¸€æ ·. æ ¹æ®è®¡ç®—èµ„æºå’Œæ•°æ®é‡ï¼Œå¯ä»¥é‡‡å–ä¸¤ç§ç­–ç•¥æ¥è°ƒå‚ Pandaï¼ˆç†ŠçŒ«ç­–ç•¥ï¼‰ï¼šå¯¹ä¸€ä¸ªæ¨¡å‹å…ˆåä¿®æ”¹å‚æ•°ï¼ŒæŸ¥çœ‹å…¶è¡¨ç°ï¼Œæœ€ç»ˆé€‰æ‹©æœ€å¥½çš„å‚æ•°ã€‚å°±åƒç†ŠçŒ«ä¸€æ ·ï¼Œä¸€æ¬¡åªæŠšå…»ä¸€ä¸ªåä»£ã€‚ Caviarï¼ˆé±¼å­é…±ç­–ç•¥ï¼‰ï¼šè®¡ç®—èµ„æºè¶³å¤Ÿï¼Œå¯ä»¥åŒæ—¶è¿è¡Œå¾ˆå¤šæ¨¡å‹å®ä¾‹ï¼Œé‡‡ç”¨ä¸åŒçš„å‚æ•°ï¼Œç„¶åæœ€ç»ˆé€‰æ‹©ä¸€ä¸ªå¥½çš„ã€‚ç±»ä¼¼é±¼ç±»ï¼Œä¸€æ¬¡ä¸‹å¾ˆå¤šåµï¼Œè‡ªåŠ¨ç«äº‰æˆæ´»ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_33.png) Â¶L 04: Normalizing Activations in a Network Â¶1. Implementing Batch Normalizing Batchå½’ä¸€åŒ–,Batchå½’ä¸€åŒ–ä¼šä½¿ä½ çš„å‚æ•°æœç´¢é—®é¢˜å˜å¾—å¾ˆå®¹æ˜“ï¼Œä½¿ç¥ç»ç½‘ç»œå¯¹è¶…å‚æ•°çš„é€‰æ‹©æ›´åŠ ç¨³å®šï¼Œè¶…å‚æ•°çš„èŒƒå›´ä¼šæ›´åŠ åºå¤§ï¼Œå·¥ä½œæ•ˆæœä¹Ÿå¾ˆå¥½ï¼Œä¹Ÿä¼šæ˜¯ä½ çš„è®­ç»ƒæ›´åŠ å®¹æ˜“ï¼Œç”šè‡³æ˜¯æ·±å±‚ç½‘ç»œã€‚ å¯ä»¥normalize $a{[l]},z{[l]}$,é€‰æ‹©$z^{[L]}$ è®¾ç½® Î³ å’Œ Î² çš„åŸå› æ˜¯ï¼Œå¦‚æœå„éšè—å±‚çš„è¾“å…¥å‡å€¼åœ¨é è¿‘ 0 çš„åŒºåŸŸï¼Œå³å¤„äºæ¿€æ´»å‡½æ•°çš„çº¿æ€§åŒºåŸŸï¼Œä¸åˆ©äºè®­ç»ƒéçº¿æ€§ç¥ç»ç½‘ç»œï¼Œä»è€Œå¾—åˆ°æ•ˆæœè¾ƒå·®çš„æ¨¡å‹ã€‚å› æ­¤ï¼Œéœ€è¦ç”¨ Î³ å’Œ Î² å¯¹æ ‡å‡†åŒ–åçš„ç»“æœåšè¿›ä¸€æ­¥å¤„ç†ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_34.png) éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒÎ²å’ŒÎ³ä¸æ˜¯è¶…å‚ï¼Œè€Œæ˜¯æ¢¯åº¦ä¸‹é™éœ€å­¦ä¹ çš„å‚æ•°ã€‚ Â¶L 05 : Fitting Batch Norm Into Neural Networks ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_35.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_36.png) æ³¨æ„ å…ˆå‰æˆ‘è¯´è¿‡æ¯å±‚çš„å‚æ•°æ˜¯$w{[l]}$å’Œ$b{[l]}$ï¼Œè¿˜æœ‰$\beta{[l]}$å’Œ$b{[l]}$ï¼Œè¯·æ³¨æ„è®¡ç®—çš„æ–¹å¼å¦‚ä¸‹ï¼Œ$z{[l]}=w{[l]} a{[l-1]}+b{[l]}$ï¼Œä½†Batchå½’ä¸€åŒ–åšçš„æ˜¯ï¼Œå®ƒè¦çœ‹è¿™ä¸ªmini-batchï¼Œå…ˆå°†$z{[l]}$å½’ä¸€åŒ–ï¼Œç»“æœä¸ºå‡å€¼0å’Œæ ‡å‡†æ–¹å·®ï¼Œå†ç”±$\beta$å’Œbé‡ç¼©æ”¾ï¼Œä½†è¿™æ„å‘³ç€ï¼Œæ— è®º$b{[l]}$çš„å€¼æ˜¯å¤šå°‘ï¼Œéƒ½æ˜¯è¦è¢«å‡å»çš„ï¼Œå› ä¸ºåœ¨Batchå½’ä¸€åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œä½ è¦è®¡ç®—çš„$z^{[l]}$å‡å€¼ï¼Œå†å‡å»å¹³å‡å€¼ï¼Œåœ¨æ­¤ä¾‹ä¸­çš„mini-batchä¸­å¢åŠ ä»»ä½•å¸¸æ•°ï¼Œæ•°å€¼éƒ½ä¸ä¼šæ”¹å˜ï¼Œå› ä¸ºåŠ ä¸Šçš„ä»»ä½•å¸¸æ•°éƒ½å°†ä¼šè¢«å‡å€¼å‡å»æ‰€æŠµæ¶ˆ. ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_37.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_38.png) æœ€åï¼Œè¯·è®°ä½çš„ç»´$z{[l]}$ï¼Œå› ä¸ºåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œç»´æ•°ä¼šæ˜¯$\left(n{[l]}, 1\right)$ï¼Œçš„å°ºå¯¸ä¸ºï¼Œå¦‚æœæ˜¯lå±‚éšè—å•å…ƒçš„æ•°é‡ï¼Œé‚£$ \beta^{[l]}$å’Œ$ \gamma{[l]}$çš„ç»´åº¦ä¹Ÿæ˜¯$\left(n{[l]}, 1\right)$ï¼Œå› ä¸ºè¿™æ˜¯ä½ éšè—å±‚çš„æ•°é‡ï¼Œä½ æœ‰éšè—å•å…ƒï¼Œæ‰€ä»¥$\gamma^{[l]}$å’Œ$ \beta^{[l]}$ç”¨æ¥å°†æ¯ä¸ªéšè—å±‚çš„å‡å€¼å’Œæ–¹å·®ç¼©æ”¾ä¸ºç½‘ç»œæƒ³è¦çš„å€¼ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_39.png) Â¶L 06 Why Doest Batch Norm Work? é¦–å…ˆï¼Œèµ·åˆ°äº†normalizationçš„ä½œç”¨ï¼ŒåŒå¯¹è¾“å…¥æ•°æ®Xçš„normalizationä½œç”¨ç±»ä¼¼ã€‚ è®©æ¯ä¸€å±‚çš„å­¦ä¹ ï¼Œä¸€å®šç¨‹åº¦è§£è€¦äº†å‰å±‚å‚æ•°å’Œåå±‚å‚æ•°ï¼Œè®©å„å±‚æ›´åŠ ç‹¬ç«‹çš„å­¦ä¹ ã€‚æ— è®ºå‰ä¸€å±‚å¦‚ä½•å˜ï¼Œæœ¬å±‚è¾“å…¥çš„æ•°æ®æ€»æ˜¯ä¿æŒç¨³å®šçš„å‡å€¼å’Œæ–¹å·®ã€‚ï¼ˆä¸»è¦åŸå› ï¼‰ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_40.png) æ‰€ä»¥ä½¿ä½ æ•°æ®æ”¹å˜åˆ†å¸ƒçš„è¿™ä¸ªæƒ³æ³•ï¼Œæœ‰ä¸ªæœ‰ç‚¹æ€ªçš„åå­—â€œCovariate shiftâ€ï¼Œæƒ³æ³•æ˜¯è¿™æ ·çš„ï¼Œå¦‚æœä½ å·²ç»å­¦ä¹ äº†åˆ° çš„æ˜ å°„ï¼Œå¦‚æœ çš„åˆ†å¸ƒæ”¹å˜äº†ï¼Œé‚£ä¹ˆä½ å¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒä½ çš„å­¦ä¹ ç®—æ³•ã€‚è¿™ç§åšæ³•åŒæ ·é€‚ç”¨äºï¼Œå¦‚æœçœŸå®å‡½æ•°ç”± åˆ° æ˜ å°„ä¿æŒä¸å˜ï¼Œæ­£å¦‚æ­¤ä¾‹ä¸­ï¼Œå› ä¸ºçœŸå®å‡½æ•°æ˜¯æ­¤å›¾ç‰‡æ˜¯å¦æ˜¯ä¸€åªçŒ«ï¼Œè®­ç»ƒä½ çš„å‡½æ•°çš„éœ€è¦å˜å¾—æ›´åŠ è¿«åˆ‡ï¼Œå¦‚æœçœŸå®å‡½æ•°ä¹Ÿæ”¹å˜ï¼Œæƒ…å†µå°±æ›´ç³Ÿäº†ã€‚ å…³äºç¬¬äºŒç‚¹ï¼Œå¦‚æœå®é™…åº”ç”¨æ ·æœ¬å’Œè®­ç»ƒæ ·æœ¬çš„æ•°æ®åˆ†å¸ƒä¸åŒï¼ˆä¾‹å¦‚ï¼Œæ©˜çŒ«å›¾ç‰‡å’Œé»‘çŒ«å›¾ç‰‡ï¼‰ï¼Œæˆ‘ä»¬ç§°å‘ç”Ÿäº†â€œCovariate Shiftâ€ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€èˆ¬è¦å¯¹æ¨¡å‹è¿›è¡Œé‡æ–°è®­ç»ƒã€‚Batch Normalization çš„ä½œç”¨å°±æ˜¯å‡å° Covariate Shift æ‰€å¸¦æ¥çš„å½±å“ï¼Œè®©æ¨¡å‹å˜å¾—æ›´åŠ å¥å£®ï¼Œé²æ£’æ€§ï¼ˆRobustnessï¼‰æ›´å¼ºã€‚ å³ä½¿è¾“å…¥çš„å€¼æ”¹å˜äº†ï¼Œç”±äº Batch Normalization çš„ä½œç”¨ï¼Œä½¿å¾—å‡å€¼å’Œæ–¹å·®ä¿æŒä¸å˜ï¼ˆç”± Î³ å’Œ Î² å†³å®šï¼‰ï¼Œé™åˆ¶äº†åœ¨å‰å±‚çš„å‚æ•°æ›´æ–°å¯¹æ•°å€¼åˆ†å¸ƒçš„å½±å“ç¨‹åº¦ï¼Œå› æ­¤åå±‚çš„å­¦ä¹ å˜å¾—æ›´å®¹æ˜“ä¸€äº›ã€‚Batch Normalization å‡å°‘äº†å„å±‚ W å’Œ b ä¹‹é—´çš„è€¦åˆæ€§ï¼Œè®©å„å±‚æ›´åŠ ç‹¬ç«‹ï¼Œå®ç°è‡ªæˆ‘è®­ç»ƒå­¦ä¹ çš„æ•ˆæœã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_41.png) å¦å¤–ï¼ŒBatch Normalization ä¹Ÿèµ·åˆ°å¾®å¼±çš„æ­£åˆ™åŒ–ï¼ˆregularizationï¼‰æ•ˆæœã€‚å› ä¸ºåœ¨æ¯ä¸ª mini-batch è€Œéæ•´ä¸ªæ•°æ®é›†ä¸Šè®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼Œåªç”±è¿™ä¸€å°éƒ¨åˆ†æ•°æ®ä¼°è®¡å¾—å‡ºçš„å‡å€¼å’Œæ–¹å·®ä¼šæœ‰ä¸€äº›å™ªå£°ï¼Œå› æ­¤æœ€ç»ˆè®¡ç®—å‡ºçš„ z(i)z(i)ä¹Ÿæœ‰ä¸€å®šå™ªå£°ã€‚ç±»ä¼¼äº dropoutï¼Œè¿™ç§å™ªå£°ä¼šä½¿å¾—ç¥ç»å…ƒä¸ä¼šå†ç‰¹åˆ«ä¾èµ–äºä»»ä½•ä¸€ä¸ªè¾“å…¥ç‰¹å¾ã€‚ å› ä¸º Batch Normalization åªæœ‰å¾®å¼±çš„æ­£åˆ™åŒ–æ•ˆæœï¼Œå› æ­¤å¯ä»¥å’Œ dropout ä¸€èµ·ä½¿ç”¨ï¼Œä»¥è·å¾—æ›´å¼ºå¤§çš„æ­£åˆ™åŒ–æ•ˆæœã€‚é€šè¿‡åº”ç”¨æ›´å¤§çš„ mini-batch å¤§å°ï¼Œå¯ä»¥å‡å°‘å™ªå£°ï¼Œä»è€Œå‡å°‘è¿™ç§æ­£åˆ™åŒ–æ•ˆæœã€‚ æœ€åï¼Œä¸è¦å°† Batch Normalization ä½œä¸ºæ­£åˆ™åŒ–çš„æ‰‹æ®µï¼Œè€Œæ˜¯å½“ä½œåŠ é€Ÿå­¦ä¹ çš„æ–¹å¼ã€‚æ­£åˆ™åŒ–åªæ˜¯ä¸€ç§éæœŸæœ›çš„å‰¯ä½œç”¨ï¼ŒBatch Normalization è§£å†³çš„è¿˜æ˜¯åå‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„æ¢¯åº¦é—®é¢˜ï¼ˆæ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸ï¼‰ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_42.png) Â¶L 07 : Batch Norm At Test Time é—®é¢˜ï¼šBNç®—æ³•åœ¨è®­ç»ƒæ—¶æ˜¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®è®­ç»ƒï¼Œèƒ½ç®—å‡ºæ¯ä¸€å±‚Zçš„å‡å€¼å’Œæ–¹å·®ï¼›è€Œåœ¨æµ‹è¯•æ—¶ï¼Œè¾“å…¥çš„åˆ™æ˜¯å•ä¸ªæ•°æ®ï¼Œå•æ¡æ•°æ®æ²¡æ³•åšå‡å€¼å’Œæ–¹å·®çš„è®¡ç®—ï¼Œæ€ä¹ˆåœ¨æµ‹è¯•æœŸè¾“å…¥å‡å€¼å’Œæ–¹å·®å‘¢? å®é™…åº”ç”¨ä¸­ä¸€èˆ¬ä¸ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œè€Œæ˜¯ä½¿ç”¨ä¹‹å‰å­¦ä¹ è¿‡çš„æŒ‡æ•°åŠ æƒå¹³å‡çš„æ–¹æ³•æ¥é¢„æµ‹æµ‹è¯•è¿‡ç¨‹å•ä¸ªæ ·æœ¬çš„ Î¼ å’Œ $Ïƒ^2$ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_43.png) è®¡ç®—$z_{\text { norm }}^{(\hat{2})}$ï¼Œç”¨$\mu$ å’Œ$ \sigma^{2}$çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œç”¨ä½ æ‰‹å¤´çš„æœ€æ–°æ•°å€¼æ¥åšè°ƒæ•´ï¼Œç„¶åä½ å¯ä»¥ç”¨å·¦è¾¹æˆ‘ä»¬åˆšç®—å‡ºæ¥çš„å’Œä½ åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­å¾—åˆ°çš„$\beta$å’Œ$\sigma$å‚æ•°æ¥è®¡ç®—ä½ é‚£ä¸ªæµ‹è¯•æ ·æœ¬çš„zå€¼ã€‚ Â¶L 08 : Softmax Regression Â¶1. [Multi-class classification] ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_44.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_45.png) æœ€åä¸€å±‚æ˜¯æ¦‚ç‡ï¼Œä¹‹å’Œä¸º1ï¼Œè¦ç”¨åˆ°Softmaxå±‚ï¼ŒSoftmaxæ¿€æ´»å‡½æ•°çš„ç‰¹æ®Šä¹‹å¤„åœ¨äºï¼Œå› ä¸ºéœ€è¦å°†æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºå½’ä¸€åŒ–ï¼Œå°±éœ€è¦è¾“å…¥ä¸€ä¸ªå‘é‡ï¼Œæœ€åè¾“å‡ºä¸€ä¸ªå‘é‡ã€‚ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_46.png) Â¶2. Softmax example æ²¡æœ‰éšè—å±‚çš„softmax,ä»£è¡¨ä¸€äº›å†³ç­–è¾¹ç•Œ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_47.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_48.png) Â¶L 09 Training SoftMax classifier ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_49.png) Softmaxè¿™ä¸ªåç§°çš„æ¥æºæ˜¯ä¸æ‰€è°“hardmaxå¯¹æ¯”,Softmaxå›å½’æˆ–Softmaxæ¿€æ´»å‡½æ•°å°†logisticæ¿€æ´»å‡½æ•°æ¨å¹¿åˆ°ç±»ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸¤ç±»ï¼Œç»“æœå°±æ˜¯å¦‚æœC=2ï¼Œé‚£ä¹ˆC=2çš„Softmaxå®é™…ä¸Šå˜å›äº†logisticå›å½’ï¼Œ Â¶Loss Function ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_50.png) $$ J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right) $$ ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_51.png) Â¶3. Gradient descent with softmax ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_52.png) æœ€åä¸€å±‚æ±‚å¯¼ï¼Œsoftmaxæ¿€æ´»å‡½æ•° $$ J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right) $$ Â¶L11 TensorFlow Â¶1. åŸºæœ¬æµç¨‹ ä½¿ç”¨tensorflowï¼Œåªè¦å‘Šè¯‰tensorflow forward propï¼Œå®ƒè‡ªå·±å°±ä¼šåšbackpropï¼Œå› æ­¤ä¸ç”¨è‡ªå·±å®ç°backprop ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_53.png) 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport tensorflow as tf#å¯¼å…¥TensorFlowâ€‹w = tf.Variable(0,dtype = tf.float32)#æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å®šä¹‰å‚æ•°wï¼Œåœ¨TensorFlowä¸­ï¼Œä½ è¦ç”¨tf.Variable()æ¥å®šä¹‰å‚æ•°â€‹#ç„¶åæˆ‘ä»¬å®šä¹‰æŸå¤±å‡½æ•°ï¼šâ€‹cost = tf.add(tf.add(w**2,tf.multiply(- 10.,w)),25)#ç„¶åæˆ‘ä»¬å®šä¹‰æŸå¤±å‡½æ•°Jç„¶åæˆ‘ä»¬å†å†™ï¼šâ€‹train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)#(è®©æˆ‘ä»¬ç”¨0.01çš„å­¦ä¹ ç‡ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±)ã€‚â€‹#æœ€åä¸‹é¢çš„å‡ è¡Œæ˜¯æƒ¯ç”¨è¡¨è¾¾å¼:â€‹init = tf.global_variables_initializer()â€‹session = tf.Session()#è¿™æ ·å°±å¼€å¯äº†ä¸€ä¸ªTensorFlow sessionã€‚â€‹session.run(init)#æ¥åˆå§‹åŒ–å…¨å±€å˜é‡ã€‚â€‹#ç„¶åè®©TensorFlowè¯„ä¼°ä¸€ä¸ªå˜é‡ï¼Œæˆ‘ä»¬è¦ç”¨åˆ°:â€‹session.run(w)â€‹#ä¸Šé¢çš„è¿™ä¸€è¡Œå°†wåˆå§‹åŒ–ä¸º0ï¼Œå¹¶å®šä¹‰æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å®šä¹‰trainä¸ºå­¦ä¹ ç®—æ³•ï¼Œå®ƒç”¨æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–å™¨ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬è¿˜æ²¡æœ‰è¿è¡Œå­¦ä¹ ç®—æ³•ï¼Œæ‰€ä»¥#ä¸Šé¢çš„è¿™ä¸€è¡Œå°†wåˆå§‹åŒ–ä¸º0ï¼Œå¹¶å®šä¹‰æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å®šä¹‰trainä¸ºå­¦ä¹ ç®—æ³•ï¼Œå®ƒç”¨æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–å™¨ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬è¿˜æ²¡æœ‰è¿è¡Œå­¦ä¹ ç®—æ³•ï¼Œæ‰€ä»¥session.run(w)è¯„ä¼°äº†wï¼Œè®©æˆ‘ï¼šï¼šâ€‹print(session.run(w))â€‹æ‰€ä»¥å¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªï¼Œå®ƒè¯„ä¼°ç­‰äº0ï¼Œå› ä¸ºæˆ‘ä»¬ä»€ä¹ˆéƒ½è¿˜æ²¡è¿è¡Œã€‚#ç°åœ¨è®©æˆ‘ä»¬è¾“å…¥ï¼šâ€‹$session.run(train)ï¼Œå®ƒæ‰€åšçš„å°±æ˜¯è¿è¡Œä¸€æ­¥æ¢¯åº¦ä¸‹é™æ³•ã€‚#æ¥ä¸‹æ¥åœ¨è¿è¡Œäº†ä¸€æ­¥æ¢¯åº¦ä¸‹é™æ³•åï¼Œè®©æˆ‘ä»¬è¯„ä¼°ä¸€ä¸‹wçš„å€¼ï¼Œå†printï¼šâ€‹print(session.run(w))#åœ¨ä¸€æ­¥æ¢¯åº¦ä¸‹é™æ³•ä¹‹åï¼Œwç°åœ¨æ˜¯0.1ã€‚ Â¶2. å¦‚ä½•ç”¨è®­ç»ƒæ•°æ® placeholder åœ¨å®é™…çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¦ç”¨ä¸åŒçš„æ ·æœ¬åå¤æ”¾åˆ°ä¸€ä¸ªå¾…ä¼˜åŒ–å‡½æ•°ä¸­çš„ï¼Œè¿™ä¸ªæ—¶å€™å°±å¯ä»¥ç”¨tensorflowçš„placeholderå®ç°,åœ¨runçš„æ—¶å€™ï¼Œå¯¹åº”ç»™å‡ºfeed_dictï¼Œè¡¨åå ä½ç¬¦xçš„å®é™…å€¼ã€‚ 123456789101112131415161718192021import numpy as npimport tensorflow as tf # å¯¼å…¥Tensorflowcoefficient = np.array([[2.],[-10.],[25.]])w = tf.Variable(0, dtype=tf.float32)x = tf.placeholder(tf.float32, [3,1]) # 3x1å¤§å°çš„placeholdercost = w**x[0][0] - x[1][0]*w + x[2][0] # è¦ä¼˜åŒ–çš„cost functionï¼ˆå³forward propçš„å½¢å¼ï¼‰train = tf.train.GradientDescentOptimizer(0.01).minimize(cost) init = tf.global_variables_initializer()session = tf.Session()session.run(init)print(session.run(w))session.run(train, feed_dict=&#123;x:coefficient&#125;) # xå ä½ç¬¦æ›¿æ¢ä¸ºcoefficientprint(session.run(w))for i in range(1000): session.run(train, feed_dict=&#123;x:coefficient&#125;) # # xå ä½ç¬¦æ›¿æ¢ä¸ºcoefficientprint(session.run(w)) Â¶3. è®¡ç®—æµ TensorFlowç¨‹åºçš„æ ¸å¿ƒæ˜¯è®¡ç®—æŸå¤±å‡½æ•°ï¼Œç„¶åTensorFlowè‡ªåŠ¨è®¡ç®—å‡ºå¯¼æ•°ï¼Œä»¥åŠå¦‚ä½•æœ€å°åŒ–æŸå¤±ï¼Œå› æ­¤è¿™ä¸ªç­‰å¼æˆ–è€…è¿™è¡Œä»£ç æ‰€åšçš„å°±æ˜¯è®©TensorFlowå»ºç«‹è®¡ç®—å›¾ï¼Œ with è¯­å¥é€‚ç”¨äºå¯¹èµ„æºè¿›è¡Œè®¿é—®çš„åœºåˆï¼Œç¡®ä¿ä¸ç®¡ä½¿ç”¨è¿‡ç¨‹ä¸­æ˜¯å¦å‘ç”Ÿå¼‚å¸¸éƒ½ä¼šæ‰§è¡Œå¿…è¦çš„â€œæ¸…ç†â€æ“ä½œï¼Œé‡Šæ”¾èµ„æºï¼Œæ¯”å¦‚æ–‡ä»¶ä½¿ç”¨åè‡ªåŠ¨å…³é—­ã€çº¿ç¨‹ä¸­é”çš„è‡ªåŠ¨è·å–å’Œé‡Šæ”¾ç­‰ã€‚å»ºç«‹è®¡ç®—æµçš„è¿‡ç¨‹ï¼Œå‰å‘ä¼ æ’­çš„è¿‡ç¨‹ï¼Œoperation ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_56.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_54.png) ![](ImprovingDeep learning.ai_Deep Neural NetworksHyperparameter tuning, Regularization and Optimization\L2_week2_55.png) Summary how to systematically organize the hyper parameter search process and batch normalization and framework http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/æ·±åº¦å­¦ä¹ çš„å®ç”¨å±‚é¢ http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3 http://dl-notes.imshuai.com/#/c2w1?id=_4-heros-of-deep-learning-yoshua-bengio-interview https://www.youtube.com/watch?v=4Ct3Yujl1dk&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=14]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å½©é“…DailyLifeStyle]]></title>
    <url>%2F2019%2F04%2F16%2F%E5%BD%A9%E9%93%85DailyLifeStyle%2F</url>
    <content type="text"><![CDATA[Day one Â¶1. å·¥å…·ç®€å•ä»‹ç» å½©é“… æ°´æº¶æ€§ï¼Œæ²¹æ€§ å½©é“…çº¸ é“…ç¬” B 2B&lt;4Bé»‘çš„ç¨‹åº¦ H 4H&lt;8Hè½¯åº¦ æ©¡çš® è½¯æ©¡çš® ç¡¬æ©¡çš® ç”µåŠ¨æ©¡çš®æ“¦ é“…ç¬”åˆ€ å¯è·³æ¡£ç±»å‹ å‹¾çº¿ç¬” é’ˆç®¡ç¬” ï¼ˆæ¨±èŠ±ï¼‰ ç¬”å¥— é«˜å…‰ç¬” å¯ä»¥ç”¨ä¿®æ­£æ¶²æ›¿æ¢ï¼ˆä¸‰æ£±) çº¸æ“¦ç¬” ç›ä¸½ åˆ·å­ ç”»æ¿ é€Ÿå†™æ¿ Â¶2. é¢œè‰² ä¸‰åŸè‰²ï¼š çº¢ é»„ è“ è‰²ç›¸ é¢œè‰² é¥±å’Œåº¦ é²œè‰³ç¨‹åº¦ æ˜åº¦ æ˜æš—ç¨‹åº¦ é‚»è¿‘è‰² å¯¹æ¯”è‰² çº¢-ç»¿ è“-æ©™ ç´«-é»„ æš–è‰²å’Œå†·è‰² Â¶3. æ’çº¿ ä¸€ä¸ªæ–¹å‘ å¾€åŒä¸€ä¸ªæ–¹å‘æ’ï¼Œæ— è¿æ¥ æ¥å› ç›¸è¿æ¥ï¼Œä¸€æ¡çº¿ ä¸åŒæ–¹å‘æ’åˆ— æ³¨æ„ï¼šåŠ›åº¦å’Œé—´è· Â¶4. å¹³æ¶‚ åŠ›åº¦ä¸€è‡´ Â¶5. æ¸å˜ åŠ›åº¦ä¸ä¸€è‡´]]></content>
      <categories>
        <category>å¨±ä¹ç”Ÿæ´»</category>
      </categories>
      <tags>
        <tag>å½©é“…</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F04%2F13%2Fmachine%20learning%20test%2F</url>
    <content type="text"><![CDATA[Logistics Regression å¦‚ä½•å‡¸æ˜¾ä½ æ˜¯ä¸€ä¸ªå¯¹é€»è¾‘å›å½’å·²ç»éå¸¸äº†è§£çš„äººå‘¢ã€‚é‚£å°±æ˜¯ç”¨ä¸€å¥è¯æ¦‚æ‹¬å®ƒï¼é€»è¾‘å›å½’å‡è®¾æ•°æ®æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒ,é€šè¿‡æå¤§åŒ–ä¼¼ç„¶å‡½æ•°çš„æ–¹æ³•ï¼Œè¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ±‚è§£å‚æ•°ï¼Œæ¥è¾¾åˆ°å°†æ•°æ®äºŒåˆ†ç±»çš„ç›®çš„ã€‚ â€‹ è¿™é‡Œé¢å…¶å®åŒ…å«äº†5ä¸ªç‚¹ 1ï¼šé€»è¾‘å›å½’çš„å‡è®¾ï¼Œ2ï¼šé€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°ï¼Œ3ï¼šé€»è¾‘å›å½’çš„æ±‚è§£æ–¹æ³•ï¼Œ4ï¼šé€»è¾‘å›å½’çš„ç›®çš„ï¼Œ5:é€»è¾‘å›å½’å¦‚ä½•åˆ†ç±»ã€‚è¿™äº›é—®é¢˜æ˜¯è€ƒæ ¸ä½ å¯¹é€»è¾‘å›å½’çš„åŸºæœ¬äº†è§£ã€‚ Â¶é€»è¾‘å›å½’çš„åŸºæœ¬å‡è®¾ ä»»ä½•çš„æ¨¡å‹éƒ½æ˜¯æœ‰è‡ªå·±çš„å‡è®¾ï¼Œåœ¨è¿™ä¸ªå‡è®¾ä¸‹æ¨¡å‹æ‰æ˜¯é€‚ç”¨çš„ã€‚é€»è¾‘å›å½’çš„ ç¬¬ä¸€ä¸ª åŸºæœ¬å‡è®¾æ˜¯ å‡è®¾æ•°æ®æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒã€‚ ä¼¯åŠªåˆ©åˆ†å¸ƒæœ‰ä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯æŠ›ç¡¬å¸ï¼ŒæŠ›ä¸­ä¸ºæ­£é¢çš„æ¦‚ç‡æ˜¯p,æŠ›ä¸­ä¸ºè´Ÿé¢çš„æ¦‚ç‡æ˜¯ 1-p,åœ¨é€»è¾‘å›å½’è¿™ä¸ªæ¨¡å‹é‡Œé¢æ˜¯å‡è®¾ $h_Î¸(x)$ä¸ºæ ·æœ¬ä¸ºæ­£çš„æ¦‚ç‡ï¼Œ $1âˆ’h_Î¸(x)$ä¸ºæ ·æœ¬ä¸ºè´Ÿçš„æ¦‚ç‡ã€‚é‚£ä¹ˆæ•´ä¸ªæ¨¡å‹å¯ä»¥æè¿°ä¸º$h_Î¸(x;Î¸)=p$ é€»è¾‘å›å½’çš„ç¬¬äºŒä¸ªå‡è®¾æ˜¯å‡è®¾æ ·æœ¬ä¸ºæ­£çš„æ¦‚ç‡æ˜¯ $p=\frac{1}{1+e{wTx}}$ æ‰€ä»¥é€»è¾‘å›å½’çš„æœ€ç»ˆå½¢å¼ $h_Î¸(x;Î¸)=\frac{1}{1+e{wTx}}$ Â¶é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•° é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°æ˜¯å®ƒçš„æå¤§ä¼¼ç„¶å‡½æ•° $LÎ¸(x)=\pi_{i=1}{m}h_Î¸(xi;Î¸)y_iâˆ—(1âˆ’h_Î¸(xi;Î¸))^{1âˆ’y_i}$ Â¶é€»è¾‘å›å½’çš„æ±‚è§£æ–¹æ³• ç”±äºè¯¥æå¤§ä¼¼ç„¶å‡½æ•°æ— æ³•ç›´æ¥æ±‚è§£ï¼Œæˆ‘ä»¬ä¸€èˆ¬é€šè¿‡å¯¹è¯¥å‡½æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™æ¥ä¸æ–­é€¼æ€¥æœ€ä¼˜è§£ã€‚åœ¨è¿™ä¸ªåœ°æ–¹å…¶å®ä¼šæœ‰ä¸ªåŠ åˆ†çš„é¡¹ï¼Œè€ƒå¯Ÿä½ å¯¹å…¶ä»–ä¼˜åŒ–æ–¹æ³•çš„äº†è§£ã€‚å› ä¸ºå°±æ¢¯åº¦ä¸‹é™æœ¬èº«æ¥çœ‹çš„è¯å°±æœ‰éšæœºæ¢¯åº¦ä¸‹é™ï¼Œæ‰¹æ¢¯åº¦ä¸‹é™ï¼Œsmall batch æ¢¯åº¦ä¸‹é™ä¸‰ç§æ–¹å¼ï¼Œé¢è¯•å®˜å¯èƒ½ä¼šé—®è¿™ä¸‰ç§æ–¹å¼çš„ä¼˜åŠ£ä»¥åŠå¦‚ä½•é€‰æ‹©æœ€åˆé€‚çš„æ¢¯åº¦ä¸‹é™æ–¹å¼ã€‚ ç®€å•æ¥è¯´ æ‰¹æ¢¯åº¦ä¸‹é™ä¼šè·å¾—å…¨å±€æœ€ä¼˜è§£ï¼Œç¼ºç‚¹æ˜¯åœ¨æ›´æ–°æ¯ä¸ªå‚æ•°çš„æ—¶å€™éœ€è¦éå†æ‰€æœ‰çš„æ•°æ®ï¼Œè®¡ç®—é‡ä¼šå¾ˆå¤§ï¼Œå¹¶ä¸”ä¼šæœ‰å¾ˆå¤šçš„å†—ä½™è®¡ç®—ï¼Œå¯¼è‡´çš„ç»“æœæ˜¯å½“æ•°æ®é‡å¤§çš„æ—¶å€™ï¼Œæ¯ä¸ªå‚æ•°çš„æ›´æ–°éƒ½ä¼šå¾ˆæ…¢ã€‚ éšæœºæ¢¯åº¦ä¸‹é™æ˜¯ä»¥é«˜æ–¹å·®é¢‘ç¹æ›´æ–°ï¼Œä¼˜ç‚¹æ˜¯ä½¿å¾—sgdä¼šè·³åˆ°æ–°çš„å’Œæ½œåœ¨æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜è§£ï¼Œç¼ºç‚¹æ˜¯ä½¿å¾—æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£çš„è¿‡ç¨‹æ›´åŠ çš„å¤æ‚ã€‚ å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ç»“åˆäº†sgdå’Œbatch gdçš„ä¼˜ç‚¹ï¼Œæ¯æ¬¡æ›´æ–°çš„æ—¶å€™ä½¿ç”¨nä¸ªæ ·æœ¬ã€‚å‡å°‘äº†å‚æ•°æ›´æ–°çš„æ¬¡æ•°ï¼Œå¯ä»¥è¾¾åˆ°æ›´åŠ ç¨³å®šæ”¶æ•›ç»“æœï¼Œä¸€èˆ¬åœ¨æ·±åº¦å­¦ä¹ å½“ä¸­æˆ‘ä»¬é‡‡ç”¨è¿™ç§æ–¹æ³•ã€‚ å…¶å®è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªéšè—çš„æ›´åŠ æ·±çš„åŠ åˆ†é¡¹ï¼Œçœ‹ä½ äº†ä¸äº†è§£è¯¸å¦‚Adamï¼ŒåŠ¨é‡æ³•ç­‰ä¼˜åŒ–æ–¹æ³•ã€‚å› ä¸ºä¸Šè¿°æ–¹æ³•å…¶å®è¿˜æœ‰ä¸¤ä¸ªè‡´å‘½çš„é—®é¢˜ã€‚ ç¬¬ä¸€ä¸ªæ˜¯å¦‚ä½•å¯¹æ¨¡å‹é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ã€‚è‡ªå§‹è‡³ç»ˆä¿æŒåŒæ ·çš„å­¦ä¹ ç‡å…¶å®ä¸å¤ªåˆé€‚ã€‚å› ä¸ºä¸€å¼€å§‹å‚æ•°åˆšåˆšå¼€å§‹å­¦ä¹ çš„æ—¶å€™ï¼Œæ­¤æ—¶çš„å‚æ•°å’Œæœ€ä¼˜è§£éš”çš„æ¯”è¾ƒè¿œï¼Œéœ€è¦ä¿æŒä¸€ä¸ªè¾ƒå¤§çš„å­¦ä¹ ç‡å°½å¿«é€¼è¿‘æœ€ä¼˜è§£ã€‚ä½†æ˜¯å­¦ä¹ åˆ°åé¢çš„æ—¶å€™ï¼Œå‚æ•°å’Œæœ€ä¼˜è§£å·²ç»éš”çš„æ¯”è¾ƒè¿‘äº†ï¼Œä½ è¿˜ä¿æŒæœ€åˆçš„å­¦ä¹ ç‡ï¼Œå®¹æ˜“è¶Šè¿‡æœ€ä¼˜ç‚¹ï¼Œåœ¨æœ€ä¼˜ç‚¹é™„è¿‘æ¥å›æŒ¯è¡ï¼Œé€šä¿—ä¸€ç‚¹è¯´ï¼Œå°±å¾ˆå®¹æ˜“å­¦è¿‡å¤´äº†ï¼Œè·‘åäº†ã€‚ ç¬¬äºŒä¸ªæ˜¯å¦‚ä½•å¯¹å‚æ•°é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ã€‚åœ¨å®è·µä¸­ï¼Œå¯¹æ¯ä¸ªå‚æ•°éƒ½ä¿æŒçš„åŒæ ·çš„å­¦ä¹ ç‡ä¹Ÿæ˜¯å¾ˆä¸åˆç†çš„ã€‚æœ‰äº›å‚æ•°æ›´æ–°é¢‘ç¹ï¼Œé‚£ä¹ˆå­¦ä¹ ç‡å¯ä»¥é€‚å½“å°ä¸€ç‚¹ã€‚æœ‰äº›å‚æ•°æ›´æ–°ç¼“æ…¢ï¼Œé‚£ä¹ˆå­¦ä¹ ç‡å°±åº”è¯¥å¤§ä¸€ç‚¹ã€‚è¿™é‡Œæˆ‘ä»¬ä¸å±•å¼€ï¼Œæœ‰ç©ºæˆ‘ä¼šä¸“é—¨å‡ºä¸€ä¸ªä¸“é¢˜ä»‹ç»ã€‚ Â¶é€»è¾‘å›å½’çš„ç›®çš„ è¯¥å‡½æ•°çš„ç›®çš„ä¾¿æ˜¯å°†æ•°æ®äºŒåˆ†ç±»ï¼Œæé«˜å‡†ç¡®ç‡ã€‚ é€»è¾‘å›å½’å¦‚ä½•åˆ†ç±» é€»è¾‘å›å½’ä½œä¸ºä¸€ä¸ªå›å½’(ä¹Ÿå°±æ˜¯yå€¼æ˜¯è¿ç»­çš„)ï¼Œå¦‚ä½•åº”ç”¨åˆ°åˆ†ç±»ä¸Šå»å‘¢ã€‚yå€¼ç¡®å®æ˜¯ä¸€ä¸ªè¿ç»­çš„å˜é‡ã€‚é€»è¾‘å›å½’çš„åšæ³•æ˜¯åˆ’å®šä¸€ä¸ªé˜ˆå€¼ï¼Œyå€¼å¤§äºè¿™ä¸ªé˜ˆå€¼çš„æ˜¯ä¸€ç±»ï¼Œyå€¼å°äºè¿™ä¸ªé˜ˆå€¼çš„æ˜¯å¦å¤–ä¸€ç±»ã€‚é˜ˆå€¼å…·ä½“å¦‚ä½•è°ƒæ•´æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©ã€‚ä¸€èˆ¬ä¼šé€‰æ‹©0.5åšä¸ºé˜ˆå€¼æ¥åˆ’åˆ†ã€‚ Â¶3.å¯¹é€»è¾‘å›å½’çš„è¿›ä¸€æ­¥æé—® â€‹ é€»è¾‘å›å½’è™½ç„¶ä»å½¢å¼ä¸Šéå¸¸çš„ç®€å•ï¼Œä½†æ˜¯å…¶å†…æ¶µæ˜¯éå¸¸çš„ä¸°å¯Œã€‚æœ‰å¾ˆå¤šé—®é¢˜æ˜¯å¯ä»¥è¿›è¡Œæ€è€ƒçš„ é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°ä¸ºä»€ä¹ˆè¦ä½¿ç”¨æå¤§ä¼¼ç„¶å‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°ï¼Ÿ æŸå¤±å‡½æ•°ä¸€èˆ¬æœ‰å››ç§ï¼Œå¹³æ–¹æŸå¤±å‡½æ•°ï¼Œå¯¹æ•°æŸå¤±å‡½æ•°ï¼ŒHingeLoss0-1æŸå¤±å‡½æ•°ï¼Œç»å¯¹å€¼æŸå¤±å‡½æ•°ã€‚å°†æå¤§ä¼¼ç„¶å‡½æ•°å–å¯¹æ•°ä»¥åç­‰åŒäºå¯¹æ•°æŸå¤±å‡½æ•°ã€‚åœ¨é€»è¾‘å›å½’è¿™ä¸ªæ¨¡å‹ä¸‹ï¼Œå¯¹æ•°æŸå¤±å‡½æ•°çš„è®­ç»ƒæ±‚è§£å‚æ•°çš„é€Ÿåº¦æ˜¯æ¯”è¾ƒå¿«çš„ã€‚è‡³äºåŸå› å¤§å®¶å¯ä»¥æ±‚å‡ºè¿™ä¸ªå¼å­çš„æ¢¯åº¦æ›´æ–° $w_j=w_jâˆ’(yiâˆ’h_w(xi;w))âˆ—x_j^i\theta$ è¿™ä¸ªå¼å­çš„æ›´æ–°é€Ÿåº¦åªå’Œ$x_j,y_j ç›¸å…³ã€‚å’Œsigmodå‡½æ•°æœ¬èº«çš„æ¢¯åº¦æ˜¯æ— å…³çš„ã€‚è¿™æ ·æ›´æ–°çš„é€Ÿåº¦æ˜¯å¯ä»¥è‡ªå§‹è‡³ç»ˆéƒ½æ¯”è¾ƒçš„ç¨³å®šã€‚ ä¸ºä»€ä¹ˆä¸é€‰å¹³æ–¹æŸå¤±å‡½æ•°çš„å‘¢ï¼Ÿå…¶ä¸€æ˜¯å› ä¸ºå¦‚æœä½ ä½¿ç”¨å¹³æ–¹æŸå¤±å‡½æ•°ï¼Œä½ ä¼šå‘ç°æ¢¯åº¦æ›´æ–°çš„é€Ÿåº¦å’Œsigmodå‡½æ•°æœ¬èº«çš„æ¢¯åº¦æ˜¯å¾ˆç›¸å…³çš„ã€‚sigmodå‡½æ•°åœ¨å®ƒåœ¨å®šä¹‰åŸŸå†…çš„æ¢¯åº¦éƒ½ä¸å¤§äº0.25ã€‚è¿™æ ·è®­ç»ƒä¼šéå¸¸çš„æ…¢ã€‚ é€»è¾‘å›å½’åœ¨è®­ç»ƒçš„è¿‡ç¨‹å½“ä¸­ï¼Œå¦‚æœæœ‰å¾ˆå¤šçš„ç‰¹å¾é«˜åº¦ç›¸å…³æˆ–è€…è¯´æœ‰ä¸€ä¸ªç‰¹å¾é‡å¤äº†100éï¼Œä¼šé€ æˆæ€æ ·çš„å½±å“ï¼Ÿ å…ˆè¯´ç»“è®ºï¼Œå¦‚æœåœ¨æŸå¤±å‡½æ•°æœ€ç»ˆæ”¶æ•›çš„æƒ…å†µä¸‹ï¼Œå…¶å®å°±ç®—æœ‰å¾ˆå¤šç‰¹å¾é«˜åº¦ç›¸å…³ä¹Ÿä¸ä¼šå½±å“åˆ†ç±»å™¨çš„æ•ˆæœã€‚ ä½†æ˜¯å¯¹ç‰¹å¾æœ¬èº«æ¥è¯´çš„è¯ï¼Œå‡è®¾åªæœ‰ä¸€ä¸ªç‰¹å¾ï¼Œåœ¨ä¸è€ƒè™‘é‡‡æ ·çš„æƒ…å†µä¸‹ï¼Œä½ ç°åœ¨å°†å®ƒé‡å¤100éã€‚è®­ç»ƒä»¥åå®Œä»¥åï¼Œæ•°æ®è¿˜æ˜¯è¿™ä¹ˆå¤šï¼Œä½†æ˜¯è¿™ä¸ªç‰¹å¾æœ¬èº«é‡å¤äº†100éï¼Œå®è´¨ä¸Šå°†åŸæ¥çš„ç‰¹å¾åˆ†æˆäº†100ä»½ï¼Œæ¯ä¸€ä¸ªç‰¹å¾éƒ½æ˜¯åŸæ¥ç‰¹å¾æƒé‡å€¼çš„ç™¾åˆ†ä¹‹ä¸€ã€‚ å¦‚æœåœ¨éšæœºé‡‡æ ·çš„æƒ…å†µä¸‹ï¼Œå…¶å®è®­ç»ƒæ”¶æ•›å®Œä»¥åï¼Œè¿˜æ˜¯å¯ä»¥è®¤ä¸ºè¿™100ä¸ªç‰¹å¾å’ŒåŸæ¥é‚£ä¸€ä¸ªç‰¹å¾æ‰®æ¼”çš„æ•ˆæœä¸€æ ·ï¼Œåªæ˜¯å¯èƒ½ä¸­é—´å¾ˆå¤šç‰¹å¾çš„å€¼æ­£è´Ÿç›¸æ¶ˆäº†ã€‚ ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜æ˜¯ä¼šåœ¨è®­ç»ƒçš„è¿‡ç¨‹å½“ä¸­å°†é«˜åº¦ç›¸å…³çš„ç‰¹å¾å»æ‰ï¼Ÿ å»æ‰é«˜åº¦ç›¸å…³çš„ç‰¹å¾ä¼šè®©æ¨¡å‹çš„å¯è§£é‡Šæ€§æ›´å¥½ å¯ä»¥å¤§å¤§æé«˜è®­ç»ƒçš„é€Ÿåº¦ã€‚å¦‚æœæ¨¡å‹å½“ä¸­æœ‰å¾ˆå¤šç‰¹å¾é«˜åº¦ç›¸å…³çš„è¯ï¼Œå°±ç®—æŸå¤±å‡½æ•°æœ¬èº«æ”¶æ•›äº†ï¼Œä½†å®é™…ä¸Šå‚æ•°æ˜¯æ²¡æœ‰æ”¶æ•›çš„ï¼Œè¿™æ ·ä¼šæ‹‰ä½è®­ç»ƒçš„é€Ÿåº¦ã€‚å…¶æ¬¡æ˜¯ç‰¹å¾å¤šäº†ï¼Œæœ¬èº«å°±ä¼šå¢å¤§è®­ç»ƒçš„æ—¶é—´ã€‚ Â¶4.é€»è¾‘å›å½’çš„ä¼˜ç¼ºç‚¹æ€»ç»“ â€‹ é¢è¯•çš„æ—¶å€™ï¼Œåˆ«äººä¹Ÿç»å¸¸ä¼šé—®åˆ°ï¼Œä½ åœ¨ä½¿ç”¨é€»è¾‘å›å½’çš„æ—¶å€™æœ‰å“ªäº›æ„Ÿå—ã€‚è§‰å¾—å®ƒæœ‰å“ªäº›ä¼˜ç¼ºç‚¹ã€‚ â€‹ åœ¨è¿™é‡Œæˆ‘ä»¬æ€»ç»“äº†é€»è¾‘å›å½’åº”ç”¨åˆ°å·¥ä¸šç•Œå½“ä¸­ä¸€äº›ä¼˜ç‚¹ï¼š å½¢å¼ç®€å•ï¼Œæ¨¡å‹çš„å¯è§£é‡Šæ€§éå¸¸å¥½ã€‚ä»ç‰¹å¾çš„æƒé‡å¯ä»¥çœ‹åˆ°ä¸åŒçš„ç‰¹å¾å¯¹æœ€åç»“æœçš„å½±å“ï¼ŒæŸä¸ªç‰¹å¾çš„æƒé‡å€¼æ¯”è¾ƒé«˜ï¼Œé‚£ä¹ˆè¿™ä¸ªç‰¹å¾æœ€åå¯¹ç»“æœçš„å½±å“ä¼šæ¯”è¾ƒå¤§ã€‚ æ¨¡å‹æ•ˆæœä¸é”™ã€‚åœ¨å·¥ç¨‹ä¸Šæ˜¯å¯ä»¥æ¥å—çš„ï¼ˆä½œä¸ºbaseline)ï¼Œå¦‚æœç‰¹å¾å·¥ç¨‹åšçš„å¥½ï¼Œæ•ˆæœä¸ä¼šå¤ªå·®ï¼Œå¹¶ä¸”ç‰¹å¾å·¥ç¨‹å¯ä»¥å¤§å®¶å¹¶è¡Œå¼€å‘ï¼Œå¤§å¤§åŠ å¿«å¼€å‘çš„é€Ÿåº¦ã€‚ è®­ç»ƒé€Ÿåº¦è¾ƒå¿«ã€‚åˆ†ç±»çš„æ—¶å€™ï¼Œè®¡ç®—é‡ä»…ä»…åªå’Œç‰¹å¾çš„æ•°ç›®ç›¸å…³ã€‚å¹¶ä¸”é€»è¾‘å›å½’çš„åˆ†å¸ƒå¼ä¼˜åŒ–sgdå‘å±•æ¯”è¾ƒæˆç†Ÿï¼Œè®­ç»ƒçš„é€Ÿåº¦å¯ä»¥é€šè¿‡å †æœºå™¨è¿›ä¸€æ­¥æé«˜ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥åœ¨çŸ­æ—¶é—´å†…è¿­ä»£å¥½å‡ ä¸ªç‰ˆæœ¬çš„æ¨¡å‹ã€‚ èµ„æºå ç”¨å°,å°¤å…¶æ˜¯å†…å­˜ã€‚å› ä¸ºåªéœ€è¦å­˜å‚¨å„ä¸ªç»´åº¦çš„ç‰¹å¾å€¼ï¼Œã€‚ æ–¹ä¾¿è¾“å‡ºç»“æœè°ƒæ•´ã€‚é€»è¾‘å›å½’å¯ä»¥å¾ˆæ–¹ä¾¿çš„å¾—åˆ°æœ€åçš„åˆ†ç±»ç»“æœï¼Œå› ä¸ºè¾“å‡ºçš„æ˜¯æ¯ä¸ªæ ·æœ¬çš„æ¦‚ç‡åˆ†æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“çš„å¯¹è¿™äº›æ¦‚ç‡åˆ†æ•°è¿›è¡Œcutoffï¼Œä¹Ÿå°±æ˜¯åˆ’åˆ†é˜ˆå€¼(å¤§äºæŸä¸ªé˜ˆå€¼çš„æ˜¯ä¸€ç±»ï¼Œå°äºæŸä¸ªé˜ˆå€¼çš„æ˜¯ä¸€ç±»)ã€‚ â€‹ ä½†æ˜¯é€»è¾‘å›å½’æœ¬èº«ä¹Ÿæœ‰è®¸å¤šçš„ç¼ºç‚¹: å‡†ç¡®ç‡å¹¶ä¸æ˜¯å¾ˆé«˜ã€‚å› ä¸ºå½¢å¼éå¸¸çš„ç®€å•(éå¸¸ç±»ä¼¼çº¿æ€§æ¨¡å‹)ï¼Œå¾ˆéš¾å»æ‹Ÿåˆæ•°æ®çš„çœŸå®åˆ†å¸ƒã€‚ å¾ˆéš¾å¤„ç†æ•°æ®ä¸å¹³è¡¡çš„é—®é¢˜ã€‚ä¸¾ä¸ªä¾‹å­ï¼šå¦‚æœæˆ‘ä»¬å¯¹äºä¸€ä¸ªæ­£è´Ÿæ ·æœ¬éå¸¸ä¸å¹³è¡¡çš„é—®é¢˜æ¯”å¦‚æ­£è´Ÿæ ·æœ¬æ¯” 10000:1.æˆ‘ä»¬æŠŠæ‰€æœ‰æ ·æœ¬éƒ½é¢„æµ‹ä¸ºæ­£ä¹Ÿèƒ½ä½¿æŸå¤±å‡½æ•°çš„å€¼æ¯”è¾ƒå°ã€‚ä½†æ˜¯ä½œä¸ºä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå®ƒå¯¹æ­£è´Ÿæ ·æœ¬çš„åŒºåˆ†èƒ½åŠ›ä¸ä¼šå¾ˆå¥½ã€‚ å¤„ç†éçº¿æ€§æ•°æ®è¾ƒéº»çƒ¦ã€‚é€»è¾‘å›å½’åœ¨ä¸å¼•å…¥å…¶ä»–æ–¹æ³•çš„æƒ…å†µä¸‹ï¼Œåªèƒ½å¤„ç†çº¿æ€§å¯åˆ†çš„æ•°æ®ï¼Œæˆ–è€…è¿›ä¸€æ­¥è¯´ï¼Œå¤„ç†äºŒåˆ†ç±»çš„é—®é¢˜ ã€‚ é€»è¾‘å›å½’æœ¬èº«æ— æ³•ç­›é€‰ç‰¹å¾ã€‚æœ‰æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šç”¨gbdtæ¥ç­›é€‰ç‰¹å¾ï¼Œç„¶åå†ä¸Šé€»è¾‘å›å½’ã€‚ Â¶æ¨¡å‹ã€ç­–ç•¥ã€ç®—æ³• Â¶Codings]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F04%2F13%2Fmachine%20learning%2F</url>
    <content type="text"><![CDATA[è‹±ä¼Ÿè¾¾:èŠ¯ç‰‡ï¼ŒGPU å¼€å‘æ¡†æ¶ï¼štensorflowï¼Œpytorch caffe Â¶ç›‘ç£å­¦ä¹  å­¦ä¹ ç›®çš„æ˜¯å­¦ä¹ ä¸€ä¸ªè¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ï¼Œç§°ä¸ºæ¨¡å‹ã€‚æ¨¡å‹çš„é›†åˆå°±æ˜¯å‡è®¾ç©ºé—´ã€‚ æ¨¡å‹ï¼šæ¦‚ç‡æ¨¡å‹ï¼›éæ¦‚ç‡æ¨¡å‹ï¼› å­¦ä¹ è¿‡ç¨‹ï¼šæœç´¢è¿‡ç¨‹]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow]]></title>
    <url>%2F2019%2F04%2F11%2Ftensorflow%2F</url>
    <content type="text"><![CDATA[official definition What is tensorflow flow of tensors â€œTensorFlow is an open source software library for numerical computation using dataflow graphs. Nodes in the graph represents mathematical operations, while graph edges represent multi-dimensional data arrays (aka tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.â€* A major difference between numpy and TensorFlow is that TensorFlow follows a lazy programming paradigm. It first builds a graph of all the operation to be done, and then when a â€œsessionâ€ is called, it â€œrunsâ€ the graph. Itâ€™s built to be scalable, by changing internal data representation to tensors (aka multi-dimensional arrays). Building a computational graph can be considered as the main ingredient of TensorFlow. Itâ€™s easy to classify TensorFlow as a neural network library, but itâ€™s not just that. Yes, it was designed to be a powerful neural network library. But it has the power to do much more than that. You can build other machine learning algorithms on it such as decision trees or k-Nearest Neighbors. You can literally do everything you normally would do in numpy! Itâ€™s aptly called â€œnumpy on steroidsâ€ The advantages of using TensorFlow are: It has an intuitive construct, because as the name suggests it has â€œflow of tensorsâ€. You can easily visualize each and every part of the graph. Easily train on cpu/gpu for distributed computing Platform flexibility. You can run the models wherever you want, whether it is on mobile, server or PC. scikit-learn 123456# define hyperparamters of ML algorithmclf = svm.SVC(gamma=0.001, C=100.)# train clf.fit(X, y)# test clf.predict(X_test) The usual workflow of running a program in TensorFlow is as follows: Build a computational graph, this can be any mathematical operation TensorFlow supports. Initialize variables, to compile the variables defined previously Create session(ä¼šè¯ï¼‰, this is where the magic starts! Run graph in session, the compiled graph is passed to the session, which starts its execution. Close session, shutdown the session. Lets write a small program to add two numbers! 12345678910111213141516171819# import tensorflowimport tensorflow as tf# build computational grapha = tf.placeholder(tf.int16)b = tf.placeholder(tf.int16)addition = tf.add(a, b)# initialize variablesinit = tf.initialize_all_variables()# create session and run the graphwith tf.Session() as sess: sess.run(init) print "Addition: %i" % sess.run(addition, feed_dict=&#123;a: 2, b: 3&#125;)# close sessionsess.close() A typical implementation of Neural Network would be as follows: Define Neural Network architecture to be compiled Transfer data to your model Under the hood, the data is first divided into batches, so that it can be ingested. The batches are first preprocessed, augmented and then fed into Neural Network for training The model then gets trained incrementally Display the accuracy for a specific number of timesteps After training save the model for future use Test the model on a new data and check how it performs Â¶ä¸‰ç±»éå¸¸é‡è¦çš„å˜é‡ Â¶å ä½ç¬¦ tensorFlowä¸­æ¥æ”¶å€¼çš„æ–¹å¼ä¸ºå ä½ç¬¦(placeholder)ï¼Œåˆ›å»ºplaceholder 123- # b = tf.placeholder(tf.float32, [None, 1], name='b')ç¬¬äºŒä¸ªå‚æ•°å€¼ä¸º[None, 1]ï¼Œå…¶ä¸­Noneè¡¨ç¤ºä¸ç¡®å®šï¼Œå³ä¸ç¡®å®šç¬¬ä¸€ä¸ªç»´åº¦çš„å¤§å°ï¼Œç¬¬ä¸€ç»´å¯ä»¥æ˜¯ä»»æ„å¤§å°ã€‚ç‰¹åˆ«å¯¹åº”tensoræ•°é‡(æˆ–è€…æ ·æœ¬æ•°é‡)ï¼Œè¾“å…¥çš„tensoræ•°ç›®å¯ä»¥æ˜¯32ã€64â€¦ placeholder: A way to feed data into the graphs feed_dict: A dictionary to pass numeric values to computational graph Â¶å¸¸é‡ tf.constant()`å®šä¹‰å¸¸é‡ 1const = tf.constant(2.0, name='const') Â¶å˜é‡ â€‹ ä½¿ç”¨tf.Variable()å®šä¹‰å˜é‡ 1c = tf.Variable(1.0, dtype=tf.float32, name='c') TensorFlowä¸­æ‰€æœ‰çš„å˜é‡å¿…é¡»ç»è¿‡åˆå§‹åŒ–æ‰èƒ½ä½¿ç”¨ï¼Œ**åˆå§‹åŒ–æ–¹å¼åˆ†ä¸¤æ­¥ï¼š å®šä¹‰åˆå§‹åŒ–operation 12# 1. å®šä¹‰init operationinit_op = tf.global_variables_initializer() è¿è¡Œåˆå§‹åŒ–operation 12# 2. è¿è¡Œinit operation sess.run(init_op) reference https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/ https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial https://github.com/aymericdamien/TensorFlow-Examples video:https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/ course: https://classroom.udacity.com/courses/ud187 tensorflow: GOOGLE å¼€æºã€Deep learning Â¶ç»ƒæ•°æˆé‡‘ Â¶C1 tensorboard ï¼ša tool;visual network;debug alter dir of jupyter é¡ºä¾¿æ”¹äº†ä¸‹æ–°ä¸‹è½½çš„è·¯å¾„ï¼ˆGOODï¼‰ CPU or GPU Â¶C2 graphs ä»£è¡¨è®¡ç®—ä»»åŠ¡ï¼ŒèŠ‚ç‚¹ï¼ˆop)ï¼Œä¸€ä¸ªopå¯ä»¥è·å¾—oä¸ªæˆ–è€…å¤šä¸ªtensor,è¾“å‡º1ä¸ªæˆ–è€…å¤šä¸ªtensor Session(ä¼šè¯)çš„ä¸Šä¸‹æ–‡ï¼ˆcontext)ä¸­æ‰§è¡Œ tensorè¡¨ç¤ºæ•°æ®,nç»´æ•°ç»„ Â¶C3 ç®€å•çš„å›å½’ç¥ç»ç½‘ç»œï¼ˆæ‹ŸåˆäºŒæ¬¡å‡½æ•°ï¼‰ï¼Œè²Œä¼¼å­¦äº†ç†è®ºæ²¡æœ‰å®è·µï¼Œè¿˜çœŸæ˜¯å¿˜å¾—å¿«å•Š æ‰‹å†™ä½“åˆ†ç±»ã€Softmaxå‡½æ•° softmaxå‡½æ•°å¯ä»¥ç»™ä¸åŒçš„å¯¹è±¡åˆ†é…æ¦‚ç‡ï¼Œsoftmax($x_i$)=$\frac{exp(x_i)}{\sum_j{exp(x_j)}}$ å¦‚è¾“å‡º[1,2,5] ,$p1=\frac{exp(1)}{exp(1)+exp(2)+exp(5)}$,$p2=\frac{exp(2)}{exp(1)+exp(2)+exp(5)}$,$p1=\frac{exp(5)}{exp(1)+exp(2)+exp(5)}$ Â¶Keras å®‰è£… backend åŸºäºä»€ä¹ˆåšè¿ç®—ï¼ˆtensorflow or theano) import keras æŸ¥çœ‹ åº•å±‚æ­å»º aï¼‰ /.keras/keras.json ç›¸å…³çš„é…ç½®ä¿¡æ¯ b) ç»ˆç«¯æ”¹ï¼Œå•æ¬¡ import os os.environ[â€˜KERAS_BACKENDâ€™]= â€˜tensorflowâ€™ import keras For example model :Sequential layer : Dense activation è®­ç»ƒç®—æ³•ï¼šmodel.compile(å‚æ•°optimizer=â€˜æ¢¯åº¦ä¸‹é™æ³•çš„å˜ç§â€™ , loss=â€˜rms/â€™) è®­ç»ƒï¼šmodel. fit (x,y) model.train_on_batch evaluate:model.evaluate prediction: model.predict(x_test, batch_size=128) https://github.com/MorvanZhou/tutorials/blob/master/kerasTUT/5-classifier_example.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 4 - Regressor exampleimport numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.models import Sequential # æŒ‰é¡ºåºå»ºç«‹from keras.layers import Dense # å…¨è¿æ¥å±‚import matplotlib.pyplot as plt# create some dataX = np.linspace(-1, 1, 200)np.random.shuffle(X) # randomize the dataY = 0.5 * X + 2 + np.random.normal(0, 0.05, (200, ))# plot dataplt.scatter(X, Y)plt.show()X_train, Y_train = X[:160], Y[:160] # first 160 data pointsX_test, Y_test = X[160:], Y[160:] # last 40 data points# build a neural network from the 1st layer to the last layermodel = Sequential()model.add(Dense(units=1, input_dim=1)) # choose loss function and optimizing methodmodel.compile(loss='mse', optimizer='sgd')# trainingprint('Training -----------')for step in range(301): cost = model.train_on_batch(X_train, Y_train) if step % 100 == 0: print('train cost: ', cost)# testprint('\nTesting ------------')cost = model.evaluate(X_test, Y_test, batch_size=40)print('test cost:', cost)W, b = model.layers[0].get_weights()print('Weights=', W, '\nbiases=', b)# plotting the predictionY_pred = model.predict(X_test)plt.scatter(X_test, Y_test)plt.plot(X_test, Y_pred)plt.show() Â¶5 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556"""To know more or get code samples, please visit my website:https://morvanzhou.github.io/tutorials/Or search: è«çƒ¦PythonThank you for supporting!"""# please note, all tutorial code are running under python3.5.# If you use the version like python2.7, please modify the code accordingly# 5 - Classifier exampleimport numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Activationfrom keras.optimizers import RMSprop# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called# X shape (60,000 28x28), y shape (10,000, )(X_train, y_train), (X_test, y_test) = mnist.load_data()# data pre-processingX_train = X_train.reshape(X_train.shape[0], -1) / 255. # normalizeX_test = X_test.reshape(X_test.shape[0], -1) / 255. # normalizey_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10)# Another way to build your neural netmodel = Sequential([ Dense(32, input_dim=784), Activation('relu'), Dense(10), Activation('softmax'),])# Another way to define your optimizerrmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)# We add metrics to get more results you want to seemodel.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy'])print('Training ------------')# Another way to train the modelmodel.fit(X_train, y_train, epochs=2, batch_size=32)print('\nTesting ------------')# Evaluate the model with the metrics we defined earlierloss, accuracy = model.evaluate(X_test, y_test)print('test loss: ', loss)print('test accuracy: ', accuracy) Â¶6 CNNå·ç§¯ç¥ç»ç½‘ç»œ ä¸æ˜¯å¯¹ https://www.cnblogs.com/skyfsm/p/6790245.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990"""To know more or get code samples, please visit my website:https://morvanzhou.github.io/tutorials/Or search: è«çƒ¦PythonThank you for supporting!"""# please note, all tutorial code are running under python3.5.# If you use the version like python2.7, please modify the code accordingly# 6 - CNN example# to try tensorflow, un-comment following two lines# import os# os.environ['KERAS_BACKEND']='tensorflow'import numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flattenfrom keras.optimizers import Adam# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called# training X shape (60000, 28x28), Y shape (60000, ). test X shape (10000, 28x28), Y shape (10000, )(X_train, y_train), (X_test, y_test) = mnist.load_data()# data pre-processingX_train = X_train.reshape(-1, 1,28, 28)/255.X_test = X_test.reshape(-1, 1,28, 28)/255.y_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10)# Another way to build your CNNmodel = Sequential()# Conv layer 1 output shape (32, 28, 28)model.add(Convolution2D( batch_input_shape=(None, 1, 28, 28), filters=32, kernel_size=5, strides=1, padding='same', # Padding method data_format='channels_first',))model.add(Activation('relu'))# Pooling layer 1 (max pooling) output shape (32, 14, 14)model.add(MaxPooling2D( pool_size=2, strides=2, padding='same', # Padding method data_format='channels_first',))# Conv layer 2 output shape (64, 14, 14)model.add(Convolution2D(64, 5, strides=1, padding='same', data_format='channels_first'))model.add(Activation('relu'))# Pooling layer 2 (max pooling) output shape (64, 7, 7)model.add(MaxPooling2D(2, 2, 'same', data_format='channels_first'))# Fully connected layer 1 input shape (64 * 7 * 7) = (3136), output shape (1024)model.add(Flatten())model.add(Dense(1024))model.add(Activation('relu'))# Fully connected layer 2 to shape (10) for 10 classesmodel.add(Dense(10))model.add(Activation('softmax'))# Another way to define your optimizeradam = Adam(lr=1e-4)# We add metrics to get more results you want to seemodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])print('Training ------------')# Another way to train the modelmodel.fit(X_train, y_train, epochs=1, batch_size=64,)print('\nTesting ------------')# Evaluate the model with the metrics we defined earlierloss, accuracy = model.evaluate(X_test, y_test)print('\ntest loss: ', loss)print('\ntest accuracy: ', accuracy)]]></content>
      <categories>
        <category>ç¼–ç¨‹è¯­è¨€</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>tensorlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Neural Network and Deep Learning]]></title>
    <url>%2F2019%2F04%2F11%2FDeep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning%2F</url>
    <content type="text"><![CDATA[Course one : Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization) C1W1 Â¶C1W1L01: Welcome AI is the new Electricity! Course 1: Neural Networks and Deep Learning Course 2: Improving Deep Neural Networks: Hyperparameter tuning,Regularization and Optimization Course 3: Structuring your Machine Learning project Course 4: Convolutional Neural Networks Course 5: Natural Langurge Processing: Building sequence models Â¶C1W1L02 : What is Neural Network Deep Learning = training (very large) neural network Â¶For example of house prize prediction : the simplest neural network å¦‚æœç°åœ¨æœ‰å…­æ ‹æˆ¿å­çš„ä¿¡æ¯ï¼Œåˆ†åˆ«æ˜¯æˆ¿å­çš„å¤§å°(size of house)å’Œå¯¹åº”çš„ä»·æ ¼(prize),ç»˜åˆ¶å‡ºå¦‚ä¸‹çš„ã€‚è‡ªç„¶çš„æƒ³æ³•ï¼šçº¿æ€§å›å½’ï¼Œå¾—åˆ°æ‹Ÿåˆçš„ç›´çº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ¿ä»·ä¸å¯èƒ½æ˜¯è´Ÿæ•°å§ï¼å› æ­¤ä¸‹å›¾ä¸­è“è‰²çš„çº¿ï¼Œå¤§è‡´å°±æ˜¯æˆ‘ä»¬æ‰€éœ€è¦çš„å‡½æ•°ã€‚è¿™ä¸ªå¯¹åº”ä¸€ä¸ªæœ€ç®€å•ç¥ç»ç½‘ç»œï¼ˆneural networkï¼‰ ![](Deep Learning.ai_Neural Networks and Deep Learning\housr_prize_1.png) ä¸Šè¿°æ˜¯ä¸€ä¸ªtiny little neural networkï¼Œæ›´å¤§çš„ï¼Œæ›´å¤æ‚çš„ç¥ç»ç½‘ç»œæ˜¯ æŠŠå¾ˆå¤šæœ€ç®€å•çš„single neuralå †ç§¯(stacking)åˆ°ä¸€èµ·ã€‚ Â¶For example of house prize prediction : stacking the neural ä¸Šé¢è¿™ä¸ªä¾‹å­ï¼Œä»…ä»…è€ƒè™‘ç‰¹å¾æ˜¯size,å®é™…æƒ…å†µä¸Šï¼Œä¸æˆ¿å±‹ç›¸å…³çš„ç‰¹å¾è¿˜æœ‰number of bedroomsã€zip codeã€wealth, number of bedrooms and size affect family size. The zip code is a feature that tells you you know walkability. The wealth tells you how good is the school quality ![](Deep Learning.ai_Neural Networks and Deep Learning\house_prize_2.png) hidden layer ç”¨è¾“å…¥å±‚è®¡ç®—å¾—åˆ°ï¼Œå› æ­¤è¯´è¾“å…¥å±‚ä¸ä¸­é—´å±‚ç´§å¯†è¿æ¥èµ·æ¥äº† Â¶The actual application of neural networks hidden layer ä¸ä¸Šä¸€å±‚çš„è¿æ¥æƒ…å†µå¹¶ä¸æ˜¯æ‰‹å·¥ç¡®å®šï¼Œæ¯ä¸€å±‚éƒ½æ˜¯ä¸Šä¸€å±‚æ‰€æœ‰çš„è¾“å…¥å‡½æ•°ï¼Œæ‰€ä»¥å»ºç«‹çš„ç¥ç»ç½‘ç»œå¦‚ä¸‹ï¼š ![](Deep Learning.ai_Neural Networks and Deep Learning\house prize 3.png) The remarkable thing about neural network Given enough data about X&amp;Y (x,y) which good at freaking out functions :map x to y Most powerful in supervised learning Â¶C2W1CL03 : Supervised Learning with Neural Network Â¶å¸¸è§çš„ç›‘ç£å­¦ä¹  æˆªæ­¢åˆ°ç›®å‰ï¼ŒNeural Networkçš„æˆåŠŸåº”ç”¨åŸºæœ¬éƒ½åœ¨Supervised Learningã€‚æ¯”å¦‚ï¼šAdï¼ŒImages vision, Audio to Text, Machine translation, Autonomous Driving ![](Deep Learning.ai_Neural Networks and Deep Learning\supervised-learning-exmples.png) Â¶å¸¸è§çš„ç¥ç»ç½‘ç»œçš„è®¾è®¡ ![](Deep Learning.ai_Neural Networks and Deep Learning\NeuralNetworkExamples.png) å·ç§¯ç¥ç»ç½‘ç»œï¼šConvolutional Neural Network (CNN) é€šå¸¸æœ‰ç”¨å›¾åƒæ•°æ® é€’å½’ç¥ç»ç½‘ç»œï¼š Recurrent Neural Network (RNN) é€šå¸¸ç”¨äºtime series å¯¹åº”å¤æ‚çš„åº”ç”¨ä¸­ï¼Œå®šåˆ¶ä¸€äº›å¤æ‚çš„æ··åˆçš„ç¥ç»ç½‘ç»œç»“æ„ Â¶ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ® ![](Deep Learning.ai_Neural Networks and Deep Learning\datastructure.png) å¤„ç†éç»“æ„åŒ–æ•°æ®æ˜¯å¾ˆéš¾çš„ï¼Œä¸ç»“æ„åŒ–æ•°æ®æ¯”è¾ƒï¼Œè®©è®¡ç®—æœºç†è§£éç»“æ„åŒ–æ•°æ®å¾ˆéš¾ Â¶C1W1L04: Why is deep learning taking off Answer: scale If you want to hit this very high level of performance ,firstly, you need to be able train a big enough neural network in order to take advantage of the huge amount of data and second you need to be out here on the x axes you do need a lot of data. ![](Deep Learning.ai_Neural Networks and Deep Learning\scale.jpg) If you do not have a lot training data is often up to your skill at hand engineering features that determines the foreman.åœ¨è¿™ä¸ªå°çš„è®­ç»ƒé›†ä¸­ï¼Œå„ç§ç®—æ³•çš„ä¼˜å…ˆçº§äº‹å®ä¸Šå®šä¹‰çš„ä¹Ÿä¸æ˜¯å¾ˆæ˜ç¡®ï¼Œæ‰€ä»¥å¦‚æœä½ æ²¡æœ‰å¤§é‡çš„è®­ç»ƒé›†ï¼Œé‚£æ•ˆæœä¼šå–å†³äºä½ çš„ç‰¹å¾å·¥ç¨‹èƒ½åŠ›ï¼Œé‚£å°†å†³å®šæœ€ç»ˆçš„æ€§èƒ½ã€‚ è¿™ä¸ªå›¾å½¢åŒºåŸŸçš„å·¦è¾¹ï¼Œå„ç§ç®—æ³•ä¹‹é—´çš„ä¼˜å…ˆçº§å¹¶ä¸æ˜¯å®šä¹‰çš„å¾ˆæ˜ç¡®ï¼Œæœ€ç»ˆçš„æ€§èƒ½æ›´å¤šçš„æ˜¯å–å†³äºä½ åœ¨ç”¨å·¥ç¨‹é€‰æ‹©ç‰¹å¾æ–¹é¢çš„èƒ½åŠ›ä»¥åŠç®—æ³•å¤„ç†æ–¹é¢çš„ä¸€äº›ç»†èŠ‚. åªæ˜¯åœ¨æŸäº›å¤§æ•°æ®è§„æ¨¡éå¸¸åºå¤§çš„è®­ç»ƒé›†ï¼Œä¹Ÿå°±æ˜¯åœ¨å³è¾¹è¿™ä¸ªä¼šéå¸¸çš„å¤§æ—¶ï¼Œæˆ‘ä»¬èƒ½æ›´åŠ æŒç»­åœ°çœ‹åˆ°æ›´å¤§çš„ç”±ç¥ç»ç½‘ç»œæ§åˆ¶å…¶å®ƒæ–¹æ³•. Â¶The Reason the scale of data the speed of computation such as GPUS innovation of algorithm è®¸å¤šç®—æ³•æ–¹é¢çš„åˆ›æ–°ï¼Œä¸€ç›´æ˜¯åœ¨å°è¯•ç€ä½¿å¾—ç¥ç»ç½‘ç»œè¿è¡Œçš„æ›´å¿« switch sigmoid function to relu function ![](Deep Learning.ai_Neural Networks and Deep Learning\algorithmâ€”â€”rul.jpg) åœ¨è¿™ä¸ªåŒºåŸŸï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªsigmoidå‡½æ•°çš„æ¢¯åº¦ä¼šæ¥è¿‘é›¶ï¼Œæ‰€ä»¥å­¦ä¹ çš„é€Ÿåº¦ä¼šå˜å¾—éå¸¸ç¼“æ…¢ï¼Œå› ä¸ºå½“ä½ å®ç°æ¢¯åº¦ä¸‹é™ä»¥åŠæ¢¯åº¦æ¥è¿‘é›¶çš„æ—¶å€™ï¼Œå‚æ•°ä¼šæ›´æ–°çš„å¾ˆæ…¢ï¼Œæ‰€ä»¥å­¦ä¹ çš„é€Ÿç‡ä¹Ÿä¼šå˜çš„å¾ˆæ…¢ï¼Œè€Œé€šè¿‡æ”¹å˜è¿™ä¸ªè¢«å«åšæ¿€æ´»å‡½æ•°çš„ä¸œè¥¿ï¼Œç¥ç»ç½‘ç»œæ¢ç”¨è¿™ä¸€ä¸ªå‡½æ•°ï¼Œå«åšReLUçš„å‡½æ•°ï¼ˆä¿®æ­£çº¿æ€§å•å…ƒï¼‰ï¼ŒReLUå®ƒçš„æ¢¯åº¦å¯¹äºæ‰€æœ‰è¾“å…¥çš„è´Ÿå€¼éƒ½æ˜¯é›¶ï¼Œå› æ­¤æ¢¯åº¦æ›´åŠ ä¸ä¼šè¶‹å‘é€æ¸å‡å°‘åˆ°é›¶ã€‚ è®­ç»ƒä½ çš„ç¥ç»ç½‘ç»œçš„è¿‡ç¨‹ï¼Œå¾ˆå¤šæ—¶å€™æ˜¯å‡­å€Ÿç›´è§‰çš„ï¼Œå¾€å¾€ä½ å¯¹ç¥ç»ç½‘ç»œæ¶æ„æœ‰äº†ä¸€ä¸ªæƒ³æ³•ï¼Œäºæ˜¯ä½ å°è¯•å†™ä»£ç å®ç°ä½ çš„æƒ³æ³•ï¼Œç„¶åè®©ä½ è¿è¡Œä¸€ä¸ªè¯•éªŒç¯å¢ƒæ¥å‘Šè¯‰ä½ ï¼Œä½ çš„ç¥ç»ç½‘ç»œæ•ˆæœæœ‰å¤šå¥½ï¼Œé€šè¿‡å‚è€ƒè¿™ä¸ªç»“æœå†è¿”å›å»ä¿®æ”¹ä½ çš„ç¥ç»ç½‘ç»œé‡Œé¢çš„ä¸€äº›ç»†èŠ‚ï¼Œç„¶åä½ ä¸æ–­çš„é‡å¤ä¸Šé¢çš„æ“ä½œï¼Œå½“ä½ çš„ç¥ç»ç½‘ç»œéœ€è¦å¾ˆé•¿æ—¶é—´å»è®­ç»ƒï¼Œéœ€è¦å¾ˆé•¿æ—¶é—´é‡å¤è¿™ä¸€å¾ªç¯ï¼Œåœ¨è¿™é‡Œå°±æœ‰å¾ˆå¤§çš„åŒºåˆ«ï¼Œæ ¹æ®ä½ çš„ç”Ÿäº§æ•ˆç‡å»æ„å»ºæ›´é«˜æ•ˆçš„ç¥ç»ç½‘ç»œã€‚å½“ä½ èƒ½å¤Ÿæœ‰ä¸€ä¸ªæƒ³æ³•ï¼Œè¯•ä¸€è¯•ï¼Œçœ‹æ•ˆæœå¦‚ä½•ã€‚åœ¨10åˆ†é’Ÿå†…ï¼Œæˆ–è€…ä¹Ÿè®¸è¦èŠ±ä¸Šä¸€æ•´å¤©ï¼Œå¦‚æœä½ è®­ç»ƒä½ çš„ç¥ç»ç½‘ç»œç”¨äº†ä¸€ä¸ªæœˆçš„æ—¶é—´ï¼Œæœ‰æ—¶å€™å‘ç”Ÿè¿™æ ·çš„äº‹æƒ…ï¼Œä¹Ÿæ˜¯å€¼å¾—çš„ï¼Œå› ä¸ºä½ å¾ˆå¿«å¾—åˆ°äº†ä¸€ä¸ªç»“æœã€‚åœ¨10åˆ†é’Ÿå†…æˆ–è€…ä¸€å¤©å†…ï¼Œä½ åº”è¯¥å°è¯•æ›´å¤šçš„æƒ³æ³•ï¼Œé‚£ææœ‰å¯èƒ½ä½¿å¾—ä½ çš„ç¥ç»ç½‘ç»œåœ¨ä½ çš„åº”ç”¨æ–¹é¢å·¥ä½œçš„æ›´å¥½ã€æ›´å¿«çš„è®¡ç®—ï¼Œåœ¨æé«˜é€Ÿåº¦æ–¹é¢çœŸçš„æœ‰å¸®åŠ©ï¼Œé‚£æ ·ä½ å°±èƒ½æ›´å¿«åœ°å¾—åˆ°ä½ çš„å®éªŒç»“æœã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\faster.jpg) Â¶Summary æ—©ä¸ŠèŠ±äº†2hå°æ—¶å­¦ä¹ ç¬¬ä¸€å‘¨çš„è§†é¢‘ï¼Œå…ˆçœ‹ä¸€éè§†é¢‘çš„å­—å¹•ï¼Œé€å­—é€å¥çš„ç†è§£ï¼Œè™½ç„¶å¾ˆå¤šæ—¶å€™éƒ½æ˜¯è‡ªå·±ä¹±çŒœçš„ï¼Œå¤§æ¦‚æ¸…æ¥šè®²çš„ä»€ä¹ˆï¼ç„¶åå†çœ‹å¤§ç‰›çš„ç¬”è®°ï¼Œç„¶åå†çœ‹ä¸€ç¯‡ç»“åˆPPTã€‚ä¸‹åˆä¹Ÿçœ‹äº†åŠä¸ªå¤šå°æ—¶ã€‚é—®é¢˜ï¼š1. è‡ªå·±çš„è‹±æ–‡æ°´å¹³ä¸å¤Ÿï¼Œè¿™ä¸ªéœ€è¦å¤§å¤§çš„æé«˜è®·ã€‚2. å…¶å®åªè¦çœ‹åˆ«äººçš„ç¬”è®°å°±å¯ä»¥çŸ¥é“å†…å®¹ï¼Œä½†æ˜¯è¿˜æ˜¯æƒ³å¬andow ngçš„è®²è§£ã€‚3. è§†é¢‘éƒ½æ¯”è¾ƒçŸ­ï¼Œæ¯ä¸ªè§†é¢‘è®¾è®¡çš„çŸ¥è¯†ç‚¹æˆ–è€…å†…å®¹ä¸å¤šï¼Œ1åˆ°3ä¸ªï¼Œåˆ†æˆçŸ¥è¯†ç‚¹åšç¬”è®°è¿˜æ˜¯ä¸é”™çš„ è¿™ä¸€å‘¨çš„å†…å®¹ï¼Œä¹Ÿå°±æ˜¯ä»Šå¤©æˆ‘å­¦ä¹ çš„çŸ¥è¯†ç®€å•å’Œå®¹æ˜“ç†è§£ã€‚å­¦ä¹ äº†ç¥ç»ç½‘ç»œçš„å¤§è‡´ç»“æ„ï¼Œç¥ç»ç½‘ç»œçš„åº”ç”¨é¢†åŸŸï¼Œæ·±åº¦å­¦ä¹ ä¸ºä»€ä¹ˆå–å¾—å¿«é€Ÿçš„å‘å±•çš„ä¸‰ç‚¹åŸå› ï¼Œå°¤å…¶æ˜¯æ•°æ®scaleä¸å…¶ä»–æ–¹æ³•å’Œç¥ç»ç½‘ç»œè§„æ¨¡çš„å¤§è‡´æ€§èƒ½å…³ç³» C1W2 Â¶C1W2L01: Binary Classification In this week, weâ€™re going to go over the basics of neural network programming. We are going to study handle data without for loop. forward password for propagation backward pass or whatâ€™s called a backward propagation step Why the computations in learning in a neural network can be organized in this board propagation and a separate backward propagation by using logistic regression to convey(ä¼ è¾¾) theses ideas. Â¶Binary Classification Inputï¼› an image . three separate matrices corresponding red green and blue color channels of this image. å¦‚æœä½ çš„å›¾ç‰‡å¤§å°ä¸º64x64åƒç´ ï¼Œé‚£ä¹ˆä½ å°±æœ‰ä¸‰ä¸ªè§„æ¨¡ä¸º64x64çš„çŸ©é˜µï¼Œåˆ†åˆ«å¯¹åº”å›¾ç‰‡ä¸­çº¢ã€ç»¿ã€è“ä¸‰ç§åƒç´ çš„å¼ºåº¦å€¼ unroll all of these pixel intensity values into a feature vector pixel intensity values of this image ![](Deep Learning.ai_Neural Networks and Deep Learning\w_piexl.jpg) ![](Deep Learning.ai_Neural Networks and Deep Learning\2_blue_green_read.jpg) notation (x,y)ï¼š a pair X comma Y $M_{train}$: M subscript train æ¯æ¡æµ‹è¯•é›†åœ¨çŸ©é˜µä¸­éƒ½æ˜¯ä»¥åˆ—å‘é‡çš„å½¢å¼å­˜åœ¨ ![](Deep Learning.ai_Neural Networks and Deep Learning\2_noation.png) Matrix capital ![](Deep Learning.ai_Neural Networks and Deep Learning\2_nation_2.png) ![](Deep Learning.ai_Neural Networks and Deep Learning\2_all_nation.jpg) ![](Deep Learning.ai_Neural Networks and Deep Learning\2_all_nation_1.jpg) Â¶Model : hypothesis Function :Logistic Regression So given an input X and the parameters W and b, how do we generate the output Y hat? Well, one thing you could try, that doesnâ€™t work, would be to have Y hat be w transpose X plus B, kind of a linear function of the input X. And in fact, this is what you use if you were doing linear regression. But this isnâ€™t a very good algorithm for binary classification because you want Y hat to be the chance that Y is equal to one. So,Y hat should really be between zero and one. This is what the sigmoid function looks like. ![](Deep Learning.ai_Neural Networks and Deep Learning\log_1.png) ![](Deep Learning.ai_Neural Networks and Deep Learning\log_3.jpg) Â¶sigmoid function $$ \sigma(z) = \frac{1}{1+e^{-z}} $$ å› ä¸ºä½ æƒ³è®©$\hat{y}$è¡¨ç¤ºå®é™…å€¼$y$ç­‰äº1çš„æœºç‡çš„è¯ï¼Œ åº”è¯¥åœ¨0åˆ°1ä¹‹é—´ã€‚è¿™æ˜¯ä¸€ä¸ªéœ€è¦è§£å†³çš„é—®é¢˜ï¼Œå› ä¸ºå¯èƒ½æ¯”1è¦å¤§å¾—å¤šï¼Œæˆ–è€…ç”šè‡³ä¸ºä¸€ä¸ªè´Ÿå€¼ã€‚å¯¹äºä½ æƒ³è¦çš„åœ¨0å’Œ1ä¹‹é—´çš„æ¦‚ç‡æ¥è¯´å®ƒæ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå› æ­¤åœ¨é€»è¾‘å›å½’ä¸­ï¼Œæˆ‘ä»¬çš„è¾“å‡ºåº”è¯¥æ˜¯ç­‰äºç”±ä¸Šé¢å¾—åˆ°çš„çº¿æ€§å‡½æ•°å¼å­ä½œä¸ºè‡ªå˜é‡çš„sigmoidå‡½æ•°ä¸­ï¼Œå…¬å¼å¦‚ä¸Šå›¾æœ€ä¸‹é¢æ‰€ç¤ºï¼Œå°†çº¿æ€§å‡½æ•°è½¬æ¢ä¸ºéçº¿æ€§å‡½æ•°ã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\log_2.jpg) æ³¨æ„ï¼šåŸæ¥$w,b$æ˜¯åˆ†å¼€åœ¨ï¼Œè¿™é‡Œå°±åˆå¹¶ï¼Œå¼•å…¥å˜é‡$x_0=1$,å¯¹åº”åç½®$b$, ![](Deep Learning.ai_Neural Networks and Deep Learning\log_3.png) Â¶Strategyï¼šCost function Firstly : Loss function $$ L(\hat{y},y)=\frac{1}{2}\sum{(y_i-\hat{y_i})^2} $$ è¿™ä¸ªä¼˜åŒ–é—®é¢˜ä¸æ˜¯å‡¸ä¼˜åŒ–é—®é¢˜(non-convex)ï¼Œå› æ­¤ä¸é€‰ç”¨è¿™ä¸ª Secondlyï¼Œ $$ L(y,\hat{y})=-(ylog{\hat{y}}+(1-y)log{1-\hat{y}}) $$ ![](Deep Learning.ai_Neural Networks and Deep Learning\log_cost_1.jpg) ![](Deep Learning.ai_Neural Networks and Deep Learning\log__cost_2.jpg) Â¶Algorithm: Gradient Descent ![](Deep Learning.ai_Neural Networks and Deep Learning\GD1.jpg) Gradient Descentç®—æ³•æ­¥éª¤ï¼š Initialize $w$, $b$ to zero repeatï¼š $w :=wâˆ’\alpha \frac{âˆ‚J(w,b)}{âˆ‚w}$ $b :=b-\alpha \frac{âˆ‚J(w,b)}{âˆ‚b}$ Â¶C1W2L05 &amp; C1W2L06 Derivatives æ±‚å¯¼ï¼Œè¿™ä¸ªæ˜¯å¾®ç§¯åˆ†çš„å†…å®¹ï¼Œä¸ç”¨å†™äº†ï¼ Â¶C1W2L07ï¼š Computation Graph Â¶C1W2L08 : Derivatives with compution graphs é“¾å¼æ³•åˆ™ $$ \frac{\partial L}{\partial v}=\frac{\partial L}{\partial u}\frac{\partial u}{\partial v} $$ Â¶C1W2L09 : Logistic Regression Gradient Descent Â¶single training example Youâ€™ve seen the loss function that measures how well youâ€™re doing on the single training example. Youâ€™ve also seen the cost function that measures how well your parameters w and b are doing on your entire training set. Youâ€™ve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives. ![](Deep Learning.ai_Neural Networks and Deep Learning\gd_1.jpg) ![](Deep Learning.ai_Neural Networks and Deep Learning\gd_2.png) $$ \frac{\partial L}{\partial w}=\frac{\partial L}{\partial \alpha }\frac{\partial \alpha }{\partial z}\frac{\partial z}{\partial w} \=-(\frac{y}{a}+\frac{1-y}{1-a})a(1-a)x=(a-y)x $$ Â¶C1W2L10 Gradient Descent on m example $$ \min L(w,b)=\sum_{i=1}^{m}L(\alpha_i,y_i)/m\ \frac{\partial L }{\partial w}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial w})/m=(\sum_{i=1}^{m}(a-y_i)x_i)/m\ \frac{\partial L }{\partial b}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial b})/m=(\sum_{i=1}^{m}(a-1)x_i)/m $$ ![](Deep Learning.ai_Neural Networks and Deep Learning\gd_3jpg.jpg) ![](Deep Learning.ai_Neural Networks and Deep Learning\cost_3.png) ä¸Šé¢çš„ä¼ªä»£ç å‘Šè¯‰æˆ‘ä»¬ï¼Œéœ€è¦å¤šæ¬¡for loopå®Œæˆä»£ç ï¼Œä½†æ˜¯è¿™ä¼šé€ æˆè¿ç®—é€Ÿåº¦ä¸‹é™ï¼å› ä¸ºæˆ‘ä»¬è¶Šæ¥è¶Šå¤šåœ°è®­ç»ƒéå¸¸å¤§çš„æ•°æ®é›†ï¼Œå› æ­¤ä½ çœŸçš„éœ€è¦ä½ çš„ä»£ç å˜å¾—éå¸¸é«˜æ•ˆã€‚æ‰€ä»¥åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬ä¼šè°ˆåˆ°å‘é‡åŒ–ï¼Œä»¥åŠå¦‚ä½•åº”ç”¨å‘é‡åŒ–è€Œè¿ä¸€ä¸ªforå¾ªç¯éƒ½ä¸ä½¿ç”¨ã€‚æ‰€ä»¥å­¦ä¹ äº†è¿™äº›ï¼Œæˆ‘å¸Œæœ›ä½ æœ‰å…³äºå¦‚ä½•åº”ç”¨é€»è¾‘å›å½’ï¼Œæˆ–æ˜¯ç”¨äºé€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™ï¼Œäº‹æƒ…ä¼šå˜å¾—æ›´åŠ æ¸…æ™° Â¶summary ä»Šå¤©ä¸»è¦å­¦ä¹ äº†ä»¥logistics regression ä¸ºä¾‹ï¼Œå¦‚ä½•é€šè¿‡é“¾å¼æ±‚å¯¼çš„è¿‡ç¨‹ï¼Œç®€å•çš„ç»ƒä¹ ä¸€ä¸‹ï¼Œä»¥åŠå†æ¬¡äº†è§£ä»€ä¹ˆæ˜¯æ¢¯åº¦ä¸‹é™æ³•ï¼Œä»¥åŠè®­ç»ƒå­¦ä¹ ç®—æ³•çš„éœ€è¦ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œè®­ç»ƒçš„è¿‡ç¨‹å°±æ˜¯æ±‚æŸå¤±å‡½æ•°æœ€ä¼˜å€¼çš„è¿‡ç¨‹ Â¶C1W2L11: Vectorization Â¶1. ä»€ä¹ˆæ˜¯Vectorizationï¼šå°† for loop å°½å¯èƒ½è½¬æ¢ä¸ºçŸ©é˜µè¿ç®— é€šè¿‡numpyå†…ç½®å‡½æ•°å’Œé¿å¼€æ˜¾å¼çš„å¾ªç¯(loop)çš„æ–¹å¼è¿›è¡Œå‘é‡åŒ–ï¼Œä»è€Œæœ‰æ•ˆæé«˜ä»£ç é€Ÿåº¦ã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\v_1.jpg) 123np.dot(a,b)å¦‚æœa,bæ˜¯ä¸€ç»´æ•°ç»„ï¼Œåˆ™è®¡ç®—ç‚¹ç§¯å¦‚æœa,bæ˜¯å¤šç»´æ•°æ®ï¼Œåˆ™çŸ©é˜µä¹˜æ³• Â¶2. An example of vectorization vectorizationçš„å¥½å¤„ï¼šconciser code, but faster execution ä¸€ä¸ªç®€å•çš„å¯¹æ¯”å®éªŒï¼š1,000,000å¤§å°çš„ä¸¤ä¸ªå‘é‡å†…ç§¯è®¡ç®—ï¼Œfor loopè¦æ¯”Vectorizationå¿«300å€ã€‚ åœ¨Deep Learningæ—¶ä»£ï¼Œvectorizationæ˜¯ä¸€é¡¹é‡è¦çš„æŠ€èƒ½ã€‚ 123456789101112131415161718192021import numpy as np #å¯¼å…¥numpyåº“a = np.array([1,2,3,4]) #åˆ›å»ºä¸€ä¸ªæ•°æ®aprint(a)# [1 2 3 4]import time #å¯¼å…¥æ—¶é—´åº“a = np.random.rand(1000000)b = np.random.rand(1000000) #é€šè¿‡roundéšæœºå¾—åˆ°ä¸¤ä¸ªä¸€ç™¾ä¸‡ç»´åº¦çš„æ•°ç»„tic = time.time() #ç°åœ¨æµ‹é‡ä¸€ä¸‹å½“å‰æ—¶é—´#å‘é‡åŒ–çš„ç‰ˆæœ¬c = np.dot(a,b)toc = time.time()print(â€œVectorized version:â€ + str(1000*(toc-tic)) +â€msâ€) #æ‰“å°ä¸€ä¸‹å‘é‡åŒ–çš„ç‰ˆæœ¬çš„æ—¶é—´â€‹#ç»§ç»­å¢åŠ éå‘é‡åŒ–çš„ç‰ˆæœ¬c = 0tic = time.time()for i in range(1000000): c += a[i]*b[i]toc = time.time()print(c)print(â€œFor loop:â€ + str(1000*(toc-tic)) + â€œmsâ€)#æ‰“å°forå¾ªç¯çš„ç‰ˆæœ¬çš„æ—¶é—´ Â¶3. GPU or CPU å¤§è§„æ¨¡çš„æ·±åº¦å­¦ä¹ å†GPUæˆ–è€…å›¾åƒå¤„ç†å•å…ƒè¿è¡Œâ€ï¼ŒCPUå’ŒGPUéƒ½æœ‰å¹¶è¡ŒåŒ–çš„æŒ‡ä»¤ï¼Œä»–ä»¬æœ‰æ—¶å€™ä¼šå«åšSIMDæŒ‡ä»¤ï¼Œè¿™ä¸ªä»£è¡¨äº†ä¸€ä¸ªå•ç‹¬æŒ‡ä»¤å¤šç»´æ•°æ®ï¼Œè¿™ä¸ªçš„åŸºç¡€æ„ä¹‰æ˜¯ï¼Œå¦‚æœä½ ä½¿ç”¨äº†built-inå‡½æ•°,åƒnp.functionæˆ–è€…å¹¶ä¸è¦æ±‚ä½ å®ç°å¾ªç¯çš„å‡½æ•°ï¼Œå®ƒå¯ä»¥è®©pythonçš„å……åˆ†åˆ©ç”¨å¹¶è¡ŒåŒ–è®¡ç®—ã€‚ åªæ˜¯åœ¨GPUå’ŒCPUä¸Šé¢è®¡ç®—ï¼ŒGPUæ›´åŠ æ“…é•¿SIMDè®¡ç®—ï¼Œä½†æ˜¯CPUäº‹å®ä¸Šä¹Ÿä¸æ˜¯å¤ªå·®ï¼Œå¯èƒ½æ²¡æœ‰GPUé‚£ä¹ˆæ“…é•¿å§ã€‚SIMD Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data) Â¶C12L12 ï¼š More Vectorization Example Â¶çŸ©é˜µå’Œå‘é‡ä¹˜æ³• ![](Deep Learning.ai_Neural Networks and Deep Learning\v_2.png) Â¶å‘é‡å‡½æ•° ![](Deep Learning.ai_Neural Networks and Deep Learning\v_3.png) åŸåˆ™ï¼šwhenever possible, avoid explict for-loops ä½¿ç”¨Element wisedçš„çŸ©é˜µè¿ç®—ï¼Œå°†å‡½æ•°ä½œç”¨åœ¨æ¯ä¸ªçŸ©é˜µå…ƒç´ ä¸Šï¼Œæ¯”å¦‚ï¼š np.exp() np.log() np.abs() np.maxium() 1/v v**2 Â¶C1W2L13: Vectorizing Logistic Regression Â¶1. å‰å‘ä¼ æ’­ ![](Deep Learning.ai_Neural Networks and Deep Learning\v_3.jpg) $$ \hat{y}=Ïƒ(w^TX+b)=(a(1),a(2),â€¦,a(mâˆ’1),a(m))=\ (\alpha(z_1),\alpha(z_m),â€¦,\alpha(z_m))=\ (\alpha(wTx_1+b),\alpha(wTx_2+b),â€¦,\alpha(w^Tx_m+b))= $$ 1234import numpy as npz=np.dot(W^T,X)+b# zè¿™é‡Œå°±æ˜¯python å·§å¦™çš„åœ°æ–¹ï¼Œbæ˜¯å®æ•°ï¼Œä½†æ˜¯å‘é‡åŠ ä¸Šå®æ•°åï¼Œbæ‰©å±•æˆå‘é‡ï¼Œè¢«ç§°ä¸ºå¹¿æ’­ï¼ˆbrosdcastingï¼‰ ä¸ªäººç»éªŒï¼š é¦–å…ˆï¼Œç†Ÿæ‚‰æ¯ä¸ªå˜é‡çš„è®°å·å’Œç»´åº¦ï¼Œå¿…è¦çš„è¯ï¼Œå¯ä»¥ç”»å‡ºæ¥ï¼Œæ›´ç›´è§‚ã€‚ å…ˆä»ä¸€ä¸ªæ ·æœ¬åšå‘é‡åŒ–ï¼Œå†æŠŠmä¸ªæ ·æœ¬çš„æ“ä½œå‘é‡åŒ–ã€‚ for-loopé‡Œé¢æ˜¯å¾ªç¯ä¹˜æ³•ï¼Œåˆ™å‘é‡åŒ–ä¸€å®šæ˜¯ä¸€ä¸ªä¹˜æ³•å½¢å¼ï¼Œè‹¥å¯¹äºä¸ç¡®å®šä¹˜æ³•çš„å·¦å³å…³ç³»ï¼Œæ˜¯å¦éœ€è½¬ç½®ï¼Œå¯ä»¥æ ¹æ®ç›®æ ‡å˜é‡çš„ç»´åº¦æ¨æµ‹ã€‚æˆ–è€…å…ˆä¹˜èµ·æ¥ï¼Œå†æ ¹æ®ç›®æ ‡å˜é‡çœ‹æ˜¯å¦è¦è½¬ç½®ã€‚ Â¶C1W2L14 : Vectorzing Logistic Regressionâ€™s Gradient Compution backforwd $$ \frac{âˆ‚J}{âˆ‚w}=\frac{1}{m}X(Aâˆ’Y)T\ \frac{âˆ‚J}{âˆ‚b}=\frac{1}{m}(a(i)âˆ’y(i)) $$ ![](Deep Learning.ai_Neural Networks and Deep Learning\v-4.jpg) é‡è¦çš„æ˜¯å¼„æ¸…æ¥šï¼Œé‡Œé¢çš„è¡Œåˆ—å…³ç³»ï¼Œä»£è¡¨çš„æ„æ€ï¼Œè¿ç®—æ—¶å€™ï¼Œå…ˆè‡ªå·±ç†æ¸…æ¥šã€‚è¿˜æœ‰ç‚¹ç§¯ã€ç­‰ç­‰è¿ç®—æ€§è´¨å¯¹åº”çš„æ“ä½œï¼Œæˆ–è€…å¯¹åº”çš„å†…ç½®å‡½æ•° Â¶C1W2L15: Broadcasting in Python Â¶One Example ![](Deep Learning.ai_Neural Networks and Deep Learning\bordacasing_4.png) A.sum(axis = 0)ä¸­çš„å‚æ•°axisã€‚axisç”¨æ¥æŒ‡æ˜å°†è¦è¿›è¡Œçš„è¿ç®—æ˜¯æ²¿ç€å“ªä¸ªè½´æ‰§è¡Œï¼Œåœ¨numpyä¸­ï¼Œ0è½´æ˜¯å‚ç›´çš„ï¼Œä¹Ÿå°±æ˜¯åˆ—ï¼Œè€Œ1è½´æ˜¯æ°´å¹³çš„ï¼Œä¹Ÿå°±æ˜¯è¡Œã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\boradcasing_5.png) ç¬¬äºŒä¸ªA/cal.reshape(1,4)æŒ‡ä»¤åˆ™è°ƒç”¨äº†numpyä¸­çš„å¹¿æ’­æœºåˆ¶ã€‚è¿™é‡Œä½¿ç”¨ 3 by 4çš„çŸ©é˜µé™¤ä»¥1 by 4 çš„çŸ©é˜µã€‚æŠ€æœ¯ä¸Šæ¥è®²ï¼Œå…¶å®å¹¶ä¸éœ€è¦å†å°†çŸ©é˜µ reshape(é‡å¡‘)æˆ ï¼Œå› ä¸ºçŸ©é˜µæœ¬èº«å·²ç»æ˜¯ äº†ã€‚ä½†æ˜¯å½“æˆ‘ä»¬å†™ä»£ç æ—¶ä¸ç¡®å®šçŸ©é˜µç»´åº¦çš„æ—¶å€™ï¼Œé€šå¸¸ä¼šå¯¹çŸ©é˜µè¿›è¡Œé‡å¡‘æ¥ç¡®ä¿å¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„åˆ—å‘é‡æˆ–è¡Œå‘é‡ã€‚é‡å¡‘æ“ä½œreshapeæ˜¯ä¸€ä¸ªå¸¸é‡æ—¶é—´çš„æ“ä½œï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯ï¼Œå®ƒçš„è°ƒç”¨ä»£ä»·æä½ã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\boradcasing_6.png) Â¶Secondly Example ![](Deep Learning.ai_Neural Networks and Deep Learning\bordacasing_2.png) pythonçš„å¹¿æ’­æœºåˆ¶ä¼šå°†å¸¸æ•°æ‰©å±•æˆ4by 1çš„åˆ—å‘é‡ ![](Deep Learning.ai_Neural Networks and Deep Learning\boradcasing_3.png) å…¶å®æ˜¯å°†1by*n çš„çŸ©é˜µå¤åˆ¶æˆä¸ºmbynçš„çŸ©é˜µ Â¶å¹¿æ’­æœºåˆ¶çš„ä¸¾ä¾‹ ![](Deep Learning.ai_Neural Networks and Deep Learning\bordcasing_1.png) Â¶axis è¡¥å……ï¼šnumpyä¸­ï¼Œç±»ä¼¼sumçš„å‡½æ•°ï¼Œç»å¸¸æ¶‰åŠaxiså‚æ•°ï¼Œå¯ä»¥å–å€¼ä¸º0æˆ–1ï¼Œç”šè‡³å…¶ä»–ã€‚ç»å¸¸è®°ä¸ä½ï¼Œè¿™é‡Œæˆ‘æŸ¥äº†äº†ä¸€ä¸‹ï¼Œæ˜¯è¿™æ ·çš„ï¼ˆåŸæ–‡ï¼‰ï¼š axisçš„æ•°å­—ï¼Œå’Œæ•°ç»„çš„shapeå‚æ•°çš„ç´¢å¼•æ˜¯å¯¹åº”çš„ã€‚æ¯”å¦‚ä¸€ä¸ªæ•°ç»„çš„shapeæ˜¯(5,6)ï¼Œåˆ™ä»£è¡¨5ä¸ªrowï¼Œ6ä¸ªcolumnã€‚å³åœ¨shapeä¸­ï¼Œrowå’Œcolumnçš„ä¸ªæ•°çš„ç´¢å¼•æ˜¯0å’Œ1ã€‚ä¹Ÿå°±ç¬¬1ä¸ªåæ ‡ï¼Œåœ¨shapeä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œç´¢å¼•æ˜¯0ï¼Œä»£è¡¨rowçš„æ–¹å‘ï¼›ç¬¬2ä¸ªåæ ‡ï¼Œåœ¨shapeä¸­çš„ç¬¬2ä¸ªå…ƒç´ ï¼Œç´¢å¼•æ˜¯1ï¼Œä»£è¡¨rowçš„æ–¹å‘ã€‚ å¯¹äºsumå‡½æ•°ï¼ŒaxisæŒ‡çš„æ˜¯sumâ€œæ²¿ç€â€çš„æ–¹å‘ï¼Œç»è¿‡è®¡ç®—ï¼Œè¿™ä¸ªæ–¹å‘çš„ç»´åº¦å› ä¸ºæ±‚å’Œåå°±æ¶ˆå¤±äº†ï¼Œæ¯”å¦‚sum(axis=0)ä»£è¡¨æ˜¯æ²¿ç€â€œrowâ€æ–¹å‘è¿›è¡Œæ±‚å’Œï¼Œ å½“ç„¶axiså¯ä»¥æ˜¯ä¸€ä¸ªtupeï¼Œé‚£å°±ç›¸å½“äºæ²¿ç€å¤šä¸ªå¤šä¸ªæ–¹å‘æ±‚å’Œã€‚ sumå¦‚æœä¸ä¼ å…¥axiså‚æ•°ï¼Œé»˜è®¤æ˜¯å¯¹æ‰€æœ‰ç»´åº¦æ±‚å’Œã€‚ Â¶broadcasting å½“ä¸¤ä¸ªæ•°ç»„çš„å½¢çŠ¶å¹¶ä¸ç›¸åŒçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰©å±•æ•°ç»„çš„æ–¹æ³•æ¥å®ç°ç›¸åŠ ã€ç›¸å‡ã€ç›¸ä¹˜ç­‰æ“ä½œï¼Œè¿™ç§æœºåˆ¶å«åšå¹¿æ’­ï¼ˆbroadcastingï¼‰ã€‚ ä¸‰ç§å¹¿æ’­æƒ…å†µ ![](Deep Learning.ai_Neural Networks and Deep Learning\bordacasing.png) Â¶C1W2L16 A Note on Python/numpy vectors æœ¬èŠ‚ä¸»è¦è®²Pythonä¸­çš„numpyä¸€ç»´æ•°ç»„çš„ç‰¹æ€§ï¼Œä»¥åŠä¸è¡Œå‘é‡æˆ–åˆ—å‘é‡çš„åŒºåˆ« Â¶1. ä¸€ç»´æ•°ç»„çš„ç‰¹æ€§ é¦–å…ˆè®¾ç½®a = np.array.random.randn(5)ï¼Œè¿™æ ·ä¼šç”Ÿæˆå­˜å‚¨åœ¨æ•°ç»„aä¸­çš„5ä¸ªé«˜æ–¯éšæœºæ•°å˜é‡ã€‚ä¹‹åè¾“å‡º ï¼Œä»å±å¹•ä¸Šå¯ä»¥å¾—çŸ¥ï¼Œæ­¤æ—¶a çš„shapeï¼ˆå½¢çŠ¶ï¼‰æ˜¯ä¸€ä¸ªçš„ç»“æ„ã€‚è¿™åœ¨Pythonä¸­è¢«ç§°ä½œä¸€ä¸ªä¸€ç»´æ•°ç»„ã€‚å®ƒæ—¢ä¸æ˜¯ä¸€ä¸ªè¡Œå‘é‡ä¹Ÿä¸æ˜¯ä¸€ä¸ªåˆ—å‘é‡ï¼Œè¿™ä¹Ÿå¯¼è‡´å®ƒæœ‰ä¸€äº›ä¸æ˜¯å¾ˆç›´è§‚çš„æ•ˆæœã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœæˆ‘è¾“å‡ºä¸€ä¸ªè½¬ç½®é˜µï¼Œæœ€ç»ˆç»“æœå®ƒä¼šå’Œçœ‹èµ·æ¥ä¸€æ ·ï¼Œæ‰€ä»¥å’Œçš„è½¬ç½®é˜µæœ€ç»ˆç»“æœçœ‹èµ·æ¥ä¸€æ ·ã€‚è€Œå¦‚æœæˆ‘è¾“å‡ºå’Œçš„è½¬ç½®é˜µçš„å†…ç§¯ï¼Œä½ å¯èƒ½ä¼šæƒ³ï¼šä¹˜ä»¥çš„è½¬ç½®è¿”å›ç»™ä½ çš„å¯èƒ½ä¼šæ˜¯ä¸€ä¸ªçŸ©é˜µã€‚ä½†æ˜¯å¦‚æœæˆ‘è¿™æ ·åšï¼Œä½ åªä¼šå¾—åˆ°ä¸€ä¸ªæ•°ã€‚ æ‰€ä»¥æˆ‘å»ºè®®å½“ä½ ç¼–å†™ç¥ç»ç½‘ç»œæ—¶ï¼Œä¸è¦åœ¨ä½¿ç”¨çš„shape(5,1)æ˜¯è¿˜æ˜¯(n,)æˆ–è€…ä¸€ç»´æ•°ç»„ã€‚ç›¸åï¼Œå¦‚æœä½ è®¾ç½®(5,1)ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯5è¡Œ1åˆ—å‘é‡ã€‚åœ¨å…ˆå‰çš„æ“ä½œé‡Œaå’Œaçš„è½¬ç½®çœ‹èµ·æ¥ä¸€æ ·ï¼Œè€Œç°åœ¨è¿™æ ·çš„ aå˜æˆä¸€ä¸ªæ–°çš„a çš„è½¬ç½®ï¼Œå¹¶ä¸”å®ƒæ˜¯ä¸€ä¸ªè¡Œå‘é‡ã€‚è¯·æ³¨æ„ä¸€ä¸ªç»†å¾®çš„å·®åˆ«ï¼Œåœ¨è¿™ç§æ•°æ®ç»“æ„ä¸­ï¼Œå½“æˆ‘ä»¬è¾“å‡ºa çš„è½¬ç½®æ—¶æœ‰ä¸¤å¯¹æ–¹æ‹¬å·ï¼Œè€Œä¹‹å‰åªæœ‰ä¸€å¯¹æ–¹æ‹¬å·ï¼Œæ‰€ä»¥è¿™å°±æ˜¯1è¡Œ5åˆ—çš„çŸ©é˜µå’Œä¸€ç»´æ•°ç»„çš„å·®åˆ«ã€‚ Â¶2. è¡Œå‘é‡å’Œåˆ—å‘é‡ rank 1 arrayé—®é¢˜ï¼šshapeæ˜¯(x,)çš„æ•°ç»„ï¼Œæ—¢ä¸æ˜¯è¡Œå‘é‡ï¼Œä¹Ÿä¸æ˜¯åˆ—å‘é‡ï¼Œæ²¡æ³•å‚ä¸æ­£å¸¸çš„çŸ©é˜µè¿ç®—ï¼Œåº”è¯¥æ€»æ˜¯ä½¿ç”¨(x,1)æˆ–(1,x)çš„shapeæ¥è¡¨ç¤ºå‘é‡ã€‚ä½†å¯ä»¥é€šè¿‡reshapeæ–¹æ³•å°†rank 1 arrayè½¬æ¢ä¸ºè¡Œå‘é‡æˆ–åˆ—å‘é‡ã€‚ï¼ˆä»€ä¹ˆæ˜¯rankï¼Œå°±æ˜¯ä¸€ä¸ªæ•°ç»„çš„ç»´åº¦ï¼‰ä¸€ç»´çš„æ•°ç»„æ—¢ä¸æ˜¯è¡Œå‘é‡ä¹Ÿä¸æ˜¯åˆ—å‘é‡ï¼Œè½¬ç½®åï¼Œä¾ç„¶æ˜¯æœ¬èº«ã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\note_1.jpg) Â¶3. è§£å†³æ–¹æ³• 12assert(a.shape=ï¼ˆ5ï¼Œ1)ï¼‰# ä¸ºäº†ç¡®ä¿ä½ çš„çŸ©é˜µæˆ–å‘é‡æ‰€éœ€è¦çš„ç»´æ•°æ—¶ï¼Œä¸è¦ç¾äº reshape æ“ä½œ Â¶C1W2L18 ï¼šQuick Tour of Jupyter/iPython Notebooks Â¶C1W2L18: Explanation of Logistic Regression Cost Function å¯¹åº”logistic regressionï¼Œè¾“å‡º$\hat{y}=p(y=1|x)$,é‚£ä¹ˆ$p(y=0|x)=1-\hat{y}$ ![](Deep Learning.ai_Neural Networks and Deep Learning\cost_2png.png) ç»¼åˆä¸Šé¢ $$ p(y|x)= \hat{y}y*(1-\hat{y}){1-y} $$ å¯¹äºæ•´ä¸ªè®­ç»ƒé›†ï¼Œ ![](Deep Learning.ai_Neural Networks and Deep Learning\cost_1png.png) å‡è®¾æ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬æœä»åŒä¸€åˆ†å¸ƒä¸”ç›¸äº’ç‹¬ç«‹ï¼Œä¹Ÿå³ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œæ‰€æœ‰è¿™äº›æ ·æœ¬çš„è”åˆæ¦‚ç‡å°±æ˜¯æ¯ä¸ªæ ·æœ¬æ¦‚ç‡çš„ä¹˜ç§¯: $$ p(labels \ in\ training\ set)=\Pi_{i=1}^mp(y_i|x_i) $$ å¦‚æœåˆ©ç”¨æå¤§ä¼¼ç„¶æ³•åšï¼Œæ‰¾åˆ°ä¸€ç»„å‚æ•°ï¼Œä½¿å¾—æ ·æœ¬è§‚æµ‹å€¼æ¦‚ç‡æœ€å¤§ $$ \max log p(label \ in \ training \ set)=log \Pi_{i=1}^mp(y_i|x_i)=\sum -L(\hat{yi},yi) $$ $$ \min cost J(w,b)=\frac{1}{m}L(\hat{yi},yi) $$ æ€»ç»“ä¸€ä¸‹ï¼Œä¸ºäº†æœ€å°åŒ–æˆæœ¬å‡½æ•°ï¼Œæˆ‘ä»¬ä»logisticå›å½’æ¨¡å‹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„è§’åº¦å‡ºå‘ï¼Œå‡è®¾è®­ç»ƒé›†ä¸­çš„æ ·æœ¬éƒ½æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„æ¡ä»¶ä¸‹ Â¶Day3 : summary ä¸»è¦å­¦ä¹ äº†pythonç¼–ç¨‹çš„å¦‚ä½•æ‰èƒ½é«˜æ•ˆç‡ï¼Œå†…ç½®å‡½æ•°çš„å…·æœ‰å¹¶è¡Œæ€§ï¼ŒsimdæŒ‡ä»¤ï¼Œä»¥åŠä¸€ç»´æ•°ç»„çš„ä½¿ç”¨æ³¨æ„äº‹é¡¹ï¼Œlogistic regressionçš„lost functionçš„åŸç†è¯æ˜ C1W3 Â¶C1W3L01 : Neural Network Overview ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_2.png) è®¸å¤šsigmoidå•å…ƒå †å èµ·æ¥å½¢æˆä¸€ä¸ªç¥ç»ç½‘ç»œã€‚ æ­£å‘ä¼ æ’­ï¼šè¾“å…¥å±‚åˆ°layer one $$ \left.\begin{array}{c}{x} \ {W^{[1]}} \ {b^{[1]}}\end{array}\right} \Longrightarrow z{[1]}=W{[1]} x+b^{[1]} \Longrightarrow a{[1]}=\sigma\left(z{[1)}\right) $$ layer one åˆ°layer two $$ \left.\begin{array}{r}{a{(1]}=\sigma\left(z{[1]}\right)} \ {W^{[2]}} \ {b^{[2]}}\end{array}\right}\begin{array}{l}{\Longrightarrow z{[2]}=W{[2]} a{[1]}+b{[2]} \Longrightarrow a{[2]}=\sigma\left(z{[2]}\right)} \ {\Longrightarrow L\left(a^{[2]}, y\right)}\end{array} $$ åå‘ä¼ æ’­ $$ \left.\begin{array}{r}{d a^{[1]}=d \sigma\left(z^{[1]}\right)} \ {d W^{[2]}} \ {d b^{[2]}}\end{array}\right}\begin{array}{l}{\Longleftarrow d z{[2]}=d\left(W{[2]} \alpha{[1]}+b{[2]}\right) \Longleftarrow d a^{[2]}=d \sigma\left(z^{[2]}\right)} \ {\Longleftarrow d L\left(a^{[2]}, y\right)}\end{array} $$ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_3.png) $W$çš„è¡Œæ•°æ˜¯æœ¬æ¬¡ç»“ç‚¹ä¸ªæ•°ï¼Œåˆ—æ•°æ˜¯ä¸Šå±‚èŠ‚ç‚¹ä¸ªæ•° Â¶C1W3L02 : Nerual Network Representations ç¬¦å·è¯´æ˜ Â¶C1W3L03ï¼š Computation Neural Network Output Â¶A simple training examples ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_5.png) å…¶ä¸­ï¼Œxè¡¨ç¤ºè¾“å…¥ç‰¹å¾ï¼Œaè¡¨ç¤ºæ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼ŒWè¡¨ç¤ºç‰¹å¾çš„æƒé‡ï¼Œä¸Šæ ‡è¡¨ç¤ºç¥ç»ç½‘ç»œçš„å±‚æ•°ï¼ˆéšè—å±‚ä¸º1ï¼‰ï¼Œä¸‹æ ‡è¡¨ç¤ºè¯¥å±‚çš„ç¬¬å‡ ä¸ªç¥ç»å…ƒã€‚è¿™æ˜¯ç¥ç»ç½‘ç»œçš„ç¬¦å·æƒ¯ä¾‹ï¼Œä¸‹åŒã€‚ ç¥ç»ç½‘ç»œçš„è®¡ç®— å…³äºç¥ç»ç½‘ç»œæ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Œä»æˆ‘ä»¬ä¹‹å‰æåŠçš„é€»è¾‘å›å½’å¼€å§‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ç”¨åœ†åœˆè¡¨ç¤ºç¥ç»ç½‘ç»œçš„è®¡ç®—å•å…ƒï¼Œé€»è¾‘å›å½’çš„è®¡ç®—æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼Œé¦–å…ˆä½ æŒ‰æ­¥éª¤è®¡ç®—å‡ºï¼Œç„¶ååœ¨ç¬¬äºŒæ­¥ä¸­ä½ ä»¥sigmoidå‡½æ•°ä¸ºæ¿€æ´»å‡½æ•°è®¡ç®—ï¼ˆå¾—å‡ºï¼‰ï¼Œä¸€ä¸ªç¥ç»ç½‘ç»œåªæ˜¯è¿™æ ·å­åšäº†å¥½å¤šæ¬¡é‡å¤è®¡ç®—ã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_6.png) ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_4.png) è¯´æ˜ï¼š$w_i{[1]}$å’Œ$W{[1]}$çš„å…³ç³»ï¼Œä¸€ä¸ªæŒ‰ç…§logistic regression ï¼Œä¸€ä¸ªæ˜¯çŸ©é˜µè¡¨ç¤ºã€‚ å‘é‡åŒ–è®¡ç®— å¦‚æœä½ æ‰§è¡Œç¥ç»ç½‘ç»œçš„ç¨‹åºï¼Œç”¨forå¾ªç¯æ¥åšè¿™äº›çœ‹èµ·æ¥çœŸçš„å¾ˆä½æ•ˆã€‚æ‰€ä»¥æ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯æŠŠè¿™å››ä¸ªç­‰å¼å‘é‡åŒ–ã€‚å‘é‡åŒ–çš„è¿‡ç¨‹æ˜¯å°†ç¥ç»ç½‘ç»œä¸­çš„ä¸€å±‚ç¥ç»å…ƒå‚æ•°çºµå‘å †ç§¯èµ·æ¥ï¼Œä¾‹å¦‚éšè—å±‚ä¸­çš„çºµå‘å †ç§¯èµ·æ¥å˜æˆä¸€ä¸ª(4,3)çš„çŸ©é˜µï¼Œç”¨ç¬¦å·$W^{[1]}$è¡¨ç¤ºã€‚å¦ä¸€ä¸ªçœ‹å¾…è¿™ä¸ªçš„æ–¹æ³•æ˜¯æˆ‘ä»¬æœ‰å››ä¸ªé€»è¾‘å›å½’å•å…ƒï¼Œä¸”æ¯ä¸€ä¸ªé€»è¾‘å›å½’å•å…ƒéƒ½æœ‰ç›¸å¯¹åº”çš„å‚æ•°â€”â€”å‘é‡ï¼ŒæŠŠè¿™å››ä¸ªå‘é‡å †ç§¯åœ¨ä¸€èµ·ï¼Œä½ ä¼šå¾—å‡ºè¿™4Ã—3çš„çŸ©é˜µã€‚ $$ z{[n]}=W{[n]}X+b^{[n]} $$ $$ a^{[1]}=\left[ \begin{array}{c}{a_{1}^{[1]}} \ {a_{2}^{[1]}} \ {a_{3}^{[1]}} \ {a_{4}{[1]}}\end{array}\right]=\sigma\left(z{[1]}\right) $$ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_7.png) Given input Xï¼ˆa single training set) $$ \begin{array}{c}{z{[1]}=W{[1]} a{[0]}+b{[1]}} \ {a{[1]}=\sigma\left(z{[1]}\right)} \ {z{[2]}=W{[2]} a{[1]}+b{[2]}} \ {a{[2]}=\sigma\left(z{[2]}\right)}\end{array} $$ è¯´æ˜ï¼š $W$çš„ç¬¬$i$è¡Œè¡¨ç¤ºï¼Œå½“å‰å±‚åˆ°ä¸Šä¸€å±‚çš„æƒé‡è¡Œå‘é‡ï¼Œå†è®¡ç®—å•ä¸ªçš„æ—¶å€™ï¼Œç”±äºæ˜¯æŒ‰ç…§logristics regressionçš„æ–¹å¼ï¼Œæ‰€ä»¥è®¤ä¸º$w_i$æ˜¯åˆ—å‘é‡ï¼Œæ‰€ä»¥è½¬ç½®æˆè¡Œå‘é‡ã€‚ä¸Šé¢çš„å›¾ä¹Ÿè¯´æ˜äº†ï¼šå¦‚ä½•ä»å•ä¸ªæ“ä½œåˆ°çŸ©é˜µæ“ä½œï¼Œæƒé‡çŸ©é˜µæ˜¯æ€ä¹ˆæ„é€ ï¼Œæ€ä¹ˆè¡¨ç¤ºçš„ã€‚ bæ˜¯åˆ—å‘é‡ã€‚ Â¶C1W3L04: Vectorizing Across Mutilple Example Different training examples in different columns of the matrix for loop ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_8.png) vectorizing : stacking training set in columns $$ x=\left[ \begin{array}{cccc}{\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \ {x^{(1)}} &amp; {x^{(2)}} &amp; {\dots} &amp; {x} \ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots}\end{array}\right] $$ å°±æœ‰ $$ \left{\begin{array}{l}{A{[1]}=\sigma\left(z{[1]}\right)} \ {z{[2]}=W{[2]} A{[1]}+b{[2]}} \ {A{[2]}=\sigma\left(z{[2]}\right)}\end{array}\right. $$ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_9.png) å½“å‚ç›´æ‰«æï¼Œæ˜¯ç´¢å¼•åˆ°éšè—å•ä½çš„æ•°å­—ã€‚å½“æ°´å¹³æ‰«æï¼Œå°†ä»ç¬¬ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹ä¸­ä»ç¬¬ä¸€ä¸ªéšè—çš„å•å…ƒåˆ°ç¬¬äºŒä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç¬¬ä¸‰ä¸ªè®­ç»ƒæ ·æœ¬â€¦â€¦ç›´åˆ°èŠ‚ç‚¹å¯¹åº”äºç¬¬ä¸€ä¸ªéšè—å•å…ƒçš„æ¿€æ´»å€¼ï¼Œä¸”è¿™ä¸ªéšè—å•å…ƒæ˜¯ä½äºè¿™ä¸ªè®­ç»ƒæ ·æœ¬ä¸­çš„æœ€ç»ˆè®­ç»ƒæ ·æœ¬ã€‚ ä»æ°´å¹³ä¸Šçœ‹ï¼ŒçŸ©é˜µä»£è¡¨äº†å„ä¸ªè®­ç»ƒæ ·æœ¬ã€‚ä»ç«–ç›´ä¸Šçœ‹ï¼ŒçŸ©é˜µçš„ä¸åŒçš„ç´¢å¼•å¯¹åº”äºä¸åŒçš„éšè—å•å…ƒã€‚ Â¶C1W3L05 : Explanation for vectorized implement ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_10.png) ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_11.png) Â¶C1W3L06 : Activation Function åœ¨è®¨è®ºä¼˜åŒ–ç®—æ³•æ—¶ï¼Œæœ‰ä¸€ç‚¹è¦è¯´æ˜ï¼šåŸºæœ¬å·²ç»ä¸ç”¨sigmoidæ¿€æ´»å‡½æ•°äº†ï¼Œtanhå‡½æ•°åœ¨æ‰€æœ‰åœºåˆéƒ½ä¼˜äºsigmoidå‡½æ•°ã€‚ sigmoidå‡½æ•°å’Œtanhå‡½æ•°ä¸¤è€…å…±åŒçš„ç¼ºç‚¹æ˜¯ï¼Œåœ¨zç‰¹åˆ«å¤§æˆ–è€…ç‰¹åˆ«å°çš„æƒ…å†µä¸‹ï¼Œå¯¼æ•°çš„æ¢¯åº¦æˆ–è€…å‡½æ•°çš„æ–œç‡ä¼šå˜å¾—ç‰¹åˆ«å°ï¼Œæœ€åå°±ä¼šæ¥è¿‘äº0ï¼Œå¯¼è‡´é™ä½æ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦ã€‚ åœ¨æœºå™¨å­¦ä¹ å¦ä¸€ä¸ªå¾ˆæµè¡Œçš„å‡½æ•°æ˜¯ï¼šä¿®æ­£çº¿æ€§å•å…ƒçš„å‡½æ•°ï¼ˆReLuï¼‰ï¼ŒReLuå‡½æ•°å›¾åƒæ˜¯å¦‚ä¸‹å›¾ã€‚$ a = max(0,z)$ï¼š æ‰€ä»¥ï¼Œåªè¦æ˜¯æ­£å€¼çš„æƒ…å†µä¸‹ï¼Œå¯¼æ•°æ’ç­‰äº1ï¼Œå½“æ˜¯è´Ÿå€¼çš„æ—¶å€™ï¼Œå¯¼æ•°æ’ç­‰äº0ã€‚ä»å®é™…ä¸Šæ¥è¯´ï¼Œå½“ä½¿ç”¨çš„å¯¼æ•°æ—¶ï¼Œ=0çš„å¯¼æ•°æ˜¯æ²¡æœ‰å®šä¹‰çš„ã€‚ä½†æ˜¯å½“ç¼–ç¨‹å®ç°çš„æ—¶å€™ï¼Œçš„å–å€¼åˆšå¥½ç­‰äº0.00000001ï¼Œè¿™ä¸ªå€¼ç›¸å½“å°ï¼Œæ‰€ä»¥ï¼Œåœ¨å®è·µä¸­ï¼Œä¸éœ€è¦æ‹…å¿ƒè¿™ä¸ªå€¼ï¼Œæ˜¯ç­‰äº0çš„æ—¶å€™ï¼Œå‡è®¾ä¸€ä¸ªå¯¼æ•°æ˜¯1æˆ–è€…0æ•ˆæœéƒ½å¯ä»¥ã€‚ å¦‚æœè¾“å‡ºæ˜¯0ã€1å€¼ï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼‰ï¼Œåˆ™è¾“å‡ºå±‚é€‰æ‹©sigmoidå‡½æ•°ï¼Œç„¶åå…¶å®ƒçš„æ‰€æœ‰å•å…ƒéƒ½é€‰æ‹©Reluå‡½æ•°ã€‚ è¿™æ˜¯å¾ˆå¤šæ¿€æ´»å‡½æ•°çš„é»˜è®¤é€‰æ‹©ï¼Œå¦‚æœåœ¨éšè—å±‚ä¸Šä¸ç¡®å®šä½¿ç”¨å“ªä¸ªæ¿€æ´»å‡½æ•°ï¼Œé‚£ä¹ˆé€šå¸¸ä¼šä½¿ç”¨Reluæ¿€æ´»å‡½æ•°ã€‚æœ‰æ—¶ï¼Œä¹Ÿä¼šä½¿ç”¨tanhæ¿€æ´»å‡½æ•°ï¼Œä½†Reluçš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼šå½“zæ˜¯è´Ÿå€¼çš„æ—¶å€™ï¼Œå¯¼æ•°ç­‰äº0ã€‚ è¿™é‡Œä¹Ÿæœ‰å¦ä¸€ä¸ªç‰ˆæœ¬çš„Reluè¢«ç§°ä¸ºLeaky Reluã€‚ å½“æ˜¯è´Ÿå€¼æ—¶ï¼Œè¿™ä¸ªå‡½æ•°çš„å€¼ä¸æ˜¯ç­‰äº0ï¼Œè€Œæ˜¯è½»å¾®çš„å€¾æ–œã€‚ ä¸¤è€…çš„ä¼˜ç‚¹æ˜¯ï¼š ç¬¬ä¸€ï¼Œåœ¨çš„åŒºé—´å˜åŠ¨å¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œæ¿€æ´»å‡½æ•°çš„å¯¼æ•°æˆ–è€…æ¿€æ´»å‡½æ•°çš„æ–œç‡éƒ½ä¼šè¿œå¤§äº0ï¼Œåœ¨ç¨‹åºå®ç°å°±æ˜¯ä¸€ä¸ªif-elseè¯­å¥ï¼Œè€Œsigmoidå‡½æ•°éœ€è¦è¿›è¡Œæµ®ç‚¹å››åˆ™è¿ç®—ï¼Œåœ¨å®è·µä¸­ï¼Œä½¿ç”¨ReLuæ¿€æ´»å‡½æ•°ç¥ç»ç½‘ç»œé€šå¸¸ä¼šæ¯”ä½¿ç”¨sigmoidæˆ–è€…tanhæ¿€æ´»å‡½æ•°å­¦ä¹ çš„æ›´å¿«ã€‚ ç¬¬äºŒï¼Œsigmoidå’Œtanhå‡½æ•°çš„å¯¼æ•°åœ¨æ­£è´Ÿé¥±å’ŒåŒºçš„æ¢¯åº¦éƒ½ä¼šæ¥è¿‘äº0ï¼Œè¿™ä¼šé€ æˆæ¢¯åº¦å¼¥æ•£ï¼Œè€ŒReluå’ŒLeaky ReLuå‡½æ•°å¤§äº0éƒ¨åˆ†éƒ½ä¸ºå¸¸æ•°ï¼Œä¸ä¼šäº§ç”Ÿæ¢¯åº¦å¼¥æ•£ç°è±¡ã€‚(åŒæ—¶åº”è¯¥æ³¨æ„åˆ°çš„æ˜¯ï¼ŒReluè¿›å…¥è´ŸåŠåŒºçš„æ—¶å€™ï¼Œæ¢¯åº¦ä¸º0ï¼Œç¥ç»å…ƒæ­¤æ—¶ä¸ä¼šè®­ç»ƒï¼Œäº§ç”Ÿæ‰€è°“çš„ç¨€ç–æ€§ï¼Œè€ŒLeaky ReLuä¸ä¼šæœ‰è¿™é—®é¢˜) åœ¨ReLuçš„æ¢¯åº¦ä¸€åŠéƒ½æ˜¯0ï¼Œä½†æ˜¯ï¼Œæœ‰è¶³å¤Ÿçš„éšè—å±‚ä½¿å¾—zå€¼å¤§äº0ï¼Œæ‰€ä»¥å¯¹å¤§å¤šæ•°çš„è®­ç»ƒæ•°æ®æ¥è¯´å­¦ä¹ è¿‡ç¨‹ä»ç„¶å¯ä»¥å¾ˆå¿«ã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_12.png) sigmoidæ¿€æ´»å‡½æ•°ï¼šé™¤äº†è¾“å‡ºå±‚æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜åŸºæœ¬ä¸ä¼šç”¨å®ƒã€‚ tanhæ¿€æ´»å‡½æ•°ï¼štanhæ˜¯éå¸¸ä¼˜ç§€çš„ï¼Œå‡ ä¹é€‚åˆæ‰€æœ‰åœºåˆã€‚ ReLuæ¿€æ´»å‡½æ•°ï¼šæœ€å¸¸ç”¨çš„é»˜è®¤å‡½æ•°ï¼Œï¼Œå¦‚æœä¸ç¡®å®šç”¨å“ªä¸ªæ¿€æ´»å‡½æ•°ï¼Œå°±ä½¿ç”¨ReLuæˆ–è€…Leaky ReLuã€‚ é€šå¸¸çš„å»ºè®®æ˜¯ï¼šå¦‚æœä¸ç¡®å®šå“ªä¸€ä¸ªæ¿€æ´»å‡½æ•°æ•ˆæœæ›´å¥½ï¼Œå¯ä»¥æŠŠå®ƒä»¬éƒ½è¯•è¯•ï¼Œç„¶ååœ¨éªŒè¯é›†æˆ–è€…å‘å±•é›†ä¸Šè¿›è¡Œè¯„ä»·ã€‚ç„¶åçœ‹å“ªä¸€ç§è¡¨ç°çš„æ›´å¥½ï¼Œå°±å»ä½¿ç”¨å®ƒã€‚ Â¶C1W3L07 : Why non-linear activation Functions ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_13.png) é€šè¿‡æ¨å¯¼å¯ä»¥å¾—å‡ºï¼Œå¦‚æœä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œç›¸å½“äºæ²¡æœ‰éšè—å±‚ã€‚æ— è®ºä½ çš„ç¥ç»ç½‘ç»œæœ‰å¤šå°‘å±‚ä¸€ç›´åœ¨åšçš„åªæ˜¯è®¡ç®—çº¿æ€§å‡½æ•°ï¼Œæ‰€ä»¥ä¸å¦‚ç›´æ¥å»æ‰å…¨éƒ¨éšè—å±‚ã€‚å½“å½“ç„¶ï¼Œåœ¨output layeræ˜¯å¯ä»¥ä¸ç”¨activation functionï¼Œæˆ–è€…ç”¨linear activation functionï¼›è¿™ç§æƒ…å†µä¸€èˆ¬æ˜¯è¦æ±‚è¾“å‡ºå®æ•°é›†ç»“æœï¼ˆæ¯”å¦‚é¢„æµ‹æˆ¿ä»·ï¼‰ã€‚å³ä¾¿å¦‚æ­¤ï¼Œåœ¨hidden layerè¿˜æ˜¯è¦ç”¨non-linear activation functionã€‚ Â¶sigmoid activation function ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_14.png) $$ \frac{d}{d z} g(z)=\frac{1}{1+e{-z}}\left(1-\frac{1}{1+e{-z}}\right)=g(z)(1-g(z)) $$ Â¶tanh activation function ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_15.png) $$ g(z)=\tanh (z)=\frac{e{z}-e{-z}}{e{x}+e{-z}} $$ $$ \frac{d}{d z} g(z)=1-(\tanh (z))^{2} $$ Â¶Rectified linear unit(RelU) ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_16.png) $$ g(z)^{\prime}=\left{\begin{array}{ll}{0} &amp; {\text { if } z&lt;0} \ {1} &amp; {\text { if } z&gt;0} \ {\text {undefined}} &amp; {\text { if } z=0}\end{array}\right. $$ æ³¨ï¼šé€šå¸¸åœ¨z= 0çš„æ—¶å€™ç»™å®šå…¶å¯¼æ•°1,0ï¼›å½“ç„¶=0çš„æƒ…å†µå¾ˆå°‘ Â¶Leaky linear unit (Leaky ReLU) $$ g(z)=\max (0.01 z, z) $$ $$ g(z)^{\prime}=\left{\begin{array}{ll}{0.01} &amp; {\text { if } z&lt;0} \ {1} &amp; {\text { if } z&gt;0} \ {\text {undefined}} &amp; {\text { if } z=0}\end{array}\right. $$ æ³¨ï¼šé€šå¸¸åœ¨çš„z=0æ—¶å€™ç»™å®šå…¶å¯¼æ•°1,0.01ï¼›å½“ç„¶çš„æƒ…å†µå¾ˆå°‘ã€‚ Â¶C1W3L09 : Gradient Descent For Neural Networks gradient descentçš„å…³é”®æ˜¯æ±‚cost functionå¯¹å‚æ•°çš„åå¯¼æ•° ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_17.png) æ±‚å¯¼è¿‡ç¨‹ä½¿ç”¨çš„æ˜¯Backpropagation é¦–å…ˆåšforward propagationï¼Œæ±‚è§£å‡ºæ¯ä¸€å±‚çš„è¾“å‡ºA $$ (1) z{[1]}=W{[1]} x+b^{[1]}\ (2) a{[1]}=\sigma\left(z{[1]}\right)\(3) z{[2]}=W{[2]}=W^{[2]} a{[1]}+b{[2]}\(4) a{[2]}=g{[2]}\left(z{[z]}\right)=\sigma\left(z{[2]}\right) $$ ç„¶åå‘åï¼Œé€å±‚æ±‚è§£å¯¹æ¯ä¸€å±‚å‚æ•°çš„åå¯¼æ•° ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_18.png) sumï¼Œkeepdimsæ˜¯é˜²æ­¢pythonè¾“å‡ºé‚£äº›å¤æ€ªçš„ç§©æ•°(n,)ï¼ŒåŠ ä¸Šè¿™ä¸ªç¡®ä¿é˜µçŸ©é˜µè¿™ä¸ªå‘é‡è¾“å‡ºçš„ç»´åº¦ä¸º(n,1ï¼‰è¿™æ ·æ ‡å‡†çš„å½¢å¼ã€‚ Â¶C1WL10: Backpropagation intuition (optional) ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_19.png) å®ç°åå‘ä¼ æ’­æœ‰ä¸ªæŠ€å·§ï¼Œå°±æ˜¯è¦ä¿è¯çŸ©é˜µçš„ç»´åº¦ç›¸äº’åŒ¹é… å…¶å®ï¼Œå¯¹äºä¸€ä¸ªç¥ç»å…ƒï¼Œè¾“å…¥éƒ¨åˆ†ï¼šæ˜¯æƒé‡å’Œä¸Šä¸€å±‚è¾“å‡ºçš„çº¿æ€§ç»„åˆï¼›è¾“å‡ºï¼šæ¿€æ´»å‡½æ•°ä½œç”¨äºè¾“å…¥ï¼Œå› æ­¤å¯¹$W$æ±‚åå¯¼æ—¶ï¼Œå¯¹æ¿€æ´»å‡½æ•°æ±‚ä¸€æ¬¡ï¼Œå†å¯¹çº¿æ€§ç»„åˆæ±‚ä¸€æ¬¡ã€‚å¯¹$b$æ±‚åå¯¼æ˜¯ï¼Œå¯¹çº¿æ€§éƒ¨åˆ†æ±‚åå¯¼æ˜¯1,è¿™é‡Œç”¨æ±‚å’Œã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_20.png) Â¶C1W3L11: Random Initialization ` ä¸logistic regressionä¸åŒï¼Œåˆå§‹åŒ–å‚æ•°ä¸å¯å›ºå®šä¸º0ï¼Œè€Œæ˜¯æ¯ä¸ªå‚æ•°éƒ½è¦éšæœºåˆå§‹åŒ–ã€‚ ä¸»è¦åŸå› æ˜¯ï¼šå¦‚æœæ¯ä¸ªå‚æ•°wå’Œbéƒ½æ˜¯0ï¼Œåˆ™åŒä¸€å±‚çš„æ¯ä¸ªneuronè®¡ç®—ç»“æœå®Œå…¨ä¸€æ ·ï¼ˆè¾“å…¥ä¸€æ ·aï¼Œå‚æ•°ä¸€æ ·wï¼Œåˆ™zä¸€æ ·,symmetry breaking problemï¼‰ï¼›æ¥ä¸‹æ¥åå‘ä¼ æ’­æ—¶çš„åå¯¼æ•°ä¹Ÿä¸€æ ·ï¼Œä¸‹ä¸€è½®è¿­ä»£åŒä¸€å±‚çš„æ¯ä¸ªneuronçš„wåˆæ˜¯ä¸€æ ·çš„ã€‚è¿™æ ·æ•´ä¸ªneural Networkä¸Šæ¯ä¸€å±‚çš„neuronæ˜¯åŒè´¨çš„ï¼Œè‡ªç„¶ä¸ä¼šæœ‰å¥½çš„performanceã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week3_13 (1).png) ä¸è¿‡ï¼Œå¯¹bå‚æ•°ï¼Œå¯ä»¥éƒ½åˆå§‹åŒ–ä¸º0ã€‚ å¦å¤–éœ€è¦æ³¨æ„ï¼Œè™½ç„¶wæ˜¯éšæœºåˆå§‹åŒ–ï¼Œä½†æœ€å¥½ä½¿ç”¨è¾ƒå°çš„éšæœºæ•°ã€‚ä¸»è¦æ˜¯é¿å…è®©zçš„è®¡ç®—å€¼è¿‡å¤§ï¼Œå¯¼è‡´activation functionå¯¹zçš„åå¯¼æ•°è¶‹äº0ï¼Œå¯¼è‡´Gradient descentä¸‹é™è¾ƒæ…¢ã€‚ é€šå¸¸çš„åšæ³•æ˜¯å¯¹randomçš„å€¼ä¹˜ä»¥ä¸€ä¸ªæ¯”ç‡ï¼Œæ¯”å¦‚0.01ï¼ˆä½†å…·ä½“æ€ä¹ˆé€‰è¿™ä¸ªæ¯”ç‡ï¼Œä¹Ÿè¦æ ¹æ®æƒ…å†µè€Œå®šï¼Œè¿™åº”è¯¥åˆæ˜¯ä¸€ä¸ªè¶…å‚äº†ï¼‰ï¼š $W[1]=np.random.randn((2,2))âˆ—0.01$ å› ä¸ºå¦‚æœä½ ç”¨tanhæˆ–è€…sigmoidæ¿€æ´»å‡½æ•°ï¼Œæˆ–è€…è¯´åªåœ¨è¾“å‡ºå±‚æœ‰ä¸€ä¸ªSigmoidï¼Œå¦‚æœï¼ˆæ•°å€¼ï¼‰æ³¢åŠ¨å¤ªå¤§ï¼Œå½“ä½ è®¡ç®—æ¿€æ´»å€¼æ—¶å¦‚æœå¾ˆå¤§ï¼Œå°±ä¼šå¾ˆå¤§æˆ–è€…å¾ˆå°ï¼Œå› æ­¤è¿™ç§æƒ…å†µä¸‹ä½ å¾ˆå¯èƒ½åœåœ¨tanh/sigmoidå‡½æ•°çš„å¹³å¦çš„åœ°æ–¹ï¼Œè¿™äº›åœ°æ–¹æ¢¯åº¦å¾ˆå°ä¹Ÿå°±æ„å‘³ç€æ¢¯åº¦ä¸‹é™ä¼šå¾ˆæ…¢ï¼Œå› æ­¤å­¦ä¹ ä¹Ÿå°±å¾ˆæ…¢ã€‚ äº‹å®ä¸Šæœ‰æ—¶æœ‰æ¯”0.01æ›´å¥½çš„å¸¸æ•°ï¼Œå½“ä½ è®­ç»ƒä¸€ä¸ªåªæœ‰ä¸€å±‚éšè—å±‚çš„ç½‘ç»œæ—¶ï¼ˆè¿™æ˜¯ç›¸å¯¹æµ…çš„ç¥ç»ç½‘ç»œï¼Œæ²¡æœ‰å¤ªå¤šçš„éšè—å±‚ï¼‰ï¼Œè®¾ä¸º0.01å¯èƒ½ä¹Ÿå¯ä»¥ã€‚ä½†å½“ä½ è®­ç»ƒä¸€ä¸ªéå¸¸éå¸¸æ·±çš„ç¥ç»ç½‘ç»œï¼Œä½ å¯èƒ½è¦è¯•è¯•0.01ä»¥å¤–çš„å¸¸æ•°ã€‚ Â¶summary å¦‚ä½•å»ºç«‹ä¸€ä¸ªä¸€å±‚çš„ç¥ç»ç½‘ç»œäº†ï¼Œåˆå§‹åŒ–å‚æ•°ï¼Œç”¨å‰å‘ä¼ æ’­é¢„æµ‹ï¼Œè¿˜æœ‰è®¡ç®—å¯¼æ•°ï¼Œç»“åˆåå‘ä¼ æ’­ç”¨åœ¨æ¢¯åº¦ä¸‹é™ä¸­ã€‚ Define the neural network structure ( # of input units, # of hidden units, etc). Initialize the modelâ€™s parameters Loop: Implement forward propagation Compute loss Implement backward propagation to get the gradients Update parameters (gradient descent) C1W4 Â¶C1W4L01 Deep-layer neural network Â¶1. logistics regression and shallow neural network and deep-layer neural network ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_1.png) ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_2.png) Â¶2. notation ç¥ç»ç½‘ç»œæ¨¡å‹ $$ \begin{array}{l}{X \in \mathbb{R}^{n_{x} \times m}} ä»£è¡¨è¾“å…¥çš„çŸ©é˜µ\{x^{(i)} \in \mathbb{R}^{n_{x}}} ä»£è¡¨ç¬¬ i ä¸ªæ ·æœ¬çš„åˆ—å‘é‡\ {Y \in \mathbb{R}^{n_{y} \times n}} æ ‡è®°çŸ©é˜µ\ {y^{(i)} \in \mathbb{R}^{n_{v}}}æ˜¯ç¬¬iæ ·æœ¬çš„è¾“å‡ºæ ‡ç­¾\ W^{[l]} \in \mathbb{R}^{l \times(l-1)}ä»£è¡¨ç¬¬[l]å±‚çš„æƒé‡çŸ©é˜µ\ b^{[l]} \in \mathbb{R}^{l}ä»£è¡¨ç¬¬[l]å±‚çš„åå·®çŸ©é˜µ\ {\hat{y}^{(i)} \in \mathbb{R}^{n_{v}}}æ˜¯é¢„æµ‹è¾“å‡ºå‘é‡\end{array} $$ $$ é€šç”¨æ¿€æ´»å…¬å¼ï¼š a_{j}{[l]}=g{[l]}\left(z_{j}{[l]}\right)=g{ | l ]}\left(\sum_{k} w_{j k}^{[l]} a_{k}{[l-1]}+b_{j}{[l]}\right) $$ Â¶C1W4L02ï¼š Forward and Backward propagation Â¶1. forward propagation ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_3.png) Â¶2. backward propagation $$ \begin{array}{l}{d z^{[l]}=d a^{[l]} * g{[l]}\left(z{l l}\right)} \ {d w^{[l]}=d z^{[l]} \cdot a^{[l-1]}}\d b^{[l]}=d z^{[l]}\ d a{[l-1]}=w{[l]} \cdot d z^{[l]}\ d z{[l]}=w{[l+1] T} d z^{[l+1]} \cdot g{[l]{\prime}}\left(z^{[l]}\right)\end{array} $$ å‘é‡åŒ– $$ \begin{array}{l}{d Z^{[l]}=d A^{[l]} * g{[l]}\left(Z{[l]}\right)} \ {d W^{[l]}=\frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T}}\ \begin{array}{l}{d b^{[l]}=\frac{1}{m} n p \cdot \operatorname{sum}\left(d z^{[l]}, \text { axis }=1, \text {keepdims}=\text {True}\right)} \ {d A{[l-1]}=W{[l] T} \cdot d Z^{[l]}}\end{array}\end{array} $$ summary ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_5.png) Â¶C1W4L03 : Forward Propagation in d deep network ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_6.png) è¿™é‡Œåªèƒ½ç”¨ä¸€ä¸ªæ˜¾å¼forå¾ªç¯ï¼Œä»1åˆ°ï¼Œç„¶åä¸€å±‚æ¥ç€ä¸€å±‚å»è®¡ç®—ã€‚ Â¶C1W4L04 Getting matrix dimension right å½“å®ç°æ·±åº¦ç¥ç»ç½‘ç»œçš„æ—¶å€™ï¼Œå…¶ä¸­ä¸€ä¸ªå¸¸ç”¨çš„æ£€æŸ¥ä»£ç æ˜¯å¦æœ‰é”™çš„æ–¹æ³•å°±æ˜¯æ‹¿å‡ºä¸€å¼ çº¸è¿‡ä¸€éç®—æ³•ä¸­çŸ©é˜µçš„ç»´æ•°ã€‚ $d_w{[l]}$å’Œ$w{[l]}$ç»´åº¦ç›¸åŒï¼Œ$db{[l]}$å’Œ$b{[l]}$ç»´åº¦ç›¸åŒï¼Œä¸”wå’Œbå‘é‡åŒ–ç»´åº¦ä¸å˜ï¼Œä½†z,aä»¥åŠxçš„ç»´åº¦ä¼šå‘é‡åŒ–åå‘ç”Ÿå˜åŒ–ã€‚ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_8.png) åå‘ä¼ æ’­çš„ç»´æ•°æ£€æŸ¥ ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_7.png) åœ¨ä½ åšæ·±åº¦ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­æ—¶ï¼Œä¸€å®šè¦ç¡®è®¤æ‰€æœ‰çš„çŸ©é˜µç»´æ•°æ˜¯å‰åä¸€è‡´çš„ï¼Œå¯ä»¥å¤§å¤§æé«˜ä»£ç é€šè¿‡ç‡ã€‚ Â¶C1W4L05 Why deep representations? ç¥ç»ç½‘ç»œä¸éœ€è¦å¾ˆå¤§ï¼Œä½†æ˜¯å¾—æœ‰æ·±åº¦ï¼Œä¹Ÿå°±æ˜¯éšè—å±‚éœ€è¦å¾ˆå¤šï¼Œ Â¶1. for example of face detector ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_9.png) Â¶C1W4L06 :Building blocks of a deep neural network ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_10.png) ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_11.png) å¯ä»¥çœ‹å¾—å‡ºï¼Œå†åå‘ä¼ æ’­çš„æ—¶å€™ï¼Œéœ€è¦ç”¨åˆ°$Z{[L]},W{[L]},b^{[L]}$,å› æ­¤cash them æ­£å‘ä¼ æ’­ï¼š$Z{[1]},A{[1]}â€¦$,åå‘ä¼ æ’­ï¼š$dA{[L]},dZ{[L]},dW{[L]}dB{[L]},dA{[L-1]}$ Â¶C1W4L07ï¼šParameters vs Hyperparameters Â¶1 What ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_12.png) Â¶2 How ![](Deep Learning.ai_Neural Networks and Deep Learning\L1_week4_13.png) Ideaâ€”Codeâ€”Experimentâ€”Ideaè¿™ä¸ªå¾ªç¯ï¼Œå°è¯•å„ç§ä¸åŒçš„å‚æ•°ï¼Œå®ç°æ¨¡å‹å¹¶è§‚å¯Ÿæ˜¯å¦æˆåŠŸï¼Œç„¶åå†è¿­ä»£ ä»Šå¤©çš„æ·±åº¦å­¦ä¹ åº”ç”¨é¢†åŸŸï¼Œè¿˜æ˜¯å¾ˆç»éªŒæ€§çš„è¿‡ç¨‹ï¼Œé€šå¸¸ä½ æœ‰ä¸ªæƒ³æ³•ï¼Œæ¯”å¦‚ä½ å¯èƒ½å¤§è‡´çŸ¥é“ä¸€ä¸ªæœ€å¥½çš„å­¦ä¹ ç‡å€¼ï¼Œå¯èƒ½è¯´æœ€å¥½ï¼Œæˆ‘ä¼šæƒ³å…ˆè¯•è¯•çœ‹ï¼Œç„¶åä½ å¯ä»¥å®é™…è¯•ä¸€ä¸‹ï¼Œè®­ç»ƒä¸€ä¸‹çœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚ç„¶ååŸºäºå°è¯•çš„ç»“æœä½ ä¼šå‘ç°ï¼Œä½ è§‰å¾—å­¦ä¹ ç‡è®¾å®šå†æé«˜åˆ°0.05ä¼šæ¯”è¾ƒå¥½ã€‚å¦‚æœä½ ä¸ç¡®å®šä»€ä¹ˆå€¼æ˜¯æœ€å¥½çš„ï¼Œä½ å¤§å¯ä»¥å…ˆè¯•è¯•ä¸€ä¸ªå­¦ä¹ ç‡ï¼Œå†çœ‹çœ‹æŸå¤±å‡½æ•°Jçš„å€¼æœ‰æ²¡æœ‰ä¸‹é™ã€‚ç„¶åä½ å¯ä»¥è¯•ä¸€è¯•å¤§ä¸€äº›çš„å€¼ï¼Œç„¶åå‘ç°æŸå¤±å‡½æ•°çš„å€¼å¢åŠ å¹¶å‘æ•£äº†ã€‚ç„¶åå¯èƒ½è¯•è¯•å…¶ä»–æ•°ï¼Œçœ‹ç»“æœæ˜¯å¦ä¸‹é™çš„å¾ˆå¿«æˆ–è€…æ”¶æ•›åˆ°åœ¨æ›´é«˜çš„ä½ç½®ã€‚ä½ å¯èƒ½å°è¯•ä¸åŒçš„å¹¶è§‚å¯ŸæŸå¤±å‡½æ•°è¿™ä¹ˆå˜äº†ï¼Œè¯•è¯•ä¸€ç»„å€¼ï¼Œç„¶åå¯èƒ½æŸå¤±å‡½æ•°å˜æˆè¿™æ ·ï¼Œè¿™ä¸ªå€¼ä¼šåŠ å¿«å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶ä¸”æ”¶æ•›åœ¨æ›´ä½çš„æŸå¤±å‡½æ•°å€¼ä¸Šï¼ˆç®­å¤´æ ‡è¯†ï¼‰ï¼Œæˆ‘å°±ç”¨è¿™ä¸ªå€¼äº†ã€‚ åœ¨å‰é¢å‡ é¡µä¸­ï¼Œè¿˜æœ‰å¾ˆå¤šä¸åŒçš„è¶…å‚æ•°ã€‚ç„¶è€Œï¼Œå½“ä½ å¼€å§‹å¼€å‘æ–°åº”ç”¨æ—¶ï¼Œé¢„å…ˆå¾ˆéš¾ç¡®åˆ‡çŸ¥é“ï¼Œç©¶ç«Ÿè¶…å‚æ•°çš„æœ€ä¼˜å€¼åº”è¯¥æ˜¯ä»€ä¹ˆã€‚æ‰€ä»¥é€šå¸¸ï¼Œä½ å¿…é¡»å°è¯•å¾ˆå¤šä¸åŒçš„å€¼ï¼Œå¹¶èµ°è¿™ä¸ªå¾ªç¯ï¼Œè¯•è¯•å„ç§å‚æ•°ã€‚è¯•è¯•çœ‹5ä¸ªéšè—å±‚ï¼Œè¿™ä¸ªæ•°ç›®çš„éšè—å•å…ƒï¼Œå®ç°æ¨¡å‹å¹¶è§‚å¯Ÿæ˜¯å¦æˆåŠŸï¼Œç„¶åå†è¿­ä»£ã€‚è¿™é¡µçš„æ ‡é¢˜æ˜¯ï¼Œåº”ç”¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œä¸€ä¸ªå¾ˆå¤§ç¨‹åº¦åŸºäºç»éªŒçš„è¿‡ç¨‹ï¼Œå‡­ç»éªŒçš„è¿‡ç¨‹é€šä¿—æ¥è¯´ï¼Œå°±æ˜¯è¯•ç›´åˆ°ä½ æ‰¾åˆ°åˆé€‚çš„æ•°å€¼ã€‚ æ‰€ä»¥æˆ‘ç»å¸¸å»ºè®®äººä»¬ï¼Œç‰¹åˆ«æ˜¯åˆšå¼€å§‹åº”ç”¨äºæ–°é—®é¢˜çš„äººä»¬ï¼Œå»è¯•ä¸€å®šèŒƒå›´çš„å€¼çœ‹çœ‹ç»“æœå¦‚ä½•ã€‚ç„¶åä¸‹ä¸€é—¨è¯¾ç¨‹ï¼Œæˆ‘ä»¬ä¼šç”¨æ›´ç³»ç»Ÿçš„æ–¹æ³•ï¼Œç”¨ç³»ç»Ÿæ€§çš„å°è¯•å„ç§è¶…å‚æ•°å–å€¼ã€‚ç„¶åå…¶æ¬¡ï¼Œç”šè‡³æ˜¯ä½ å·²ç»ç”¨äº†å¾ˆä¹…çš„æ¨¡å‹ï¼Œå¯èƒ½ä½ åœ¨åšç½‘ç»œå¹¿å‘Šåº”ç”¨ï¼Œåœ¨ä½ å¼€å‘é€”ä¸­ï¼Œå¾ˆæœ‰å¯èƒ½å­¦ä¹ ç‡çš„æœ€ä¼˜æ•°å€¼æˆ–æ˜¯å…¶ä»–è¶…å‚æ•°çš„æœ€ä¼˜å€¼æ˜¯ä¼šå˜çš„ï¼Œæ‰€ä»¥å³ä½¿ä½ æ¯å¤©éƒ½åœ¨ç”¨å½“å‰æœ€ä¼˜çš„å‚æ•°è°ƒè¯•ä½ çš„ç³»ç»Ÿï¼Œä½ è¿˜æ˜¯ä¼šå‘ç°ï¼Œæœ€ä¼˜å€¼è¿‡ä¸€å¹´å°±ä¼šå˜åŒ–ï¼Œå› ä¸ºç”µè„‘çš„åŸºç¡€è®¾æ–½ï¼ŒCPUæˆ–æ˜¯GPUå¯èƒ½ä¼šå˜åŒ–å¾ˆå¤§ã€‚æ‰€ä»¥æœ‰ä¸€æ¡ç»éªŒè§„å¾‹å¯èƒ½æ¯å‡ ä¸ªæœˆå°±ä¼šå˜ã€‚å¦‚æœä½ æ‰€è§£å†³çš„é—®é¢˜éœ€è¦å¾ˆå¤šå¹´æ—¶é—´ï¼Œåªè¦ç»å¸¸è¯•è¯•ä¸åŒçš„è¶…å‚æ•°ï¼Œå‹¤äºæ£€éªŒç»“æœï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰æ›´å¥½çš„è¶…å‚æ•°æ•°å€¼ï¼Œç›¸ä¿¡ä½ æ…¢æ…¢ä¼šå¾—åˆ°è®¾å®šè¶…å‚æ•°çš„ç›´è§‰ï¼ŒçŸ¥é“ä½ çš„é—®é¢˜æœ€å¥½ç”¨ä»€ä¹ˆæ•°å€¼ã€‚ æœ‰ä¸€æ¡ç»éªŒè§„å¾‹ï¼šç»å¸¸è¯•è¯•ä¸åŒçš„è¶…å‚æ•°ï¼Œå‹¤äºæ£€æŸ¥ç»“æœï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰æ›´å¥½çš„è¶…å‚æ•°å–å€¼ï¼Œä½ å°†ä¼šå¾—åˆ°è®¾å®šè¶…å‚æ•°çš„ç›´è§‰ã€‚ æ€»ç»“ï¼šè¶…å‚æ•°çš„è®¾å®šï¼Œé ç»éªŒï¼Œå°è¯•ï¼Œå¹¶è°ƒï¼Œæ ¹æ®ç»“æœè°ƒï¼Œ Â¶C1W4L08 : What does this have to do with the brain? # summary : forward prop and back prop Â¶1. logistics regression,shallow neural network and deep neural network logistics regression $$ Z = W^TX+B\ A = \frac{1}{1+e^{-Z}}\ L(A,Y)=-\frac{1}{m}(YlogA+(1-Y)log{1-A}\ \frac{\partial L}{\partial Z}=(A-Y)\ \frac{\partial L}{\partial W}=X(A-Y)\ $$ è¯´æ˜ï¼šXæ˜¯æ ·æœ¬æŒ‰åˆ—å †ç§¯ï¼ŒWæ˜¯åˆ—å‘é‡ shallow neural network ä»¥äºŒåˆ†é—®é¢˜ä¸ºä¾‹ $$ Z{[1]}=W{[1]}A{[0]}+b{[1]}\ A{[1]}=g{[1]}(Z^{[1]})\ \ \ Z{[2]}=W{[2]}A{[1]}+b{[2]}\ A{[2]}=g{[2]}(Z^{[2]})\ \ \ \ \ \ L(A{[2]},Y)=-\frac{1}{m}(Ylog{A}+(1-Y)log^{1-A})\ \frac{\partial L}{\partial Z{[2]}}=(A{[2]}-Y)\ \frac{\partial L}{\partial W{[2]}}=(A{[2]}-Y)A{[1]T}\ \frac{\partial L}{\partial b{[2]}}=(A{[2]}-Y)1_{1m}^T\ \frac{\partial L}{\partial a{[1]}}=W{[2]T}(A{[2]}-Y)\ \ \ \frac{\partial L}{\partial Z{[1]}}=W{[2]T}(A{[2]}-Y) g{â€™[1]}(Z{[1]})\ $$ è¯´æ˜ï¼šWæ˜¯æŒ‰åˆ—æ’$W{[L]}$æ˜¯$n{[L]}*n^{[L-1]}$çŸ©é˜µï¼ŒA,Zæ˜¯æŒ‰åˆ—å †ç§¯ï¼Œè®°å¾—æ£€æŸ¥çŸ©é˜µç»´æ•°å°±å¥½äº† Â¶deep neural network $$ Z{[l]}=W{[l]}A{[l-1]}+b{[l]}\ A{[l]}=g{[l]}(Z^{[l]})\ \ \ \ \ \ \frac{\partial L}{\partial Z^{[l]}}=\partial A&lt;!â€“ï¿¼0â€“&gt;*g{â€™[l]}(Z^{l})\ \frac{\partial L}{\partial W^{[l]}}=\partial Z^{[l]} A{[1-1]T}\ \frac{\partial L}{\partial b^{[l]}}=\partial Z^{[l]}\ \frac{\partial L}{\partial a{[l-1]}}=W{[l]^T}\partial Z^{[l]}\ \ \ \frac{\partial L}{\partial Z{[l-1]}}=W{[l]^T}\partial Z^{[l]}* g{â€™[l-1]}(Z{[l-1]})\ $$ Â¶2. vectorization æ¨å¯¼çš„æ—¶å€™è¦å‘é‡åŒ–ï¼Œæ³¨æ„çŸ©é˜µç»´æ•°è¡¨ç¤ºï¼Œå¯ä»¥ä»å•ä¸ªæ¨å¯¼åˆ°mutli å……åˆ†åˆ©ç”¨pythonçš„å¹¿æ’­å±æ€§ï¼Œå’Œå†…ç½®å‡½æ•°çš„å¹¶è¡ŒåŒ– pythonä¸€ç»´ï¼ŒäºŒç»´æ•°ç»„çš„ç‰¹æ€§ Â¶3. çŸ¥è¯†ç»“æ„ ![](Deep Learning.ai_Neural Networks and Deep Learning\C1.png)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep_learning.aiæ·±åº¦å­¦ä¹ ç¬”è®°]]></title>
    <url>%2F2019%2F04%2F11%2Fdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[C5: Sequence Models Â¶W1 : Recurrent Neural Networks (å¾ªç¯åºåˆ—æ¨¡å‹) Â¶L1 ï¼š Why Sequence Models? å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ä¹‹ç±»çš„æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå…¶ä»–é¢†åŸŸä¸­å¼•èµ·å˜é©ã€‚ åºåˆ—æ¨¡å‹çš„åˆ—å­ Â¶L2 : Notation æ•°å­¦ç¬¦å· NLP æˆ‘ä»¬ç”¨$X{(i)}$æ¥è¡¨ç¤ºç¬¬ä¸ªiè®­ç»ƒæ ·æœ¬ï¼Œæ‰€ä»¥ä¸ºäº†æŒ‡ä»£ç¬¬ä¸ªtå…ƒç´ ï¼Œæˆ–è€…è¯´æ˜¯è®­ç»ƒæ ·æœ¬içš„åºåˆ—ä¸­ç¬¬tä¸ªå…ƒç´ ç”¨$X{(i)}$è¿™ä¸ªç¬¦å·æ¥è¡¨ç¤ºã€‚å¦‚æœæ˜¯åºåˆ—é•¿åº¦$T_x$ï¼Œé‚£ä¹ˆä½ çš„è®­ç»ƒé›†é‡Œä¸åŒçš„è®­ç»ƒæ ·æœ¬å°±ä¼šæœ‰ä¸åŒçš„é•¿åº¦ï¼Œæ‰€ä»¥$T_x{(i)}$å°±ä»£è¡¨ç¬¬ä¸ªè®­ç»ƒæ ·æœ¬çš„è¾“å…¥åºåˆ—é•¿åº¦ã€‚åŒæ ·$y{(i)}$ä»£è¡¨ç¬¬iä¸ªè®­ç»ƒæ ·æœ¬ä¸­ç¬¬tä¸ªå…ƒç´ ï¼Œ$T_y^{(i)}$å°±æ˜¯ç¬¬iä¸ªè®­ç»ƒæ ·æœ¬çš„è¾“å‡ºåºåˆ—çš„é•¿åº¦ã€‚ é¢„å…ˆæœ‰ä¸€ä¸ªè¯å…¸ Â¶L3 : Recurrent Neural Network Model (å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹) ç°åœ¨æˆ‘ä»¬è®¨è®ºä¸€ä¸‹æ€æ ·æ‰èƒ½å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œå»ºç«‹ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥å­¦ä¹ Xåˆ°Yçš„æ˜ å°„ $a^{&lt;0&gt;}$é€šå¸¸ æ˜¯é›¶å‘é‡ Næ¨¡å‹åŒ…å«ä¸‰ç±»æƒé‡ç³»æ•°ï¼Œåˆ†åˆ«æ˜¯Waxï¼ŒWaaï¼ŒWyaã€‚ä¸”ä¸åŒå…ƒç´ ä¹‹é—´åŒä¸€ä½ç½®å…±äº«åŒä¸€æƒé‡ç³»æ•°ã€‚ RNNçš„æ­£å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰è¿‡ç¨‹ä¸ºï¼š å¾ªç¯ç¥ç»ç½‘ç»œç”¨çš„æ¿€æ´»å‡½æ•°ç»å¸¸æ˜¯tanhï¼Œä¸è¿‡æœ‰æ—¶å€™ä¹Ÿä¼šç”¨ReLUï¼Œä½†æ˜¯tanhæ˜¯æ›´é€šå¸¸çš„é€‰æ‹©ï¼Œæˆ‘ä»¬æœ‰å…¶ä»–æ–¹æ³•æ¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œæˆ‘ä»¬å°†åœ¨ä¹‹åè¿›è¡Œè®²è¿°ã€‚é€‰ç”¨å“ªä¸ªæ¿€æ´»å‡½æ•°æ˜¯å–å†³äºä½ çš„è¾“å‡ºyï¼Œå¦‚æœå®ƒæ˜¯ä¸€ä¸ªäºŒåˆ†é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘çŒœä½ ä¼šç”¨sigmoidå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œå¦‚æœæ˜¯kç±»åˆ«åˆ†ç±»é—®é¢˜çš„è¯ï¼Œé‚£ä¹ˆå¯ä»¥é€‰ç”¨softmaxä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚ä¸è¿‡è¿™é‡Œæ¿€æ´»å‡½æ•°çš„ç±»å‹å–å†³äºä½ æœ‰ä»€ä¹ˆæ ·ç±»å‹çš„è¾“å‡ºyï¼Œå¯¹äºå‘½åå®ä½“è¯†åˆ«æ¥è¯´yåªå¯èƒ½æ˜¯0æˆ–è€…1ï¼Œé‚£æˆ‘çŒœè¿™é‡Œç¬¬äºŒä¸ªæ¿€æ´»å‡½æ•°gå¯ä»¥æ˜¯sigmoidæ¿€æ´»å‡½æ•°ã€‚ Â¶c4: Backpropagation through time ( é€šè¿‡æ—¶é—´çš„åå‘ä¼ æ’­) å‚æ•°çš„å…³ç³»* å•ä¸ªå…ƒç´ çš„Loss function: è¯¥æ ·æœ¬æ‰€æœ‰å…ƒç´ çš„Loss functionä¸ºï¼š ç„¶åï¼Œåå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰è¿‡ç¨‹å°±æ˜¯ä»å³åˆ°å·¦åˆ†åˆ«è®¡ç®—L(y^,y)å¯¹å‚æ•°Waï¼ŒWyï¼Œbaï¼Œbyçš„åå¯¼æ•°ã€‚æ€è·¯ä¸åšæ³•ä¸æ ‡å‡†çš„ç¥ç»ç½‘ç»œæ˜¯ä¸€æ ·çš„ã€‚ä¸€èˆ¬å¯ä»¥é€šè¿‡æˆç†Ÿçš„æ·±åº¦å­¦ä¹ æ¡†æ¶è‡ªåŠ¨æ±‚å¯¼ï¼Œä¾‹å¦‚PyTorchã€Tensorflowç­‰ã€‚è¿™ç§ä»å³åˆ°å·¦çš„æ±‚å¯¼è¿‡ç¨‹è¢«ç§°ä¸ºBackpropagation through time Â¶L5: Different types of RNNs (ä¸åŒç±»å‹çš„å¾ªç¯ç¥ç»ç½‘ç»œ) Â¶L6 : Language model and sequence generation (è¯­è¨€æ¨¡å‹å’Œåºåˆ—ç”Ÿæˆ) Â¶L7 : Sampling novel sequences (å¯¹æ–°åºåˆ—é‡‡æ ·) Â¶Vanishing gradients with RNNs (å¾ªç¯ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±) é¦–å…ˆä»å·¦åˆ°å³å‰å‘ä¼ æ’­ï¼Œç„¶ååå‘ä¼ æ’­ã€‚ä½†æ˜¯åå‘ä¼ æ’­ä¼šå¾ˆå›°éš¾ï¼Œå› ä¸ºåŒæ ·çš„æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œåé¢å±‚çš„è¾“å‡ºè¯¯å·®ï¼ˆä¸Šå›¾ç¼–å·6æ‰€ç¤ºï¼‰å¾ˆéš¾å½±å“å‰é¢å±‚ï¼ˆä¸Šå›¾ç¼–å·7æ‰€ç¤ºçš„å±‚ï¼‰çš„è®¡ç®—ã€‚è¿™å°±æ„å‘³ç€ï¼Œå®é™…ä¸Šå¾ˆéš¾è®©ä¸€ä¸ªç¥ç»ç½‘ç»œèƒ½å¤Ÿæ„è¯†åˆ°å®ƒè¦è®°ä½çœ‹åˆ°çš„æ˜¯å•æ•°åè¯è¿˜æ˜¯å¤æ•°åè¯ï¼Œç„¶ååœ¨åºåˆ—åé¢ç”Ÿæˆä¾èµ–å•å¤æ•°å½¢å¼çš„wasæˆ–è€…wereã€‚è€Œä¸”åœ¨è‹±è¯­é‡Œé¢ï¼Œè¿™ä¸­é—´çš„å†…å®¹ï¼ˆä¸Šå›¾ç¼–å·8æ‰€ç¤ºï¼‰å¯ä»¥ä»»æ„é•¿ï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥ä½ éœ€è¦é•¿æ—¶é—´è®°ä½å•è¯æ˜¯å•æ•°è¿˜æ˜¯å¤æ•°ï¼Œè¿™æ ·åé¢çš„å¥å­æ‰èƒ½ç”¨åˆ°è¿™äº›ä¿¡æ¯ã€‚ä¹Ÿæ­£æ˜¯è¿™ä¸ªåŸå› ï¼Œæ‰€ä»¥åŸºæœ¬çš„RNNæ¨¡å‹ä¼šæœ‰å¾ˆå¤šå±€éƒ¨å½±å“ http://www.ai-start.com/dl2017/html/lesson5-week1.html https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect]]></content>
      <categories>
        <category>å­¦ä¹ ã®å†ç¨‹(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>æˆ‘çš„è¯»ä¹¦ç¬”è®°</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[[^]: [TOC] åˆ©ç”¨å¬åŠ›ææ–™å­¦è‹±è¯­æœ€é«˜æ•ˆ æœ‰æ•ˆè¾“å‡ºåº”è¯¥æ˜¯ç¨é«˜äºç°æœ‰æ°´å¹³çš„ã€æ›´å‡†ç¡®ã€æ›´å¾—ä½“çš„è¡¨è¾¾è¾“å‡º å…ˆè®°ä½ä»¥ä¸‹ä¸¤ç‚¹ï¼Œä¸‹é¢æˆ‘ä»¬å†ä¸€ä¸€è§£æåº”è¯¥è¦æ€ä¹ˆåšåˆ°ã€‚ 1. å­¦ä¹ éœ€è¦åé¦ˆï¼Œæ¥å‘Šè¯‰æˆ‘ä»¬è¿™è¾“å‡ºæ˜¯æ­£ç¡®çš„ï¼Œå¯ä»¥ç»§ç»­ï¼›æˆ–è€…æ˜¯é”™è¯¯çš„ï¼Œéœ€è¦ä¿®æ­£ã€‚ 2. å°†è¾“å…¥å†…åŒ–ï¼Œå˜æˆè‡ªå·±çš„çŸ¥è¯†ã€‚ æˆ‘ä¸€èˆ¬ä¼šå¬å››éï¼Œä½†ä¸æ˜¯å…¨å¬åŸæ–‡ã€‚ **ç¬¬ä¸€éï¼šæ³›å¬åŸæ–‡ï¼Œä¸è¦çœ‹å­—å¹•æˆ–è„šæœ¬ï¼Œæ¸…æ¥šå½•éŸ³çš„å†…å®¹ã€‚**å¬ç¬¬ä¸€éçš„æ—¶å€™æˆ‘é€šå¸¸è¿ç¬”è®°éƒ½ä¸åšï¼Œç›®çš„æ˜¯ä¸ºäº†è®©è‡ªå·±æµç•…çš„å¬å®Œï¼Œå¯¹å¬åŠ›çš„å†…å®¹æœ‰ä¸€ä¸ªæ•´ä½“çš„æŒæ¡ã€‚ ç¬¬äºŒéï¼šå…ˆå¬åŸæ–‡ï¼Œæ ¹æ®å†…å®¹æ®µè½ï¼Œå¼€å§‹å¤è¿°å†…å®¹ï¼Œå¹¶å½•éŸ³ã€‚è¿™ä¸€æ­¥çš„ç›®çš„æ˜¯å¼ºè¿«è‡ªå·±è°ƒç”¨å·²ç»å­¦è¿‡çš„çŸ¥è¯†ï¼Œç»„ç»‡è¯­è¨€å’Œè¿›è¡Œç»ƒä¹ ã€‚ **ç¬¬ä¸‰éï¼Œå¬è‡ªå·±çš„å½•éŸ³ï¼Œç„¶ååšå‡ºä¿®æ­£ã€‚**è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼Œå¯ä»¥è®©ä½ äº†è§£è‡ªå·±çš„å‘éŸ³é—®é¢˜å’Œè¯­æ³•é—®é¢˜ï¼Œå¹¶æŠŠå¯å¬å‡ºæ¥çš„è¯­æ³•é—®é¢˜è¿›è¡Œä¿®æ”¹ã€‚é€šè¿‡è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠç®€å•é‡å¤è¾“å…¥çš„è¯­è¨€ææ–™ï¼Œè½¬åŒ–ä¸ºæœ‰æ•ˆè¾“å‡ºã€‚ ç¬¬å››éï¼Œå¬åŸæ–‡çœ‹å­—å¹•å’Œè„šæœ¬ï¼Œçœ‹æŠŠå¬ä¸æ‡‚çš„åœ°æ–¹æ ‡æ³¨ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆå¬ä¸æ‡‚ï¼ˆæ¯”å¦‚æ˜¯å› ä¸ºè‡ªå·±å‘éŸ³ä¸å‡†å¯¼è‡´çš„å¬ä¸å‡ºï¼Œæˆ–è€…å°±æ˜¯å› ä¸ºè¿™ä¸ªè¯æ²¡èƒŒè¿‡ã€ä¸ç†Ÿæ‚‰ï¼‰ã€‚ä¸ç†Ÿæ‚‰çš„ç”¨æ³•å’Œè‡ªå·±ç”¨é”™çš„åœ°æ–¹æ€»ç»“ï¼ŒèƒŒä¸‹æ¥ï¼Œä¸‹æ¬¡è¯•ç€ç”¨ã€‚ Tipsï¼š ä¸è¦é€‰æ‹©å¤ªéš¾çš„ææ–™ï¼Œå¤ªéš¾çš„ææ–™å®¹æ˜“ä½¿è‡ªå·±ä¸§å¤±å­¦ä¹ å…´è¶£ã€‚ ä¸€å¼€å§‹ï¼Œä¸è¦é€‰æ‹©å¤ªé•¿çš„å¬åŠ›ææ–™ã€‚10åˆ†é’Ÿå·¦å³æœ€ä½³ã€‚åœ¨è¿™é‡Œæ¨ètedï¼Œå¯ä»¥é€‰æ‹©æœ‰å­—å¹•æˆ–å…³é—­å­—å¹•ã€‚ åœ¨ä¸€ä¸ªç›¸è¿‘çš„æ—¶é—´æ®µå†…ï¼Œé€‰æ‹©ç›¸è¿‘é¢˜æçš„ææ–™ã€‚æ¯”å¦‚æˆ‘ä¼šåœ¨ä¸¤ä¸ªæ˜ŸæœŸå†…é€‰æ‹©â€œå¿ƒç†â€é¢˜æçš„å½•éŸ³ã€‚è¿™æ ·æˆ‘å°±ä¼šæœ‰æ›´å¤§çš„å‡ ç‡ç”¨ä¸Šåˆšå­¦è¿‡çš„ç»“æ„å’Œè¯æ±‡ã€‚ åŠæ—¶æ€»ç»“ï¼ŒåŠæ—¶å¤ä¹ å·²èƒŒè¿‡çš„ææ–™ï¼Œå¤ä¹ çš„é‡è¦æ€§å¤§å®¶éƒ½æ‡‚ï¼Œè¿™é‡Œå°±ä¸å¤šè¯´äº†ã€‚ çœŸé¢˜å¬å†™ ææ–™é€‰æ‹© èƒ½å¤Ÿå¬å¾—æ‡‚ 70%çš„ææ–™ 2 å…·ä½“æ‰§è¡Œæ–¹æ³• å…ˆæ³›å¬ä¸€ç¯‡ å†å¾ªç¯å¬å‡ é å†é€å¥é€å¥çš„å¬ Â¶ç¾å‰§ç²¾å¬ å…ˆçœ‹ä¸­æ–‡å¬ è‹±æ–‡ï¼ŒæŸ¥ å¬æ‰¾ å°è¯ï¼Œè·Ÿè¯» é‡å¤ä¸‰å›› è‡³å°‘10]]></content>
  </entry>
  <entry>
    <title><![CDATA[deeplearningvideo]]></title>
    <url>%2F2019%2F04%2F03%2Fdeeplearningvideo%2F</url>
    <content type="text"><![CDATA[Courseraæ·±åº¦å­¦ä¹ æ•™ç¨‹ä¸­æ–‡ç¬”è®° è¯¾ç¨‹æ¦‚è¿° https://mooc.study.163.com/university/deeplearning_ai#/c è¿™äº›è¯¾ç¨‹ä¸“ä¸ºå·²æœ‰ä¸€å®šåŸºç¡€ï¼ˆåŸºæœ¬çš„ç¼–ç¨‹çŸ¥è¯†ï¼Œç†Ÿæ‚‰Pythonã€å¯¹æœºå™¨å­¦ä¹ æœ‰åŸºæœ¬äº†è§£ï¼‰ï¼Œæƒ³è¦å°è¯•è¿›å…¥äººå·¥æ™ºèƒ½é¢†åŸŸçš„è®¡ç®—æœºä¸“ä¸šäººå£«å‡†å¤‡ã€‚ä»‹ç»æ˜¾ç¤ºï¼šâ€œæ·±åº¦å­¦ä¹ æ˜¯ç§‘æŠ€ä¸šæœ€çƒ­é—¨çš„æŠ€èƒ½ä¹‹ä¸€ï¼Œæœ¬è¯¾ç¨‹å°†å¸®ä½ æŒæ¡æ·±åº¦å­¦ä¹ ã€‚â€ åœ¨è¿™5å ‚è¯¾ä¸­ï¼Œå­¦ç”Ÿå°†å¯ä»¥å­¦ä¹ åˆ°æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œå­¦ä¼šæ„å»ºç¥ç»ç½‘ç»œï¼Œå¹¶ç”¨åœ¨åŒ…æ‹¬å´æ©è¾¾æœ¬äººåœ¨å†…çš„å¤šä½ä¸šç•Œé¡¶å°–ä¸“å®¶æŒ‡å¯¼ä¸‹åˆ›å»ºè‡ªå·±çš„æœºå™¨å­¦ä¹ é¡¹ç›®ã€‚Deep Learning Specializationå¯¹å·ç§¯ç¥ç»ç½‘ç»œ (CNN)ã€é€’å½’ç¥ç»ç½‘ç»œ (RNN)ã€é•¿çŸ­æœŸè®°å¿† (LSTM) ç­‰æ·±åº¦å­¦ä¹ å¸¸ç”¨çš„ç½‘ç»œç»“æ„ã€å·¥å…·å’ŒçŸ¥è¯†éƒ½æœ‰æ¶‰åŠã€‚ ç¬”è®°æ˜¯æ ¹æ®è§†é¢‘å’Œå­—å¹•å†™çš„ï¼Œæ²¡æœ‰æŠ€æœ¯å«é‡ï¼Œåªéœ€è¦ä¸“æ³¨å’Œä¸¥è°¨ã€‚ 2018-04-14 æœ¬è¯¾ç¨‹è§†é¢‘æ•™ç¨‹åœ°å€ï¼šhttps://mooc.study.163.com/university/deeplearning_ai#/c ï¼ˆè¯¥è§†é¢‘ä»www.deeplearning.ai ç½‘ç«™ä¸‹è½½ï¼Œå› ä¼—æ‰€å‘¨çŸ¥çš„åŸå› ï¼Œå›½å†…ç”¨æˆ·è§‚çœ‹æŸäº›åœ¨çº¿è§†é¢‘éå¸¸ä¸å®¹æ˜“ï¼Œæ•…ä¸€äº›å­¦è€…ä¸€èµ·åˆ¶ä½œäº†ç¦»çº¿è§†é¢‘ï¼Œæ—¨åœ¨æ–¹ä¾¿å›½å†…ç”¨æˆ·ä¸ªäººå­¦ä¹ ä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºå•†ä¸šç”¨é€”ã€‚è§†é¢‘å†…åµŒä¸­è‹±æ–‡å­—å¹•ï¼Œæ¨èä½¿ç”¨potplayeræ’­æ”¾ã€‚ç‰ˆæƒå±äºå´æ©è¾¾è€å¸ˆæ‰€æœ‰ï¼Œè‹¥åœ¨çº¿è§†é¢‘æµç•…ï¼Œè¯·åˆ°å®˜æ–¹ç½‘ç«™è§‚çœ‹ã€‚ï¼‰ ç¬”è®°ç½‘ç«™(é€‚åˆæ‰‹æœºé˜…è¯») å´æ©è¾¾è€å¸ˆçš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ç¬”è®°å’Œè§†é¢‘ï¼šhttps://github.com/fengdu78/Coursera-ML-AndrewNg-Notes æ·±åº¦å­¦ä¹ ç¬”è®°ç›®å½• Â¶ç¬¬ä¸€é—¨è¯¾ ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ (Neural Networks and Deep Learning) ç¬¬ä¸€å‘¨ï¼šæ·±åº¦å­¦ä¹ å¼•è¨€(Introduction to Deep Learning) 1.1 æ¬¢è¿(Welcome) 1.2 ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ(What is a Neural Network) 1.3 ç¥ç»ç½‘ç»œçš„ç›‘ç£å­¦ä¹ (Supervised Learning with Neural Networks) 1.4 ä¸ºä»€ä¹ˆç¥ç»ç½‘ç»œä¼šæµè¡Œï¼Ÿ(Why is Deep Learning taking off?) 1.5 å…³äºæœ¬è¯¾ç¨‹(About this Course) 1.6 è¯¾ç¨‹èµ„æº(Course Resources) 1.7 Geoffery Hinton ä¸“è®¿(Geoffery Hinton interview) ç¬¬äºŒå‘¨ï¼šç¥ç»ç½‘ç»œçš„ç¼–ç¨‹åŸºç¡€(Basics of Neural Network programming) 2.1 äºŒåˆ†ç±»(Binary Classification) 2.2 é€»è¾‘å›å½’(Logistic Regression) 2.3 é€»è¾‘å›å½’çš„ä»£ä»·å‡½æ•°ï¼ˆLogistic Regression Cost Functionï¼‰ 2.4 æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰ 2.5 å¯¼æ•°ï¼ˆDerivativesï¼‰ 2.6 æ›´å¤šçš„å¯¼æ•°ä¾‹å­ï¼ˆMore Derivative Examplesï¼‰ 2.7 è®¡ç®—å›¾ï¼ˆComputation Graphï¼‰ 2.8 è®¡ç®—å›¾å¯¼æ•°ï¼ˆDerivatives with a Computation Graphï¼‰ 2.9 é€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™ï¼ˆLogistic Regression Gradient Descentï¼‰ 2.10 æ¢¯åº¦ä¸‹é™çš„ä¾‹å­(Gradient Descent on m Examples) 2.11 å‘é‡åŒ–(Vectorization) 2.12 æ›´å¤šçš„å‘é‡åŒ–ä¾‹å­ï¼ˆMore Examples of Vectorizationï¼‰ 2.13 å‘é‡åŒ–é€»è¾‘å›å½’(Vectorizing Logistic Regression) 2.14 å‘é‡åŒ–é€»è¾‘å›å½’çš„æ¢¯åº¦è®¡ç®—ï¼ˆVectorizing Logistic Regressionâ€™s Gradientï¼‰ 2.15 Pythonä¸­çš„å¹¿æ’­æœºåˆ¶ï¼ˆBroadcasting in Pythonï¼‰ 2.16 å…³äº Pythonä¸numpyå‘é‡çš„ä½¿ç”¨ï¼ˆA note on python or numpy vectorsï¼‰ 2.17 Jupyter/iPython Notebookså¿«é€Ÿå…¥é—¨ï¼ˆQuick tour of Jupyter/iPython Notebooksï¼‰ 2.18 é€»è¾‘å›å½’æŸå¤±å‡½æ•°è¯¦è§£ï¼ˆExplanation of logistic regression cost functionï¼‰ ç¬¬ä¸‰å‘¨ï¼šæµ…å±‚ç¥ç»ç½‘ç»œ(Shallow neural networks) 3.1 ç¥ç»ç½‘ç»œæ¦‚è¿°ï¼ˆNeural Network Overviewï¼‰ 3.2 ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºï¼ˆNeural Network Representationï¼‰ 3.3 è®¡ç®—ä¸€ä¸ªç¥ç»ç½‘ç»œçš„è¾“å‡ºï¼ˆComputing a Neural Networkâ€™s outputï¼‰ 3.4 å¤šæ ·æœ¬å‘é‡åŒ–ï¼ˆVectorizing across multiple examplesï¼‰ 3.5 å‘é‡åŒ–å®ç°çš„è§£é‡Šï¼ˆJustification for vectorized implementationï¼‰ 3.6 æ¿€æ´»å‡½æ•°ï¼ˆActivation functionsï¼‰ 3.7 ä¸ºä»€ä¹ˆéœ€è¦éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Ÿï¼ˆwhy need a nonlinear activation function?ï¼‰ 3.8 æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ï¼ˆDerivatives of activation functionsï¼‰ 3.9 ç¥ç»ç½‘ç»œçš„æ¢¯åº¦ä¸‹é™ï¼ˆGradient descent for neural networksï¼‰ 3.10ï¼ˆé€‰ä¿®ï¼‰ç›´è§‚ç†è§£åå‘ä¼ æ’­ï¼ˆBackpropagation intuitionï¼‰ 3.11 éšæœºåˆå§‹åŒ–ï¼ˆRandom+Initializationï¼‰ ç¬¬å››å‘¨ï¼šæ·±å±‚ç¥ç»ç½‘ç»œ(Deep Neural Networks) 4.1 æ·±å±‚ç¥ç»ç½‘ç»œï¼ˆDeep L-layer neural networkï¼‰ 4.2 å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼ˆForward and backward propagationï¼‰ 4.3 æ·±å±‚ç½‘ç»œä¸­çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼ˆForward propagation in a Deep Networkï¼‰ 4.4 æ ¸å¯¹çŸ©é˜µçš„ç»´æ•°ï¼ˆGetting your matrix dimensions rightï¼‰ 4.5 ä¸ºä»€ä¹ˆä½¿ç”¨æ·±å±‚è¡¨ç¤ºï¼Ÿï¼ˆWhy deep representations?ï¼‰ 4.6 æ­å»ºç¥ç»ç½‘ç»œå—ï¼ˆBuilding blocks of deep neural networksï¼‰ 4.7 å‚æ•°VSè¶…å‚æ•°ï¼ˆParameters vs Hyperparametersï¼‰ 4.8 æ·±åº¦å­¦ä¹ å’Œå¤§è„‘çš„å…³è”æ€§ï¼ˆWhat does this have to do with the brain?ï¼‰ Â¶ç¬¬äºŒé—¨è¯¾ æ”¹å–„æ·±å±‚ç¥ç»ç½‘ç»œï¼šè¶…å‚æ•°è°ƒè¯•ã€æ­£åˆ™åŒ–ä»¥åŠä¼˜åŒ–(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization) ç¬¬ä¸€å‘¨ï¼šæ·±åº¦å­¦ä¹ çš„å®ç”¨å±‚é¢(Practical aspects of Deep Learning) 1.1 è®­ç»ƒï¼ŒéªŒè¯ï¼Œæµ‹è¯•é›†ï¼ˆTrain / Dev / Test setsï¼‰ 1.2 åå·®ï¼Œæ–¹å·®ï¼ˆBias /Varianceï¼‰ 1.3 æœºå™¨å­¦ä¹ åŸºç¡€ï¼ˆBasic Recipe for Machine Learningï¼‰ 1.4 æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰ 1.5 ä¸ºä»€ä¹ˆæ­£åˆ™åŒ–æœ‰åˆ©äºé¢„é˜²è¿‡æ‹Ÿåˆå‘¢ï¼Ÿï¼ˆWhy regularization reduces overfitting?ï¼‰ 1.6 dropout æ­£åˆ™åŒ–ï¼ˆDropout Regularizationï¼‰ 1.7 ç†è§£ dropoutï¼ˆUnderstanding Dropoutï¼‰ 1.8 å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆOther regularization methodsï¼‰ 1.9 æ ‡å‡†åŒ–è¾“å…¥ï¼ˆNormalizing inputsï¼‰ 1.10 æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸ï¼ˆVanishing / Exploding gradientsï¼‰ 1.11 ç¥ç»ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–ï¼ˆWeight Initialization for Deep NetworksVanishing /Exploding gradientsï¼‰ 1.12 æ¢¯åº¦çš„æ•°å€¼é€¼è¿‘ï¼ˆNumerical approximation of gradientsï¼‰ 1.13 æ¢¯åº¦æ£€éªŒï¼ˆGradient checkingï¼‰ 1.14 æ¢¯åº¦æ£€éªŒåº”ç”¨çš„æ³¨æ„äº‹é¡¹ï¼ˆGradient Checking Implementation Notesï¼‰ ç¬¬äºŒå‘¨ï¼šä¼˜åŒ–ç®—æ³• (Optimization algorithms) 2.1 Mini-batch æ¢¯åº¦ä¸‹é™ï¼ˆMini-batch gradient descentï¼‰ 2.2 ç†è§£Mini-batch æ¢¯åº¦ä¸‹é™ï¼ˆUnderstanding Mini-batch gradient descentï¼‰ 2.3 æŒ‡æ•°åŠ æƒå¹³å‡ï¼ˆExponentially weighted averagesï¼‰ 2.4 ç†è§£æŒ‡æ•°åŠ æƒå¹³å‡ï¼ˆUnderstanding Exponentially weighted averagesï¼‰ 2.5 æŒ‡æ•°åŠ æƒå¹³å‡çš„åå·®ä¿®æ­£ï¼ˆBias correction in exponentially weighted averagesï¼‰ 2.6 momentumæ¢¯åº¦ä¸‹é™ï¼ˆGradient descent with momentumï¼‰ 2.7 RMSpropâ€”â€”root mean square propï¼ˆRMSpropï¼‰ 2.8 Adamä¼˜åŒ–ç®—æ³•ï¼ˆAdam optimization algorithmï¼‰ 2.9 å­¦ä¹ ç‡è¡°å‡ï¼ˆLearning rate decayï¼‰ 2.10 å±€éƒ¨æœ€ä¼˜é—®é¢˜ï¼ˆThe problem of local optimaï¼‰ ç¬¬ä¸‰å‘¨è¶…å‚æ•°è°ƒè¯•ï¼Œbatchæ­£åˆ™åŒ–å’Œç¨‹åºæ¡†æ¶ï¼ˆHyperparameter tuning, Batch Normalization and Programming Frameworks) 3.1 è°ƒè¯•å¤„ç†ï¼ˆTuning processï¼‰ 3.2 ä¸ºè¶…å‚æ•°é€‰æ‹©å’Œé€‚åˆèŒƒå›´ï¼ˆUsing an appropriate scale to pick hyperparametersï¼‰ 3.3 è¶…å‚æ•°è®­ç»ƒçš„å®è·µï¼šPandas vs. Caviarï¼ˆHyperparameters tuning in practice: Pandas vs. Caviarï¼‰ 3.4 ç½‘ç»œä¸­çš„æ­£åˆ™åŒ–æ¿€æ´»å‡½æ•°ï¼ˆNormalizing activations in a networkï¼‰ 3.5 å°† Batch Normæ‹Ÿåˆè¿›ç¥ç»ç½‘ç»œï¼ˆFitting Batch Norm into a neural networkï¼‰ 3.6 ä¸ºä»€ä¹ˆBatch Normå¥æ•ˆï¼Ÿï¼ˆWhy does Batch Norm work?ï¼‰ 3.7 æµ‹è¯•æ—¶çš„Batch Normï¼ˆBatch Norm at test timeï¼‰ 3.8 Softmax å›å½’ï¼ˆSoftmax Regressionï¼‰ 3.9 è®­ç»ƒä¸€ä¸ªSoftmax åˆ†ç±»å™¨ï¼ˆTraining a softmax classifierï¼‰ 3.10 æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆDeep learning frameworksï¼‰ 3.11 TensorFlowï¼ˆTensorFlowï¼‰ Â¶ç¬¬ä¸‰é—¨è¯¾ ç»“æ„åŒ–æœºå™¨å­¦ä¹ é¡¹ç›® (Structuring Machine Learning Projects) ç¬¬ä¸€å‘¨ï¼šæœºå™¨å­¦ä¹ ç­–ç•¥ï¼ˆ1ï¼‰(ML Strategy (1)) 1.1 ä¸ºä»€ä¹ˆæ˜¯MLç­–ç•¥ï¼Ÿ (Why ML Strategy) 1.2 æ­£äº¤åŒ–(Orthogonalization) 1.3 å•ä¸€æ•°å­—è¯„ä¼°æŒ‡æ ‡(Single number evaluation metric) 1.4 æ»¡è¶³å’Œä¼˜åŒ–æŒ‡æ ‡ (Satisficing and Optimizing metric) 1.5 è®­ç»ƒé›†ã€å¼€å‘é›†ã€æµ‹è¯•é›†çš„åˆ’åˆ†(Train/dev/test distributions) 1.6 å¼€å‘é›†å’Œæµ‹è¯•é›†çš„å¤§å° (Size of the dev and test sets) 1.7 ä»€ä¹ˆæ—¶å€™æ”¹å˜å¼€å‘é›†/æµ‹è¯•é›†å’Œè¯„ä¼°æŒ‡æ ‡(When to change dev/test sets and metrics) 1.8 ä¸ºä»€ä¹ˆæ˜¯äººçš„è¡¨ç° (Why human-level performance?) 1.9 å¯é¿å…åå·®(Avoidable bias) 1.10 ç†è§£äººç±»çš„è¡¨ç° (Understanding human-level performance) 1.11 è¶…è¿‡äººç±»çš„è¡¨ç°(Surpassing human-level performance) 1.12 æ”¹å–„ä½ çš„æ¨¡å‹è¡¨ç° (Improving your model performance) ç¬¬äºŒå‘¨ï¼šæœºå™¨å­¦ä¹ ç­–ç•¥ï¼ˆ2ï¼‰(ML Strategy (2)) 2.1 è¯¯å·®åˆ†æ (Carrying out error analysis) 2.2 æ¸…é™¤æ ‡æ³¨é”™è¯¯çš„æ•°æ®(Cleaning up incorrectly labeled data) 2.3 å¿«é€Ÿæ­å»ºä½ çš„ç¬¬ä¸€ä¸ªç³»ç»Ÿï¼Œå¹¶è¿›è¡Œè¿­ä»£(Build your first system quickly, then iterate) 2.4 åœ¨ä¸åŒçš„åˆ†å¸ƒä¸Šçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›† (Training and testing on different distributions) 2.5 æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„åå·®ä¸æ–¹å·®åˆ†æ (Bias and Variance with mismatched data distributions) 2.6 å¤„ç†æ•°æ®ä¸åŒ¹é…é—®é¢˜(Addressing data mismatch) 2.7 è¿ç§»å­¦ä¹  (Transfer learning) 2.8 å¤šä»»åŠ¡å­¦ä¹ (Multi-task learning) 2.9 ä»€ä¹ˆæ˜¯ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ ï¼Ÿ (What is end-to-end deep learning?) 2.10 æ˜¯å¦ä½¿ç”¨ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ–¹æ³• (Whether to use end-to-end deep learning) Â¶ç¬¬å››é—¨è¯¾ å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Networksï¼‰ ç¬¬ä¸€å‘¨ å·ç§¯ç¥ç»ç½‘ç»œ(Foundations of Convolutional Neural Networks) 1.1 è®¡ç®—æœºè§†è§‰ï¼ˆComputer visionï¼‰ 1.2 è¾¹ç¼˜æ£€æµ‹ç¤ºä¾‹ï¼ˆEdge detection exampleï¼‰ 1.3 æ›´å¤šè¾¹ç¼˜æ£€æµ‹å†…å®¹ï¼ˆMore edge detectionï¼‰ 1.4 Padding 1.5 å·ç§¯æ­¥é•¿ï¼ˆStrided convolutionsï¼‰ 1.6 ä¸‰ç»´å·ç§¯ï¼ˆConvolutions over volumesï¼‰ 1.7 å•å±‚å·ç§¯ç½‘ç»œï¼ˆOne layer of a convolutional networkï¼‰ 1.8 ç®€å•å·ç§¯ç½‘ç»œç¤ºä¾‹ï¼ˆA simple convolution network exampleï¼‰ 1.9 æ± åŒ–å±‚ï¼ˆPooling layersï¼‰ 1.10 å·ç§¯ç¥ç»ç½‘ç»œç¤ºä¾‹ï¼ˆConvolutional neural network exampleï¼‰ 1.11 ä¸ºä»€ä¹ˆä½¿ç”¨å·ç§¯ï¼Ÿï¼ˆWhy convolutions?ï¼‰ ç¬¬äºŒå‘¨ æ·±åº¦å·ç§¯ç½‘ç»œï¼šå®ä¾‹æ¢ç©¶(Deep convolutional models: case studies) 2.1 ä¸ºä»€ä¹ˆè¦è¿›è¡Œå®ä¾‹æ¢ç©¶ï¼Ÿï¼ˆWhy look at case studies?ï¼‰ 2.2 ç»å…¸ç½‘ç»œï¼ˆClassic networksï¼‰ 2.3 æ®‹å·®ç½‘ç»œï¼ˆResidual Networks (ResNets)ï¼‰ 2.4 æ®‹å·®ç½‘ç»œä¸ºä»€ä¹ˆæœ‰ç”¨ï¼Ÿï¼ˆWhy ResNets work?ï¼‰ 2.5 ç½‘ç»œä¸­çš„ç½‘ç»œä»¥åŠ 1Ã—1 å·ç§¯ï¼ˆNetwork in Network and 1Ã—1 convolutionsï¼‰ 2.6 è°·æ­Œ Inception ç½‘ç»œç®€ä»‹ï¼ˆInception network motivationï¼‰ 2.7 Inception ç½‘ç»œï¼ˆInception networkï¼‰ 2.8 ä½¿ç”¨å¼€æºçš„å®ç°æ–¹æ¡ˆï¼ˆUsing open-source implementationsï¼‰ 2.9 è¿ç§»å­¦ä¹ ï¼ˆTransfer Learningï¼‰ 2.10 æ•°æ®æ‰©å……ï¼ˆData augmentationï¼‰ 2.11 è®¡ç®—æœºè§†è§‰ç°çŠ¶ï¼ˆThe state of computer visionï¼‰ ç¬¬ä¸‰å‘¨ ç›®æ ‡æ£€æµ‹ï¼ˆObject detectionï¼‰ 3.1 ç›®æ ‡å®šä½ï¼ˆObject localizationï¼‰ 3.2 ç‰¹å¾ç‚¹æ£€æµ‹ï¼ˆLandmark detectionï¼‰ 3.3 ç›®æ ‡æ£€æµ‹ï¼ˆObject detectionï¼‰ 3.4 å·ç§¯çš„æ»‘åŠ¨çª—å£å®ç°ï¼ˆConvolutional implementation of sliding windowsï¼‰ 3.5 Bounding Boxé¢„æµ‹ï¼ˆBounding box predictionsï¼‰ 3.6 äº¤å¹¶æ¯”ï¼ˆIntersection over unionï¼‰ 3.7 éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNon-max suppressionï¼‰ 3.8 Anchor Boxes 3.9 YOLO ç®—æ³•ï¼ˆPutting it together: YOLO algorithmï¼‰ 3.10 å€™é€‰åŒºåŸŸï¼ˆé€‰ä¿®ï¼‰ï¼ˆRegion proposals (Optional)ï¼‰ ç¬¬å››å‘¨ ç‰¹æ®Šåº”ç”¨ï¼šäººè„¸è¯†åˆ«å’Œç¥ç»é£æ ¼è½¬æ¢ï¼ˆSpecial applications: Face recognition &amp;Neural style transferï¼‰ 4.1 ä»€ä¹ˆæ˜¯äººè„¸è¯†åˆ«ï¼Ÿ(What is face recognition?) 4.2 One-Shotå­¦ä¹ ï¼ˆOne-shot learningï¼‰ 4.3 Siamese ç½‘ç»œï¼ˆSiamese networkï¼‰ 4.4 Triplet æŸå¤±ï¼ˆTriplet æŸå¤±ï¼‰ 4.5 é¢éƒ¨éªŒè¯ä¸äºŒåˆ†ç±»ï¼ˆFace verification and binary classificationï¼‰ 4.6 ä»€ä¹ˆæ˜¯ç¥ç»é£æ ¼è½¬æ¢ï¼Ÿï¼ˆWhat is neural style transfer?ï¼‰ 4.7 ä»€ä¹ˆæ˜¯æ·±åº¦å·ç§¯ç½‘ç»œï¼Ÿï¼ˆWhat are deep ConvNets learning?ï¼‰ 4.8 ä»£ä»·å‡½æ•°ï¼ˆCost functionï¼‰ 4.9 å†…å®¹ä»£ä»·å‡½æ•°ï¼ˆContent cost functionï¼‰ 4.10 é£æ ¼ä»£ä»·å‡½æ•°ï¼ˆStyle cost functionï¼‰ 4.11 ä¸€ç»´åˆ°ä¸‰ç»´æ¨å¹¿ï¼ˆ1D and 3D generalizations of modelsï¼‰ ç¬¬äº”é—¨è¯¾ åºåˆ—æ¨¡å‹(Sequence Models) ç¬¬ä¸€å‘¨ å¾ªç¯åºåˆ—æ¨¡å‹ï¼ˆRecurrent Neural Networksï¼‰ 1.1 ä¸ºä»€ä¹ˆé€‰æ‹©åºåˆ—æ¨¡å‹ï¼Ÿï¼ˆWhy Sequence Models?ï¼‰ 1.2 æ•°å­¦ç¬¦å·ï¼ˆNotationï¼‰ 1.3 å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆRecurrent Neural Network Modelï¼‰ 1.4 é€šè¿‡æ—¶é—´çš„åå‘ä¼ æ’­ï¼ˆBackpropagation through timeï¼‰ 1.5 ä¸åŒç±»å‹çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆDifferent types of RNNsï¼‰ 1.6 è¯­è¨€æ¨¡å‹å’Œåºåˆ—ç”Ÿæˆï¼ˆLanguage model and sequence generationï¼‰ 1.7 å¯¹æ–°åºåˆ—é‡‡æ ·ï¼ˆSampling novel sequencesï¼‰ 1.8 å¾ªç¯ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±ï¼ˆVanishing gradients with RNNsï¼‰ 1.9 GRUå•å…ƒï¼ˆGated Recurrent Unitï¼ˆGRUï¼‰ï¼‰ 1.10 é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼ˆlong short term memoryï¼‰unitï¼‰ 1.11 åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆBidirectional RNNï¼‰ 1.12 æ·±å±‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆDeep RNNsï¼‰ ç¬¬äºŒå‘¨ è‡ªç„¶è¯­è¨€å¤„ç†ä¸è¯åµŒå…¥ï¼ˆNatural Language Processing and Word Embeddingsï¼‰ 2.1 è¯æ±‡è¡¨å¾ï¼ˆWord Representationï¼‰ 2.2 ä½¿ç”¨è¯åµŒå…¥ï¼ˆUsing Word Embeddingsï¼‰ 2.3 è¯åµŒå…¥çš„ç‰¹æ€§ï¼ˆProperties of Word Embeddingsï¼‰ 2.4 åµŒå…¥çŸ©é˜µï¼ˆEmbedding Matrixï¼‰ 2.5 å­¦ä¹ è¯åµŒå…¥ï¼ˆLearning Word Embeddingsï¼‰ 2.6 Word2Vec 2.7 è´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰ 2.8 GloVe è¯å‘é‡ï¼ˆGloVe Word Vectorsï¼‰ 2.9 æƒ…ç»ªåˆ†ç±»ï¼ˆSentiment Classificationï¼‰ 2.10 è¯åµŒå…¥é™¤åï¼ˆDebiasing Word Embeddingsï¼‰ ç¬¬ä¸‰å‘¨ åºåˆ—æ¨¡å‹å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSequence models &amp; Attention mechanismï¼‰ 3.1 åŸºç¡€æ¨¡å‹ï¼ˆBasic Modelsï¼‰ 3.2 é€‰æ‹©æœ€å¯èƒ½çš„å¥å­ï¼ˆPicking the most likely sentenceï¼‰ 3.3 é›†æŸæœç´¢ï¼ˆBeam Searchï¼‰ 3.4 æ”¹è¿›é›†æŸæœç´¢ï¼ˆRefinements to Beam Searchï¼‰ 3.5 é›†æŸæœç´¢çš„è¯¯å·®åˆ†æï¼ˆError analysis in beam searchï¼‰ 3.6 Bleu å¾—åˆ†ï¼ˆé€‰ä¿®ï¼‰ï¼ˆBleu Score (optional)ï¼‰ 3.7 æ³¨æ„åŠ›æ¨¡å‹ç›´è§‚ç†è§£ï¼ˆAttention Model Intuitionï¼‰ 3.8æ³¨æ„åŠ›æ¨¡å‹ï¼ˆAttention Modelï¼‰ 3.9è¯­éŸ³è¯†åˆ«ï¼ˆSpeech recognitionï¼‰ 3.10è§¦å‘å­—æ£€æµ‹ï¼ˆTrigger Word Detectionï¼‰ 3.11ç»“è®ºå’Œè‡´è°¢ï¼ˆConclusion and thank youï¼‰ äººå·¥æ™ºèƒ½å¤§å¸ˆè®¿è°ˆ å´æ©è¾¾é‡‡è®¿ Geoffery Hinton å´æ©è¾¾é‡‡è®¿ Ian Goodfellow å´æ©è¾¾é‡‡è®¿ Ruslan Salakhutdinov å´æ©è¾¾é‡‡è®¿ Yoshua Bengio å´æ©è¾¾é‡‡è®¿ æ—å…ƒåº† å´æ©è¾¾é‡‡è®¿ Pieter Abbeel å´æ©è¾¾é‡‡è®¿ Andrej Karpathy é™„ä»¶ æ·±åº¦å­¦ä¹ ç¬¦å·æŒ‡å—ï¼ˆåŸè¯¾ç¨‹ç¿»è¯‘ï¼‰]]></content>
      <categories>
        <category>è§†é¢‘å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æ·±åº¦å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[è´å¶æ–¯åˆ†ç±»å™¨]]></title>
    <url>%2F2019%2F03%2F28%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[[TOC] æ¦‚ç‡è®ºçš„çŸ¥è¯† Â¶æ¡ä»¶æ¦‚ç‡ $$ P(A|B)=P(A\cap B)/P(B) $$ å·²çŸ¥Bå‘ç”Ÿçš„æ¦‚ç‡ï¼Œæ±‚Aå‘ç”Ÿçš„æ¦‚ç‡ Â¶å…¨æ¦‚ç‡ $$ P(B) = \sum_{i=1}^{N}P(B \cap A_i)P(A_i) $$ Â¶è´å¶æ–¯æ¨æ–­ $$ P(A|B)=P(A)\frac{P(B|A)}{P(B)} $$ $$ P(A_i|B)=P(A_i)\frac{P(B|A_i)}{\sum P(A_i)P(B|A_i)} $$ $P(A)$ï¼šPrior probability å…ˆéªŒæ¦‚ç‡ï¼Œåœ¨Bäº‹ä»¶å‘ç”Ÿä¹‹å‰ï¼Œå¯¹Aäº‹ä»¶åšä¸€ä¸ªåˆ¤æ–­ $P(A|B)$:Posterior probability åéªŒæ¦‚ç‡ï¼Œåœ¨Bäº‹ä»¶å‘ç”Ÿä¹‹åï¼Œå¯¹Aäº‹ä»¶çš„æ¦‚ç‡é‡æ–°è¯„ä¼° $P(B|A)/P(B)$:ç§°ä¸ºå¯èƒ½æ€§å‡½æ•°ï¼Œä¸€ä¸ªè°ƒæ•´å› å­ åéªŒæ¦‚ç‡=å…ˆéªŒæ¦‚ç‡*è°ƒæ•´å› å­ ï¼ˆå¯çŸ¥ï¼Œè°ƒæ•´å› æ­¤&gt;1,å‘ç”Ÿæ¦‚ç‡å¢å¤§äº†ï¼Œ è´å¶æ–¯å†³ç­–è®º è‹±æ–‡ï¼šBayesian decision theory è®¾æœ‰$N$ç§å¯èƒ½çš„ç±»åˆ«, å³Î³=${c_1,c_2,â€¦,c_N}$. $Î»_ij$æ˜¯å°†ä¸€ä¸ªçœŸå®ç±»åˆ«ä¸º$c_j$çš„æ ·æœ¬åˆ¤ä¸º$c_x$çš„æŸå¤±ã€‚ åŸºäºåéªŒæ¦‚ç‡å¯å¾—å°†æ ·æœ¬åˆ†ç±»æ‰€äº§ç”Ÿçš„æœŸæœ›æŸå¤±, æˆ–è€…æˆä¸ºæ¡ä»¶é£é™©(Conditional Risk) $$ R(C_i|x)=âˆ‘_{j=1}^NÎ»_{ij}P(c_j|x) $$ äºæ˜¯ï¼Œ æˆ‘ä»¬çš„ä»»åŠ¡å°±æ˜¯å¯»æ‰¾åˆ¤å®šå‡†åˆ™hï¼Œ ä»¤$Ï‡â†’Î³$ ä½¿å¾—æœ€å°åŒ–æ€»ä½“é£é™©ï¼Œ$R(h)=E_x[R(h(x)|x]$æœ€å°. å¯¹äºæ¯ä¸€ä¸ª$x$ï¼Œè‹¥$h$éƒ½èƒ½æœ€å°åŒ–æ¡ä»¶é£é™©ï¼Œé‚£ä¹ˆæ€»ä½“ä¹Ÿè¢«æœ€å°åŒ–äº†ã€‚ å¯ä»¥ç®€åŒ–ä¸ºå¯¹æ¯ä¸ªæ ·æœ¬é€‰æ‹©å…¶æ¡ä»¶é£é™©æœ€å°çš„åˆ†ç±», å³: $$ h(x)=arg \min_{câŠ‚Î»}R(c|x) $$ æ­¤$h(x)$å°±æ˜¯è´å¶æ–¯æœ€ä¼˜åˆ†ç±»å™¨ã€‚ $R(h)$ä¸ºè´å¶æ–¯é£é™©(Bayes Risk), $1âˆ’R(h)$åæ˜ äº†åˆ†ç±»å™¨çš„æœ€ä¼˜æ€§èƒ½. å…·ä½“æ¥è¯´ï¼Œå¦‚æœç›®æ ‡æ˜¯æœ€å°åŒ–åˆ†ç±»é”™è¯¯ç‡ï¼Œ $$\lambda_{ij}=\begin{cases} 0\ \ i==j\1 \ \ \ i!=j \end{cases}$$ åˆ™$R(c|x)=1-p(c|x)$ï¼Œå› æ­¤å¯çŸ¥ï¼Œ$h(x)=\max_{c\in C} p(c|x)$ å¯¹äºæ ·æœ¬$x$,é€‰æ‹©åéªŒæ¦‚ç‡$P(c|X)$æœ€å¤§çš„ç±»åˆ«ä¸ºæ ‡è®°ã€‚ é—®é¢˜è½¬æ¢ä¸º $$ P(c_i|x)=\frac{P(c_i)P(x|c_i)}{\sum P(x)} $$ æ±‚å…ˆéªŒæ¦‚ç‡å’Œä¼¼ç„¶($P(x|c)$) å…¶ä¸­ $PÂ©$è¡¨è¾¾äº†æ ·æœ¬ç©ºé—´ç§å„ç±»æ ·æœ¬æ‰€å çš„æ¯”åˆ—ï¼Œæ ¹æ®å¤§æ•°å®šå¾‹ï¼Œå½“æ ·æœ¬è¶³å¤Ÿå……åˆ†çš„ç‹¬ç«‹åŒåˆ†å¸ƒæ ·æœ¬æ˜¯ï¼Œå¯ä»¥é¢‘ç‡ä¼°è®¡ $P(x|c)$,æ¶‰åŠå…³äºxæ‰€ä»¥å±æ€§çš„è”åˆæ¦‚ç‡ï¼Œç”¨é¢‘ç‡ä¼°è®¡æ¦‚ç‡å¯èƒ½ä¸å¤ªå¥½ï¼Œå¯¹äºä¼°è®¡ç±»æ¡ä»¶æ¦‚ç‡çš„ä¸€ç§å® ç”¨ç­–ç•¥æ˜¯å…ˆå‡è®¾å…·æœ‰æŸç§ç¡®å®šçš„æ¦‚ç‡åˆ†å¸ƒå½¢å¼ï¼Œå†åŸºäºè®­ç»ƒæ ·æœ¬å¯¹æ¦‚ç‡åˆ†å¸ƒçš„å‚æ•°è¿›è¡Œä¼°è®¡ã€‚ $P(x|c)$æ˜¯ç±»æ¡ä»¶æ¦‚ç‡ï¼Œç”±æŸä¸ªåˆ†å¸ƒå†³å®šï¼Œ$P(x|\theta_c)$æ¥è¡¨ç¤ºäº† é¢‘ç‡æ³¨æ„æ´¾è®¤ä¸ºå¯ä»¥é€šè¿‡ä¼˜åŒ–ä¼¼ç„¶å‡½æ•°ä¼°è®¡å‚æ•°ã€‚$D_c$ç±»åˆ«cçš„æ ·æœ¬é›†åˆï¼Œç‹¬ç«‹åŒåˆ†å¸ƒ $$ P(D_c|\theta_c)=\Pi_{x \in D_c}P(x|\theta_c) $$ $$ LL(\theta_c)=log P(D_c|\theta_c) $$ æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ è‹±æ–‡ï¼šnaive Bayes classifier å‡è®¾ï¼šå±æ€§æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ï¼Œæ¯ä¸ªå±æ€§ç‹¬ç«‹æ€§å¯¹åˆ†ç±»ç»“æœå‘ç”Ÿå½±å“ $$ P(c|x)=\frac{PÂ©P(x|c)}{P(x)}=\frac{PÂ©\Pi_{i=1}^{d}P(x_i|c)}{P(x)} $$ å¯¹äºä¸€ä¸ª$x$ï¼Œ$P(x)$éƒ½æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤è´å¶æ–¯æ¨¡å‹å¯å†™ä¸º $$ h_{nb}(x)=arg max_{c\in y}PÂ©\Pi_{i=1}^{d}P(x_i|c) $$ Â¶è®¡ç®—è¿‡ç¨‹ å‡è®¾$D_{c_i}$è¡¨ç¤ºç¬¬iç±»çš„æ ·æœ¬é›†åˆï¼Œ $P(c_i)=\frac{|D_{c_i}|}{|D|}$ å¦‚æœæ˜¯ç¦»æ•£å±æ€§ $$ P(x_i|c_i)=\frac{|D_{c,x_i}|}{|D_{c_i}|} $$ å¦‚æœæ˜¯è¿ç»­å±æ€§ï¼Œ$P(x_i|c_i)$æœä»$N(u_{c,i},\theta_{c,i}^2)$çš„åˆ†å¸ƒ $$ P(x_i|c)=\frac{1}{\sqrt{2\pi}\theta_{c,i}}exp(-\frac{(x_i-u_i)2}{2\theta_{c,i}2}) $$ $P(c_i)\Pi_{i=1}^{N}P(x_i|c_i)$ æ³¨æ„ä¸ºäº†é¿å…å…¶ä»–å±æ€§æºå¸¦çš„ä¿¡æ¯è¢«è®­ç»ƒé›†ä¸­æœªå‡ºç°çš„å±æ€§å€¼æŠ¹å»ï¼Œå› æ­¤ç”¨æ‹‰æ™®æ‹‰æ–¯ä¿®æ­£ï¼ˆLaplacian correction) $$ PÂ©=\frac{|D_{c_i}|+1}{|D|+N}\ P(x_i|c)=\frac{|D_{x_i,c}|+1}{|D_c|+N_i} $$ $N$:è®­ç»ƒé›†å¯èƒ½å‡ºç°çš„ç±»åˆ«æ•° $N_i$:ç¬¬iä¸ªå±æ€§å¯èƒ½çš„å–å€¼æ•° æ˜¾ç„¶ï¼Œæ‹‰æ™®æ‹‰æ–¯ä¿®æ­£é¿å…å› è®­ç»ƒé›†ä¸å……åˆ†å¯¼å‡ºçš„æ¦‚ç‡ä¼°å€¼ä¸º0çš„æƒ…å†µ æœ´ç´ è´å¶æ–¯çš„ç§ç±» å†scikit-learnä¸­ï¼Œä¸€å…±æœ‰ä¸‰ä¸ªæœ´ç´ è´å¶æ–¯ï¼Œåˆ†åˆ«æ˜¯ Â¶GaussianNB $$ P(x_i|C_i)=\frac{1}{\sqrt{2\pi}\theta_{c,i}}exp(-\frac{(x_i-u_i)2}{2\theta_{c,i}2}) $$ 12345678910111213141516171819202122#å¯¼å…¥åŒ…import pandas as pdfrom sklearn.naive_bayes import GaussianNBfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score#å¯¼å…¥æ•°æ®é›†from sklearn import datasetsiris=datasets.load_iris()#åˆ‡åˆ†æ•°æ®é›†Xtrain, Xtest, ytrain, ytest = train_test_split(iris.data, iris.target, random_state=42)#å»ºæ¨¡clf = GaussianNB()clf.fit(Xtrain, ytrain)#åœ¨æµ‹è¯•é›†ä¸Šæ‰§è¡Œé¢„æµ‹ï¼Œprobaå¯¼å‡ºçš„æ˜¯æ¯ä¸ªæ ·æœ¬å±äºæŸç±»çš„æ¦‚ç‡clf.predict(Xtest)clf.predict_proba(Xtest) #æ¯ä¸€ç±»è®¡ç®—ç»“æœéƒ½è¾“å‡º#æµ‹è¯•å‡†ç¡®ç‡accuracy_score(ytest, clf.predict(Xtest)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import numpy as npimport pandas as pdimport randomdataSet =pd.read_csv('iris.txt',header = None)dataSet.head()def randSplit(dataSet, rate): l = list(dataSet.index) #æå–å‡ºç´¢å¼• random.shuffle(l) #éšæœºæ‰“ä¹±ç´¢å¼• dataSet.index = l #å°†æ‰“ä¹±åçš„ç´¢å¼•é‡æ–°èµ‹å€¼ç»™åŸæ•°æ®é›† n = dataSet.shape[0] #æ€»è¡Œæ•° m = int(n * rate) #è®­ç»ƒé›†çš„æ•°é‡ train = dataSet.loc[range(m), :] #æå–å‰mä¸ªè®°å½•ä½œä¸ºè®­ç»ƒé›† test = dataSet.loc[range(m, n), :] #å‰©ä¸‹çš„ä½œä¸ºæµ‹è¯•é›† dataSet.index = range(dataSet.shape[0]) #æ›´æ–°åŸæ•°æ®é›†çš„ç´¢å¼• test.index = range(test.shape[0]) #æ›´æ–°æµ‹è¯•é›†çš„ç´¢å¼•train,test=randSplit(dataSet, 0.8)def gnb_classify(train,test): labels = train.iloc[:,-1].value_counts().index #æå–è®­ç»ƒé›†çš„æ ‡ç­¾ç§ç±» mean =[] #å­˜æ”¾æ¯ä¸ªç±»åˆ«çš„å‡å€¼ std =[] #å­˜æ”¾æ¯ä¸ªç±»åˆ«çš„æ–¹å·® result = [] #å­˜æ”¾æµ‹è¯•é›†çš„é¢„æµ‹ç»“æœ for i in labels: item = train.loc[train.iloc[:,-1]==i,:] #åˆ†åˆ«æå–å‡ºæ¯ä¸€ç§ç±»åˆ« m = item.iloc[:,:-1].mean() #å½“å‰ç±»åˆ«çš„å¹³å‡å€¼ s = np.sum((item.iloc[:,:-1]-m)**2)/(item.shape[0]) #å½“å‰ç±»åˆ«çš„æ–¹å·® mean.append(m) #å°†å½“å‰ç±»åˆ«çš„å¹³å‡å€¼è¿½åŠ è‡³åˆ—è¡¨ std.append(s) #å°†å½“å‰ç±»åˆ«çš„æ–¹å·®è¿½åŠ è‡³åˆ—è¡¨ means = pd.DataFrame(mean,index=labels) #å˜æˆDFæ ¼å¼ï¼Œç´¢å¼•ä¸ºç±»æ ‡ç­¾ stds = pd.DataFrame(std,index=labels) #å˜æˆDFæ ¼å¼ï¼Œç´¢å¼•ä¸ºç±»æ ‡ç­¾ for j in range(test.shape[0]): iset = test.iloc[j,:-1].tolist() #å½“å‰æµ‹è¯•å®ä¾‹ iprob = np.exp(-1*(iset-means)**2/(stds*2))/(np.sqrt(2*np.pi*stds)) #æ­£æ€åˆ†å¸ƒå…¬å¼ prob = train.iloc[:,-1].value_counts()/len(train.iloc[:,-1]) #åˆå§‹åŒ–å½“å‰å®ä¾‹æ€»æ¦‚ç‡ for k in range(test.shape[1]-1): #éå†æ¯ä¸ªç‰¹å¾ prob *= iprob[k] #ç‰¹å¾æ¦‚ç‡ä¹‹ç§¯å³ä¸ºå½“å‰å®ä¾‹æ¦‚ç‡ cla = prob.index[np.argmax(prob.values)] #è¿”å›æœ€å¤§æ¦‚ç‡çš„ç±»åˆ« result.append(cla) test['predict']=result acc = (test.iloc[:,-1]==test.iloc[:,-2]).mean() #è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡ print(f'æ¨¡å‹é¢„æµ‹å‡†ç¡®ç‡ä¸º&#123;acc&#125;') return testgnb_classify(train,test)for i in range(20): train,test= randSplit(dataSet, 0.8) gnb_classify(train,test) Â¶MultinomialNB å…ˆéªŒæ¦‚ç‡å¤šé¡¹å¼åˆ†å¸ƒçš„æœ´ç´ è´å¶æ–¯ï¼Œå‡è®¾ç‰¹å¾æ˜¯ç”±ä¸€å…±ç®€å•å¤šé¡¹å¼åˆ†å¸ƒç”Ÿæˆï¼Œå¤šé¡¹åˆ†å¸ƒå¯ä»¥æè¿°å„ç§ç±»å‹æ ·æœ¬å‡ºç°çš„é¢‘ç‡ï¼Œè¯¥æ¨¡å‹å¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œç‰¹åˆ«è¡¨ç¤ºæ¬¡æ•°ã€‚$\lambda$å¸¸å–å€¼1 $$ P(x_{il}|c)=\frac{x_{il}+\lambda}{m_k+n\lambda} $$ 12345678910def loadDataSet(): dataSet=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] #åˆ‡åˆ†å¥½çš„è¯æ¡ classVec = [0,1,0,1,0,1] #ç±»åˆ«æ ‡ç­¾å‘é‡ï¼Œ1ä»£è¡¨ä¾®è¾±æ€§è¯æ±‡ï¼Œ0ä»£è¡¨éä¾®è¾±æ€§è¯æ±‡ return dataSet,classVecdataSet,classVec = loadDataSet() 12345678def createVocabList(dataSet): vocabSet = set() #åˆ›å»ºä¸€ä¸ªç©ºçš„é›†åˆ for doc in dataSet: #éå†dataSetä¸­çš„æ¯ä¸€æ¡è¨€è®º vocabSet = vocabSet | set(doc) #å–å¹¶é›† vocabList = list(vocabSet) return vocabListvocabList = createVocabList(dataSet) 12345678def setOfWords2Vec(vocabList, inputSet): returnVec = [0] * len(vocabList) #åˆ›å»ºä¸€ä¸ªå…¶ä¸­æ‰€å«å…ƒç´ éƒ½ä¸º0çš„å‘é‡ for word in inputSet: #éå†æ¯ä¸ªè¯æ¡ if word in vocabList: #å¦‚æœè¯æ¡å­˜åœ¨äºè¯æ±‡è¡¨ä¸­ï¼Œåˆ™å˜ä¸º1 returnVec[vocabList.index(word)] = 1 else: print(f" &#123;word&#125; is not in my Vocabulary!" ) return returnVec #è¿”å›æ–‡æ¡£å‘é‡ 12345678def get_trainMat(dataSet): trainMat = [] #åˆå§‹åŒ–å‘é‡åˆ—è¡¨ vocabList = createVocabList(dataSet) #ç”Ÿæˆè¯æ±‡è¡¨ for inputSet in dataSet: #éå†æ ·æœ¬è¯æ¡ä¸­çš„æ¯ä¸€æ¡æ ·æœ¬ returnVec=setOfWords2Vec(vocabList, inputSet) #å°†å½“å‰è¯æ¡å‘é‡åŒ– trainMat.append(returnVec) #è¿½åŠ åˆ°å‘é‡åˆ—è¡¨ä¸­ return trainMattrainMat = get_trainMat(dataSet) 1234567891011121314151617181920def trainNB(trainMat,classVec): n = len(trainMat) #è®¡ç®—è®­ç»ƒçš„æ–‡æ¡£æ•°ç›® m = len(trainMat[0]) #è®¡ç®—æ¯ç¯‡æ–‡æ¡£çš„è¯æ¡æ•° pAb = sum(classVec)/n #æ–‡æ¡£å±äºä¾®è¾±ç±»çš„æ¦‚ç‡ p0Num = np.zeros(m) #è¯æ¡å‡ºç°æ•°åˆå§‹åŒ–ä¸º0 p1Num = np.zeros(m) #è¯æ¡å‡ºç°æ•°åˆå§‹åŒ–ä¸º0 p0Denom = 0 #åˆ†æ¯åˆå§‹åŒ–ä¸º0 p1Denom = 0 #åˆ†æ¯åˆå§‹åŒ–ä¸º0 for i in range(n): #éå†æ¯ä¸€ä¸ªæ–‡æ¡£ if classVec[i] == 1: #ç»Ÿè®¡å±äºä¾®è¾±ç±»çš„æ¡ä»¶æ¦‚ç‡æ‰€éœ€çš„æ•°æ® p1Num += trainMat[i] p1Denom += sum(trainMat[i]) else: #ç»Ÿè®¡å±äºéä¾®è¾±ç±»çš„æ¡ä»¶æ¦‚ç‡æ‰€éœ€çš„æ•°æ® p0Num += trainMat[i] p0Denom += sum(trainMat[i]) p1V = p1Num/p1Denom p0V = p0Num/p0Denom return p0V,p1V,pAb #è¿”å›å±äºéä¾®è¾±ç±»,ä¾®è¾±ç±»å’Œæ–‡æ¡£å±äºä¾®è¾±ç±»çš„æ¦‚ç‡p0V,p1V,pAb=trainNB(trainMat,classVec) 1234567891011121314151617181920212223from functools import reducedef classifyNB(vec2Classify, p0V, p1V, pAb): p1 = reduce(lambda x,y:x*y, vec2Classify * p1V) * pAb #å¯¹åº”å…ƒç´ ç›¸ä¹˜ p0 = reduce(lambda x,y:x*y, vec2Classify * p0V) * (1 - pAb) print('p0:',p0) print('p1:',p1) if p1 &gt; p0: return 1 else: return 0def testingNB(testVec): dataSet,classVec = loadDataSet() #åˆ›å»ºå®éªŒæ ·æœ¬ vocabList = createVocabList(dataSet) #åˆ›å»ºè¯æ±‡è¡¨ trainMat= get_trainMat(dataSet) #å°†å®éªŒæ ·æœ¬å‘é‡åŒ– p0V,p1V,pAb = trainNB(trainMat,classVec) #è®­ç»ƒæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ thisone = setOfWords2Vec(vocabList, testVec) #æµ‹è¯•æ ·æœ¬å‘é‡åŒ– if classifyNB(thisone,p0V,p1V,pAb): print(testVec,'å±äºä¾®è¾±ç±»') #æ‰§è¡Œåˆ†ç±»å¹¶æ‰“å°åˆ†ç±»ç»“æœ else: print(testVec,'å±äºéä¾®è¾±ç±»') #æ‰§è¡Œåˆ†ç±»å¹¶æ‰“å°åˆ†ç±»ç»“æœ testVec1 = ['love', 'my', 'dalmation']testingNB(testVec1) Â¶BernoulliNB ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œå¦‚æœæ˜¯äºŒå…ƒä¼¯åŠªåˆ©åˆ†å¸ƒ $$ P(x_{il}|C_i)=P(i|Y=C_i)x_{il}+(1-P(i|Y=C_i))(1-x_{il}) $$ å¦‚æœæ ·æœ¬å±æ€§å¤§å¤šæ•°å±äºè¿ç»­ï¼ŒGaussionNB å¦‚æœæ˜¯ç¦»æ•£å€¼ï¼Œä½¿ç”¨MultinomialNB å¦‚æœæ ·æœ¬ç‰¹å¾æ˜¯äºŒå…ƒç¦»æ•£å€¼æˆ–è€…ç¨€ç–ç¦»æ•£å€¼ï¼ŒBernoulliNB åŠæœ´ç´ è´å¶æ–¯ Â¶ä¿¡æ¯é‡ã€ç†µã€è”åˆç†µã€æ¡ä»¶ç†µã€äº’ä¿¡æ¯ Â¶ä¿¡æ¯é‡ ååº”äº†éšæœºå˜é‡å–æŸä¸ªå€¼å«çš„å¯èƒ½æ€§å¤§å°ï¼Œæˆ–è€…æ˜¯å«æœ‰çš„ä¿¡æ¯å¤šå°‘ $$ I(X=x)=-log_2^{p(xï¼‰} $$ Â¶ç†µ(entropy) ååº”äº†ä¿¡æºå¹³å‡æ¯ä¸ªç¬¦å·çš„ä¿¡æ¯é‡,æˆ–è€…æ˜¯éšæœºå˜é‡ä¸ç¡®å®šæ€§çš„è¡¡é‡ $$ H(X)=E(I(X))=\sum p(X=x)(-log_2^{p(x)}) $$ Â¶è”åˆç†µ ååº”äº†å¤šä¸ªéšæœºå˜é‡çš„å¹³å‡ä¿¡æ¯é‡ $$ H(X,Y)=\sum p(x,y)(-log_2^{p(x,y)}) $$ Â¶æ¡ä»¶ç†µï¼ˆConditional entropyï¼‰ ååº”äº†å·²çŸ¥ä¸€ä¸ªéšæœºå˜é‡ä¸‹ï¼Œå¦ä¸€ä¸ªéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§ $$ H(X|Y)=-\sum p(y)H(X|Y=y)=-\sum p(x,y)log_2^{p(x|y)} $$ Â¶äº’ä¿¡æ¯(mutual information) ååº”äº†å·²çŸ¥ä¸€ä¸ªéšæœºå˜é‡çš„æƒ…å†µä¸‹ï¼Œå¦å¤–ä¸€ä¸ªéšæœºå˜é‡ä¸ç¡®å®šæ€§å‡å°‘äº†å¤šå°‘,å¯ä»¥æŠŠäº’ä¿¡æ¯çœ‹æˆç”±äºçŸ¥é“ y å€¼è€Œé€ æˆçš„ x çš„ä¸ç¡®å®šæ€§çš„å‡å° $$ I(X;Y)=\sum \sum p(x,y)log(\frac{p(x,y)}{p(x)p(y)})\ =H(X)-H(X|Y)=H(Y)-H(Y|X) $$ å¦‚æœä¸¤ä¸ªéšæœºå˜é‡ç‹¬ç«‹ï¼Œåˆ™äº’ä¿¡æ¯ä¸º0,å› æ­¤ï¼Œäº’ä¿¡æ¯å¯ä»¥è¡¡é‡ä¸¤ä¸ªéšæœºå˜é‡çš„ç›¸å…³ç¨‹åº¦ Â¶æ¡ä»¶äº’ä¿¡æ¯ åœ¨æ¡ä»¶zå‘ç”Ÿæ—¶çš„æ¡ä»¶äº’ä¿¡æ¯ $$ I(X;Y|Z) = \sum\sum p(x,y|z)log_2^{\frac{p(x,y|z)}{p(x|z)p(y|z)}} $$ Â¶åŠæœ´ç´ è´å¶æ–¯ é€‚å½“çš„è€ƒè™‘ä¸€éƒ¨åˆ†å±æ€§é—´çš„ç›¸äº’ä¾èµ–å…³ç³»ï¼Œè¿™ä¸ªå…³ç³»å¯ä»¥ç”¨äº’ä¿¡æ¯æè¿° Â¶ç‹¬ä¾èµ– å‡è®¾æ¯ä¸ªå±æ€§åªæœ‰ä¸€ä¸ªå…¶ä»– çš„å±æ€§.åˆ™è®¡ç®—å…¬å¼æ”¹ä¸‹å¦‚ä¸‹ $$ pÂ©\Pi_{i=1}^{d} P(x_i|C_i,pa_i) $$ $pa_i$æ˜¯å±æ€§$x_i$æ‰€ä¾èµ–çš„å±æ€§ï¼Œè¢«ç§°ä¸º$x_i$çš„çˆ¶å±æ€§ **SPODE **æœ€ç®€å•çš„æ–¹æ³•æ˜¯ï¼šéƒ½é€‰ä¸€ä¸ªå±æ€§ä½œä¸ºçˆ¶å±æ€§ å¯ä»¥é€šè¿‡äº¤å‰éªŒè¯çš„æ–¹æ³• TAN :æœ€å¤§å¸¦æƒç”Ÿæˆæ ‘ æƒé‡ï¼šå½“yåˆ’åˆ†ä¸º$c_k$ç±»æ—¶æ¡ä»¶ç†µ $$ I(x_i;y_i|y)=\sum_{x_i,y_i,c_k}p(x_i,y_j|c_k)log^{\frac{p(x_i;y_j|c_k)}{p(x_i|c_k)p(y_i|c_k)}} $$ step 1: è®¡ç®—ä»»æ„ä¸¤ä¸ªå±æ€§ä¹‹é—´æ¡ä»¶äº’ä¿¡æ¯ $$ I(X;Y|Y)=\sum_{i}I(X;Y|c_i) $$ step 2: ä»¥å±æ€§ä¸ºç»“ç‚¹æ„å»ºå®Œå…¨å›¾ step 3: æœ€å¤§å¸¦æƒç”Ÿæˆæ ‘ï¼ŒæŒ‘é€‰æ ¹å˜é‡ step 4: åŠ å…¥ç±»åˆ«ç»“ç‚¹y,å¢åŠ åˆ°æ¯ä¸ªå±æ€§çš„æœ‰å‘è¾¹ æ¡ä»¶äº’ä¿¡æ¯ååº”äº†å±æ€§åœ¨å·²çŸ¥ç±»åˆ«ä¸‹çš„ç›¸å…³æ€§å¤§å° Â¶é›†æˆå­¦ä¹  AODEé€‰æ‹©æ¨¡å‹å°è¯•å°†æ¯ä¸ªå±æ€§ä½œä¸ºè¶…çˆ¶æ„å»ºSPODE $$ P(c_i|X)æ­£æ¯”äº \sum_{i=1,|D_{x_i}&gt;=m}p(c,x_i)\Pi_{j=1}^{d}p(x_j|c_i,x_i) $$ $m$é€šå¸¸å–30, $$ P(c,x_i)=\frac{|D_{c,x_i}|+1}{|{D}|+N*N_i}\ P(x_j|c,x_i)=\frac{|D_{c,x_i,x_j}+1|}{|D_{c,xi}|+N_j} $$ è´å¶æ–¯ç½‘(Bayesian network) å€ŸåŠ©æœ‰å‘æ— ç¯å›¾æ¥åˆ»ç”»å±æ€§ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæ¡ä»¶æ¦‚ç‡è¡¨æ¥æè¿°å±æ€§çš„è”åˆæ¦‚ç‡åˆ†å¸ƒã€‚ ä¸€ä¸ªè´å¶æ–¯ç½‘ç»œ$B$,åŒ…æ‹¬ç»“æ„$G$å’Œå‚æ•°$\Theta$ ,$B(G,\Theta)$,å¦‚æœä¸¤ä¸ªå±æ€§æœ‰ç›´æ¥ä¾èµ–å…³ç³»ï¼Œç”¨è¾¹è¿æ¥ï¼Œå¯¹äºå±æ€§$x_i$,å…¶çˆ¶èŠ‚ç‚¹é›†åˆ$G_i$,åˆ™$\Theta$åŒ…æ‹¬æ¯ä¸ªå±æ€§æ¡ä»¶æ¦‚ç‡$\Theta_{x_i|\pi_i}=P_B(x_i|\pi_i)$ Â¶ç»“æ„ $$ p(x_1,x_2,â€¦,x_n)=\Pi_{i=1}{n}p_{B}(x_i|\pi_i)=\Pi_{i=1}{d}\Theta_{xi|\pi_i}\ =\Pi_{i=1}^{d}P(x_i|Parents(x_i)) $$ Â¶æ¨æ–­ ä¸€æ—¦è®­ç»ƒå¥½è´å¶æ–¯ç½‘åï¼Œå°±èƒ½å›ç­”query,é€šè¿‡ä¸€äº›å±æ€§çš„è§‚æµ‹è€…æ¥æ¨æ–­å…¶ä»–å±æ€§å˜é‡çš„å–å€¼ï¼Œå…¶ä¸­ï¼Œå·²çŸ¥å˜é‡çš„å€¼è§‚æµ‹æ¨æµ‹å¾…æŸ¥è¯¢çš„è¿‡ç¨‹â€œæ¨æ–­&quot;,å·²çŸ¥å˜é‡çš„è§‚æµ‹è€…â€è¯æ®â€œ]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>è´å¶æ–¯åˆ†ç±»å™¨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[äºŒæ¬¡è§„åˆ’]]></title>
    <url>%2F2019%2F03%2F25%2F%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[[TOC] KKT(Karush-Kuhn-Tucher)æ¡ä»¶ ç»™å®šä¼˜åŒ–é—®é¢˜ $$ \min f(x)\ subject\ to \begin{cases} g_i(x) = 0 (i=1,m\ h_i(x) &lt;= 0 (i=m+1,â€¦,n) \end{cases} $$ æ„é€ lagrangeå‡½æ•° $$ L(x,\lambda) = f(x)+\sum_{i=1}{m}\lambda_ig_i(x)+\sum_{i=m+1}{n}\lambda_ih_i(x) $$ KKTæ¡ä»¶ $$ \frac{\partial L}{\partial x}=0\ g_i(x)=0\ \lambda_i&gt;=0 (i=m+1,â€¦,n)\ \lambda_i h_i(x)=0(i=m+1,â€¦,n) $$ äºŒæ¬¡è§„åˆ’é—®é¢˜ é—®é¢˜çš„æ•°å­¦è¡¨è¾¾ $$ \min Q(x) = \frac{1}{2}xTHx+gTx\ s.t. a_i^Tx = b_i (i=1,â€¦,m)\ \ \ \ \ \ \ \ a_i^Tx &lt;= b_i(i=m+1,â€¦n) $$ KKTæ¡ä»¶ $$ \bigtriangledown f(x)-A^T\lambda =0\ A_{E}x - b_{E}=0\ A_{L}x-b_L&lt;=0\ \lambda_L&gt;=0\ \lambda_L^T(A_LX_L-b_L)=0 $$ å¦‚æœ$H$åŠæ­£å®šï¼ŒäºŒæ¬¡è§„åˆ’é—®é¢˜çš„å…¨å±€æå°å€¼çš„å……è¦æ¡ä»¶ï¼Œ$x^{*}$æ˜¯ä¸€ä¸ªK-Tæ¡ä»¶ è¯æ˜ï¼š å¿…è¦æ€§ï¼šKKT å……åˆ†æ€§ï¼š $$ f(x)-f(x{*})=\frac{1}{2}xTHx+gTx-\frac{1}{2}x{T}Hx{*}-gTx^{}\ =\frac{1}{2}(x-x{*})TH(x-x{*})+x{T}H(x-x{*})+gT(x-x^{})\ =x{*T}H(x-x{})+gT(x-x{})=\lambdaTA(x-x{*}) $$ http://www.hankcs.com/ml/lagrange-duality.html#h3-7 SMO ï¼šSequential minimal optimization æ”¯æŒå‘é‡æœºçš„å¯¹å¶é—®é¢˜ $$ \min \frac{1}{2}\sum_{i=1}{m}\sum_{j=1}{m}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^{m}\alpha_i\ s.t. \sum_{i=1}^{m}\alpha_iy_i=0\ 0&lt;=\alpha_i&lt;=C $$ è¿™ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå¯ä»¥æ ¹æ®äºŒæ¬¡è§„åˆ’æ±‚è§£ï¼Œä½†æ˜¯å¦‚æœæ ·æœ¬ è¿‡å¤šï¼Œç‰¹åˆ«æ…¢ Plattæå‡ºäº†ä¸€ç§æ›´å¿«çš„æ–¹æ³• SMOç®—æ³•æ˜¯ä¸€ç§å¯å‘å¼ç®—æ³•ï¼Œå…¶åŸºæœ¬æ€è·¯æ˜¯ï¼š å¦‚æœæ‰€æœ‰å˜é‡çš„è§£éƒ½æ»¡è¶³æ­¤æœ€ä¼˜åŒ–é—®é¢˜çš„KKTæ¡ä»¶ï¼ˆKarush-Kuhn-Tuckerconditions)ï¼Œé‚£ä¹ˆè¿™ä¸ªæœ€ä¼˜åŒ–é—®é¢˜çš„è§£å°±å¾—åˆ°äº†ã€‚å› ä¸ºKKTæ¡ä»¶æ˜¯è¯¥æœ€ä¼˜åŒ–é—®é¢˜çš„å……åˆ†å¿…è¦æ¡ä»¶ã€‚ å¦åˆ™ï¼Œé€‰æ‹©ä¸¤ä¸ªå˜é‡ï¼Œå›ºå®šå…¶ä»–å˜é‡ï¼Œé’ˆå¯¹è¿™ä¸¤ä¸ªå˜é‡æ„å»ºä¸€ä¸ªäºŒæ¬¡è§„åˆ’é—®é¢˜ã€‚è¿™ä¸ªäºŒæ¬¡è§„åˆ’é—®é¢˜å…³äºè¿™ä¸¤ä¸ªå˜é‡çš„è§£åº”è¯¥æ›´æ¥è¿‘åŸå§‹äºŒæ¬¡è§„åˆ’é—®é¢˜çš„è§£ï¼Œå› ä¸ºè¿™ä¼šä½¿å¾—åŸå§‹äºŒæ¬¡è§„åˆ’é—®é¢˜çš„ç›®æ ‡å‡½æ•°å€¼å˜å¾—æ›´å°ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™æ—¶å­é—®é¢˜å¯ä»¥é€šè¿‡è§£ææ–¹æ³•æ±‚è§£ï¼Œè¿™æ ·å°±å¯ä»¥å¤§å¤§æé«˜æ•´ä¸ªç®—æ³•çš„è®¡ç®—é€Ÿåº¦ã€‚å­é—®é¢˜æœ‰ä¸¤ä¸ªå˜é‡ï¼Œä¸€ä¸ªæ˜¯è¿åKKTæ¡ä»¶æœ€ä¸¥é‡çš„é‚£ä¸€ä¸ªï¼Œå¦ä¸€ä¸ªç”±çº¦æŸæ¡ä»¶è‡ªåŠ¨ç¡®å®šã€‚å¦‚æ­¤ï¼ŒSMOç®—æ³•å°†åŸé—®é¢˜ä¸æ–­åˆ†è§£ä¸ºå­é—®é¢˜å¹¶å¯¹å­é—®é¢˜æ±‚è§£ï¼Œè¿›è€Œè¾¾åˆ°æ±‚è§£åŸé—®é¢˜çš„ç›®çš„ã€‚ å‡è®¾$\alpha_i,\alpha_j$ä¸ºé€‰æ‹©çš„ä¸¤ä¸ªä¼˜åŒ–å˜é‡ï¼Œåˆ™ä¼˜åŒ–é—®é¢˜ $$ \min W(\alpha_i,\alpha_j)=\frac{1}{2}\alpha_i\alpha_iy_iy_iK(x_i,x_i)+\frac{1}{2}\alpha_j\alpha_jy_jy_jK(x_j,x_j)+\alpha_i\alpha_jy_iy_jK(x_i,x_j)\ +\sum_{k_1,k_2!=i,j}^{m}\alpha_{k_1}\alpha_{k_2}y_{k_1}y_{k_2}K(x_{k_1},x_{k_2})-(\alpha_i+\alpha_j)\ s.t\ \ \ \alpha_iy_i+\alpha_jy_j=-\sum_{k!=i,j}^{m}\alpha_ky_k=\xi\ 0&lt;=\alpha_i&lt;=C $$ ä¸Šè¿°é—®é¢˜å°±æ˜¯å…³äº]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
        <category>æ•°å­¦</category>
      </categories>
      <tags>
        <tag>äºŒæ¬¡è§„åˆ’</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle]]></title>
    <url>%2F2019%2F03%2F24%2Fkaggle%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>ç«èµ›</category>
      </categories>
      <tags>
        <tag>ç«èµ›</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn]]></title>
    <url>%2F2019%2F03%2F23%2Fscikit-learn%2F</url>
    <content type="text"><![CDATA[Cross-validation: evaluating estimator performanceÂ¶ 12345import numpy as npfrom sklearn.model_selection import train_test_split# è°ƒç”¨train_test_splitå‡½æ•° è‡ªåŠ¨åˆ’åˆ†æ•°æ®é›† 40%for testingX_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target, test_size=0.4, random_state=0) Â¶corss validation 1234567from sklearn.model_selection import cross_validatefrom sklearn.metrics import recall_scorescoring = [&apos;precision_macro&apos;, &apos;recall_macro&apos;]clf = svm.SVC(kernel=&apos;linear&apos;, C=1, random_state=0)scores = cross_validate(clf, iris.data, iris.target, scoring=scoring, cv=5, return_train_score=False)sorted(scores.keys()) Â¶Cross validation of time series data Tuning the hyper-parameters of an estimator A search consists of: an estimator (regressor or classifier such as sklearn.svm.SVC()); a parameter space; a method for searching or sampling candidates; a cross-validation scheme; and a score function. Â¶Grid Search 1234param_grid = [ &#123;&apos;C&apos;: [1, 10, 100, 1000], &apos;kernel&apos;: [&apos;linear&apos;]&#125;, &#123;&apos;C&apos;: [1, 10, 100, 1000], &apos;gamma&apos;: [0.001, 0.0001], &apos;kernel&apos;: [&apos;rbf&apos;]&#125;, ] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from __future__ import print_functionfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import classification_reportfrom sklearn.svm import SVCprint(__doc__)# Loading the Digits datasetdigits = datasets.load_digits()# To apply an classifier on this data, we need to flatten the image, to# turn the data in a (samples, feature) matrix:n_samples = len(digits.images)X = digits.images.reshape((n_samples, -1))y = digits.target# Split the dataset in two equal partsX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=0)# Set the parameters by cross-validationtuned_parameters = [&#123;'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]&#125;, &#123;'kernel': ['linear'], 'C': [1, 10, 100, 1000]&#125;]scores = ['precision', 'recall']for score in scores: print("# Tuning hyper-parameters for %s" % score) print() clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='%s_macro' % score) clf.fit(X_train, y_train) print("Best parameters set found on development set:") print() print(clf.best_params_) print() print("Grid scores on development set:") print() means = clf.cv_results_['mean_test_score'] stds = clf.cv_results_['std_test_score'] for mean, std, params in zip(means, stds, clf.cv_results_['params']): print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params)) print() print("Detailed classification report:") print() print("The model is trained on the full development set.") print("The scores are computed on the full evaluation set.") print() y_true, y_pred = y_test, clf.predict(X_test) print(classification_report(y_true, y_pred)) print()# Note the problem is too easy: the hyperparameter plateau is too flat and the# output model is the same for precision and recall with ties in quality. Â¶Randomized Parameter Optimization 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566print(__doc__)import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) step1ï¼š äº¤å‰éªŒè¯ï¼ˆè¯„ä»·æ¨¡å‹ï¼‰ step2: è¶…å‚æ•°é€‰æ‹©ï¼Œæ¯ä¸€ç»„å‚æ•°ï¼šå¯¹åº”ä¸€æ¬¡äº¤å‰éªŒè¯ step 3: é›†æˆå­¦ä¹  ä¹Ÿå¯è¿›è¡Œå‚æ•°çš„è°ƒè§£ 12345678from sklearn.model_selection import cross_val_scorefrom sklearn.datasets import load_irisfrom sklearn.ensemble import AdaBoostClassifieriris = load_iris()clf = AdaBoostClassifier(n_estimators=100)scores = cross_val_score(clf, iris.data, iris.target, cv=5)scores.mean() 1234567891011121314151617181920212223from sklearn import datasetsfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom itertools import productfrom sklearn.ensemble import VotingClassifier# Loading some example datairis = datasets.load_iris()X = iris.data[:, [0, 2]]y = iris.target# Training classifiersclf1 = DecisionTreeClassifier(max_depth=4)clf2 = KNeighborsClassifier(n_neighbors=7)clf3 = SVC(gamma=&apos;scale&apos;, kernel=&apos;rbf&apos;, probability=True)eclf = VotingClassifier(estimators=[(&apos;dt&apos;, clf1), (&apos;knn&apos;, clf2), (&apos;svc&apos;, clf3)], voting=&apos;soft&apos;, weights=[2, 1, 2])clf1 = clf1.fit(X, y)clf2 = clf2.fit(X, y)clf3 = clf3.fit(X, y)eclf = eclf.fit(X, y)]]></content>
      <categories>
        <category>ç¼–ç¨‹è¯­è¨€</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Boosting]]></title>
    <url>%2F2019%2F03%2F22%2FBoosting%2F</url>
    <content type="text"><![CDATA[[TOC] Boosting Â¶åŸç† Boostingç®—æ³•æ˜¯å°†â€œå¼±å­¦ä¹ ç®—æ³•â€œæå‡ä¸ºâ€œå¼ºå­¦ä¹ ç®—æ³•â€çš„è¿‡ç¨‹ã€‚ åŠ æ³•æ¨¡å‹ $$ F_n(x;P) = \sum_{t=1}^{n}\alpha_th_t(x;a_t) $$ å‰å‘åˆ†æ­¥ $$ F_m(x) = F_{m-1}(x)+\alpha_mh_m(x,a_m) $$ å¦‚æœé€‰å–ä¸åŒæŸå¤±å‡½æ•°ï¼Œåˆ™äº§ç”Ÿä¸åŒçš„ç±»å‹ AdaBoost AdaBoostå°±æ˜¯æŸå¤±å‡½æ•°ä¸ºæŒ‡æ•°æŸå¤±çš„Boostingç®—æ³•ã€‚ æ¯ä¸€æ¬¡è¿­ä»£çš„å¼±å­¦ä¹ $h(x;a_m)$æœ‰ä½•ä¸ä¸€æ ·ï¼Œå¦‚ä½•å­¦ä¹ ï¼Ÿ AdaBoostæ”¹å˜äº†è®­ç»ƒæ•°æ®çš„æƒå€¼ï¼Œä¹Ÿå°±æ˜¯æ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå…¶æ€æƒ³æ˜¯å°†å…³æ³¨ç‚¹æ”¾åœ¨è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ä¸Šï¼Œå‡å°ä¸Šä¸€è½®è¢«æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æƒå€¼ï¼Œæé«˜é‚£äº›è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬æƒå€¼ã€‚ å¼±åˆ†ç±»å™¨æƒå€¼$Î²_m$å¦‚ä½•ç¡®å®šï¼Ÿ AdaBoosté‡‡ç”¨åŠ æƒå¤šæ•°è¡¨å†³çš„æ–¹æ³•ï¼ŒåŠ å¤§åˆ†ç±»è¯¯å·®ç‡å°çš„å¼±åˆ†ç±»å™¨çš„æƒé‡ï¼Œå‡å°åˆ†ç±»è¯¯å·®ç‡å¤§çš„å¼±åˆ†ç±»å™¨çš„æƒé‡ã€‚è¿™ä¸ªå¾ˆå¥½ç†è§£ï¼Œæ­£ç¡®ç‡é«˜åˆ†å¾—å¥½çš„å¼±åˆ†ç±»å™¨åœ¨å¼ºåˆ†ç±»å™¨ä¸­å½“ç„¶åº”è¯¥æœ‰è¾ƒå¤§çš„å‘è¨€æƒã€‚ Â¶åŸç†ç†è§£ åŸºäºBoostingçš„ç†è§£ï¼Œå¯¹äºAdaBoostï¼Œæˆ‘ä»¬è¦ææ¸…æ¥šä¸¤ç‚¹ï¼š æ¯ä¸€æ¬¡è¿­ä»£çš„å¼±å­¦ä¹ h(x;am)æœ‰ä½•ä¸ä¸€æ ·ï¼Œå¦‚ä½•å­¦ä¹ ï¼Ÿ å¼±åˆ†ç±»å™¨æƒå€¼Î²må¦‚ä½•ç¡®å®šï¼Ÿ å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼ŒAdaBoostæ”¹å˜äº†è®­ç»ƒæ•°æ®çš„æƒå€¼ï¼Œä¹Ÿå°±æ˜¯æ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå…¶æ€æƒ³æ˜¯å°†å…³æ³¨ç‚¹æ”¾åœ¨è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ä¸Šï¼Œå‡å°ä¸Šä¸€è½®è¢«æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æƒå€¼ï¼Œæé«˜é‚£äº›è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬æƒå€¼ã€‚ç„¶åï¼Œå†æ ¹æ®æ‰€é‡‡ç”¨çš„ä¸€äº›åŸºæœ¬æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œå­¦ä¹ ï¼Œæ¯”å¦‚é€»è¾‘å›å½’ã€‚ å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼ŒAdaBoosté‡‡ç”¨åŠ æƒå¤šæ•°è¡¨å†³çš„æ–¹æ³•ï¼ŒåŠ å¤§åˆ†ç±»è¯¯å·®ç‡å°çš„å¼±åˆ†ç±»å™¨çš„æƒé‡ï¼Œå‡å°åˆ†ç±»è¯¯å·®ç‡å¤§çš„å¼±åˆ†ç±»å™¨çš„æƒé‡ã€‚è¿™ä¸ªå¾ˆå¥½ç†è§£ï¼Œæ­£ç¡®ç‡é«˜åˆ†å¾—å¥½çš„å¼±åˆ†ç±»å™¨åœ¨å¼ºåˆ†ç±»å™¨ä¸­å½“ç„¶åº”è¯¥æœ‰è¾ƒå¤§çš„å‘è¨€æƒã€‚ Â¶å…¬å¼æ¨å¯¼ æŒ‡æ•°æŸå¤±å‡½æ•° $$ L(Y,f(x))=exp(-Yf(x)) $$ æƒé‡æ›´æ–°å…¬å¼: é‡‡ç”¨çš„æŒ‡æ•°è¯¯å·®å‡½æ•° $$ l_{exp}(a_th_t|D_t)=E(exp(-f(x)a_th_t(x)))\ =p(f(x)=h_t(x))e{-at}+p(f(x)!=h_t(x))e{at}\ =e{-at}(1-\xi)+e{at}\xi $$ $$ a_t=\frac{1}{2}ln \frac{1-\xi}{\xi} $$ åˆ†å¸ƒæ›´æ–°å…¬å¼ $$ \begin{aligned} l\left(H_{t-1}(x)+\alpha h_{t}(x) | D\right) &amp;=E_{X \sim D}\left(\exp \left(-y(x)\left(H_{t-1}(x)+\alpha h_{t}(x)\right)\right)\right) \ &amp;=E_{x \sim D}\left(\exp \left(-y(x) H_{t-1}(x)\right) \exp \left(-y(x) \alpha h_{t}(x)\right)\right) \end{aligned} $$ åœ¨æ³°å‹’å±•å¼€$exp(-y(x)h_t(x))$ $$ \begin{aligned} l\left(H_{t-1}(x)+h_{t}(x) | D\right) &amp; \approx E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-\alpha y(x) h_{t}(x)+\frac{\alpha^{2} y^{2}(x) h_{t}^{2}(x)}{2}\right)\right] \ &amp;=E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-y(x) h_{t}(x)+0.5 \alpha^{2}\right)\right] \end{aligned} $$ $$ \begin{aligned} h(x) &amp;=\arg \min {h} l\left(H{t-1}(x)+\alpha h_{t} | D\right) \ &amp;=\arg \max {h} E{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right) \alpha y(x) h_{t}(x)\right] \ &amp;=\arg \max {h}\left[\frac{\exp \left(-y(x) H{t-1}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]} y(x) h(x)\right] \end{aligned} $$ $$ $$ ä»¤ä¸€ä¸ªæ–°åˆ†å¸ƒ,æ³¨æ„åˆ†å­æ˜¯å¸¸æ•° $$ D_{t}(x)=\frac{D(x) \exp \left(-y(x) H_{t-1}(x)\right)^{L}}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]} $$ $$ \begin{aligned} h(x) &amp;=\arg \max {h} E{x \sim D,}(y(x) h(x)) \ &amp;=\arg \max {h} E{x \sim D_{t}}(1-2 \mathcal{I}(y(x) \neq h(x))) \ &amp;=\arg \min {h} E{x \sim D_{i}}(\mathcal{I}(y(x) \neq h(x))) \end{aligned} $$ åŒç†å¯å¾— $$ \begin{aligned} D_{t+1} &amp;=\frac{D(x) \exp \left(-y(x) H_{t}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \ &amp;=\frac{D_{t}(x) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right] \cdot \exp \left(-y(x) H_{t}(x)\right)}{\exp \left(-y(x) H_{t-1}(x)\right) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \ &amp;=D_{t}(x) \exp \left(-y(x) \alpha h_{t}(x)\right) \cdot C . \quad(C i s a \text {constant}) \end{aligned} $$ $$ Z_{t}=\sum_{i}^{m} D_{t}(x) \exp \left(-y(x) \alpha_{t} h_{y}(x)\right) $$ æŒ‡æ•°è¯¯å·®å‡½æ•° $$ \begin{aligned} l(H(x) | D) &amp;=\frac{1}{m} \sum_{i}^{m} \exp \left(-y_{i} H\left(x_{i}\right)\right) \ &amp;=\frac{1}{m} \sum_{i}^{m} \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \ &amp;=\sum_{i}^{m} D_{1}\left(x_{i}\right) \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \ &amp;=Z_{1} Z_{2}\left(x_{i}\right) \exp \left(-\sum_{j=2}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \ &amp; \vdots \ &amp;=\prod_{i=1}^{T} Z_{i} \end{aligned} $$ Â¶ç®—æ³•æè¿° æ€»ç»“ä¸€ä¸‹ï¼Œå¾—åˆ°AdaBoostçš„ç®—æ³•æµç¨‹ï¼š è¾“å…¥ï¼šè®­ç»ƒæ•°æ®é›†$T={(x1,y1),(x2,y2),(xN,yN)}T={(x1,y1),(x2,y2),(xN,yN)}$ï¼Œå…¶ä¸­ï¼Œ$xiâˆˆXâŠ†RnxiâˆˆXâŠ†Rnï¼ŒyiâˆˆY=âˆ’1,1yiâˆˆY=âˆ’1,1ï¼Œ$è¿­ä»£æ¬¡æ•°M åˆå§‹åŒ–è®­ç»ƒæ ·æœ¬çš„æƒå€¼åˆ†å¸ƒï¼š$D1=(w1,1,w1,2,â€¦,w1,i),w,i=1,2,â€¦,N$ã€‚ å¯¹äº$m=1,2,â€¦,M$ (a) ä½¿ç”¨å…·æœ‰æƒå€¼åˆ†å¸ƒ$D_m$çš„è®­ç»ƒæ•°æ®é›†è¿›è¡Œå­¦ä¹ ï¼Œå¾—åˆ°å¼±åˆ†ç±»å™¨$h_m(x)$ (b) è®¡ç®—$h_m(x)$åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„åˆ†ç±»è¯¯å·®ç‡ï¼š $e_m=âˆ‘_{i=1}^{N}w_m,iI(h_m(xi)â‰ y_i)$ Â© è®¡ç®—$h_m(x)$åœ¨å¼ºåˆ†ç±»å™¨ä¸­æ‰€å çš„æƒé‡ï¼š $\alpha_m=\frac{1}{2}log(\frac{1âˆ’e_m}{e_m})$ (d) æ›´æ–°è®­ç»ƒæ•°æ®é›†çš„æƒå€¼åˆ†å¸ƒï¼ˆè¿™é‡Œï¼Œ$z_mæ˜¯å½’ä¸€åŒ–å› å­ï¼Œä¸ºäº†ä½¿æ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒå’Œä¸º1ï¼‰ï¼š $$w_{m+1,i}=\frac{w_{m,i}}exp(âˆ’Î±_my_ih_m(xi))ï¼Œi=1,2,â€¦,10$$ $$z_m=âˆ‘_{i=1}^{N}w_{m,i}exp(âˆ’Î±_my_ih_m(xi))$$ å¾—åˆ°æœ€ç»ˆåˆ†ç±»å™¨ï¼š $$F(x)=sign(âˆ‘_{i=1}^{N}Î±_mh_m(x))$$ Â¶é¢ç» ä»Šå¹´8æœˆå¼€å§‹æ‰¾å·¥ä½œï¼Œå‚åŠ å¤§å‚é¢è¯•é—®åˆ°çš„ç›¸å…³é—®é¢˜æœ‰å¦‚ä¸‹å‡ ç‚¹ï¼š æ‰‹æ¨AdaBoost ä¸GBDTæ¯”è¾ƒ AdaBoostå‡ ç§åŸºæœ¬æœºå™¨å­¦ä¹ ç®—æ³•å“ªä¸ªæŠ—å™ªèƒ½åŠ›æœ€å¼ºï¼Œå“ªä¸ªå¯¹é‡é‡‡æ ·ä¸æ•æ„Ÿï¼Ÿ Â¶ç®—æ³•æµç¨‹ Â¶å®ä¾‹è®¡ç®— Â¶Pythonå®ç° https://www.cnblogs.com/davidwang456/articles/8927029.html é›†æˆå­¦ä¹ ]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>Boosting, AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ”¯æŒå‘é‡å›å½’]]></title>
    <url>%2F2019%2F03%2F19%2FSVR%2F</url>
    <content type="text"><![CDATA[[TOC] æ”¯æŒå‘é‡æœºç”¨äºåˆ†ç±»:ç¡¬é—´éš”å’Œè½¯ä»¶é—´éš”æ”¯æŒå‘é‡æœºã€‚å°½å¯èƒ½åˆ†å¯¹ æ”¯æŒå‘é‡æœºå›å½’ï¼š å¸Œæœ›$f(x)$ä¸$y$å°½å¯èƒ½çš„æ¥è¿‘ã€‚ æ”¯æŒå‘é‡æœºåŸºæœ¬æ€æƒ³ è‹±æ–‡å:support vector regression ç®€è®°ï¼šSVR Â¶æ ‡å‡†çš„çº¿æ€§æ”¯æŒå‘é‡å›å½’æ¨¡å‹ å­¦ä¹ çš„æ¨¡å‹: $$f(x)=w^Tx+b$$ å‡è®¾èƒ½å®¹å¿$f(x)$ä¸$y$ä¹‹é—´å·®åˆ«ç»å¯¹å€¼$\xi$,è¿™å°±ä»¥$f(x)=w^Tx+b$å½¢æˆäº†ä¸€ä¸ª$2\xi$çš„é—´éš”å¸¦ï¼Œå› æ­¤æ¨¡å‹ $$ \min \frac{1}{2}w^Tw\ s.t -\xi&lt;=f(x_i)-y_i&lt;=\xi $$ ä½†æ˜¯ä¸Šè¿°æ¡ä»¶å¤ªè¿‡ä¸¥è‹›ï¼Œå› æ­¤å¢åŠ æƒ©ç½šé¡¹ï¼Œ $$ \min \frac{1}{2}w^Tw+C\sum(\epsilon_i+\hat{\epsilon}i)\ s.t. \begin{cases}f(x_i)-y_i&lt;=\xi+\epsilon_i\ y_i-f(x_i)&lt;=\xi+\hat{\epsilon}i\ \hat{\epsilon}i&gt;=0,\epsilon_i&gt;=0 \end{cases} $$ æ„é€ Lagrangeå‡½æ•° $$ \begin{aligned} L :=\frac{1}{2}|\omega|^{2} &amp;+C \sum\left(\xi_i+\xi{\prime}_i\right)-\sum_{i=1}{N}\left(\eta{i} \xi{i}+\eta{i}^{â€™} \xi_{i}6{â€™}\right) \ &amp;+\sum \alpha_{i}\left(y_{i}-\omega^{T} x_{i}-b-\varepsilon-\xi_{i}\right) \ &amp;+\sum \alpha_{i}{â€™}\left(\omega{T} x_{i}+b-y_{i}-\varepsilon-\xi_{i}^{\prime}\right) \end{aligned}\tag{1} $$ æ±‚åå¯¼ $$ \frac{\partial L}{\partial \omega}=\omega-\sum\left(\alpha_{i}-\alpha_{i}\right) x_{i}=0 \Rightarrow \omega=\sum\left(\alpha_{i}-\alpha_{i}^{\prime}\right) x_{i}\tag{2} $$ $$ \frac{\partial L}{\partial b}=\sum_{i=1}{N}\left(\alpha_{i}-\alpha_{i}{\prime}\right)=0 \tag{3} $$ $$ \frac{\partial L}{\partial \xi_{i}{\prime}}=C-\alpha_{i}{â€™}-\eta_{i}^{\prime}=0 \tag{4} $$ $$ \frac{\partial L}{\partial \xi_{i}}=C-\alpha_{i}-\eta_{i}=0 \tag{5} $$ å°†(2)-(4)å¸¦å›(1),å¯å¾—å¯¹å¶é—®é¢˜ $$ \begin{aligned} \min L(\boldsymbol{\alpha})=&amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}{N}\left(\alpha_{i}-\alpha_{i}{}\right)\left(\alpha_{j}-\alpha_{j}^{}\right)\left\langle x_{i}, x_{j}\right\rangle \ &amp;+\varepsilon \sum_{i=1}{N}\left(\alpha_{i}+\alpha_{i}{}\right)-\sum_{i=1}^{N} y_{i}\left(\alpha_{i}-\alpha_{i}^{}\right) \ \text { s.t. } &amp; \sum_{n=1}{N}\left(\alpha_{n}-\alpha_{n}{}\right)=0 \end{aligned} $$ å†å°†(2)å¸¦å›$Y=w^Tx+b$,å¯å¾—çº¿æ€§å›å½’æ¨¡å‹ $$ y(x)=\sum_{i=1}{N}\left(\alpha_{i}-\alpha_{i}{}\right) x_{i}^{T} x+b $$ Â¶éçº¿æ€§æ”¯æŒå‘é‡æœº è€ƒè™‘æ¨¡å‹ $$ y=f(x)+b $$ $f(x)$æ˜¯éçº¿æ€§å‡½æ•°ï¼Œå­˜åœ¨ä¸€ä¸ªç”±$X$æ‰€åœ¨ç©ºé—´åˆ°å¸Œå°”ä¼¯ç‰¹ç©ºé—´çš„æ˜ å°„ï¼Œä½¿å¾— $$ f(x)=w^T\varphi(x) $$ å› æ­¤ï¼Œå»ºç«‹å¦‚ä¸‹çš„ä¼˜åŒ–é—®é¢˜ $$ \min \frac{1}{2}|\omega|^{T}+C \sum_{i}\left(\xi_{i}+\xi_{i}^{\prime}\right)\ \begin{cases} y\left(x_{i}\right)-\omega^{T} \varphi\left(x_{i}\right)-b \leq \xi_{i} \ \omega^{T} \varphi\left(x_{i}\right)+b-y\left(x_{i}\right) &amp; \leq \xi_{i} \ \xi_{i} &amp; \geq 0 \ \xi_{i} &amp; \geq 0 \end{cases} $$ æ„é€ lagrangeå‡½æ•° $$ \begin{aligned} L :=\frac{1}{2}|\omega|^{2} &amp;+C \sum\left(\xi+\xi^{\prime}\right)-\sum\left(\eta_{i} \xi_{i}+\eta_{i} \xi_{i}^{\prime}\right) \ &amp;+\sum \alpha_{i}\left(y_{i}-w^{T} \varphi\left(x_{i}\right)-b-\varepsilon_{i}-\xi_{i}\right) \ &amp;+\sum \alpha_{\mathrm{i}}{\prime}\left(w{T} \varphi\left(x_{i}\right)+b-y_{i}-\varepsilon_{i}{â€™}-\xi_{i}{\prime}\right) \end{aligned} $$ æ±‚åå¯¼ $$ \begin{cases}\frac{\partial L}{\partial w}=w-\sum\left(\alpha_{i}-\alpha_{i}\right) \varphi\left(x_{i}\right)=0\ \frac{\partial L}{\partial b} =\sum\left(\alpha_{i}-\alpha_{i}^{\prime}\right)=0 \ \frac{\partial L}{\partial \xi_{i}^{\prime}} =C-\alpha_{i}{â€™}-\eta_{i}{\prime}=0 \ \frac{\partial L}{\partial \xi_{i}} =C-\alpha_{i}-\eta_{i}=0 \end{cases} $$ å†å¸¦å›ä¼˜åŒ–é—®é¢˜å¯å¾— $$\min {t}-\frac{1}{2} \sum\left(\alpha{i}-\alpha_{i}{\prime}\right)\left(\alpha_{j}-\alpha_{j}{\prime}\right) \varphi\left(x_{i}\right)^{T} \varphi\left(x_{j}\right)-\varepsilon \sum\left(\alpha_{i}+\alpha_{i}^{\prime}\right)+\sum y_{i}\left(\alpha_{i}-\alpha_{i}^{â€™}\right)\s t . \sum\left(\alpha_{i}-\alpha_{i}^{\prime}\right)=0$$ å†æ¬¡å°†$w$å¸¦å›æ¨¡å‹ $$ y=\sum\left(\alpha_{i}-\alpha_{i}^{â€™}\right) \varphi\left(x_{i}\right)^{T} \varphi(x)+b $$]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æ”¯æŒå‘é‡æœºå›å½’</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æ”¯æŒå‘é‡æœº(SVM) ----- åˆ†ç±»å™¨]]></title>
    <url>%2F2019%2F03%2F17%2FSVMClassifiar%2F</url>
    <content type="text"><![CDATA[[TOC] é¢„å¤‡çš„æ•°å­¦çŸ¥è¯† Â¶çº¦æŸä¼˜åŒ–é—®é¢˜ åŸé—®é¢˜,å¸¦ç­‰å¼çº¦æŸï¼Œä¹Ÿå¸¦ä¸ç­‰å¼çº¦æŸçš„ä¸€èˆ¬çº¦æŸé—®é¢˜ $$ \begin{cases} \min_{x}f(x)\ s.t \begin{cases} m_i(x)&gt;=0, i=1,â€¦,m\ n_j(x)=0ï¼Œj=1,â€¦,m\ \end{cases} \end{cases}\tag{1} $$ æ„é€ lagrangeä¹˜å­æ³• $$ L(x,\lambda_i,\eta_j)= f(x)-\sum_{i=1}{m}\lambda_im_i(x)-\sum_{j=1}{n}\eta_j \tag{2} $$ $$ \begin{cases} \min_{x} max_{\lambda_i,\eta_j} L(R^p)\ s.t \lambda_i&gt;=0 \end{cases} $$ ä¸Šè¿°ä¸¤ä¸ªé—®é¢˜çš„ç­‰ä»·æ€§è¯æ˜ å¦‚æœxä¸æ»¡è¶³çº¦æŸ$m_i(x)$,åˆ™$\lambda_i&gt;=0$,åŒæ—¶$m_i(x)&lt;$,åˆ™$L(R^{p},\lambda,\eta)$è¶‹è¿‘æ— ç©·ï¼Œåä¹‹ï¼Œåˆ™å­˜åœ¨æœ€å¤§å€¼ $$ min_{x} max_{\lambda,\eta}=min_{x}(max fæ»¡è¶³æ¡ä»¶,max fä¸æ»¡è¶³çº¦æŸ)\=min_{x} max_{\lambda,\eta}{fæ»¡è¶³æ¡ä»¶} $$ å¯¹å¶é—®é¢˜: å…³äº$\lambda,\eta$çš„æœ€å¤§åŒ–é—®é¢˜ $$max min L(x,\lambda,\eta)\ s.t \lambda_i&gt;=0$$ å¼±å¯¹å¶é—®é¢˜ï¼šå¯¹å¶é—®é¢˜&lt;=åŸé—®é¢˜ è¯æ˜: $max_{x} min(\lambda \eta ) L&lt;=min_{\eta,\lambda } max_{x} L$ $$ \underbrace{\min_{x}L(x,\lambda,\eta)}{A(\lambda,\eta)}&lt;=L(x,\lambda,\eta)&lt;=\underbrace{\max{\lambda,\eta} L(x,\lambda,\eta)}_{B(x)} $$ åˆ†ç±» hard-margin SVMã€ soft-margin SVM ã€kernel SVM çº¿æ€§å¯åˆ†æ”¯æŒå‘é‡æœº å¯¹äºAå­å›¾ï¼Œå¯ä»¥ç”¨ä¸€ä¸ªè¶…å¹³é¢($w^Tx+b$)å»åˆ†ç±»ä¸¤ç±»æ•°æ®ï¼Œå»ºç«‹å¦‚ä¸‹çš„æ•°å­¦æ¨¡å‹ $$ f(w,b)=sign(w^Tx+b)$$ B,C,Då­å›¾æä¾›äº†è¶…å¹³é¢éƒ½å¯ä»¥åˆ†ç±»ï¼Œæ˜¾ç„¶B,Cå›¾çš„è¶…å¹³é¢çš„é²æ£’æ€§ä¸å¦‚Då›¾ã€‚SVMå°±æ˜¯æ‰¾åˆ°æœ€å¥½çš„ä¸€ä¸ªè¶…å¹³é¢ï¼Œæ€ä¹ˆè¡¡é‡å¥½å‘¢ï¼Ÿæ‰¾åˆ°å¹³é¢ç¦»æ ·æœ¬ç‚¹çš„è·ç¦»æœ€å¤§ Â¶hard-margin SVMï¼š æœ€å¤§é—´éš”SVM Â¶ç¬¬ä¸€å® é—´éš” é¦–å…ˆï¼Œçœ‹ä¸‹marginçš„å®šä¹‰ $$ margin(w,b) = min(\frac{|w^Tx_i+b|}{||w||})$$ æ¥ä¸‹æ¥ æ•°å­¦æ¨¡å‹ï¼š $$\begin{cases} \max margin(w,b)\ st. y_i(w^Tx_i+b)&gt;0\end{cases}$$ $$\Longrightarrow\begin{cases} max \frac{1}{||w||}min(y_i(w^Tx_i+b))\ st. y_i(w^Tx_i+b)&gt;0\end{cases}$$ æ³¨æ„ï¼Œ$y_i(w^Tx_i+b)&gt;0$,æ‰€ä»¥$\exists r&gt;0, min(y_i(wTx_i+b))=r$,å¯ä»¤$r=1$,è¿™æ˜¯å¯¹è¶…å¹³é¢èŒƒæ•°çš„å›ºå®šä½œç”¨ï¼Œå› ä¸º$y=wTx+b$å’Œ$y=2w^T+2b$æ˜¯åŒä¸€ä¸ªè¶…å¹³é¢ï¼Œæ€»èƒ½æ‰¾åˆ°ç¼©æ”¾$w,b$ä½¿å¾—ï¼Œå¯ä»¥å°†$r$ç¼©æ”¾åˆ°1 $$\Longrightarrow\begin{cases} max \frac{1}{||w||}\ st. y_i(w^Tx_i+b)&gt;=1\end{cases}$$ $$\Longrightarrow\begin{cases} \min \frac{1}{2}w^Tw\ st. y_i(w^Tx_i+b)&gt;=1\end{cases}$$ è¿™æ˜¯ä¸€ä¸ªåœŸäºŒæ¬¡è§„åˆ’é—®é¢˜ Â¶ç¬¬äºŒå® å¯¹å¶ åˆ©ç”¨lagrangeä¹˜å­æ³•å¾—å‡ºå¯¹å¶é—®é¢˜ å¸¦çº¦æŸ $$\begin{cases} \min \frac{1}{2}w^Tw\ st. y_i(w^Tx_i+b)-1&gt;=0\end{cases}$$ $$ \Longrightarrow L(w,b,\lambdaï¼‰=\frac{1}{2}wTw-\sum_{i=1}{N}\lambda_i(1-y_i(w^Tx_i+b)$$ æ— çº¦æŸ $$ \begin{cases}min_{w,b} max_{\lambda}L(w,b,\lambda) \ s.t \lambda_i&gt;=0\end{cases}$$ æ­¤æ—¶å…³äº$w,b$æ— çº¦æŸçš„ã€‚ å¯¹$(L(w,b,\lambda))$ å¯¹$w$,$b$æ±‚åå¯¼ $$ \frac{\partial L}{\partial w}=w+\sum_{i=1}^{N}y_ix_i\lambda_i=0 \Longrightarrow w=-\sum_{i=1}^{N}y_ix_i\lambda_i\ \frac{\partial L}{\partial b}=-\sum_{i=1}^{N}\lambda_iy_i=0 $$ å¸¦å›$L(w,b,\lambda)$,å¯å¾—å¯¹å¶é—®é¢˜ $$ \begin{cases} max_{\lambda}L(w,b,\lambda ) =-\frac{1}{2}\sum_iN\sum_jN\lambda_i \lambda_jy_iy_jx_i^Tx_j +\sum_i^N\lambda_i \ s .t. \sum_{i=1}^N\lambda_iy_i,\lambda_i&gt;=0\end{cases} \Longrightarrow\\begin{cases} min_{\lambda}L(w,b,\lambda ) =\frac{1}{2}\sum_iN\sum_jN\lambda_i \lambda_jy_iy_jx_i^Tx_j -\sum_i^N\lambda_i \ s .t. \sum_{i=1}^N\lambda_iy_i,\lambda_i&gt;=0\end{cases}$$ Â¶åŸé—®é¢˜å’Œå¯¹å¶é—®é¢˜æœ‰ç›¸åŒè§£çš„å……è¦æ¡ä»¶ æ»¡è¶³ KKT $$ \begin{cases} \frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0,\frac{\partial L}{\partial \lambda}=0\ \lambda_i(y_i(w^Tx_i+b)-1)=0\ \lambda_i&gt;=0\ y_i(w^Tx_i+b)-1&gt;=0 \end{cases} $$ å¦‚æœå­˜åœ¨$(x_k,y_k)=+1or -1$ä½¿å¾—$y_i(wTx_i+b)-1=0$å³å¯æ±‚è§£$b=y_k-\sum_{i=0}{N}\lambda_ix_i^Tx_k$ ä»£å…¥æ¨¡å‹ $$ f(x)=sign(\sum_iNa_iy_ix_iTx+y_k-\sum_{i=0}{N}\lambda_ix_iTx_k)$$ æ³¨æ„ï¼Œå¯¹äºä»»æ„çš„è®­ç»ƒæ ·æœ¬ï¼Œæ€»æœ‰$\lambda_i=0$æˆ–è€…$y_if(x_i)=1$,å¦‚æœ$\lambda_i&gt;0$,è¯´æ˜æ ·æœ¬ç‚¹è½åœ¨æœ€å¤§é—´éš”çš„è¾¹ç•Œä¸Šï¼Œè¿™äº›ç‚¹å°±æ˜¯æ”¯æŒå‘é‡ï¼Œè¿™æ¡è¾¹ç•Œ$w^Tx+b=1or-1$ soft-marign è½¯é—´éš” æƒ³æ³•ï¼šå…è®¸ä¸€éƒ¨åˆ†æ ·æœ¬å¯ä»¥ä¸è¢«æ­£ç¡®åˆ†ç±» Â¶ä¼˜åŒ–ç›®æ ‡ $$ \min_{w,b} \frac{1}{2}w^Tw+loss $$ Â¶ä¸€äº›æŸå¤±å‡½æ•° 0-1æŸå¤± ä¸ªæ•° $$loss=\sum_{i=1}NI{y_i(wTx+b)&lt;1}$$ æ•°å­¦æ€§è´¨ä¸å¥½ï¼Œä¸è¿ç»­ 0-1æŸå¤± è·ç¦» hinge loss $$ loss = \begin{cases} 0 , y_i(w^Tx_i+b)&gt;=0,\ 1-y_i(w^tx_i+b), y_i(w^Tx_i+b)&lt;1\ \end{cases} $$ $$ loss_{max} = max(0,1-y_i(w^Tx_i+b)=1-z) $$ æ­¤æ—¶ä¼˜åŒ–é—®é¢˜ï¼Œä»¤$\xi_i=1-y_i(w^Tx_i+b)$ $$ \min \frac{1}{2}wTw+\sum_{i=1}{N}\xi_i\ s.t \begin{cases} y_i(w^Tx_i+b)&gt;=1-\xi_i\ \xi_i&gt;=0 \end{cases} $$ æŒ‡æ•°æŸå¤±ï¼ˆ**exponential loss **) $$ l_{exp}(z)=exp(-z) $$ å¯¹ç‡æŸå¤±logistic loss $$ l_{log}(z)=log(1+exp(-z)ï¼‰ $$ æ ¸æ–¹æ³• Â¶æ ¸å‡½æ•°çš„å®šä¹‰ è®¾ $\chi$ä¸ºè¾“å…¥ç©ºé—´ï¼ˆInput Spaceï¼‰ï¼Œ $\mathrm{H}$ä¸ºç‰¹å¾ç©ºé—´(Feature Space,ä¸€å®šæ˜¯å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼‰ï¼Œå­˜åœ¨ä¸€ä¸ªæ˜ å°„ $$ \varphi : \chi \rightarrow \mathrm{H} $$ å¯¹ä»»æ„çš„ $x, y \in \mathrm{X}$ï¼Œå‡½æ•° $K(x, y)$ï¼Œæ»¡è¶³ $$ K(x, y)=&lt;\varphi(x), \varphi(y)&gt; $$ åˆ™ç§° $K(x, y)$ä¸ºæ ¸å‡½æ•°ã€‚å¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦çŸ¥é“è¾“å…¥ç©ºé—´å’Œç‰¹å¾ç©ºé—´æ»¡è¶³çš„æ˜ å°„å…³ç³» ï¼Œåªéœ€è¦çŸ¥é“æ ¸å‡½æ•°å°±å¯ä»¥ç®—å‡ºï¼Œè¾“å…¥ç©ºé—´ä¸­ä»»æ„ä¸¤ç‚¹æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´çš„å†…ç§¯ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æ”¯æŒå‘é‡æœº</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å›å½’æ ‘]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%9B%9E%E5%BD%92%E6%A0%91%2F</url>
    <content type="text"><![CDATA[[TOC] åˆ†ç±»æ ‘ä¸å›å½’æ ‘ åˆ†ç±»æ ‘ç”¨äºåˆ†ç±»é—®é¢˜ã€‚åˆ†ç±»å†³ç­–æ ‘åœ¨é€‰å–åˆ’åˆ†ç‚¹ï¼Œç”¨ä¿¡æ¯ç†µã€ä¿¡æ¯å¢ç›Šã€æˆ–è€…ä¿¡æ¯å¢ç›Šç‡ã€æˆ–è€…åŸºå°¼ç³»æ•°ä¸ºæ ‡å‡†ã€‚ Classification tree analysis is when the predicted outcome is the class to which the data belongs. å›å½’å†³ç­–æ ‘ç”¨äºå¤„ç†è¾“å‡ºä¸ºè¿ç»­å‹çš„æ•°æ®ã€‚å›å½’å†³ç­–æ ‘åœ¨é€‰å–åˆ’åˆ†ç‚¹ï¼Œå°±å¸Œæœ›åˆ’åˆ†çš„ä¸¤ä¸ªåˆ†æ”¯çš„è¯¯å·®è¶Šå°è¶Šå¥½ã€‚ Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patientâ€™s length of stay in a hospital)ã€‚ å›å½’æ ‘ è‹±æ–‡åå­—ï¼šRegression Tree Â¶åŸç†ä»‹ç» å†³ç­–æ ‘æœ€ç›´è§‚çš„ç†è§£å…¶å®å°±æ˜¯ï¼Œè¾“å…¥ç‰¹å¾ç©ºé—´($R^n$)ï¼Œç„¶åå¯¹ç‰¹å¾ç©ºé—´åšåˆ’åˆ†ï¼Œæ¯ä¸€ä¸ªåˆ’åˆ†å±äºåŒä¸€ç±»æˆ–è€…å¯¹äºä¸€ä¸ªè¾“å‡ºçš„é¢„æµ‹å€¼ã€‚é‚£ä¹ˆè¿™ä¸ªç®—æ³•éœ€è¦è§£å†³çš„é—®é¢˜æ˜¯1. å¦‚ä½•å†³ç­–è¾¹ç•Œ(åˆ’åˆ†ç‚¹)ï¼Ÿ2. å°½å¯èƒ½å°‘çš„æ¯”è¾ƒæ¬¡æ•°(å†³ç­–æ ‘çš„å½¢çŠ¶) å¦‚ä¸Šå›¾ï¼Œæ¯ä¸€ä¸ªéå¶å­å¯¹äºæŸä¸ªç‰¹å¾çš„åˆ’åˆ†ã€‚ Â¶æœ€å°äºŒä¹˜å›å½’æ ‘ç”Ÿæˆç®—æ³• Q1: é€‰æ‹©åˆ’åˆ†ç‚¹ï¼Ÿéå†æ‰€æœ‰çš„ç‰¹å¾($n$),å¯¹äºæ¯ä¸€ä¸ªç‰¹å¾å¯¹åº”$s_i$ä¸ªå–å€¼ï¼Œå°è¯•å®Œæ‰€æœ‰ç‰¹å¾ï¼Œä»¥åŠç‰¹å¾æ‰€ä»¥æœ‰åˆ’åˆ†ï¼Œé€‰æ‹©ä½¿å¾—æŸå¤±å‡½æ•°æœ€å°çš„é‚£ç»„ç‰¹å¾ä»¥åŠç‰¹å¾çš„åˆ’åˆ†å–å€¼ã€‚ Q2: å¶èŠ‚ç‚¹çš„è¾“å‡ºï¼Ÿå–æ¯ä¸ªåŒºåŸŸæ‰€ä»¥ç»“æœçš„å¹³å‡æ•°ä½œä¸ºè¾“å‡º èŠ‚ç‚¹çš„æŸå¤±å‡½æ•°çš„å½¢å¼ $$ \min _{j, s}\left[\min {c{1}} Loss(y_i,c_1)+\min {c{2}} Loss(y_i,c_2)\right] $$ èŠ‚ç‚¹æœ‰ä¸¤æ¡åˆ†æ”¯ï¼Œ$c1$æ˜¯å·¦èŠ‚ç‚¹çš„å¹³å‡å€¼ï¼Œ$c2$æ˜¯å³èŠ‚ç‚¹çš„å¹³å‡å€¼ï¼Œæ¢å¥è¯è¯´ï¼Œåˆ†ä¸€æ¬¡åˆ’åˆ†éƒ½æ˜¯ä½¿å¾—åˆ’åˆ†å‡ºçš„ä¸¤ä¸ªåˆ†æ”¯çš„è¯¯å·®å’Œæœ€å°ã€‚æœ€ç»ˆå¾—åˆ°å‡½æ•°æ˜¯åˆ†æ®µå‡½æ•° Â¶CARTç®—æ³• è¾“å…¥ï¼š è®­ç»ƒæ•°æ®é›† è¾“å‡ºï¼šå›å½’æ ‘$f(x)$ é€‰æ‹©æœ€ä¼˜çš„ç‰¹å¾$j$å’Œåˆ†åˆ‡ç‚¹$s$ $$ \min {j, s}\left[\min {c{1}} \sum{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min {c{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right] $$ å¯¹äºé€‰å®šçš„$(j,s)$åˆ’åˆ†åŒºåŸŸï¼Œå¹¶ç¡®å®šè¯¥åŒºåŸŸçš„é¢„æµ‹å€¼ å¯¹ä¸¤ä¸ªåŒºåŸŸé€’å½’1. 2. ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ è¿”å›ç”Ÿæˆæ ‘ æ³¨ï¼šåˆ†åˆ‡ç‚¹é€‰æ‹©ï¼šå…ˆæ’åºï¼ŒäºŒåˆ†ã€‚ Pythonä»£ç  Â¶èŠ‚ç‚¹ç±» å±æ€§ï¼šå·¦å³èŠ‚ç‚¹ã€lossã€ç‰¹å¾ç¼–å·æˆ–è€…ç‰¹å¾ã€åˆ†å‰²ç‚¹ 12345678class Node(object): def __init__(self, score=None): # æ„é€ å‡½æ•° self.score = score self.left = None self.right = None self.feature = None self.split = None Â¶å›å½’æ ‘ç±» æ„é€ æ–¹æ³• 1234class RegressionTree(object): def __init__(self): self.root = Node() self.height = 0 ç»™å®šç‰¹å¾ã€åˆ’åˆ†ç‚¹ï¼Œè¿”å›è®¡ç®—MAPE 12345678910111213141516def _get_split_mse(self, X, y, idx, feature, split): ''' X:è®­ç»ƒæ ·æœ¬è¾“å…¥ y:è®­ç»ƒæ ·æœ¬è¾“å‡º idx:è¯¥åˆ†æ”¯å¯¹åº”çš„æ ·æœ¬ç¼–å· feaure: ç‰¹å¾ split: åˆ’åˆ†ç‚¹ ''' split_x1=X[X[idex,feature]&lt;split] split_y1=y[X[idex,feature]&lt;split] split_x2=X[X[idex,feature]&gt;=split] split_y2=y[X[idex,feature]&gt;=split] split_avg = [np.mean(split_y1), np.mean(split_y2)] split_mape = [np.sum((split_y1-split_avg[0])**2),np.sum((split_y2-split_avg[1])**2)] return split_mse, split, split_avg è®¡ç®—ç»™å®šç‰¹å¾çš„æœ€ä½³åˆ†å‰²ç‚¹ éå†ç‰¹å¾æŸä¸€åˆ—çš„æ‰€æœ‰çš„ä¸é‡å¤çš„ç‚¹ï¼Œæ‰¾å‡ºMAPEæœ€å°çš„ç‚¹ä½œä¸ºæœ€ä½³åˆ†å‰²ç‚¹ã€‚å¦‚æœç‰¹å¾ä¸­æ²¡æœ‰ä¸é‡å¤çš„å…ƒç´ åˆ™è¿”å›Noneã€‚ 12345678910def _choose_split_point(self, X, y, idx, feature): feature_x = X[idx,feature] uniques = np.unique(feature_x) if len(uniques)==1: return Noe mape, split, split_avg = min( (self._get_split_mse(X, y, idx, feature, split) for split in unique[1:]), key=lambda x: x[0]) return mape, feature, split, split_avg é€‰æ‹©ç‰¹å¾ éå†å…¨éƒ¨ç‰¹å¾ï¼Œè®¡ç®—mape,ç„¶åç¡®å®šç‰¹å¾å’Œå¯¹åº”çš„åˆ‡å‰²ç‚¹ï¼Œæ³¨æ„å¦‚æœæŸä¸ªç‰¹å¾çš„å€¼æ˜¯ä¸€æ ·çš„ï¼Œåˆ™è¿”å›None 12345678910111213141516171819def _choose_feature(self, X, y, idx): m = len(X[0]) split_rets = [x for x in map(lambda x: self._choose_split_point( X, y, idx, x), range(m)) if x is not None] if split_rets == []: return None _, feature, split, split_avg = min( split_rets, key=lambda x: x[0]) idx_split = [[], []] while idx: i = idx.pop() xi = X[i][feature] if xi &lt; split: idx_split[0].append(i) else: idx_split[1].append(i) return feature, split, split_avg, idx_split å¯¹åº”å¶å­èŠ‚ç‚¹ï¼Œæ‰“å°ç›¸å…³çš„ä¿¡æ¯ 1234def _expr2literal(self, expr): feature, op, split = expr op = "&gt;=" if op == 1 else "&lt;" return "Feature%d %s %.4f" % (feature, op, split) å»ºç«‹å¥½äºŒå‰æ ‘ä»¥åï¼Œéå†æ“ä½œ 12345678910111213141516171819def _get_rules(self): que = [[self.root, []]] self.rules = [] while que: nd, exprs = que.pop(0) if not(nd.left or nd.right): literals = list(map(self._expr2literal, exprs)) self.rules.append([literals, nd.score]) if nd.left: rule_left = [] rule_left.append([nd.feature, -1, nd.split]) que.append([nd.left, rule_left]) if nd.right: rule_right =[] rule_right.append([nd.feature, 1, nd.split]) que.append([nd.right, rule_right]) å»ºç«‹äºŒå‰æ ‘çš„è¿‡ç¨‹ï¼Œä¹Ÿå°±æ˜¯è®­ç»ƒçš„è¿‡ç¨‹ æ§åˆ¶æ·±åº¦ æ§åˆ¶èŠ‚å¶å­èŠ‚ç‚¹çš„æœ€å°‘æ ·æœ¬æ•°é‡ è‡³å°‘æœ‰ä¸€ä¸ªç‰¹å¾æ˜¯ä¸é‡å¤çš„ 12345678910111213141516171819202122232425def fit(self, X, y, max_depth=5, min_samples_split=2): self.root = Node() que = [[0, self.root, list(range(len(y)))]] while que: depth, nd, idx = que.pop(0) if depth == max_depth: break if len(idx) &lt; min_samples_split or set(map(lambda i: y[i,0], idx)) == 1: continue feature_rets = self._choose_feature(X, y, idx) if feature_rets is None: continue nd.feature, nd.split, split_avg, idx_split = feature_rets nd.left = Node(split_avg[0]) nd.right = Node(split_avg[1]) que.append([depth+1, nd.left, idx_split[0]]) que.append([depth+1, nd.right, idx_split[1]]) self.height = depth self._get_rules() æ‰“å°å¶å­èŠ‚ç‚¹ 12345def print_rules(self): for i, rule in enumerate(self.rules): literals, score = rule print("Rule %d: " % i, ' | '.join( literals) + ' =&gt; split_hat %.4f' % score) é¢„æµ‹å•æ ·æœ¬ 123456789101112def _predict(self, row): nd = self.root while nd.left and nd.right: if row[nd.feature] &lt; nd.split: nd = nd.left else: nd = nd.right return nd.score # é¢„æµ‹å¤šæ¡æ ·æœ¬def predict(self, X): return [self._predict(Xi) for Xi in X] 1234567891011121314 def main(): print("Tesing the accuracy of RegressionTree...") X_train=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]]) y_train=np.array([[5.56 ],[5.7],[5.91],[6.4 ],[6.8],[7.05],[8.9],[8.7 ],[9 ],[9.05]]) reg = RegressionTree() print(reg) reg.fit(X=X_train, y=y_train, max_depth=3) reg.print_rules()main() ç®€å•çš„ä¾‹å­ è®­ç»ƒæ•°æ® x 1 2 3 4 5 6 7 8 9 10 y 5.56 5.7 5.91 6.4 6.8 7.05 8.9 8.7 9 9.05 æ ¹æ®ä¸Šè¡¨ï¼Œåªæœ‰ä¸€ä¸ªç‰¹å¾$x$. é€‰æ‹©æœ€ä¼˜çš„ç‰¹å¾$j$å’Œåˆ†åˆ‡ç‚¹$s$ åˆ†åˆ‡ç‚¹(s) 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 $c_1$ 5.56 5.63 5.72 5.89 6.07 6.24 6.62 6.88 7.11 $c_2$ 7.5 7.73 7.99 8.25 8.54 8.91 8.92 9.03 9.05 loss 15.72 12.07 8.36 5.78 3.91 1.93 8.01 11.73 15.74 å½“åˆ†åˆ‡ç‚¹å–$s=6.5$,æŸå¤±æœ€å°$l(s=6.5)=1.93$,æ­¤æ—¶åˆ’åˆ†å‡ºä¸¤ä¸ªåˆ†æ”¯ï¼Œåˆ†åˆ«æ˜¯$R_1={1,2,3,4,5,6}$,$c_1=6.42$,$R_2={7,8,9,10}$,$c_2=8.91$ a) å¯¹R1ç»§ç»­åˆ’åˆ† x 1 2 3 4 5 6 y 5.56 5.7 5.91 6.4 6.8 7.05 åˆ†åˆ‡ç‚¹(s) 1.5 2.5 3.5 4.5 5.5 $c_1$ 5.56 5.63 5.72 5.89 6.07 $c_2$ 6.37 6.54 6.75 6.93 7.05 loss 1.3087 0.754 0.2771 0.4368 1.0644 å½“åˆ†åˆ‡ç‚¹å–$s=3.5$,æŸå¤±å‡½æ•°$l(s=3.6)=0.2771$(å‡è®¾æ­¤æ—¶æ»¡è¶³åœæ­¢æ¡ä»¶ï¼‰,æ­¤æ—¶å¾—åˆ°ä¸¤ä¸ªåˆ†æ”¯ï¼Œåˆ†åˆ«æ˜¯$R_1={1,2,3}$ï¼Œ$c_1=5.72$,$R_2={4,5,6}$,$c_2=6.75$ b) å¯¹R2ç»§ç»­åˆ’åˆ† x 7 8 9 10 y 8.9 8.7 9 9.05 åˆ†åˆ‡ç‚¹(s) 7.5 8.5 9.5 $c_1$ 8.9 8.8 8.87 $c_2$ 8.92 9.03 9.05 loss 0.0717 0.0213 0.0467 å½“åˆ†åˆ‡ç‚¹å–$s=8.5$,æŸå¤±å‡½æ•°$l(s=8,5)=0.0213$(å‡è®¾æ­¤æ—¶æ»¡è¶³åœæ­¢æ¡ä»¶ï¼‰,æ­¤æ—¶å¾—åˆ°ä¸¤ä¸ªåˆ†æ”¯ï¼Œåˆ†åˆ«æ˜¯$R_1={7,8}$ï¼Œ$c_1=8.8$,$R_2={9,10}$,$c_2=9.03$ å‡½æ•°è¡¨è¾¾å¼ $$ \begin{equation} f(x)=\left\{ \begin{aligned} 5.72 &amp; &amp; x&lt;3.5\\ 6.7 5&amp; &amp;3.5&lt;=x&lt;6.5\\ 8.8&amp; &amp;6.5&lt;=x&lt;8.5\\ 9.03&amp; &amp;8.5&lt;=x&lt;10\\ \end{aligned} \right. \end{equation} $$ Pythonåº“ 1class sklearn.tree.DecisionTreeClassifier(criterion=â€™giniâ€™, splitter=â€™bestâ€™, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False) 12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-"""Created on Wed Mar 13 19:59:53 2019@author: 23230"""import numpy as npfrom sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as pltX=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]])y=np.array([[5.56 ],[5.7],[5.91],[6.4],[6.8],[7.05],[8.9],[8.7],[9 ],[9.05]])# Fit regression modelregr_1 = DecisionTreeRegressor(max_depth=2)regr_2 = DecisionTreeRegressor(max_depth=3)regr_3 = DecisionTreeRegressor(max_depth=4)regr_1.fit(X, y)regr_2.fit(X, y)regr_3.fit(X, y)X_test = np.copy(X)y_1 = regr_1.predict(X_test)y_2 = regr_2.predict(X_test)y_3 = regr_3.predict(X_test) # Plot the resultsplt.figure()plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data")plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2)plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=4", linewidth=2)plt.plot(X_test, y_3, color="r", label="max_depth=8", linewidth=2)plt.xlabel("data")plt.ylabel("target")plt.title("Decision Tree Regression")plt.legend()]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>å›å½’æ ‘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BPç®—æ³•]]></title>
    <url>%2F2019%2F03%2F05%2FBP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[[TOC] 1. éœ€è¦çš„å¾®ç§¯åˆ†çŸ¥è¯† Â¶1.1 å¯¼æ•° å¯¹äºä¸€å…ƒå‡½æ•°ï¼Œåœ¨å¯¼æ•°å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œåœ¨æŸä¸€ç‚¹çš„å¯¼æ•°ï¼Œä¹Ÿå°±æ˜¯è¯¥ç‚¹çš„æ–œç‡ã€‚ å¯¹äºå¤šå…ƒå‡½æ•°ï¼Œå¯¹äºæŸä¸€ç‚¹æ±‚å¯¼ï¼Œåˆ™éœ€è¦æŒ‡æ˜æ–¹å‘ï¼Œä¸¤ä¸ªç‰¹æ®Šçš„æ–¹å‘ï¼Œ1. åå¯¼ï¼šåœ¨åæ ‡è½´æ–¹å‘çš„å¯¼æ•° 2. æ¢¯åº¦çš„æ–¹å‘:æ€»æœ‰ä¸€ä¸ªæ–¹å‘æ˜¯å˜åŒ–æœ€å¿«çš„ã€‚ Â¶1.2 æ±‚å¯¼çš„é“¾å¼æ³•åˆ™ $x \in R$, $z=g(f(x))$, $y=f(x)$ $$ \frac{\partial z}{\partial x}=\frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$ $ x \in R^m $, $f(x)$æ˜¯$RM$åˆ°$Rn$çš„æ˜ å°„ï¼Œ$g(f)$æ˜¯$R^n$åˆ°Rçš„æ˜ å°„ $$ \frac{\partial g}{\partial x_i}=\sum_j^n \frac{\partial g}{\partial f_i} \frac{\partial f_i}{\partial x_i}$$ å¦‚æœä½¿ç”¨å‘é‡è¡¨ç¤º $$ \nabla_x^z=(\frac{\partial f}{\partial x})^T \nabla_y^z$$ 2. æ¢¯åº¦ä¸‹é™æ³• Â¶2.1 æ¢¯åº¦ æ¢¯åº¦å…¶å®æœ¬è´¨ä¹Ÿæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå¯¹äºå‡½æ•°$f(X,y)$ åœ¨$(W,y)$è¿™ä¸€ç‚¹çš„æ¢¯åº¦ $(\frac{\partial f}{\partial X},\frac{\partial f}{\partial y})$ æ¢¯åº¦çš„å‡ ä½•æ„ä¹‰ï¼šåœ¨è¯¥åº—å˜åŒ–å¢åŠ æœ€å¿«çš„åœ°æ–¹ Â¶2.2 æ¢¯åº¦ç®—æ³•çš„è§£é‡Š å›¾æ¥è‡ªå´æ©è¾¾çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ é¢œè‰²åçº¢(A)çš„åœ°æ–¹å¼€å§‹ï¼Œæ ¹æ®æ¢¯åº¦çš„è´Ÿæ–¹å‘é€šè¿‡9æ¬¡æ›´æ–°ï¼Œè¾¾åˆ°äº†æœ€å°å€¼(B)ã€‚ ç°åœ¨ç»™å®šä¸€ä¸ªç‚¹$A(\theta_0,\theta_1)$,å¹²å˜›å‘¢ï¼Œæˆ‘ä»¬æƒ³ä»Aåˆ°Bç‚¹ï¼ˆæœ€å°å€¼ç‚¹),ç±»ä¼¼äººç±»ä¸‹å±±ï¼Œéœ€è¦çŸ¥é“å¾€é‚£ä¸ªæ–¹å‘å§ã€èµ°å¤§å¤šä¸€æ­¥å‘¢ï¼Ÿ æ–¹å‘ï¼šæ¢¯åº¦çš„è´Ÿæ–¹å‘ $ \delta=(\frac{\partial L}{\partial \theta_0},\frac{\partial L}{\partial \theta_1})$) æ­¥é•¿ï¼šå­¦ä¹ ç‡ï¼ˆ$\alpha$) å› æ­¤ï¼Œè®¡ç®—ä¸€æ¬¡é‡Œç›®æ ‡æ›´è¿‘äº† $(\theta_0,\theta_1)=(\theta_0,\theta_1)-\alpha \dot (\delta)$ åœ¨é‡å¤ä¸Šä¸¤æ­¥ï¼Œç›´åˆ°æ»¡æ„ä¸ºæ­¢ã€‚ 3.è¯¯å·®åå‘ä¼ æ’­ç®—æ³• Â¶3.1 ç†è®ºæ¨å¯¼ Â¶3.1.1 ç¬¦å·è¯´æ˜ ä¸Šå›¾æ˜¯ä¸€ä¸ªLå±‚çš„ç¥ç»ç½‘ç»œï¼Œè¾“å…¥å±‚ä¸ºç¬¬ä¸€å±‚ï¼Œéšè—å±‚ï¼š2è‡³$L-1$å±‚ï¼Œè¾“å‡ºå±‚L ä»¤ è¾“å…¥å‘é‡ $\vec{X}$ $$ \vec{X} = (x_1,x_2,â€¦,x_{m-1},x_m)$$ è¾“å‡ºå‘é‡ $\vec{Y}$ $$ \vec{Y}=(y_1,y_2,â€¦,y_{n-1},y_n)$$a ç¬¬jå±‚éšè—å±‚çš„è¾“å‡ºå‘é‡ $\vec{h^{(j)}}$ $$\vec{h{(j)}}=(h_1{(j)},h_2&lt;!â€“ï¿¼0â€“&gt;,â€¦,h_{t-1}{(j)},h_tj^{(j)})$$ å…¶ä¸­ï¼Œ$tj$:è¡¨ç¤ºç¬¬jçš„éšè—å±‚ä¸ªæ•° ç¬¬$(l-1)$å±‚çš„ç¬¬iä¸ªç¥ç»å…ƒåˆ°ç¬¬$l$å±‚çš„ç¬¬jä¸ªç¥ç»å…ƒçš„è¿æ¥æƒé‡ï¼š$w_{ij}^{(l)}$ï¼Œåˆ™ç¬¬$(l-1)$å±‚ç¥ç»å…ƒåˆ°ç¬¬$l$å±‚ç¥ç»å…ƒçš„è¿æ¥æƒé‡çŸ©é˜µ $$W^{(l)}=\left( \begin{matrix}w_{11}^{(l)}&amp; \cdots &amp; w_{1(tj)}\ &amp; \dots &amp;\ w_{s(l-1)}{l}&amp;\cdots&amp;w_{s(l-1)s(l)}{l} \end{matrix}\right)$$ Â¶3.1.2 æ¨å¯¼è¿‡ç¨‹ Â¶3.1.2.1 è¯¯å·® å®šä¹‰çš„è¯¯å·®å‡½æ•°,å¸¸è§çš„è¡¡é‡æ€§æŒ‡æ ‡è§ æˆ³æˆ‘,è¿™é‡Œé€‰æ‹©çš„è¯¯å·®å¹³æ–¹å’Œæœ€å° ç¬¬$i$ä¸ªè¾“å‡ºçš„è¯¯å·®,å‡è®¾å®é™…è¾“å‡º$(d(1),d(2),â€¦,d(n))$ï¼š,ä¸€ä¸ªè¾“å…¥æ ·æœ¬å¯¹åº”çš„è¯¯å·® $$E(i)=\frac{1}{2}\sum_{k=1}n(y(i)-d(i))2=\frac{1}{2}||y-d||^2$$ æ‰€æœ‰è®­ç»ƒæ ·æœ¬($N$)çš„è¯¯å·®ï¼š $$E(i)=\frac{1}{2}\sum_{j=1}{N}(\sum_{k=1}n(y(i)-d(i))2)=\frac{1}{2N}\sum_{j=1}{N}(||y(i)-d(i)||^2)$$ å› æ­¤ï¼Œ $$ E = \frac{1}{2N}\sum_{i=1}N(||y(i)-d(i)||2)$$ å…¶å®ï¼Œç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯å…³äºèŠ‚ç‚¹çš„å¤åˆå‡½æ•°ã€‚ä»£ä»·å‡½æ•°æ˜¯å…³äº$W$å’Œ$b$çš„å‡½æ•°ã€‚ Â¶3.1.2.2 æ­£å‘ä¼ æ’­ è¾“å…¥å±‚$\hat{X}$ï¼š $$ X =(x_1,x_2,x_3,â€¦,x_m)$$ å½“æœ‰$N$ä¸ªè®­ç»ƒæ ·æœ¬æ—¶ï¼Œå¯ç”¨çŸ©é˜µè¡¨ç¤º $$ X=\left( \begin{matrix} x_{11} &amp;x_{12}&amp;â€¦&amp;x_{1m}\ x_{21} &amp; x_{22}&amp;â€¦&amp;x_{2m}\ \vdots &amp; \vdots&amp;\dots&amp;\vdots\ x_{N1} &amp; \vdots&amp;\vdots&amp;x_{Nm}\ \end{matrix} \right)$$ ç¬¬äºŒå±‚ $h^{(2)}$,ä¸€å…±$s2$ä¸ªèŠ‚ç‚¹: ç¬¬iä¸ªèŠ‚ç‚¹çš„è®¡ç®— $$h{(2)}(i)=f(\sum_{j=1}{s2}x(j)w_{ji}^{(l)}+b_i)=f(xw(:,i)+b_i)$$ çŸ©é˜µè¡¨ç¤º $$ h{(2)}=f(x*W{(l)}+b^{(2)})$$ ç¬¬iå±‚ çŸ©é˜µå½¢å¼ $$ h{(l)}=f(h{(l-1)}*W^{(l)}+b)$$ Â¶3.1.2.3 åå‘ä¼ æ’­ æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æƒé‡ï¼Œä¸æ–­è¿­ä»£åˆ°æœ€ä¼˜è§£ã€‚ å¯¹$w_{ij}$æ±‚å¯¼æ•°å¯å¾—,å¯æ›´æ–°$w_{ij}$æ›´æ–°å…¬å¼ï¼š $$ w_{ij}=w_{ij}-\alpha \frac{\partial E}{\partial w_{ij}}$$ å½“ç„¶ç®€å•çš„æƒ…å†µä¸‹ï¼Œå¯ç›´æ¥å†™å‡ºå…¬å¼ï¼Œå½“å¤ªå¤æ‚çš„æ—¶å€™ï¼Œå¼•å…¥BPç®€åŒ–æ±‚å¯¼ æ–¹ä¾¿ä¹¦å†™å…¬å¼ï¼Œå¯¹äºç¬¬içš„è¾“å…¥$h{(i-1)}*W{(i)}+b{(i)}$è®°ä½œ$net{(i)}$,å…¶ä¸­ï¼Œç¬¬$i$çš„è¾“å…¥å’Œè¾“å‡ºçš„å…³ç³»ï¼Œ$è¾“å…¥=f(è¾“å‡º)$ ä¸‹é¢å¼€å§‹æ¨å¯¼ é¦–å…ˆï¼Œå¯¹äº$L$å±‚ï¼Œ å¯¹äº$W{(L)}$ï¼Œå…ˆçœ‹å¯¹$W_{ij}{(L)}$æ±‚å¯¼ï¼Œ $$ \frac{\partial E}{\partial W_{ij}^{(L)}} =\frac{\partial E}{\partial y(j)} * \frac{\partial y(i)}{\partial net_{j}^{L}} * \frac{\partial net_{j}^{L}}{\partial W_{ij}^{(L)}}\ =(y(j)-d(j))*f(x){â€™}|_{x=net_j{(L)}}h_i^{(L-1)}$$ ä»¤$\delta_i^{(L)}=y(i)-d(i)$ ä¸Šè¿°ç»™å‡ºäº†å•ä¸ªåˆ†é‡çš„æ±‚åå¯¼çš„ç»“æœï¼Œå¯¹äº$W^{(L)}$ $$ \frac{\partial E}{\partial W^{(L)}} =\left[\begin{matrix} \frac{\partial E}{\partial W_{11}^{(L)}} &amp; \frac{\partial E}{\partial W_{12}^{(L)}}&amp;\dots &amp; \frac{\partial E}{\partial W_{1n}^{(L)}}\ \frac{\partial E}{\partial W_{21}^{(L)}} &amp; \frac{\partial E}{\partial W_{22}^{(L)}}&amp;\dots&amp; \frac{\partial E}{\partial W_{2n}^{(L)}}\ \vdots&amp; \dots&amp; \dots&amp; \dots\ \frac{\partial E}{\partial W_{sL,1}^{(L)}} &amp; \frac{\partial E}{\partial W_{sL,2}^{(L)}}&amp;\dots&amp; \frac{\partial E}{\partial W_{sL,n}^{(L)}} \end{matrix}\right] \= \left[ \begin{matrix} h{(L-1)}_1\h{(L-1)}2\ \dots\h^{(L-1)}n \end{matrix} \right] *\left[\begin{matrix} \delta_1{(L)}f(x){â€™}|{x=net_1^{(L)}}\ \delta_2{(L)}f(x){â€™}|{x=net_2^{(L)}}\ \dots\ \delta_n{(L)}f(x){â€™}|_{x=net_n^{(L)}} \end{matrix}\right] ^T =h{(L-1)}S{(L)} $$ å…¶ä¸­ï¼Œ $$ S^{(L)}=\left[\begin{matrix} \delta_1{(L)}f(x){â€™}|{x=net_1^{(L)}}\ \delta_2{(L)}f(x){â€™}|{x=net_2^{(L)}}\ \dots\ \delta_n{(L)}f(x){â€™}|{x=net_n^{(L)}} \end{matrix}\right]^T $$ åŒç†å¯å¾—ï¼Œ $$ \frac{\partial E}{\partial b_k{(L)}}=(y(j)-d(j))*f(x){â€™}|{x=net_j^{(L)}} $$ å…¶æ¬¡ï¼Œå¯¹äºéšå«å±‚$L-1$å±‚ï¼Œå¯¹$W_{ij}^{(L)}$æ±‚å¯¼ $$ \frac{\partial E}{\partial W_{ij}^{(L-1)}} =\sum_{k=1}^{n}\frac{\partial E}{\partial y(k)} * \frac{\partial y(k)}{\partial net_{k}^{L}} * \frac{\partial net_{k}^{L}}{\partial f(net_j^{(L-1)})}\frac{\partial f(net_j^{(L-1)})}{\partial net_j^{(L-1)}}\frac{\partial net_j^{(L-1)}}{\partial W_{ij}^{(L-1)}}\ =\sum_{k=1}^{n} (y(j)-d(j))*f(x){â€™}|_{x=net_j{(L)}}W_{kj}{(L)}f(x){â€™}|{x=net_j{L-1}}h_i{L-2}\ =\sum{k=1}{n}S_i{(L)}W_{kj}{(L)}f(x){â€™}|_{x=net_j{L-1}}h_i{L-2}\ $$ å†™å‡ºçŸ©é˜µå½¢å¼,å¯¹$W^{(L-1)}$ $$ \frac{\partial E}{\partial W^{(L-1)}}=\left[\begin{matrix} h{(L-2)}_1\h{(L-2)}2\\vdots\h^{(L-2)}{s(L-2)}\end{matrix}\right] \left[\begin{matrix} \delta_1{(L)}f(x){â€™}|{x=net_1^{(L)}}\ \delta_2{(L)}f(x){â€™}|{x=net_2^{(L)}}\ \dots\ \delta_n{(L)}f(x){â€™}|{x=net_n^{(L)}} \end{matrix}\right]^T \left[\begin{matrix} W{11}^{(L)} &amp; W_{12}^{(L)}&amp;\dots &amp; W_{1n}^{(L)}\ W_{21}^{(L)} &amp; W_{22}^{(L)}&amp;\dots&amp; W_{2n}^{(L)}\ \vdots&amp; \dots&amp; \dots&amp; \dots\ W_{s(L-1),1}^{(L)} &amp; W_{s(L-1),2}^{(L)}&amp;\dots&amp; W_{s(L-1),n}^{(L)} \end{matrix}\right]^T \ \left[ \begin{array}{ccc}{f{â€™(L-1)}\left(net{(L-1)}{(1)}\right)} &amp; {0} &amp; {0}&amp;{0} \ {0} &amp; {f{â€™(L-1)}\left(net{(L-1)}{(2)}\right)} &amp; {0} &amp;{0}\ 0 &amp; \dots &amp; \vdots &amp; 0\{0} &amp; {0} &amp; {0}&amp;{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]\ =h{(L-2)}S{(L-1)} $$ $$ S^{(L-1)}=\left(\left[\begin{matrix} f(x){â€™(L)}|_{x=net_1{(L)}}&amp;0&amp; \dots&amp; 0\ 0&amp;f(x){â€™}|_{x=net_2{(L)}}0&amp; \dots&amp; 0\ 0&amp;\dots&amp;\dots&amp;0\ 0&amp;0&amp;0&amp;f(x){â€™(L)}|_{x=net_n{(L)}} \end{matrix}\right]\left[\begin{matrix} \delta_1{(L)}\\delta_2{(L)}\\vdots\\delta_n^{(L)}\end{matrix}\right] \right)^T\ \left[\begin{matrix} W_{11}^{(L)} &amp; W_{12}^{(L)}&amp;\dots &amp; W_{1n}^{(L)}\ W_{21}^{(L)} &amp; W_{22}^{(L)}&amp;\dots&amp; W_{2n}^{(L)}\ \vdots&amp; \dots&amp; \dots&amp; \dots\ W_{s(L-1),1}^{(L)} &amp; W_{s(L-1),2}^{(L)}&amp;\dots&amp; W_{s(L-1),n}^{(L)}* \end{matrix}\right]^T \left[ \begin{array}{ccc}{f{â€™(L-1)}\left(net{(L-1)}{(1)}\right)} &amp; {0} &amp; {0}&amp;{0} \ {0} &amp; {f{â€™(L-1)}\left(net{(L-1)}{(2)}\right)} &amp; {0} &amp;{0}\ 0 &amp; \dots &amp; \vdots &amp; 0\{0} &amp; {0} &amp; {0}&amp;{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]\ =S^{(L)}\left[\begin{matrix} W_{11}^{(L)} &amp; W_{12}^{(L)}&amp;\dots &amp; W_{1n}^{(L)}\ W_{21}^{(L)} &amp; W_{22}^{(L)}&amp;\dots&amp; W_{2n}^{(L)}\ \vdots&amp; \dots&amp; \dots&amp; \dots\ W_{s(L-1),1}^{(L)} &amp; W_{s(L-1),2}^{(L)}&amp;\dots&amp; W_{s(L-1),n}^{(L)}* \end{matrix}\right]^T\left[ \begin{array}{ccc}{f{â€™(L-1)}\left(net{(L-1)}{(1)}\right)} &amp; {0} &amp; {0}&amp;{0} \ {0} &amp; {f{â€™(L-1)}\left(net{(L-1)}{(2)}\right)} &amp; {0} &amp;{0}\ 0 &amp; \dots &amp; \vdots &amp; 0\{0} &amp; {0} &amp; {0}&amp;{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]*\ $$ å¯¹$1&lt;l&lt;L$,æ±‚$W^{(l)}$çš„åå¯¼, æœ€åï¼Œæ ¹æ®ä¸Šè¿°çš„æ¨å¯¼å–”ï¼Œå¾ˆå®¹æ˜“å¾—å‡º$S{(l)}$å’Œ$S{(l+1)}$, $$ S{(l)}=S{(l+1)}W{(l+1)T}F{â€™(l)}(net{(l)})\ S{(L)}=(Y-\hat{Y})F{â€™(L)}(net^{(L)}) $$ $$ \frac{\partial E}{\part W{(l)}}=\left[\begin{matrix}h{(l-1)}1\h^{(l-1)}2 \\dots \h{(l-1)}_{sl}\end{matrix}\right]S{(l+1)} \left[\begin{matrix}W{11}{(l+1)}&amp;W_{12}{(l+1)} &amp;\dots&amp; W{2(sl+1)}^{(l+1)}\ W_{21}{(l+1)}&amp;W_{22}{(l+1)} &amp;\dots&amp; W_{2(sl+1)}^{(l+1)}\ \dots&amp;\dots&amp;\dots&amp;\dots\ W_{sl1}{(l+1)}&amp;W_{sl2}{(l+1)} &amp;\dots&amp; W_{sl(sl+1)}^{(l+1)}\ \end{matrix} \right]^T\left[\begin{matrix} \part f{â€™(l)}(net_1{l})&amp;0&amp;\dots &amp; 0\ 0\0 &amp;\part f{â€™(l)}(net_2{l})&amp;\dots&amp;0\ 0 &amp; 0&amp;\dots&amp;0\ 0&amp;0&amp;\dots&amp;\part f{â€™(l)}(net_l{l})\end{matrix}\right] $$ Â¶3.2 BPç®—æ³•çš„å°ç»“ ç®—æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå‰å‘é˜¶æ®µå’Œåå‘ä¼ æ’­é˜¶æ®µ åå‘é˜¶æ®µç®—æ³•ï¼š Step 1: è®¡ç®—$\hat{y}^{(L)}$ Step 2: for l =L:2 â€‹ è®¡ç®—$S{(l)}=S{(l+1)}W{(l+1)}Fâ€™(net{(l)})$ â€‹ è®¡ç®— $\Delta W{(l)}=h{(l-1)}S^{(l)} $ â€‹ è®¡ç®—$W{(l)}=W{(l)}-\delta \Delta W^{(l)}$ Â¶3.3 Pythonå®ç° Â¶3.3.1 æœ€ç®€å•ä¸‰å±‚ç½‘ç»œ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071'''ä¸ç”¨ä»»ä½•æ¡†æ¶ï¼Œè‡ªå·±å†™ä¸€ä¸ªä¸‰å±‚çš„ç¥ç»ç½‘ç»œ# input-3,hidden-4 output-1'''import numpy as npnp.random.seed(1)# Input MatrixX = np.array([[0, 0, 1], [0, 1, 1], [1, 0 ,1], [1, 1, 1],])# Output Matrixy = np.array([[0], [1], [1], [0]])# Nonlinear functiondef sigmoid(X,derive=False): if not derive: return 1 / (1 + np.exp(-X)) else: return X*(1-X)# reludef relu(X,derive = False): if not derive: return np.maximum(0,X) else: return (X&gt;0).astype(float) # Weight biasW1 = 2 * np.random.random((3, 4))-1b1 = 0.1 * np.ones((4,)) W2 = 2 * np.random.random((4,1))-1b2 = 0.1 * np.ones((1,)) rate = 0.1noline = relu# Trainingtrain_times = 200 for time in range(train_times): # Layer one A1 = np.dot(X,W1)+b1 Z1 = noline(A1) # Layer two A2 = np.dot(Z1, W2)+b2 Z2 = noline(A2) cost = -y+Z2 # Calc deltas S2= cost*noline(A2,True) delta_W2 = np.dot(Z1.T,S2) bias2 = S2.sum(axis=0) S1 = np.dot(S2, W2.T)*noline(A1,True) delta_W1= np.dot(X.T, S1) bias1 = S1.sum(axis=0) # update W1 = W1-rate*delta_W1 b1 = b1-rate*bias1 W2 = W2-rate*delta_W2 b2 = b2-rate*bias2 print('error',np.mean(((y-Z2)*(y-Z2))**2))print("prediction",Z2) Â¶3.4 é™„å½•ï¼š Name Abbreviation Mean absolute percentage error MAPE Root mean squares percentage error RMSPE Mean absolute percentage error MAE Mean squares error MSE Index of agreement IA Theil U statistic 1 U1 Theil U statistic 2 U2 Correlation coefficient R MAPE = $\frac{1}{n} \sum_{k=1}{n}\left|\frac{x{(0)}(k)-\hat{x}{(0)}(k)}{x{(0)}(k)}\right| \times 100$ RMSPE = $\sqrt{\frac{1}{n} \sum_{k=1}{n}\left(\frac{\hat{x}{(0)}(k)-x{(0)}(k)}{x{(0)}(k)}\right)^{2}} \times 100$ MAE = $\frac{1}{n} \sum_{k=1}{n}\left|\hat{x}{(0)}(k)-x^{(0)}(k)\right|$ MSE = $\frac{1}{n} \sum_{k=1}{n}\left(\hat{x}{(0)}(k)-x{(0)}(k)\right){2}$ IA = $1-\frac{\sum_{k=1}{n}\left(\hat{x}{(0)}(k)-x{(0)}(k)\right){2}}{\sum_{k=1}^{n} \left( \left| \hat{x}^{(0)}(k)-\overline{x} \right|+\left| x^{(0)}(k)-\overline{x}\right| \right)^{2}}$ U1 = $\frac{\sqrt{\frac{1}{n} \sum_{k=1}{n}\left(x{(0)}(k)-x{(0)}(k)\right){2}}}{\sqrt{\frac{1}{n} \sum_{k=1}^{n} x{(0)}(k){2}}+\sqrt{\frac{1}{n} \sum_{k=1}^{n} x{(0)}(k){2}}}$ U2 = $\frac{\left[\sum_{k=1}{n}\left(\hat{x}{(0)}(k)-x{(0)}(k)\right){2}\right]^{1 / 2}}{\left[\sum_{k=1}^{n} x{(0)}(k){2}\right]^{1 / 2}}$ R = $\frac{\operatorname{Cov}(\hat{x}^{(0)}, x{(0)})}{\sqrt{\operatorname{Var}[\hat{x}{(0)}] \operatorname{Var}[x^{(0)}]}}$]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å¸Œè…Šå­—æ¯]]></title>
    <url>%2F2019%2F03%2F03%2F%E5%B8%8C%E8%85%8A%E5%AD%97%E6%AF%8D%2F</url>
    <content type="text"><![CDATA[$\alpha$ $\beta$ $\gamma$ $\Gamma$ $\delta$ $\Delta$ $\epsilon$ $\varepsilon$ $\zeta$ $\eta$ $\theta$ $\Theta$ $\vartheta$ $\iota$ $\kappa$ $\lambda$ $\Lambda$ $\mu$ $\nu$ $\xi$ $\Xi$ $\pi$ $\Pi$ $\varpi$ $\rho$ $\varrho$ $\sigma$ $\Sigma$ $\varsigma$ $\tau$ $\upsilon$ $\Upsilon$ $\phi$ $\Phi$ $\varphi$ $\chi$ $\psi$ $\Psi$ $\Omega$ $\omega$ alpha beta gamma delta epsilon theta]]></content>
      <categories>
        <category>æ‚é¡¹</category>
      </categories>
      <tags>
        <tag>å¸Œè…Šå­—æ¯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å´æ©è¾¾]]></title>
    <url>%2F2019%2F03%2F03%2F%E5%90%B4%E6%81%A9%E8%BE%BE%2F</url>
    <content type="text"><![CDATA[Neural Networks and Deep Learning 4 å‘¨]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>å†³ç­–æ ‘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å†³ç­–æ ‘]]></title>
    <url>%2F2019%2F03%2F03%2F%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[ä¸»è¦æ˜¯åˆ†äº«å†³ç­–çš„åŸºæœ¬çŸ¥è¯†ç‚¹ï¼Œé‡ç‚¹åœ¨åˆ†ç±»å†³ç­–æ ‘ä¸Šï¼Œå¯¹äºå›å½’çš„å†³ç­–æ ‘åé¢åœ¨ç»™å‡ºã€‚å¸Œæœ›å¤§å®¶å’Œæˆ‘ä¸€èµ·åšçŸ¥è¯†çš„ä¼ æ’­è€…å•¦ï¼ğŸ˜„ ğŸ˜ƒ ğŸ˜ ğŸ˜® [TOC] å†³ç­–æ ‘ è‹±æ–‡åå­—ï¼šDescision Tree Â¶ä»€ä¹ˆæ˜¯å†³ç­–æ ‘ ä¸¾ä¸ªæ ¡å›­ç›¸äº²çš„ä¾‹å­ï¼Œä»Šå¤©æ ¡å›­çš„å°çŒ«(å¥³)å’Œå°ç‹—(ç”·)å‡†å¤‡é…å¯¹ï¼Œå°çŒ«å¦‚ä½•æ‰èƒ½åœ¨ä¼—å¤šçš„ä¼˜è´¨ğŸ¶çš„å¿ƒä»ªçš„ç‹—å‘¢ï¼Ÿäºæ˜¯å‘¢ï¼Ÿæœ‰ä¸€åªç‰¹ä¹–å·§çš„å°çŒ«æ‰¾åˆ°äº†ä½ ï¼Œä½ æ­£åœ¨å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Œåˆšå¥½å­¦ä¹ äº†å†³ç­–æ ‘ï¼Œå‡†å¤‡ç»™è¿™åªçŒ«çŒ«æŒ‘é€‰ä¼˜è´¨ç‹—ï¼Œå½“ç„¶ï¼Œä½ ä¸ä»…ä»…æ˜¯ç›´æ¥å‘Šè¯‰çŒ«å“ªäº›ç‹—æ˜¯åˆé€‚ä½ çš„ï¼Ÿä½ æ›´åº”è¯¥è¯¦ç»†çš„ç»™çŒ«è®²è§£å†³ç­–æ ‘æ˜¯å¦‚ä½•æ ¹æ®å®ƒæå‡ºçš„æ ‡å‡†é€‰å‡ºçš„ç¬¦åˆè¦æ±‚çš„ç‹—å‘¢ï¼Ÿ çŒ«ç»™å‡ºå¦‚ä¸‹ä¿¡æ¯ï¼š å¹´é¾„&lt;0.5 ä¸å¿ƒä»ªï¼›å¹´é¾„å¤§äº&gt;=0.5 6.5&lt;=ä½“é‡&lt;=8.5;å¿ƒä»ª; å¹´é¾„&gt;=0.5 ä½“é‡&gt;8.5 é•¿ç›¸å¥½ å¿ƒä»ª;å…¶ä½™æƒ…å†µä¸å¿ƒä»ª; æ ¹æ®ä¸Šè¿°æ¡ä»¶å¯ä»¥æ„é€ ä¸€é¢—æ ‘ï¼š ä¸Šé¢çš„å›¾å°±æ˜¯å†³ç­–æ ‘ï¼Œæœ€ç»ˆçš„ç»“æœæ˜¯å¿ƒä»ªæˆ–è€…ä¸å¿ƒä»ªã€‚å†³ç­–æ ‘ç®—æ³•ä»¥æ ‘å½¢ç»“æ„è¡¨ç¤ºæ•°æ®åˆ†ç±»çš„ç»“æœ Â¶åŸºæœ¬æ¦‚å¿µ å†³ç­–æ ‘å±äºä¹Ÿåªèƒ½éå‚æ•°å­¦ä¹ ç®—æ³•ã€å¯ä»¥ç”¨äºè§£å†³(å¤š)åˆ†ç±»é—®é¢˜ï¼Œå›å½’é—®é¢˜ã€‚ å›å½’é—®é¢˜çš„ç»“æœï¼Œå¶å­ç»“ç‚¹çš„å¹³å‡å€¼æ˜¯å›å½’é—®é¢˜çš„è§£ã€‚ æ ¹èŠ‚ç‚¹ï¼šå†³ç­–æ ‘å…·æœ‰æ•°æ®ç»“æ„é‡Œé¢çš„äºŒå‰æ ‘ã€æ ‘çš„å…¨éƒ¨å±æ€§ éå¶å­èŠ‚ç‚¹ ï¼šï¼ˆå†³ç­–ç‚¹ï¼‰ ä»£è¡¨æµ‹è¯•çš„æ¡ä»¶ï¼Œæ•°æ®çš„å±æ€§çš„æµ‹è¯• å¶å­èŠ‚ç‚¹ ï¼šåˆ†ç±»åè·å¾—åˆ†ç±»æ ‡è®° åˆ†æ”¯ï¼š æµ‹è¯•çš„ç»“æœ Â¶æ•°å­¦é—®é¢˜-ç†µ-Giniç³»æ•° ä»€ä¹ˆæ˜¯ç†µï¼šç†µçš„æ¦‚å¿µæºäºç‰©ç†å­¦ï¼Œç”¨äºåº¦é‡ä¸€ä¸ªçƒ­åŠ›å­¦ç³»ç»Ÿçš„æ— åºç¨‹åº¦ã€‚ ä¿¡æ¯ç†µï¼šä¸å¾—ä¸æé¦™å†œè¿™ä¸ªå¤§å†™çš„äººå•¦ï¼ä¿¡æ¯è®ºé‡Œé¢çš„çŸ¥è¯†ã€‚åœ¨ä¿¡æ¯è®ºé‡Œé¢ï¼Œä¿¡æ¯ç†µè¡¡é‡ä¿¡æ¯é‡çš„å¤§å°ï¼Œä¹Ÿå°±æ˜¯å¯¹éšæœºå˜é‡ä¸ç¡®å®šåº¦çš„ä¸€ä¸ªè¡¡é‡ã€‚ç†µè¶Šå¤§ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§ï¼›æ ·æœ¬çº¯åº¦è¶Šå¤§è¶Šå¥½ã€‚ å¯¹äºæŸä¸ªå•ç¬¦å·æ— è®°å¿†ä¿¡æºï¼Œå‘å‡ºç¬¦å·($x_i$)çš„æ¦‚ç‡æ˜¯$p_i$,æ¦‚ç‡è¶Šå¤§ï¼Œç¬¦å·çš„ä¿¡æ¯é‡å°±è¶Šå°ï¼Œé¦™å†œå…¬å¼ $I(x_i)=-log_{p_i}$ã€‚ä¿¡æºæ‰€å«çš„ä¿¡æ¯ç†µå°±æ˜¯ä¿¡æ¯é‡çš„æœŸæœ›] $H(x)=-\sum p_i*log_{p_i}$ Giniç³»æ•°ï¼š $GimiÂ§ = 1-\sum_{k=1}{K}p_k2$ Â¶å†³ç­–æ ‘å¦‚ä½•æ„å»ºçš„é—®é¢˜ è‡ªæˆ‘æé—®é˜¶æ®µï¼š æ¯ä¸ªèŠ‚ç‚¹çš„ä½ç½®å¦‚ä½•ç¡®å®šï¼Ÿ ç‰¹å¾çš„é€‰æ‹©ï¼šæ¯æ¬¡é€‰å…¥çš„ç‰¹å¾ä½œä¸ºåˆ†è£‚çš„æ ‡å‡†ï¼Œéƒ½æ˜¯ä½¿å¾—å†³ç­–æ ‘åœ¨è¿™ä¸ªèŠ‚ç‚¹çš„æ ¹æ®ä½ è‡ªå·±é€‰æ‹©çš„æ ‡å‡†ï¼ˆä¿¡æ¯ç†µæœ€å°ã€ä¿¡æ¯å¢ç›Šæœ€å¤§ã€giniç³»æ•°æœ€å°ï¼‰. é€‰å–çš„æ ‡å‡†ï¼šå°½å¿«èƒ½çš„åˆ’åˆ†å‡ºç»“æœï¼Œä½¿å¾—åˆ†çš„ç»“æœæœ€å¥½ã€‚ æ¯ä¸ªèŠ‚ç‚¹åœ¨å“ªä¸ªå€¼ä¸Šåšåˆ’åˆ†ï¼Œç¡®å®šåˆ†æ”¯ç»“æ„å‘¢ï¼Ÿ éå†åˆ’åˆ†çš„èŠ‚ç‚¹çš„åˆ†ç•Œå€¼æ“ä½œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ å¯ä»¥æƒ³è±¡ï¼Œæˆ‘ä»¬æ„é€ çš„å†³ç­–æ ‘è¶³å¤Ÿåºå¤§ï¼Œå†³ç­–æ ‘å¯ä»¥æŠŠæ¯ä¸€ä¸ªæ ·æœ¬éƒ½åˆ†å¯¹ï¼Œé‚£ä¹ˆå†³ç­–æ ‘çš„æ³›åŒ–èƒ½åŠ›å°±å¯ä»¥å¾ˆå·®äº† ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°±éœ€è¦å‰ªææ“ä½œäº† Â¶è®­ç»ƒç®—æ³• Â¶åŸºäºä¿¡æ¯ç†µçš„æ„é€  å½“é€‰æ‹©æŸä¸ªç‰¹å¾ä½œä¸ºèŠ‚ç‚¹æ—¶ï¼Œæˆ‘ä»¬å°±å¸Œæœ›è¿™ä¸ªç‰¹å¾çš„ä½¿å¾—åˆ†ç±»ç»“æœä¿¡æ¯ç†µè¶Šå°è¶Šå¥½ï¼Œé‚£ä¹ˆä¸ç¡®å®šæ€§è¶Šå°ã€‚ è®¡ç®—ç‰¹å¾çš„ä¿¡æ¯ç†µå…¬å¼å¦‚ä¸‹ï¼š $$ H(x) = -p_i(x)log^{p_i(x)} = -\frac{n_j}{S}log^{\frac{n_j}{S}}$$ $n_j$: ç¬¬jä¸ªç±»åˆ«ï¼Œåœ¨æ ·æœ¬ä¸­å‡ºç°çš„é¢‘æ•° $S$: æ ·æœ¬ä¸ªæ•° å¯¹äºç¦»æ•£å±æ€§ï¼Œç›´æ¥è®¡ç®—ä¿¡æ¯ç†µï¼Œè¿ç»­å±æ€§ï¼Œå°±éœ€è¦åˆ’åˆ†åŒºé—´ï¼ŒæŒ‰åŒºé—´è®¡ç®—ä¿¡æ¯ç†µã€‚ åŸºäºæŸä¸€å±‚çš„æ•°æ®é›† a. éå†è®¡ç®—æ‰€æœ‰å±æ€§ï¼Œéå†ç›¸åº”å±æ€§ä»¥ä¸åŒå€¼ä¸ºåˆ†æˆªç‚¹çš„ä¿¡æ¯ç†µ b. é€‰æ‹©ä¿¡æ¯ç†µæœ€å°çš„ä½œä¸ºèŠ‚ç‚¹ å¦‚æœåˆ°è¾¾ç»ˆæ­¢æ¡ä»¶ï¼Œè¿”å›ç›¸åº”ä¿¡æ¯ï¼Œå¦åˆ™ï¼ŒæŒ‰ç…§åˆ†æ”¯é‡å¤æ­¥éª¤1 Â¶ID3 ç®—æ³•ï¼š ä¿¡æ¯å¢ç›Šæœ€å¤§åŒ– å»ºç«‹åœ¨å¥¥å¡å§†å‰ƒåˆ€çš„åŸºç¡€ä¸Šã€‚ æ€æƒ³ é›†åˆCçš„ä¿¡æ¯ç†µ $$HÂ©=-\sum_{i=1}^{m}p_i log 2^{p_i}$$ æŒ‰ç…§Dç»„åˆ’åˆ†Cï¼Œæ•°æ®é›†Cçš„æ¡ä»¶ç†µï¼Œ $$H(C/D)=\sum{i=1}^{v}\frac{|C_i|}{|C|}H(C_i) = \sum_{i=1}^{v}\frac{|C_i|}{|C|}\sum_{j = 1}^{m}\frac{|C_{ik}|}{|C_i|}log_2\frac{|C_{ik}|}{|C_2|}$$ ä¿¡æ¯å¢ç›Š = ä¿¡æ¯ç†µ-æ¡ä»¶ç†µ $$ gain(C,D) = gainÂ©-H(C/D)$$ è¿™é‡Œæˆ‘å°±ä»¥ç½‘ä¸Šç»™å‡ºçš„æ•°æ®ä¸ºä¾‹ï¼Œç»™å‡ºæ ¹æ®ä¿¡æ¯ç†µæ„æˆå†³ç­–æ ‘çš„è®¡ç®—è¿‡ç¨‹ã€‚ ç¡®å®šç‰¹å¾ï¼Œç»Ÿè®¡å±æ€§å€¼å’Œåˆ†è§£ç»“æœï¼Œæ€»å…±å››ä¸ªç‰¹å¾ï¼Œå››ç§ç‰¹å¾çš„ç»Ÿè®¡ç»“æœå¦‚ä¸‹å›¾ï¼š æ ¹æ®å†å²æ•°æ®ï¼Œåœ¨ä¸çŸ¥åˆ°ä»»ä½•æƒ…å†µä¸‹ï¼Œè®¡ç®—æ•°æ®æœ¬èº«çš„ç†µä¸º $$ - \frac{9}{14}log_2 \frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940$$ è®¡ç®—æ¯ä¸ªç‰¹å¾åšä¸ºèŠ‚ç‚¹çš„ä¿¡æ¯ç†µ ä»¥å¤©æ°”ä¸ºä¾‹ï¼Œå¤©æ°”ä¸‰ç§å±æ€§ï¼Œå½“Outlook = sunnyæ—¶ï¼ŒH(x) = $-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5}$; å½“Outlook= overcast,$H(x)=0$,å½“Outlook = rainy ,$H(x) = 0.971$ æ‰€ä»¥ï¼Œå½“é€‰å¤©æ°”ä½œä¸ºèŠ‚ç‚¹æ—¶ï¼Œæ­¤æ—¶$H(x)=\frac{5}{14}*0.971+\frac{4}{14}*0+\frac{5}{14}*0.971 = 0.693$,gain(å¤©æ°”) = 0.247 åŒç†ï¼Œå¯å¾—gain(æ¸©åº¦) =0.029 gain(æ¹¿åº¦)=0.152ï¼Œgain(é£)=0.048 å› æ­¤é€‰æ‹©å¤©æ°”èŠ‚ç‚¹ï¼Œåœ¨é€’å½’å®ç°å…¶ä»–èŠ‚ç‚¹çš„é€‰æ‹©ã€‚ ä¿¡æ¯å¢ç›Šçš„æ–¹æ³•åå‘é€‰æ‹©å…·æœ‰å¤§é‡å€¼çš„å±æ€§ï¼Œä¹Ÿå°±æ˜¯è¯´æŸä¸ªå±æ€§ç‰¹å¾ç´¢å–çš„ä¸åŒå€¼è¶Šå¤šï¼Œé‚£ä¹ˆè¶Šæœ‰å¯èƒ½ä½œä¸ºåˆ†è£‚å±æ€§ï¼Œè¿™æ ·æ˜¯ä¸åˆç†çš„ï¼› ç¼ºç‚¹ æ²¡æœ‰å‰ªçº¸ç­–ç•¥ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ ä¿¡æ¯å¢ç›Šå‡†åˆ™è¡¨ç°å‡ºå¯¹å–å€¼è¾ƒå¤šçš„ç‰¹å¾ï¼Œåˆ—å¦‚ç¼–å·ï¼Œç”Ÿæ—¥è¿™ç§ æ²¡æœ‰è€ƒè™‘ç¼ºå¤±å€¼ Â¶C4.5: ä¿¡æ¯å¢ç›Šç‡ C4.5 ç›¸å¯¹äºID3çš„ç¼ºç‚¹æ”¹è¿›å¦‚ä¸‹ï¼š å¼•å…¥äº†å‰ªçº¸ç­–ç•¥ å¯¹äºå…·æœ‰ç¼ºå¤±å€¼ç‰¹å¾ï¼Œç”¨æ²¡æœ‰ç¼ºå¤±çš„æ ·æœ¬å­é›†æ‰€å æ¯”é‡æ¥æŠ˜ç®—ï¼› å¼•å…¥ä¿¡æ¯å¢ç›Šç‡ä½œä¸ºåˆ’åˆ†æ ‡å‡† è¿ç»­ç‰¹å¾ç¦»æ•£åŒ– ç¼ºå¤±å€¼å¤„ç†ã€‚ ä»¥ä¸åŒæ¦‚ç‡åˆ’åˆ†åˆ°ä¸åŒèŠ‚ç‚¹ä¸­ å¦‚æœè¿™é‡Œè€ƒè™‘äº†ä¸€åˆ—ID,æ¯ä¸ªIDå‡ºç°ä¸€æ¬¡ï¼Œæ‰€ä»¥ç®—å‡ºçš„ä¿¡æ¯å¢ç›Šå¤§ã€‚ $ H(x) = 0$,ä¿¡æ¯å¢ç›Šæœ€å¤§åŒ–äº†ï¼Œå¯ä»¥å¼•å…¥ä¿¡æ¯å¢ç›Šç‡ $$C(T) = \frac{ä¿¡æ¯å¢ç›Š}{H(T)} =\frac{HÂ©-H(C/T)}{H(T)}$$ Â¶CART:åŸºå°¼(Gini)ç³»æ•° $$G = 1-\sum_{i=l_k}{k}p_i2$$,ä¹Ÿæ˜¯å¯¹éšæœºå˜é‡ä¸ç¡®å®šæ€§çš„ä¸€ä¸ªè¡¡é‡ï¼Œginiè¶Šå¤§ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§ Â¶è¿ç»­å±æ€§çš„å¤„ç†æ–¹æ³• é€‰å–åˆ†è§£ç‚¹çš„é—®é¢˜ï¼š åˆ†æˆä¸åŒçš„åŒºé—´ï¼ˆäºŒåˆ†ã€ä¸‰åˆ†â€¦)ï¼Œåˆ†åˆ«è®¡ç®—å¢ç›Šå€¼ï¼Œç„¶åæ¯”è¾ƒé€‰æ‹©ã€‚ å°†éœ€è¦å¤„ç†çš„æ ·æœ¬ï¼ˆå¯¹åº”æ ¹èŠ‚ç‚¹ï¼‰æˆ–æ ·æœ¬å­é›†ï¼ˆå¯¹åº”å­æ ‘ï¼‰æŒ‰ç…§è¿ç»­å˜é‡çš„å¤§å°ä»å°åˆ°å¤§è¿›è¡Œæ’åº å‡è®¾è¯¥å±æ€§å¯¹åº”ä¸åŒçš„å±æ€§å€¼å…±Nä¸ªï¼Œé‚£ä¹ˆæ€»å…±æœ‰N-1ä¸ªå¯èƒ½çš„å€™é€‰åˆ†å‰²å€¼ç‚¹ï¼Œæ¯ä¸ªå€™é€‰çš„åˆ†å‰²é˜ˆå€¼ç‚¹çš„å€¼ä¸ºä¸Šè¿°æ’åºåçš„å±æ€§å€¼ä¸­ä¸¤ä¸¤å‰åè¿ç»­å…ƒç´ çš„ä¸­ç‚¹ã€‚ é˜™å€¼ï¼šthreshold Â¶è¯„ä»· è¯„ä»·å‡½æ•°ï¼š $$C(T) = \sum_{releaf} N_t*H(T)$$ $ N_t$ï¼šæ¯ä¸ªå¶å­èŠ‚ç‚¹é‡Œé¢å«æœ‰çš„æ ·æœ¬ä¸ªæ•° $H(T)$:å¶å­èŠ‚ç‚¹å«æœ‰çš„ä¿¡æ¯ç†µ Â¶è¿‡æ‹Ÿåˆ å¦‚æœå†³ç­–æ ‘è¿‡äºåºå¤§ï¼Œåˆ†æ”¯å¤ªå¤šï¼Œå¯èƒ½é€ æˆè¿‡æ‹Ÿåˆã€‚å¯¹åº”è®­ç»ƒæ ·æœ¬éƒ½å°½å¯èƒ½çš„åˆ†å¯¹ï¼Œä¹Ÿè®¸æ ·æœ¬æœ¬èº«å°±å­˜åœ¨å¼‚å¸¸ç‚¹å‘¢ï¼Ÿ I. é¢„å‰ªæï¼šè¾¹æ„å»ºï¼Œè¾¹å‰ªæ æŒ‡å®šæ·±åº¦d èŠ‚ç‚¹çš„min_sample èŠ‚ç‚¹ç†µå€¼æˆ–è€…giniå€¼å°äºé˜™å€¼ ç†µå’ŒåŸºå°¼å€¼çš„å¤§å°è¡¨ç¤ºæ•°æ®çš„å¤æ‚ç¨‹åº¦ï¼Œå½“ç†µæˆ–è€…åŸºå°¼å€¼è¿‡å°æ—¶ï¼Œè¡¨ç¤ºæ•°æ®çš„çº¯åº¦æ¯”è¾ƒå¤§ï¼Œå¦‚æœç†µæˆ–è€…åŸºå°¼å€¼å°äºä¸€å®šç¨‹åº¦æ•°ï¼ŒèŠ‚ç‚¹åœæ­¢åˆ†è£‚ã€‚ å½“æ‰€æœ‰7ç‰¹å¾éƒ½ç”¨å®Œäº† æŒ‡å®šèŠ‚ç‚¹ä¸ªæ•° å½“èŠ‚ç‚¹çš„æ•°æ®é‡å°äºä¸€ä¸ªæŒ‡å®šçš„æ•°é‡æ—¶ï¼Œä¸ç»§ç»­åˆ†è£‚ã€‚ä¸¤ä¸ªåŸå› ï¼šä¸€æ˜¯æ•°æ®é‡è¾ƒå°‘æ—¶ï¼Œå†åšåˆ†è£‚å®¹æ˜“å¼ºåŒ–å™ªå£°æ•°æ®çš„ä½œç”¨ï¼›äºŒæ˜¯é™ä½æ ‘ç”Ÿé•¿çš„å¤æ‚æ€§ã€‚æå‰ç»“æŸåˆ†è£‚ä¸€å®šç¨‹åº¦ä¸Šæœ‰åˆ©äºé™ä½è¿‡æ‹Ÿåˆçš„å½±å“ã€‚ II. åå‰ªæï¼š æ„å»ºå¥½åï¼Œç„¶åæ‰å¼€å§‹è£å‰ª $$ C_\alpha(T) = C(T)+\alpha|T_{leaf}|$$ åœ¨æ„é€ å«ä¸€æ£µæ ‘åï¼Œé€‰ä¸€äº›èŠ‚ç‚¹åšè®¡ç®—ï¼Œçœ‹æ˜¯å¦éœ€è¦å‰ªæã€‚ åå‰ªæå†³ç­–æ ‘çš„æ¬ æ‹Ÿåˆé£é™©å¾ˆå°ï¼Œæ³›åŒ–æ€§èƒ½å¾€å¾€ä¼˜äºé¢„å‰ªæå†³ç­–æ ‘ã€‚ä½†åŒæ—¶å…¶è®­ç»ƒæ—¶é—´ä¼šå¤§çš„å¤š Â¶ç†µ bias ç”Ÿæ—¥è¿™ç§å±æ€§ï¼ŒæŠŠå±æ€§åˆ†çš„å¤ªå¤šäº†ï¼Œåˆ†çš„è¶Šç»†ï¼Œå¾€å¾€ç†µè¶Šå¤§ã€‚ Â¶å†³ç­–æ ‘å•ä¸ªèŠ‚ç‚¹é€‰æ‹©çš„ä»£ç å®ç° Â¶ç®€å•å®ç°äº†å•ä¸ªèŠ‚ç‚¹å†³ç­–æ„é€ è¿‡ç¨‹ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182def split(X,y,d,value):'''åœ¨dçº¬åº¦ä¸Šï¼ŒæŒ‰ç…§valueè¿›è¡Œåˆ’åˆ†''' index_a =(X[:,d]&lt;=value) index_b =(X[:,d]&gt;value) return X[index_a],X[index_b],y[index_a],y[index_b]from collections import Counterfrom math import log from numpy as npdef entropy(y): counter = Counter(y) # å­—å…¸ res = 0.0 for num in counter.values(): p = num/len(y) res+=-p*log(p) return resdef gain(X,y,d,v): X_l,X_r,y_l,y_r = split(X,y,d,v) e = len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r) return (entropy(y)-e)def gainratio(X,y,d,v): X_l,X_r,y_l,y_r = split(X,y,d,v) gain =entropy(y) - len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r) return gain/(entropy(y_l)+entropy(y_r))def gini(y): counter = Counter(y) res = 1.0 for num in counter.values(): p = num / len(y) res += -p**2 return res #X_l,X_r,y_l,y_r = split(X,y,d,v) #return 1-(len(y_l)/len(y))**2-(len(y_r)/len(y))**2def try_split(X,y): best_entropy = float('inf') best_d,best_v=-1,-1 for d in range(X.shape[1]): sorted_index = np.argsort(X[:,d]) for i in range(1, len(X)): if (X[sorted_index[i],d] != X[sorted_index[i-1],d]): v = (X[sorted_index[i-1],d]+X[sorted_index[i],d])/2 X_l,X_r,y_l,y_r = split(X,y,d,v) # ä¿¡æ¯ç†µ e = entropy(y_l)+entropy(y_r) #gini e = gini(y_l) + gini(y_r) # ä¿¡æ¯å¢ç›Š e = -gain(X,y,d,v) if e &lt; best_entropy: best_entropy, best_d,best_v = e,d,v return best_entropy, best_d, best_v# æ‰‹åŠ¨æ¥åˆ’åˆ†data =np.array([[ 0.3 , 5 , 2 , 0 ],[ 0.4 , 6 , 0 , 0 ],[ 0.5 , 6.5 , 1 , 1 ],[ 0.6 , 6 , 0 , 0 ],[ 0.7 , 9 , 2 , 1 ],[ 0.5 , 7 , 1 , 0 ],[ 0.4 , 6 , 0 , 0 ],[ 0.6 , 8.5 , 0 , 1 ],[ 0.3 , 5.5 , 2 , 0 ],[ 0.9 , 10 , 0 , 1 ],[ 1 , 12 , 1 , 0 ],[ 0.6 , 9 , 1 , 0 ],])X =data[:,0:3]y = data[:,-1]# æ‰‹åŠ¨æ¥åˆ’åˆ†best_entropy, best_d, best_v = try_split(X, y)print(best_entropy, best_d, best_v)X1_l, X1_r, y1_l, y1_r = split(X,y,best_d,best_v)print(X1_l, X1_r, y1_l, y1_r)best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)X2_l, X2_r, y2_l, y2_r = split(X1_r,y1_r,best_d2,best_v2)entropy(y2_l) Â¶Python skleané‡Œé¢treeæ¨¡å—é‡Œé¢çš„DecisionTreeClassifier 1234from sklearn import treeclf =tree.DecisionTreeClassifier(max_depth=1,criterion ='gini') # criterion='entropy|gini'clf = clf.fit(X,y) è®­ç»ƒå¥½ä¸€é¢—å†³ç­–æ ‘ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨export_graphvizå¯¼å‡ºå™¨ä»¥Graphvizæ ¼å¼å¯¼å‡ºæ ‘ã€‚ 1234import graphviz dot_data = tree.export_graphviz(clf, out_file=None,) graph = graphviz.Source(dot_data) graph.render("data") åœ¨è¿è¡Œæ—¶å¯ä»¥å‡ºé”™ï¼š ExecutableNotFound: failed to execute [â€˜dotâ€™, â€˜-Tpdfâ€™, â€˜-Oâ€™, â€˜dataâ€™], make sure the Graphviz executables are on your systemsâ€™ PATH åŸå› ï¼šgraphvizæœ¬èº«æ˜¯ä¸€ä¸ªè½¯ä»¶ï¼Œéœ€è¦é¢å¤–ä¸‹è½½ï¼Œå¹¶å°†å…¶binåŠ å…¥ç¯å¢ƒå˜é‡ä¹‹ä¸­ã€‚ä¸‹è½½]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>å†³ç­–æ ‘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å†³ç­–æ ‘]]></title>
    <url>%2F2019%2F03%2F03%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[ä¸»è¦æ˜¯åˆ†äº«å†³ç­–çš„åŸºæœ¬çŸ¥è¯†ç‚¹ï¼Œé‡ç‚¹åœ¨åˆ†ç±»å†³ç­–æ ‘ä¸Šï¼Œå¯¹äºå›å½’çš„å†³ç­–æ ‘åé¢åœ¨ç»™å‡ºã€‚å¸Œæœ›å¤§å®¶å’Œæˆ‘ä¸€èµ·åšçŸ¥è¯†çš„ä¼ æ’­è€…å•¦ï¼ğŸ˜„ ğŸ˜ƒ ğŸ˜ ğŸ˜® [TOC] å†³ç­–æ ‘ è‹±æ–‡åå­—ï¼šDescision Tree Â¶ä»€ä¹ˆæ˜¯å†³ç­–æ ‘ ä¸¾ä¸ªæ ¡å›­ç›¸äº²çš„ä¾‹å­ï¼Œä»Šå¤©æ ¡å›­çš„å°çŒ«(å¥³)å’Œå°ç‹—(ç”·)å‡†å¤‡é…å¯¹ï¼Œå°çŒ«å¦‚ä½•æ‰èƒ½åœ¨ä¼—å¤šçš„ä¼˜è´¨ğŸ¶çš„å¿ƒä»ªçš„ç‹—å‘¢ï¼Ÿäºæ˜¯å‘¢ï¼Ÿæœ‰ä¸€åªç‰¹ä¹–å·§çš„å°çŒ«æ‰¾åˆ°äº†ä½ ï¼Œä½ æ­£åœ¨å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Œåˆšå¥½å­¦ä¹ äº†å†³ç­–æ ‘ï¼Œå‡†å¤‡ç»™è¿™åªçŒ«çŒ«æŒ‘é€‰ä¼˜è´¨ç‹—ï¼Œå½“ç„¶ï¼Œä½ ä¸ä»…ä»…æ˜¯ç›´æ¥å‘Šè¯‰çŒ«å“ªäº›ç‹—æ˜¯åˆé€‚ä½ çš„ï¼Ÿä½ æ›´åº”è¯¥è¯¦ç»†çš„ç»™çŒ«è®²è§£å†³ç­–æ ‘æ˜¯å¦‚ä½•æ ¹æ®å®ƒæå‡ºçš„æ ‡å‡†é€‰å‡ºçš„ç¬¦åˆè¦æ±‚çš„ç‹—å‘¢ï¼Ÿ çŒ«ç»™å‡ºå¦‚ä¸‹ä¿¡æ¯ï¼š å¹´é¾„&lt;0.5 ä¸å¿ƒä»ªï¼›å¹´é¾„å¤§äº&gt;=0.5 6.5&lt;=ä½“é‡&lt;=8.5;å¿ƒä»ª; å¹´é¾„&gt;=0.5 ä½“é‡&gt;8.5 é•¿ç›¸å¥½ å¿ƒä»ª;å…¶ä½™æƒ…å†µä¸å¿ƒä»ª; æ ¹æ®ä¸Šè¿°æ¡ä»¶å¯ä»¥æ„é€ ä¸€é¢—æ ‘ï¼š ä¸Šé¢çš„å›¾å°±æ˜¯å†³ç­–æ ‘ï¼Œæœ€ç»ˆçš„ç»“æœæ˜¯å¿ƒä»ªæˆ–è€…ä¸å¿ƒä»ªã€‚å†³ç­–æ ‘ç®—æ³•ä»¥æ ‘å½¢ç»“æ„è¡¨ç¤ºæ•°æ®åˆ†ç±»çš„ç»“æœ Â¶åŸºæœ¬æ¦‚å¿µ å†³ç­–æ ‘å±äºä¹Ÿåªèƒ½éå‚æ•°å­¦ä¹ ç®—æ³•ã€å¯ä»¥ç”¨äºè§£å†³(å¤š)åˆ†ç±»é—®é¢˜ï¼Œå›å½’é—®é¢˜ã€‚ å›å½’é—®é¢˜çš„ç»“æœï¼Œå¶å­ç»“ç‚¹çš„å¹³å‡å€¼æ˜¯å›å½’é—®é¢˜çš„è§£ã€‚ æ ¹èŠ‚ç‚¹ï¼šå†³ç­–æ ‘å…·æœ‰æ•°æ®ç»“æ„é‡Œé¢çš„äºŒå‰æ ‘ã€æ ‘çš„å…¨éƒ¨å±æ€§ éå¶å­èŠ‚ç‚¹ ï¼šï¼ˆå†³ç­–ç‚¹ï¼‰ ä»£è¡¨æµ‹è¯•çš„æ¡ä»¶ï¼Œæ•°æ®çš„å±æ€§çš„æµ‹è¯• å¶å­èŠ‚ç‚¹ ï¼šåˆ†ç±»åè·å¾—åˆ†ç±»æ ‡è®° åˆ†æ”¯ï¼š æµ‹è¯•çš„ç»“æœ Â¶æ•°å­¦é—®é¢˜-ç†µ-Giniç³»æ•° ä»€ä¹ˆæ˜¯ç†µï¼šç†µçš„æ¦‚å¿µæºäºç‰©ç†å­¦ï¼Œç”¨äºåº¦é‡ä¸€ä¸ªçƒ­åŠ›å­¦ç³»ç»Ÿçš„æ— åºç¨‹åº¦ã€‚ ä¿¡æ¯ç†µï¼šä¸å¾—ä¸æé¦™å†œè¿™ä¸ªå¤§å†™çš„äººå•¦ï¼ä¿¡æ¯è®ºé‡Œé¢çš„çŸ¥è¯†ã€‚åœ¨ä¿¡æ¯è®ºé‡Œé¢ï¼Œä¿¡æ¯ç†µè¡¡é‡ä¿¡æ¯é‡çš„å¤§å°ï¼Œä¹Ÿå°±æ˜¯å¯¹éšæœºå˜é‡ä¸ç¡®å®šåº¦çš„ä¸€ä¸ªè¡¡é‡ã€‚ç†µè¶Šå¤§ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§ï¼› å¯¹äºæŸä¸ªå•ç¬¦å·æ— è®°å¿†ä¿¡æºï¼Œå‘å‡ºç¬¦å·($x_i$)çš„æ¦‚ç‡æ˜¯$p_i$,æ¦‚ç‡è¶Šå¤§ï¼Œç¬¦å·çš„ä¿¡æ¯é‡å°±è¶Šå°ï¼Œé¦™å†œå…¬å¼ $I(x_i)=-log_{p_i}$ã€‚ä¿¡æºæ‰€å«çš„ä¿¡æ¯ç†µå°±æ˜¯ä¿¡æ¯é‡çš„æœŸæœ›] $H(x)=-\sum p_i*log_{p_i}$ Giniç³»æ•°ï¼š $GimiÂ§ = 1-\sum_{k=1}{K}p_k2$ Â¶å†³ç­–æ ‘å¦‚ä½•æ„å»ºçš„é—®é¢˜ è‡ªæˆ‘æé—®é˜¶æ®µï¼š æ¯ä¸ªèŠ‚ç‚¹çš„ä½ç½®å¦‚ä½•ç¡®å®šï¼Ÿ ç‰¹å¾çš„é€‰æ‹©ï¼šæ¯æ¬¡é€‰å…¥çš„ç‰¹å¾ä½œä¸ºåˆ†è£‚çš„æ ‡å‡†ï¼Œéƒ½æ˜¯ä½¿å¾—å†³ç­–æ ‘åœ¨è¿™ä¸ªèŠ‚ç‚¹çš„æ ¹æ®ä½ è‡ªå·±é€‰æ‹©çš„æ ‡å‡†ï¼ˆä¿¡æ¯ç†µæœ€å°ã€ä¿¡æ¯å¢ç›Šæœ€å¤§ã€giniç³»æ•°æœ€å°ï¼‰. æ¯ä¸ªèŠ‚ç‚¹åœ¨å“ªä¸ªå€¼ä¸Šåšåˆ’åˆ†ï¼Œç¡®å®šåˆ†æ”¯ç»“æ„å‘¢ï¼Ÿ éå†åˆ’åˆ†çš„èŠ‚ç‚¹çš„åˆ†ç•Œå€¼æ“ä½œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ å¯ä»¥æƒ³è±¡ï¼Œæˆ‘ä»¬æ„é€ çš„å†³ç­–æ ‘è¶³å¤Ÿåºå¤§ï¼Œå†³ç­–æ ‘å¯ä»¥æŠŠæ¯ä¸€ä¸ªæ ·æœ¬éƒ½åˆ†å¯¹ï¼Œé‚£ä¹ˆå†³ç­–æ ‘çš„æ³›åŒ–èƒ½åŠ›å°±å¯ä»¥å¾ˆå·®äº† ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°±éœ€è¦å‰ªææ“ä½œäº† Â¶è®­ç»ƒç®—æ³• Â¶åŸºäºä¿¡æ¯ç†µçš„æ„é€  å½“é€‰æ‹©æŸä¸ªç‰¹å¾ä½œä¸ºèŠ‚ç‚¹æ—¶ï¼Œæˆ‘ä»¬å°±å¸Œæœ›è¿™ä¸ªç‰¹å¾çš„ä¿¡æ¯ç†µè¶Šå°è¶Šå¥½ï¼Œé‚£ä¹ˆä¸ç¡®å®šæ€§è¶Šå°ã€‚ è®¡ç®—ç‰¹å¾çš„ä¿¡æ¯ç†µå…¬å¼å¦‚ä¸‹ï¼š $$ H(x) = -p_i(x)log^{p_i(x)} = -\frac{n_j}{S}log^{\frac{n_j}{S}}$$ $n_j$: ç¬¬jä¸ªç±»åˆ«ï¼Œåœ¨æ ·æœ¬ä¸­å‡ºç°çš„é¢‘æ•° $S$: æ ·æœ¬ä¸ªæ•° å¯¹äºç¦»æ•£å±æ€§ï¼Œç›´æ¥è®¡ç®—ä¿¡æ¯ç†µï¼Œè¿ç»­å±æ€§ï¼Œå°±éœ€è¦åˆ’åˆ†åŒºé—´ï¼ŒæŒ‰åŒºé—´è®¡ç®—ä¿¡æ¯ç†µã€‚ åŸºäºæŸä¸€å±‚çš„æ•°æ®é›† a. éå†è®¡ç®—æ‰€æœ‰å±æ€§ï¼Œéå†ç›¸åº”å±æ€§ä»¥ä¸åŒå€¼ä¸ºåˆ†æˆªç‚¹çš„ä¿¡æ¯ç†µ b. é€‰æ‹©ä¿¡æ¯ç†µæœ€å°çš„ä½œä¸ºèŠ‚ç‚¹ å¦‚æœåˆ°è¾¾ç»ˆæ­¢æ¡ä»¶ï¼Œè¿”å›ç›¸åº”ä¿¡æ¯ï¼Œå¦åˆ™ï¼ŒæŒ‰ç…§åˆ†æ”¯é‡å¤æ­¥éª¤1 Â¶ID3ç®—æ³•ï¼š ä¿¡æ¯å¢ç›Šæœ€å¤§åŒ– C:ç±»åˆ« $$HÂ©=-\sum_{i=1}^{m}p_i log 2^{p_i}$$ æŒ‰ç…§Dç»„åˆ’åˆ†C $$H(C/D)=\sum{i=1}^{v}\frac{|C_i|}{|C|}H(C_i)$$ ä¿¡æ¯å¢ç›Š $$ gain(D) = gainÂ©-H(C/D)$$ è¿™é‡Œæˆ‘å°±ä»¥ç½‘ä¸Šç»™å‡ºçš„æ•°æ®ä¸ºä¾‹ï¼Œç»™å‡ºæ ¹æ®ä¿¡æ¯ç†µæ„æˆå†³ç­–æ ‘çš„è®¡ç®—è¿‡ç¨‹ã€‚ ç¡®å®šç‰¹å¾ï¼Œç»Ÿè®¡å±æ€§å€¼å’Œåˆ†è§£ç»“æœï¼Œæ€»å…±å››ä¸ªç‰¹å¾ï¼Œå››ç§ç‰¹å¾çš„ç»Ÿè®¡ç»“æœå¦‚ä¸‹å›¾ï¼š æ ¹æ®å†å²æ•°æ®ï¼Œåœ¨ä¸çŸ¥åˆ°ä»»ä½•æƒ…å†µä¸‹ï¼Œè®¡ç®—æ•°æ®æœ¬èº«çš„ç†µä¸º $$ - \frac{9}{14}log_2 \frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940$$ è®¡ç®—æ¯ä¸ªç‰¹å¾åšä¸ºèŠ‚ç‚¹çš„ä¿¡æ¯ç†µ ä»¥å¤©æ°”ä¸ºä¾‹ï¼Œå¤©æ°”ä¸‰ç§å±æ€§ï¼Œå½“Outlook = sunnyæ—¶ï¼ŒH(x) = $-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5}$; å½“Outlook= overcast,$H(x)=0$,å½“Outlook = rainy ,$H(x) = 0.971$ æ‰€ä»¥ï¼Œå½“é€‰å¤©æ°”ä½œä¸ºèŠ‚ç‚¹æ—¶ï¼Œæ­¤æ—¶$H(x)=\frac{5}{14}*0.971+\frac{4}{14}*0+\frac{5}{14}*0.971 = 0.693$,gain(å¤©æ°”) = 0.247 åŒç†ï¼Œå¯å¾—gain(æ¸©åº¦) =0.029 gain(æ¹¿åº¦)=0.152ï¼Œgain(é£)=0.048 å› æ­¤é€‰æ‹©å¤©æ°”èŠ‚ç‚¹ï¼Œåœ¨é€’å½’å®ç°å…¶ä»–èŠ‚ç‚¹çš„é€‰æ‹©ã€‚ ä¿¡æ¯å¢ç›Šçš„æ–¹æ³•åå‘é€‰æ‹©å…·æœ‰å¤§é‡å€¼çš„å±æ€§ï¼Œä¹Ÿå°±æ˜¯è¯´æŸä¸ªå±æ€§ç‰¹å¾ç´¢å–çš„ä¸åŒå€¼è¶Šå¤šï¼Œé‚£ä¹ˆè¶Šæœ‰å¯èƒ½ä½œä¸ºåˆ†è£‚å±æ€§ï¼Œè¿™æ ·æ˜¯ä¸åˆç†çš„ï¼› Â¶C4.5: ä¿¡æ¯å¢ç›Šç‡ å¦‚æœè¿™é‡Œè€ƒè™‘äº†ä¸€åˆ—ID,æ¯ä¸ªIDå‡ºç°ä¸€æ¬¡ï¼Œæ‰€ä»¥ç®—å‡ºçš„ä¿¡æ¯å¢ç›Šå¤§ã€‚ $ H(x) = 0$,ä¿¡æ¯å¢ç›Šæœ€å¤§åŒ–äº†ï¼Œå¯ä»¥å¼•å…¥ä¿¡æ¯å¢ç›Šç‡ $$C(T) = \frac{ä¿¡æ¯å¢ç›Š}{H(T)} =\frac{HÂ©-H(C/T)}{H(T)}$$ Â¶CART:åŸºå°¼(Gini)ç³»æ•° $$G = 1-\sum_{i=l_k}{k}p_i2$$,ä¹Ÿæ˜¯å¯¹éšæœºå˜é‡ä¸ç¡®å®šæ€§çš„ä¸€ä¸ªè¡¡é‡ï¼Œginiè¶Šå¤§ï¼Œä¸ç¡®å®šæ€§è¶Šå¤§ Â¶è¿ç»­å±æ€§çš„å¤„ç†æ–¹æ³• é€‰å–åˆ†è§£ç‚¹çš„é—®é¢˜ï¼š åˆ†æˆä¸åŒçš„åŒºé—´ï¼ˆäºŒåˆ†ã€ä¸‰åˆ†â€¦)ï¼Œåˆ†åˆ«è®¡ç®—å¢ç›Šå€¼ï¼Œç„¶åæ¯”è¾ƒé€‰æ‹©ã€‚ å°†éœ€è¦å¤„ç†çš„æ ·æœ¬ï¼ˆå¯¹åº”æ ¹èŠ‚ç‚¹ï¼‰æˆ–æ ·æœ¬å­é›†ï¼ˆå¯¹åº”å­æ ‘ï¼‰æŒ‰ç…§è¿ç»­å˜é‡çš„å¤§å°ä»å°åˆ°å¤§è¿›è¡Œæ’åº å‡è®¾è¯¥å±æ€§å¯¹åº”ä¸åŒçš„å±æ€§å€¼å…±Nä¸ªï¼Œé‚£ä¹ˆæ€»å…±æœ‰N-1ä¸ªå¯èƒ½çš„å€™é€‰åˆ†å‰²å€¼ç‚¹ï¼Œæ¯ä¸ªå€™é€‰çš„åˆ†å‰²é˜ˆå€¼ç‚¹çš„å€¼ä¸ºä¸Šè¿°æ’åºåçš„å±æ€§å€¼ä¸­ä¸¤ä¸¤å‰åè¿ç»­å…ƒç´ çš„ä¸­ç‚¹ Â¶è¯„ä»· è¯„ä»·å‡½æ•°ï¼š $$C(T) = \sum_{releaf} N_t*H(T)$$ $ N_t$ï¼šæ¯ä¸ªå¶å­èŠ‚ç‚¹é‡Œé¢å«æœ‰çš„æ ·æœ¬ä¸ªæ•° $H(T)$:å¶å­èŠ‚ç‚¹å«æœ‰çš„ä¿¡æ¯ç†µ Â¶è¿‡æ‹Ÿåˆ å¦‚æœå†³ç­–æ ‘è¿‡äºåºå¤§ï¼Œåˆ†æ”¯å¤ªå¤šï¼Œå¯èƒ½é€ æˆè¿‡æ‹Ÿåˆã€‚å¯¹åº”è®­ç»ƒæ ·æœ¬éƒ½å°½å¯èƒ½çš„åˆ†å¯¹ï¼Œä¹Ÿè®¸æ ·æœ¬æœ¬èº«å°±å­˜åœ¨å¼‚å¸¸ç‚¹å‘¢ï¼Ÿ I. é¢„å‰ªæï¼šè¾¹æ„å»ºï¼Œè¾¹å‰ªæ æŒ‡å®šæ·±åº¦d èŠ‚ç‚¹çš„min_sample èŠ‚ç‚¹ç†µå€¼æˆ–è€…giniå€¼å°äºé˜™å€¼ ç†µå’ŒåŸºå°¼å€¼çš„å¤§å°è¡¨ç¤ºæ•°æ®çš„å¤æ‚ç¨‹åº¦ï¼Œå½“ç†µæˆ–è€…åŸºå°¼å€¼è¿‡å°æ—¶ï¼Œè¡¨ç¤ºæ•°æ®çš„çº¯åº¦æ¯”è¾ƒå¤§ï¼Œå¦‚æœç†µæˆ–è€…åŸºå°¼å€¼å°äºä¸€å®šç¨‹åº¦æ•°ï¼ŒèŠ‚ç‚¹åœæ­¢åˆ†è£‚ã€‚ å½“æ‰€ä»¥ç‰¹å¾éƒ½ç”¨å®Œäº† æŒ‡å®šèŠ‚ç‚¹ä¸ªæ•° å½“èŠ‚ç‚¹çš„æ•°æ®é‡å°äºä¸€ä¸ªæŒ‡å®šçš„æ•°é‡æ—¶ï¼Œä¸ç»§ç»­åˆ†è£‚ã€‚ä¸¤ä¸ªåŸå› ï¼šä¸€æ˜¯æ•°æ®é‡è¾ƒå°‘æ—¶ï¼Œå†åšåˆ†è£‚å®¹æ˜“å¼ºåŒ–å™ªå£°æ•°æ®çš„ä½œç”¨ï¼›äºŒæ˜¯é™ä½æ ‘ç”Ÿé•¿çš„å¤æ‚æ€§ã€‚æå‰ç»“æŸåˆ†è£‚ä¸€å®šç¨‹åº¦ä¸Šæœ‰åˆ©äºé™ä½è¿‡æ‹Ÿåˆçš„å½±å“ã€‚ II. åå‰ªæï¼š æ„å»ºå¥½åï¼Œç„¶åæ‰å¼€å§‹è£å‰ª $$ C_\alpha(T) = C(T)+\alpha|T_{leaf}|$$ åœ¨æ„é€ å«ä¸€æ£µæ ‘åï¼Œé€‰ä¸€äº›èŠ‚ç‚¹åšè®¡ç®—ï¼Œçœ‹æ˜¯å¦éœ€è¦å‰ªæ Â¶å†³ç­–æ ‘å•ä¸ªèŠ‚ç‚¹é€‰æ‹©çš„ä»£ç å®ç° Â¶ç®€å•å®ç°äº†å•ä¸ªèŠ‚ç‚¹å†³ç­–æ„é€ è¿‡ç¨‹ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182def split(X,y,d,value):'''åœ¨dçº¬åº¦ä¸Šï¼ŒæŒ‰ç…§valueè¿›è¡Œåˆ’åˆ†''' index_a =(X[:,d]&lt;=value) index_b =(X[:,d]&gt;value) return X[index_a],X[index_b],y[index_a],y[index_b]from collections import Counterfrom math import log from numpy as npdef entropy(y): counter = Counter(y) # å­—å…¸ res = 0.0 for num in counter.values(): p = num/len(y) res+=-p*log(p) return resdef gain(X,y,d,v): X_l,X_r,y_l,y_r = split(X,y,d,v) e = len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r) return (entropy(y)-e)def gainratio(X,y,d,v): X_l,X_r,y_l,y_r = split(X,y,d,v) gain =entropy(y) - len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r) return gain/(entropy(y_l)+entropy(y_r))def gini(y): counter = Counter(y) res = 1.0 for num in counter.values(): p = num / len(y) res += -p**2 return res #X_l,X_r,y_l,y_r = split(X,y,d,v) #return 1-(len(y_l)/len(y))**2-(len(y_r)/len(y))**2def try_split(X,y): best_entropy = float('inf') best_d,best_v=-1,-1 for d in range(X.shape[1]): sorted_index = np.argsort(X[:,d]) for i in range(1, len(X)): if (X[sorted_index[i],d] != X[sorted_index[i-1],d]): v = (X[sorted_index[i-1],d]+X[sorted_index[i],d])/2 X_l,X_r,y_l,y_r = split(X,y,d,v) # ä¿¡æ¯ç†µ e = entropy(y_l)+entropy(y_r) #gini e = gini(y_l) + gini(y_r) # ä¿¡æ¯å¢ç›Š e = -gain(X,y,d,v) if e &lt; best_entropy: best_entropy, best_d,best_v = e,d,v return best_entropy, best_d, best_v# æ‰‹åŠ¨æ¥åˆ’åˆ†data =np.array([[ 0.3 , 5 , 2 , 0 ],[ 0.4 , 6 , 0 , 0 ],[ 0.5 , 6.5 , 1 , 1 ],[ 0.6 , 6 , 0 , 0 ],[ 0.7 , 9 , 2 , 1 ],[ 0.5 , 7 , 1 , 0 ],[ 0.4 , 6 , 0 , 0 ],[ 0.6 , 8.5 , 0 , 1 ],[ 0.3 , 5.5 , 2 , 0 ],[ 0.9 , 10 , 0 , 1 ],[ 1 , 12 , 1 , 0 ],[ 0.6 , 9 , 1 , 0 ],])X =data[:,0:3]y = data[:,-1]# æ‰‹åŠ¨æ¥åˆ’åˆ†best_entropy, best_d, best_v = try_split(X, y)print(best_entropy, best_d, best_v)X1_l, X1_r, y1_l, y1_r = split(X,y,best_d,best_v)print(X1_l, X1_r, y1_l, y1_r)best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)X2_l, X2_r, y2_l, y2_r = split(X1_r,y1_r,best_d2,best_v2)entropy(y2_l) Â¶Python skleané‡Œé¢treeæ¨¡å—é‡Œé¢çš„DecisionTreeClassifier 1234from sklearn import treeclf =tree.DecisionTreeClassifier(max_depth=1,criterion ='gini') # criterion='entropy|gini'clf = clf.fit(X,y) è®­ç»ƒå¥½ä¸€é¢—å†³ç­–æ ‘ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨export_graphvizå¯¼å‡ºå™¨ä»¥Graphvizæ ¼å¼å¯¼å‡ºæ ‘ã€‚ 1234import graphviz dot_data = tree.export_graphviz(clf, out_file=None,) graph = graphviz.Source(dot_data) graph.render("data") åœ¨è¿è¡Œæ—¶å¯ä»¥å‡ºé”™ï¼š ExecutableNotFound: failed to execute [â€˜dotâ€™, â€˜-Tpdfâ€™, â€˜-Oâ€™, â€˜dataâ€™], make sure the Graphviz executables are on your systemsâ€™ PATH åŸå› ï¼šgraphvizæœ¬èº«æ˜¯ä¸€ä¸ªè½¯ä»¶ï¼Œéœ€è¦é¢å¤–ä¸‹è½½ï¼Œå¹¶å°†å…¶binåŠ å…¥ç¯å¢ƒå˜é‡ä¹‹ä¸­ã€‚ä¸‹è½½]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>å†³ç­–æ ‘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æˆ‘çš„è¯»ä¹¦ç¬”è®°]]></title>
    <url>%2F2019%2F02%2F28%2FSVD%2F</url>
    <content type="text"><![CDATA[ç›®å½• ğŸ˜„ 1ï¸âƒ£ ç®€å•è¯´ä¸€ä¸‹ç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡ä¸ç‰¹å¾åˆ†è§£ I. ç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡ä¸ç‰¹å¾åˆ†è§£ II. å‡ ä½•æ„ä¹‰ III. å¦‚ä½•å®ç°é€šè¿‡Matlabã€Pythonå®ç° 2ï¸âƒ£è¯¦ç»†è§£è¯´SVD I. å‡ ä½•æ„ä¹‰ I. å¥‡å¼‚å€¼åˆ†è§£çš„æ¨å¯¼è¿‡ç¨‹ I. SVDç®—ä¾‹ I. å¦‚ä½•é€šè¿‡Matlabå’ŒPython 3ï¸âƒ£åº”ç”¨ä¸¾ä¾‹ I. ç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡ä¸ç‰¹å¾åˆ†è§£ 4ï¸âƒ£ç‰¹å¾åˆ†è§£ã€å¥‡å¼‚å€¼åˆ†è§£çš„åŒºåˆ« I. ç‰¹å¾åˆ†è§£ã€å¥‡å¼‚å€¼åˆ†è§£çš„åŒºåˆ« ç®€å•è¯´ä¸€ä¸‹ç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡ä¸ç‰¹å¾åˆ†è§£ Â¶ç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡ä¸ç‰¹å¾åˆ†è§£ Theory: å¯¹äºä¸€ä¸ªæ­£é˜µ$M$ï¼Œæ»¡è¶³å¦‚ä¸‹ï¼š $$Mx=\lambda x $$ å…¶ä¸­$\lambda$è¢«æˆä¸ºç‰¹å¾å€¼ï¼Œæ»¡è¶³$||M-\lambda E||=0$å†æœ‰$(M-\lambda E)x=0$ï¼Œå¯è®¡ç®—å…¶ç‰¹å¾å‘é‡ã€‚ å¦‚æœæœ‰äº†ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡åå‘¢ï¼Œåˆ™å¯ä»¥å°†çŸ©é˜µ$M$ç”¨ç‰¹å¾åˆ†è§£ï¼š $$ M=W\sum W^{-1}$$ $W={w_1,w_2,â€¦,w_n}$åˆ†åˆ«æ˜¯ç‰¹å¾å€¼$\lambda_1,\lambda_2,â€¦,\lambda_n$å¯¹åº”çš„ç‰¹å¾å‘é‡æ„æˆçš„æ–¹é˜µ Â¶ å‡ ä½•æ„ä¹‰ å¯¹åº”çŸ©é˜µM,å…¶å¯¹åº”çš„çº¿æ€§å˜åŒ– $$Mx = xâ€™$$ ä¸Šé¢è¿™ä¸ªå¼å­ï¼Œ$Mxï¼Œxâ€™$æ˜¯ä¸€ä¸ªå‘é‡ï¼Œ$x,xâ€™$å¯èƒ½æ˜¯ä¸å…±çº¿çš„(å¦‚å›¾(b))ï¼Œå¦‚æœå‘é‡$Mx,xâ€™$æ»¡è¶³$Mx=xâ€™=\lambda x$,åˆ™å¦‚å›¾(b)ï¼Œè¿™è¯´æ˜äº†è¿™ä¸ªå˜æ¢å°±æ˜¯å¯¹å‘é‡xåšä¸€ä¸ªæ‹‰ä¼¸æˆ–è€…å‹ç¼©ã€‚ Â¶å¦‚ä½•å®ç°é€šè¿‡Matlabã€Pythonå®ç° æ•°å­¦æ¨å¯¼ï¼š $$ Mx = \lambda x$$ $$ Mx-\lambda x=(M-\lambda E)x=0$$ é½æ¬¡çº¿æ€§æ–¹ç¨‹ç»„æœ‰éé›¶è§£ï¼Œåˆ™$||M-\lambda E||=0$å¯æ±‚å¾—ç‰¹å¾å‘é‡ å†å¸¦å›ï¼Œå¯å¾—ç‰¹å¾å‘é‡ã€‚ Matlab: 123d = eig(M) % æ±‚å–çŸ©é˜µMçš„ç‰¹å¾å€¼ï¼Œå‘é‡å½¢å¼å­˜å‚¨[V,D] = eig(M) % è®¡ç®—Mçš„ç‰¹å¾å€¼å¯¹è§’é˜µDå’Œç‰¹å¾å‘é‡Vï¼Œä½¿å¾—MV = VDæˆç«‹[V,D] = eig(M,'nobalance') %å½“çŸ©é˜µMä¸­æœ‰ä¸æˆªæ–­è¯¯å·®æ•°é‡çº§ç›¸å·®ä¸è¿œçš„å€¼æ—¶ï¼Œè¯¥æŒ‡ä»¤å¯èƒ½æ›´ç²¾ç¡®ã€‚'nobalance'èµ·è¯¯å·®è°ƒèŠ‚ä½œç”¨ Python numpyç§‘å­¦è®¡ç®—åº“æä¾›ç›¸åº”çš„æ–¹æ³• 1234import numpy as npx = np.diag((1,2,3)) # è¿™æ˜¯ä½ æƒ³è¦æ±‚å–ç‰¹å¾å€¼çš„æ•°ç»„a,b = numpy.linalg.elg(x) # ç‰¹å¾å€¼èµ‹å€¼ç»™a,å¯¹åº”çš„ç‰¹å¾å‘é‡èµ‹å€¼ç»™b è¯¦ç»†è§£è¯´SVD SVDçš„è‹±æ–‡å…¨ç§°ï¼š Singular Value Decompositionï¼Œä¸­æ–‡åå­—ï¼šå¥‡å¼‚å€¼åˆ†è§£ Â¶å‡ ä½•æ„ä¹‰ å›¾æ¥æº ä»¥äºŒç»´ç©ºé—´ä¸ºä¾‹ å‡ ä½•æ„ä¹‰å°±æ˜¯æŠŠä¸€ä¸ªå•ä½æ­£äº¤çš„ç½‘æ ¼ï¼Œè½¬æ¢ä¸ºå¦å¤–ä¸€ä¸ªå•ä½æ­£äº¤çš„ç½‘æ ¼ å‡å¦‚é€‰å–äº†ä¸€ç»„å•ä½æ­£äº¤åŸº{$\vec{v}1$,$\vec{v}2$},åˆšå¥½çŸ©é˜µ$M$çš„çº¿æ€§å˜åŒ–$M\vec{v}1 $,$M\vec{v}2 $ ä¹Ÿæ­£äº¤ï¼Œç”¨$\vec{u}1,\vec{u}2 $åˆ†åˆ«è¡¨ç¤º$M\vec{v}1 $,$M\vec{v}2 $ çš„å•ä½å‘é‡ï¼Œç”¨$\lambda_1,\lambda_2 $è¡¨ç¤º$M\vec{v}1 $,$M\vec{v}2$çš„é•¿åº¦ï¼Œæè¿°ç½‘æ ¼åœ¨è¿™äº›ç‰¹å®šæ–¹å‘ä¸Šçš„æ‹‰ä¼¸é‡ï¼Œä¹Ÿè¢«ç§°ä½œçŸ©é˜µMçš„å¥‡å¼‚å€¼ã€‚ $M\vec{v}1 =\lambda_1\vec{u}1 $ $M\vec{v}2 =\lambda_2\vec{u}2 $ å¯¹ä»»æ„ç»™å®šçš„å‘é‡ $\vec{x}$ ,åˆ™æœ‰ $$ \mathbf{x}=\left(\mathbf{v}{1} \cdot \mathbf{x}\right) \mathbf{v}{1}+\left(\mathbf{v}{2} \cdot \mathbf{x}\right) \mathbf{v}{2} $$ å†å°†Mçš„çº¿æ€§å˜æ¢ $$ \begin{aligned} M \mathbf{x} &amp;=\left(\mathbf{v}{1} \cdot \mathbf{x}\right) M \mathbf{N}{1}+\left(\mathbf{v}{2} \cdot \mathbf{x}\right) M \mathbf{v}{2} \ M \mathbf{x} &amp;=\left(\mathbf{v}{1} \cdot \mathbf{x}\right) \sigma{1} \mathbf{u}{1}+\left(\mathbf{v}{2} \cdot \mathbf{x}\right) \sigma{2} \mathbf{u}{2} \end{aligned} $$ $$ \begin{array}{c}{M \mathbf{x}=\mathbf{u}{1} \sigma{1} \mathbf{v}{1}^{\top} \mathbf{x}+\mathbf{u}{2} \sigma_{2} \mathbf{v}{2}^{\top} \mathbf{x}} \ {M=\mathbf{u}{1} \sigma_{1} \mathbf{v}{1}^{\top}+\mathbf{u}{2} \sigma_{2} \mathbf{v}_{2}^{\top}}\end{array} $$ so $$ M=U \Sigma V^{T} $$ Â¶å¥‡å¼‚å€¼åˆ†è§£çš„æ¨å¯¼è¿‡ç¨‹ $u=(u_1,u_2,â€¦,u_m)$ $v=(v_1,v_2,â€¦,v_n)$ $u,v$éƒ½æ˜¯ç©ºé—´çš„åŸº,æ˜¯æ­£äº¤çŸ©é˜µ $uTu=E,vTv = E$ ä»»ä½•ä¸€ä¸ªçŸ©é˜µ$M_{m*n}$ï¼Œ$rank(M)=k$ï¼Œä¸€å®šå­˜åœ¨ï¼³ï¼¶ï¼¤,æ¢å¥è¯è¯´ï¼ŒMå¯ä»¥å°†ä¸€ç»„å•ä½æ­£äº¤åŸºæ˜ å°„åˆ°å¦ä¸€ç»„å•ä½æ­£äº¤åŸºã€‚ç­”æ¡ˆæ˜¯è‚¯å®šçš„ è¯æ˜å¦‚ä¸‹ï¼š åœ¨nä¸ºç©ºé—´ä¸­ï¼Œæœ‰ä¸€ç»„å•ä½æ­£äº¤åŸº{$\vec{v}1,\vec{v}2,â€¦,\vec{v}n$},çº¿æ€§å˜åŒ–ä½œç”¨ä»¥å $$ {M\vec{v}1,M\vec{v}2,â€¦,M\vec{v}n} $$ ä¹Ÿæ˜¯æ­£äº¤çš„ï¼Œåˆ™æœ‰ $$ (M\vec{v}i,M\vec{v}j) = (M\vec{x}i)TM\vec{v}_j=\vec{v}_iTM^TM\vec{v}j=0 $$ æ³¨æ„å–”ï¼Œ$MTM$æ˜¯çŸ©é˜µå–”ï¼Œåˆ™ä¼šæœ‰$MTM\vec{v}j=\lambda \vec{v}j$ æ¥ä¸‹å»ï¼Œ $$ \begin{aligned} v{i}^{T} M^{T} \mathrm{M} v{j}=&amp; v{i}^{T} \lambda{j} v{j} \ &amp;=\lambda{j} v{i}^{T} v{j} \ &amp;=\lambda{j} v{i}\dot v{j}=0 \end{aligned} $$ ä¸Šè¿°å°±è¯æ˜äº†æ˜¯æœ‰çš„ï¼šä»»ä½•ä¸€ä¸ªçŸ©é˜µï¼Œéƒ½å¯ä»¥å°†ä¸€ç»„å•ä½æ­£äº¤åŸºè½¬æ¢æˆå¦å¤–ä¸€ç»„æ­£äº¤åŸºã€‚ å½“$i=j$,$&lt;M\vec{v}i,M \vec{v}i&gt;=\lambda_i \vec{v}i \vec{v}i=\lambda_i$ è¿›è¡Œä¸€äº›å•ä½åŒ–ï¼Œè®°$u_i=\frac{A\vec{v}i}{|M\vec{v}i|}=\frac{1}{\sqrt{\lambda_i}}M\vec{v}i$ åˆ™ $$ A v{i}=\sigma{i} u{i}, \sigma{i}(\operatorname{å¥‡å¼‚å€¼})=\sqrt{\lambda{i}}, 0 \leq i \leq \mathrm{k}, \mathrm{k}=\operatorname{Rank}(\mathrm{A}) $$ å½“$k &lt; i &lt;= m$æ—¶ï¼Œå¯¹$u1ï¼Œu2ï¼Œâ€¦ï¼Œuk$è¿›è¡Œæ‰©å±•$u(k+1),â€¦,um$ï¼Œä½¿å¾—$u1ï¼Œu2ï¼Œâ€¦ï¼Œum$ä¸º$m$ç»´ç©ºé—´ä¸­çš„ä¸€ç»„æ­£äº¤åŸº.ä¹Ÿå¯å¯¹$\vec{v}1,\vec{v}2,â€¦,\vec{v}k$è¿›è¡Œæ‰©å±•ï¼Œæ‰©å±•çš„$\vec{v}{k+1},â€¦,\vec{v}{n}$å­˜åœ¨é›¶å­ç©ºé—´é‡Œé¢ã€‚ $$ M\left[ \begin{array}{lll}{\vec{v}{1}} &amp; {\cdots} &amp; {\vec{v}{k}}\end{array}\right| \vec{v}{k+1} \quad \cdots \quad \vec{v}{m} ]= \left[ \begin{array}{c}{\vec{u}{1}^{T}} \ {\vdots} \ {\frac{\vec{u}{k}^{T}}{\vec{u}{k+1}}} \ {\vdots} \ {\vec{u}{n}^{T}}\end{array}\right] \left[ \begin{array}{ccc|c}\sigma_{1} &amp; &amp; 0 &amp; 0\ &amp; {\ddots} &amp; \sigma_{k} &amp; 0 \ \hline 0 &amp; &amp; 0 &amp;0\end{array}\right] $$ $$ M=\left[ \begin{array}{lll}{\vec{u}{1}} &amp; {\cdots} &amp; {\vec{u}{k}}\end{array}\right] \left [ \begin{array}{ccc}\sigma_{1} &amp; &amp; \ &amp; {\ddots} &amp; \ &amp; &amp; {\sigma_{k}}\end{array}\right] \left[ \begin{array}{c}{\vec{v}{1}^{T}} \ {\vdots} \ {\vec{v}{k}^{T}}\end{array}\right]+ \left[ \begin{array}{ccc}{\vec{u}{k+1}} &amp; {\cdots} &amp; {\vec{u}{m}}\end{array}\right] \left[\begin{array}{c} 0 \end{array} \right] \left[ \begin{array}{c}{\vec{v}{k+1}^{T}} \ {\vdots} \ {\vec{v}{n}^{T}}\end{array}\right] $$ Â¶SVDç®—ä¾‹ Uï¼š$AA^T$çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œç”¨å•ä½åŒ–çš„ç‰¹å¾å‘é‡æ„æˆ U V: $A^TA$ çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œç”¨å•ä½åŒ–çš„ç‰¹å¾å‘é‡æ„æˆ V $\sum_{mn} $ :å°†$ AA^{T} $æˆ–è€… A^{T}A çš„ç‰¹å¾å€¼æ±‚å¹³æ–¹æ ¹ï¼Œç„¶åæ„æˆ Î£ ä»¥çŸ©é˜µ$A = \left[\begin{matrix} 1 &amp; 1\1 &amp;1\ 0 &amp;0\\end{matrix} \right]$ ç¬¬ä¸€æ­¥ U ï¼Œä¸‹é¢æ˜¯ä¸€ç§è®¡ç®—æ–¹æ³• å¯¹çŸ©é˜µ $$ A A^{T}=\left[ \begin{array}{lll}{2} &amp; {2} &amp; {0} \ {2} &amp; {2} &amp; {0} \ {0} &amp; {0} &amp; {0}\end{array}\right] $$ ç‰¹å¾åˆ†è§£ï¼Œ ç‰¹å¾æ˜¯4ï¼Œ0ï¼Œ0 ç‰¹å¾å‘é‡æ˜¯ $\left[\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0\right]^{T},\left[-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0\right]{T},[0,0,1]{T}$,å¯å¾—åˆ° $$ U=\left[ \begin{array}{ccc}{\frac{1}{\sqrt{2}}} &amp; {-\frac{1}{\sqrt{2}}} &amp; {0} \ {\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}} &amp; {0} \ {0} &amp; {0} &amp; {1}\end{array}\right] $$ ç¬¬äºŒæ­¥ è®¡ç®—çŸ©é˜µ$A^TA$çš„ç‰¹å¾åˆ†è§£ï¼Œå¯å¾— ç‰¹å¾å€¼4ï¼Œ0ï¼Œ $$ V=\left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {-\frac{1}{\sqrt{2}}} \ {\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}}\end{array}\right] $$ ç¬¬ä¸‰æ­¥ è®¡ç®—$\sum_{mn}$ $$ \Sigma=\left[ \begin{array}{ll}{2} &amp; {0} \ {0} &amp; {0} \ {0} &amp; {0}\end{array}\right] $$ æœ€åï¼Œ $$ A=U \Sigma V^{T}=\left[ \begin{array}{ccc}{\frac{1}{\sqrt{2}}} &amp; {-\frac{1}{\sqrt{2}}} &amp; {0} \ {\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}} &amp; {0} \ {0} &amp; {0} &amp; {1}\end{array}\right] \left[ \begin{array}{ll}{2} &amp; {0} \ {0} &amp; {0} \ {0} &amp; {0}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} &amp; {-\frac{1}{\sqrt{2}}} \ {\frac{1}{\sqrt{2}}} &amp; {\frac{1}{\sqrt{2}}}\end{array}\right]^{T}=\left[ \begin{array}{cc}{1} &amp; {1} \ {1} &amp; {1} \ {0} &amp; {0}\end{array}\right] $$ Â¶å¦‚ä½•é€šè¿‡Matlabå’ŒPython Matlabï¼š 1234567891011s = svd(A)[U,S,V] = svd(A)[U,S,V] = svd(A,'econ')[U,S,V] = svd(A,0)input: A çŸ©é˜µoutput: s:å¥‡å¼‚å€¼ï¼Œä»¥åˆ—å‘é‡å½¢å¼è¿”å›ã€‚å¥‡å¼‚å€¼æ˜¯ä»¥é™åºé¡ºåºåˆ—å‡ºçš„éè´Ÿå®æ•° Sï¼š U:å·¦å¥‡å¼‚å‘é‡ï¼Œä»¥çŸ©é˜µçš„åˆ—å½¢å¼è¿”å›ã€‚ V:å¥‡å¼‚å€¼ï¼Œä»¥å¯¹è§’çŸ©é˜µå½¢å¼è¿”å›ã€‚S çš„å¯¹è§’å…ƒç´ æ˜¯ä»¥é™åºæ’åˆ—çš„éè´Ÿå¥‡å¼‚å€¼ã€‚ å³å¥‡å¼‚å‘é‡ï¼Œä»¥çŸ©é˜µçš„åˆ—å½¢å¼è¿”å›ã€‚ Python 123import numpy as npM = np.array([ [1,1,2],[0,0,1]])U,S,V = np.linalg.svd(M) åº”ç”¨ä¸¾ä¾‹ Â¶åº”ç”¨ 2.1 ä¿¡æ¯æ£€ç´¢ 2.2 æ¨èç³»ç»Ÿ 2.3 åŸºäºååŒè¿‡æ»¤çš„æ¨èç³»ç»Ÿ 2.4 å›¾åƒå‹ç¼© ç‰¹å¾å€¼åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£çš„åŒºåˆ« ç‰¹å¾å€¼åˆ†è§£åªèƒ½æ˜¯æ–¹é˜µï¼Œè€Œå¥‡å¼‚å€¼åˆ†è§£æ˜¯çŸ©é˜µå°±å¯ä»¥ ç‰¹å¾å€¼åˆ†è§£åªè€ƒè™‘äº†å¯¹çŸ©é˜µç¼©æ”¾æ•ˆæœï¼Œå¥‡å¼‚å€¼åˆ†è§£å¯¹çŸ©é˜µæœ‰é€‰æ‹©ã€æ”¶ç¼©ã€æŠ•å½±çš„æ•ˆæœ]]></content>
      <categories>
        <category>æ•°å­¦</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonåº“]]></title>
    <url>%2F2019%2F02%2F24%2Fpython%E5%BA%93%2F</url>
    <content type="text"><![CDATA[å¼€å§‹æ¥è§¦Pythonæ˜¯å¤§äºŒç»“æŸçš„æ—¶å€™ï¼Œåˆ°ç°åœ¨éƒ½å¿«ä¸¤å¹´äº†ï¼Œå…¶å®ä¸€ç›´å¹¶ä¸æ˜¯å¾ˆç»†èŠ‚çš„å­¦ä¹ ï¼Œåªæ˜¯å¸Œæœ›èƒ½å¤Ÿè·‘ä¸ªç»“æœã€‚ä¸è¿‡å‘¢ï¼Ÿï¼Œä»¥åè‚¯å®šæ˜¯ä¼šç»å¸¸ç”¨Pythonï¼Œæ‰€ä»¥å‘¢ï¼Ÿæˆ‘æ¥ä¸‹æ¥ä¼šè®¤çœŸå­¦ä¹ Python Â¶Python é«˜çº§ç”¨æ³•æ€»ç»“ åŸºæœ¬æ•°æ®ç±»å‹ï¼šæ•´å‹ã€æµ®ç‚¹å‹ã€å¸ƒå°”ç±»å‹ Â¶å®¹å™¨ï¼š Containers å®¹å™¨æ˜¯ä¸€ç§æŠŠå¤šä¸ªå…ƒç´ ç»„ç»‡åœ¨ä¸€èµ·çš„æ•°æ®ç»“æ„ï¼Œå®¹å™¨ä¸­çš„å…ƒç´ å¯ä»¥é€ä¸ªåœ°è¿­ä»£è·å–ï¼Œå¯ä»¥ç”¨in, not inå…³é”®å­—åˆ¤æ–­å…ƒç´ æ˜¯å¦åŒ…å«åœ¨å®¹å™¨ä¸­ã€‚é€šå¸¸è¿™ç±»æ•°æ®ç»“æ„æŠŠæ‰€æœ‰çš„å…ƒç´ å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼ˆä¹Ÿæœ‰ä¸€äº›ç‰¹ä¾‹ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„å…ƒç´ éƒ½æ”¾åœ¨å†…å­˜ï¼Œæ¯”å¦‚è¿­ä»£å™¨å’Œç”Ÿæˆå™¨å¯¹è±¡ï¼‰åœ¨Pythonä¸­ï¼Œå¸¸è§çš„å®¹å™¨å¯¹è±¡æœ‰ï¼š list, deque set, frozensets dict, defaultdict, OrderedDict, Counter tuple, namedtuple str Â¶listæ¨å¯¼ï¼ˆlist comprehensions) å®˜æ–¹è§£é‡Šï¼šåˆ—è¡¨è§£æå¼æ˜¯Pythonå†…ç½®çš„éå¸¸ç®€å•å´å¼ºå¤§çš„å¯ä»¥ç”¨æ¥åˆ›å»ºlistçš„ç”Ÿæˆå¼ã€‚ 1å¯¹äºä¸€ä¸ªåˆ—è¡¨ï¼Œæ—¢è¦éå†ç´¢å¼•åˆè¦éå†å…ƒç´ ã€‚ 123array = ['I', 'love', 'Python']for i, element in enumerate(array): array[i] = '%d: %s' % (i, seq[i]) 12345def getitem(index, element): return '%d: %s' % (index, element)array = ['I', 'love', 'Python']arrayIndex = [getitem(index, element) for index, element in enumerate(array)] Â¶è¿­ä»£å™¨å’Œç”Ÿæˆå™¨ Â¶å¯è¿­ä»£å¯¹è±¡ï¼š å‡¡æ˜¯å¯ä»¥è¿”å›ä¸€ä¸ªè¿­ä»£å™¨çš„å¯¹è±¡éƒ½å¯ç§°ä¹‹ä¸ºå¯è¿­ä»£å¯¹è±¡ ä¾‹å¦‚ï¼šlist dic str set tuple range() enumerate(æšä¸¾) f=open()ï¼ˆæ–‡ä»¶å¥æŸ„ï¼‰ 123456789### è¿­ä»£å™¨(iterator)æ˜¯ä¸€ä¸ªå¸¦çŠ¶æ€çš„å¯¹è±¡ï¼Œä»–èƒ½åœ¨ä½ è°ƒç”¨next()æ–¹æ³•çš„æ—¶å€™è¿”å›å®¹å™¨ä¸­çš„ä¸‹ä¸€ä¸ªå€¼ï¼Œä»»ä½•å®ç°äº†__iter__å’Œ__next__()ï¼ˆpython2ä¸­å®ç°next()ï¼‰æ–¹æ³•çš„å¯¹è±¡éƒ½æ˜¯è¿­ä»£å™¨ï¼Œ__iter__è¿”å›è¿­ä»£å™¨è‡ªèº«ï¼Œ__next__è¿”å›å®¹å™¨ä¸­çš„ä¸‹ä¸€ä¸ªå€¼ï¼Œå¦‚æœå®¹å™¨ä¸­æ²¡æœ‰æ›´å¤šå…ƒç´ äº†ï¼Œåˆ™æŠ›å‡ºStopIterationå¼‚å¸¸### ç”Ÿæˆå™¨(generator)ç”Ÿæˆå™¨å…¶å®æ˜¯ä¸€ç§ç‰¹æ®Šçš„è¿­ä»£å™¨ï¼Œä¸è¿‡è¿™ç§è¿­ä»£å™¨æ›´åŠ ä¼˜é›…ã€‚å®ƒä¸éœ€è¦å†åƒä¸Šé¢çš„ç±»ä¸€æ ·å†™__iter__()å’Œ__next__()æ–¹æ³•äº†ï¼Œåªéœ€è¦ä¸€ä¸ªyiledå…³é”®å­—ã€‚ ç”Ÿæˆå™¨ä¸€å®šæ˜¯è¿­ä»£å™¨ï¼ˆåä¹‹ä¸æˆç«‹ï¼‰#åˆ—è¡¨ç”Ÿæˆå¼lis = [x*x for x in range(10)]# å—åˆ°å†…å­˜é™åˆ¶ï¼Œåˆ—è¡¨å®¹é‡è‚¯å®šæ˜¯æœ‰é™çš„#ç”Ÿæˆå™¨è¡¨è¾¾å¼generator_ex = (x*x for x in range(10)) ç”Ÿæˆå™¨ï¼š ä¸ç”¨åˆ›å»ºå®Œæ•´çš„listï¼Œä¸ºèŠ‚çœå¤§é‡çš„ç©ºé—´ï¼Œåœ¨Pythonä¸­ï¼Œè¿™ç§ä¸€è¾¹å¾ªç¯ä¸€è¾¹è®¡ç®—çš„æœºåˆ¶ï¼Œç§°ä¸ºç”Ÿæˆå™¨ï¼šgenerator Tuples:() å­—å…¸ï¼š{ï¼šï¼Œ} Sets: {,} å‡½æ•° ç±» Â¶Pythonåº“----numpy Â¶What NumPy=Numerical+Python ä¸»è¦æ˜¯æä¾›äº†é«˜æ€§èƒ½å¤šç»´æ•°ç»„è¿™ä¸ªå¯¹è±¡ï¼Œä»¥åŠå¤„ç†ç›¸å…³çš„æ–¹æ³• Â¶How è‡ªå®šä¹‰ä¸€ä¸ªï¼ˆ1D or MD)æ•°ç»„æˆ–è€…ç‰¹æ®Šçš„æ•°ç»„,ä¸€ç»´ï¼ŒäºŒç»´ æ•°ç»„åˆ‡ç‰‡ï¼ˆä¹Ÿå°±æ˜¯æå–æ•°ç»„å…ƒç´ ï¼‰ï¼Œæ³¨æ„ a[:,0]å’Œa[:,0:1]æ˜¯ä¸åŒçš„å–” å…³äºæ•°ç»„å±æ€§çš„æ–¹æ³• æ•°ç»„è¿ç®— ç´¢å¼• where å‡½æ•° ç´¢å¼•çš„å¸ƒå°”æ•°ç»„ å¹¿æ’­ï¼ˆBroadcastingï¼‰ ç”¨äºå¤„ç†ä¸åŒæ€§çŠ¶çš„ æ•°ç»„ã€‚ Broadcastingæä¾›äº†ä¸€ç§çŸ¢é‡åŒ–æ•°ç»„æ“ä½œçš„æ–¹æ³•ï¼Œä½¿å¾—å¾ªç¯å‘ç”Ÿåœ¨Cè€Œä¸æ˜¯Pythonã€‚æ ‡é‡ä¹˜ä»¥ä¸€ä¸ªçŸ¢é‡çš„æ—¶å€™ï¼Œç”¨Boradcastingæ›´å¿«ï¼Œå› ä¸º broadcastingåœ¨ä¹˜æ³•æœŸé—´ç§»åŠ¨è¾ƒå°‘çš„å†…å­˜ array å’Œ matrix é€‰æ‹©å“ªä¸ª? æˆ³æˆ‘ çŸ¢é‡åŒ–å’Œå¹¿æ’­ã€ç´¢å¼• åœ¨Pythonä¸­å¾ªç¯æ•°ç»„æˆ–ä»»ä½•æ•°æ®ç»“æ„æ—¶ï¼Œä¼šæ¶‰åŠå¾ˆå¤šå¼€é”€ã€‚ NumPyä¸­çš„å‘é‡åŒ–æ“ä½œå°†å†…éƒ¨å¾ªç¯å§”æ‰˜ç»™é«˜åº¦ä¼˜åŒ–çš„Cå’ŒFortranå‡½æ•°ï¼Œä»è€Œå®ç°æ›´æ¸…æ™°ï¼Œæ›´å¿«é€Ÿçš„Pythonä»£ç ã€‚ Â¶stack|vstack|hstack 1234567891011121314151617181920212223242526272829303132a = np.array([1, 2, 3])b = np.array([2, 3, 4])np.stack((a, b))array([[1, 2, 3], [2, 3, 4]])% hstacka = np.array((1,2,3))b = np.array((2,3,4))np.hstack((a,b))array([1, 2, 3, 2, 3, 4])a = np.array([[1],[2],[3]])b = np.array([[2],[3],[4]])np.hstack((a,b))array([[1, 2], [2, 3], [3, 4]])% vstacka = np.array([1, 2, 3])b = np.array([2, 3, 4])np.vstack((a,b))array([[1, 2, 3], [2, 3, 4]])a = np.array([[1], [2], [3]])b = np.array([[2], [3], [4]])np.vstack((a,b))array([[1], [2], [3], [2], [3], [4]]) Â¶mean 123456a = np.array([[1, 2], [3, 4]])np.mean(a)np.mean(a, axis=0)np.mean(a, axis=1) Â¶reshape reshape(x, y)ï¼Œå…¶ä¸­xè¡¨ç¤ºè½¬æ¢åæ•°ç»„çš„è¡Œæ•°ï¼Œyè¡¨ç¤ºè½¬æ¢åæ•°ç»„çš„åˆ—æ•°ã€‚å½“xæˆ–è€…yä¸º-1æ—¶ï¼Œè¡¨ç¤ºè¯¥å…ƒç´ éšæœºåˆ†é…ï¼Œå¦‚reshape(2, -1)è¡¨ç¤ºåˆ—æ•°éšæœºï¼Œè¡Œæ•°ä¸ºä¸¤è¡Œã€‚ 123456789æ ¼å¼ï¼šnp.reshape((x, y, z))å‚æ•°çš„å«ä¹‰ï¼šxï¼šè¡¨ç¤ºç”Ÿæˆçš„ä¸‰ç»´æ•°ç»„ä¸­äºŒç»´æ•°ç»„çš„ä¸ªæ•°yï¼šè¡¨ç¤ºå•ä¸ªäºŒç»´æ•°ç»„ä¸­ä¸€ç»´æ•°ç»„çš„ä¸ªæ•°zï¼šè¡¨ç¤ºä¸‰ç»´æ•°ç»„çš„åˆ—æ•° Â¶numpyæ•°ç»„å»æ‰å†—ä½™çš„ç»´åº¦-----squeeze()å‡½æ•° import numpy as np a = [[[10, 2, 3]]] a = np.array(a) a_sque = np.squeeze(a) print(a) print(a_sque) Â¶Pythonåº“----pandas è®°å¾—å­¦ä¹ pandasæ˜¯åœ¨å¤§ä¸‰æ—¶å€™çš„ç¾èµ›ï¼ŒèŠ±äº†ä¸€å¤©å¤šæ—¶é—´å­¦ä¹ pandasï¼Œç„¶åé¢„å¤„ç†æ•°æ®ï¼Œå½“æ—¶ä¸‰ä¸ªé˜Ÿå‹éƒ½æ˜¯å„è‡ªçš„å®¶ï¼Œæ˜¯éå¸¸æ„‰å¿«çš„ï¼ï¼ï¼ Â¶what Python Data Analysis Library ä¸‰ç§æ•°æ®ç»“æ„ åºåˆ—ï¼š Series 1D æ•°æ®å¸§ï¼š DataFrame 2D é¢æ¿ï¼š Panel &gt;2D è‡ªå®šä¹‰åˆ›å»º å¯ä»¥é€šè¿‡å­—æ®µã€æ•°æ®ã€seriesã€åˆ—è¡¨ åˆ—è¡¨ä¼ å…¥çš„æ—¶å€™ï¼Œä¸»è¦è¡Œåˆ—ï¼Œå¦‚æœå•ä¸ªåˆ—è¡¨ï¼šåˆ—ï¼›å¦‚æœæ˜¯[[],[]]æ˜¯æŒ‰è¡Œ[] å¦‚æœä½ç½®ä¸å¯¹å¯è½¬ç½® åˆ›å»ºç©º pd.DataFrame() é€‰æ‹©åŒºå— a) Series [] b) DataFrame åˆ—é€‰æ‹© [â€˜columsçš„åå­—â€™] è¡Œåˆ—é€‰æ‹©ï¼š.loc[åˆ—å,è¡Œå]åç§° .iloc[åˆ—ç´¢å¼•,è¡Œç´¢å¼•]æ•´æ•° array .value ç»Ÿè®¡æè¿° .descibe(include = â€˜allâ€™) .head() .tail() .select_dtype(include=[]) .columns .dtype ç¼ºå°‘æ•°æ® æŸ¥çœ‹ç¼ºå¤±å€¼ isnull() notnull() ä¹Ÿå¯ä»¥ åšä¸€äº›ç»Ÿè®¡ï¼Œsum, any,all æ¸…ç†ç¼ºå¤±å€¼ dropna(axis=0)ï¼šaxis = 0:index axis=1,columns å¡«å……ç¼ºå°‘æŒ‡ fillna() æ ‡é‡æ›¿æ¢ æ›¿æ¢ ç»Ÿè®¡å‡½æ•° Pandas å‡½æ•°åº”ç”¨ è¡¨åˆç†å‡½æ•°åº”ç”¨ï¼špipe() è¡Œæˆ–åˆ—å‡½æ•°åº”ç”¨ï¼šapply() å…ƒç´ å‡½æ•°åº”ç”¨ï¼šapplymap() egï¼š pd.pipe(lambda x: x*100) ç±»åˆ«å˜é‡å‘é‡åŒ– éæ•°å€¼ç±»å‹çš„å¤„ç†æ–¹æ³• æ—¶é—´åºåˆ—ç”Ÿæˆ data_range pandas.date_range(â€œ11:00â€, â€œ21:30â€, freq=â€œ30minâ€) å‚æ•° 1Return a fixed frequency DatetimeIndex. Parameters startstr or datetime-like, optional Left bound for generating dates. endstr or datetime-like, optional Right bound for generating dates. periodsint, optional Number of periods to generate. freqstr or DateOffset, default â€˜Dâ€™ Frequency strings can have multiples, e.g. â€˜5Hâ€™. See here for a list of frequency aliases. tzstr or tzinfo, optional Time zone name for returning localized DatetimeIndex, for example â€˜Asia/Hong_Kongâ€™. By default, the resulting DatetimeIndex is timezone-naive. normalizebool, default False Normalize start/end dates to midnight before generating date range. namestr, default None Name of the resulting DatetimeIndex. closed{None, â€˜leftâ€™, â€˜rightâ€™}, optional Make the interval closed with respect to the given frequency to the â€˜leftâ€™, â€˜rightâ€™, or both sides (None, the default). **kwargs For compatibility. Has no effect on the result. Returns rngDatetimeIndex 12345678910111213141516171819202111. DataFrame.stackParameterslevelint, str, list, default -1Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels.dropnabool, default TrueWhether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section.ReturnsDataFrame or SeriesStacked dataframe or series.â€‹```pythondf_single_level_cols weight heightcat 0 1dog 2 3df_single_level_cols.stack()cat weight 0 height 1dog weight 2 height DataFrame.value_connts()è¿”å›åºåˆ—ï¼Œindex=ç»Ÿè®¡å€¼ï¼Œå€¼ï¼šç»Ÿè®¡ä¸ªæ•° Â¶Matplotlib matplotlib.pyplot as plt çª—å£ï¼šfigure: ä¸€ä¸ªçª—å£ï¼Œplt.figure(num=,figsize=(h,w))ä¸‹é¢æ•°æ®éƒ½å±äºå½“å‰çš„figure,æœ‰ä¸€å®šçš„é¡ºåºå–” ç”»å›¾ï¼šplt.plot(x,y,color=,linewidth=,linestyle,label=) æ ‡æ³¨ä¿¡æ¯ï¼š plt.xlim((,)), plt.yxlim((,)),plt.xlabel(),plt.ylabel(),ticks:å›¾åƒçš„å°æ ‡ï¼Œplt.xticks(),plt.yticks([å€¼1ï¼Œå€¼2],[râ€™$å€¼1\ å¯¹åº”çš„æ–‡å­—$â€™,râ€™å€¼2çš„æ–‡å­— \alpha]) åæ ‡è½´ï¼šaxis gac='get current axisâ€™ ax = plt.gca() # è½´ # è·å–å››ä¸ªè½´ ax.spines[â€˜right|left|top|â€™].set_color(â€˜noneâ€™) ax.xaxis.set_ticks_position(â€˜bottomâ€™) ax.spines[â€˜bottomâ€™].set_position((â€˜dataâ€™,-1)) å›¾ä¾‹ï¼šlegend: a. plt.plot(,label=), plt.legend() b. l1, = plt.plot() plt.legend(handles=[l1,],labels=[,],loc=â€˜best|upper right|â€™) æ³¨è§£ annotation a. ç‚¹çš„ä½ç½®(x0ï¼Œy0) plt.scatter(). plt.plot([x0,y0],[y0,0],â€˜kâ€“â€™,lw=) b . method 1: plt.annotate(râ€™nameâ€™,xy=(,)èµ·å§‹ç‚¹ï¼Œxycoords=â€˜dataâ€™//åŸºäºxy,xytext=(+30,30),textcoords=â€˜offseet pointsâ€™//æ–‡æœ¬åŸºäºxy,arrowprops=dict(arrowstyle=â€™-&gt;'ç®­å¤´,connectionstyle=â€˜arc3,rad=.2â€™)å¼§åº¦) Bar æŸ±çŠ¶å›¾ plt.bar(x,+|-y,facecolor=&quot;&quot;,edgecolor,) |# ha horizontal alignment å¯¹é½æ–¹å¼ for x,y in zip(x,y): plt.text(x+0,4,y+0.05,â€™%.2fâ€™%y,ha=â€˜centerâ€™,va=â€˜bottomâ€™) å¾ˆå¤šè‡ªåŠ¨ subplot(æ€»è¡Œï¼Œå½“å‰è¡Œçš„åˆ—ï¼Œæ€»çš„æŒ‰æœ€å°åˆ†çš„ç¬¬å‡ ä¸ª) subplot(,) Â¶index reset_index:é™äºDataFrame set_index index scikit-learn å®˜æ–¹æ•™ç¨‹ç»å¯¹æ˜¯æœ€å¥½æœ€æ£’çš„é€‰æ‹©ï¼Œæœ‰ç®€å•æ•°å­¦æ¨å¯¼ã€ç›´è§‚ç«‹é©¬å°±èƒ½ä¸Šæ‰‹çš„æ¡ˆä¾‹ï¼Œè¿˜èƒ½æé˜…è¯»è‹±æ–‡çš„èƒ½åŠ›å–”ï¼Œå®åœ¨æ˜¯ä¸€ä¸¾å¤šå¾—å•Šï¼ï¼ï¼ï¼ scikit-learn.org Â¶regression Â¶Feature selection Â¶Method from sklearn.feature_selection import VarianceThreshold Â¶sklearn.feature_selection.SelectFromModel class sklearn.feature_selection.SelectFromModel(estimator, , threshold=None, prefit=False, norm_order=1, max_features=None) seaborn seaborn.jointplot(x, y, data=None, kind=â€˜scatterâ€™, stat_func=None, color=None, height=6, ratio=5, space=0.2, dropna=True, xlim=None, ylim=None, joint_kws=None, marginal_kws=None, annot_kws=None, **kwargs) Parameters x, ystrings or vectorsData or names of variables in data.dataDataFrame, optionalDataFrame when x and y are variable names.kind{ â€œscatterâ€ | â€œregâ€ | â€œresidâ€ | â€œkdeâ€ | â€œhexâ€ }, optionalKind of plot to draw.stat_funccallable or None, optionalDeprecatedcolormatplotlib color, optionalColor used for the plot elements.heightnumeric, optionalSize of the figure (it will be square).rationumeric, optionalRatio of joint axes height to marginal axes height.spacenumeric, optionalSpace between the joint and marginal axesdropnabool, optionalIf True, remove observations that are missing from x and y.{x, y}limtwo-tuples, optionalAxis limits to set before plotting.{joint, marginal, annot}_kwsdicts, optionalAdditional keyword arguments for the plot components.kwargskey, value pairingsAdditional keyword arguments are passed to the function used to draw the plot on the joint Axes, superseding items in the joint_kws dictionary. Returns gridJointGridJointGrid object with the plot on it. http://seaborn.pydata.org/generated/seaborn.JointGrid.html#seaborn.JointGrid g = sns.jointplot(x=â€œxâ€, y=â€œyâ€, kind = â€˜regâ€™ , space=0,color = â€˜gâ€™, data=df11,stat_func=sci.pearsonr) sns.set() sns.axes_style(â€œdarkgridâ€) sns.set_context(â€œpaperâ€) https://blog.mazhangjing.com/2018/03/29/learn_seaborn/ https://blog.csdn.net/weiyudang11/article/details/51549672 123456789101112131415#åˆå§‹åŒ–ç±»g=sns.JointGrid(x=&apos;v_ma5&apos;,y=&apos;price_change&apos;,data=stock,space=0.5,ratio=5)g=sns.JointGrid(x=&apos;v_ma5&apos;,y=&apos;price_change&apos;,data=stock,space=0.5,ratio=5)g=g.plot_joint(plt.scatter,color=&apos;.3&apos;,edgecolor=&apos;r&apos;)g=g.plot_marginals(sns.distplot,kde=False)from scipy import statsg=sns.JointGrid(x=&apos;v_ma5&apos;,y=&apos;price_change&apos;,data=stock,space=0.5,ratio=5)g=g.plot_joint(plt.scatter,color=&apos;.3&apos;,edgecolor=&apos;r&apos;)_=g.ax_marg_x.hist(stock.v_ma10,color=&apos;r&apos;,alpha=.6,bins=50)_=g.ax_marg_y.hist(stock.low,color=&apos;y&apos;,orientation=&quot;horizontal&quot;,bins=20)rquare=lambda a,b:stats.pearsonr(a,b)[0]**2g=g.annotate(rquare,template=&apos;&#123;stat&#125;:&#123;val:.2f&#125;&apos;,stat=&apos;$R^2$&apos;,loc=&apos;upper left&apos;,fontsize=12) Â¶é¢œè‰²å’Œé£æ ¼è®¾ç½® Â¶è°ƒè‰²æ¿ ä¸»è¦ä½¿ç”¨ä»¥ä¸‹å‡ ä¸ªå‡½æ•°è®¾ç½®é¢œè‰²ï¼š color_palette() èƒ½ä¼ å…¥ä»»ä½•Matplotlibæ‰€æœ‰æ”¯æŒçš„é¢œè‰² color_palette() ä¸å†™å‚æ•°åˆ™é»˜è®¤é¢œè‰² current_palette = sns.color_palette() sns.palplot(current_palette) plt.show() set_palette() è®¾ç½®æ‰€æœ‰å›¾çš„é¢œè‰² sns.palplot(sns.color_palette(â€œhlsâ€,8)) plt.show() Â¶é¢œè‰²çš„äº®åº¦åŠé¥±å’Œåº¦ l-å…‰åº¦ lightness s-é¥±å’Œ saturation sns.palplot(sns.hls_palette(8,l=.7,s=.9)) plt.show() Â¶xkcdé€‰å–é¢œè‰² xkcdåŒ…å«äº†ä¸€å¥—ä¼—åŒ…åŠªåŠ›çš„é’ˆå¯¹éšæœºRGBè‰²çš„å‘½åã€‚äº§ç”Ÿäº†954ä¸ªå¯ä»¥éšæ—¶é€šè¿‡xkcd_rgbå­—å…¸ä¸­è°ƒç”¨çš„å‘½åé¢œè‰² plt.plot([0,1],[0,1],sns.xkcd_rgb[â€˜pale redâ€™],lw = 3) #lw = çº¿å®½åº¦ plt.plot([0,1],[0,2],sns.xkcd_rgb[â€˜medium greenâ€™],lw = 3) plt.plot([0,1],[0,3],sns.xkcd_rgb[â€˜denim blueâ€™],lw = 3) plt.show() Â¶æ±‡æ€» http://seaborn.pydata.org/api.html# https://github.com/mwaskom/seaborn/blob/master/seaborn/rcmod.py https://xkcd.com/color/rgb/]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æˆ‘çš„è¯»ä¹¦ç¬”è®°]]></title>
    <url>%2F2019%2F02%2F22%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Â¶2019 ç¬¬åäº”å‘¨ ä¸‰æœˆä»½è‡³2019.4.9è¿™æ®µæ—¶é—´ï¼Œæ‰å‘ç°æˆ‘æ˜¯å¦‚æ­¤æ²¡æœ‰è‡ªå¾‹çš„äººï¼Œå……åˆ†ä½“ç°äº†æˆ‘æ˜¯äººçš„ç‰¹æ€§ï¼Œé‚£å°±æ˜¯æˆ‘æ˜¯ç¾¤ä½“åŠ¨ç‰©ï¼Œè‹¦ç¬‘.jpg,è‹¦ç¬‘.jpg, en, æœ€è¿‘çªç„¶æƒ³ç»™è‡ªå·±æ‰“ä¸Šå¨å¨˜çš„èº«ä»½ï¼Œå¦‚æœå¯ä»¥æ¯å¤©èŠ±ä¸¤ä¸ªå°æ—¶åšé¥­å°±å¥½äº† Â¶æ„¿ä½ è¢«ä¸–ç•Œæ¸©æŸ”çš„ç›¸å¾… æ¥è§¦çš„ä¸œè¥¿è¶Šå¤šï¼Œè¶Šæ·±å…¥ï¼Œå°±ä¼šå‘ç°æˆ‘æ˜¯å¦‚æ­¤çš„èœï¼Œå¼€å§‹æœ‰äº›çŸ¥è¯†ç„¦è™‘äº†ï¼ŒçŸ¥è¯†é‚£ä¹ˆå¤š~~~ï¼Œå¯æ˜¯æˆ‘åªæœ‰ä¸€ä¸ªå¤´è„‘å•Š~~~ å¼€å§‹ä¸æƒ³å†™ä¸€äº›ç‰¹åˆ«ä½ä¿—çš„åšå®¢äº†ï¼Œä¸€æ˜¯è§‰å¾—æµªè´¹æ—¶é—´ï¼ŒäºŒæ˜¯è¾“å‡ºæ•ˆæœå¤ªå·®ï¼Œå¼•ä¸èµ·ç‰¹åˆ«å¤§çš„å…³æ³¨ï¼Œè™½ç„¶æˆ‘å†™åšå®¢ï¼Œå®Œå…¨æ˜¯ç«™åœ¨è‡ªå·±çš„è§’åº¦ï¼Œæ²¡æœ‰è€ƒè™‘è¯»è€…çš„æ„æ„¿ï¼Œï¼ˆæ»‘ç¨½.jpg)ã€‚ ç°åœ¨çš„è‡ªå·±ï¼Œä¸æ˜¯åœç•™åœ¨åŸºæœ¬çš„é—®é¢˜ä¸Šï¼Œæ›´åº”è¯¥å»æ¢ç´¢æœªçŸ¥ çš„çŸ¥è¯†ä¸–ç•Œï¼Œè™½ç„¶ç¦»è¿™ä¸ªflagå¯èƒ½è¿˜æœ‰å‡ å¹´çš„æ—¶é—´ï¼Œèƒ½å¤Ÿç»™ä¸–ç•Œçš„çŸ¥è¯†åˆ›é€ ä¸€ç‚¹ç‚¹ä»·å€¼ï¼Œå“ªæ€•åªæ˜¯ä¸€å°ç‚¹ç‚¹ã€‚ç¦»è¿™ä¸ªç›®æ ‡è¿˜éœ€è¦åŠªåŠ›å•Šï¼ï¼ï¼ï¼ï¼ æˆ‘æƒ³æˆ‘åº”è¯¥å»è®°å½•å­¦ä¹ çŸ¥è¯†çš„è¿‡ç¨‹ï¼Œçªç ´æ›´å¤§çš„æ›´å›°éš¾çš„é—®é¢˜ã€‚ Â¶2019-ç¬¬å››å‘¨è¯»ä¹¦ç¬”è®° è¿™å‘¨è¯»äº†ä¸€æœ¬å°è¯´ï¼Œæ˜¯å¼ çˆ±ç²çš„ã€Šå€¾åŸä¹‹æ‹ã€‹ï¼ŒåŸæ¥å’Œç”µè§†å‰§çš„ä½•æ™Ÿé“­ä¸»æ¼”ã€Šå€¾åŸä¹‹æ‹ã€‹ä¸æ˜¯åŒä¸€ä¸ªäº‹æƒ…å•Šï¼ çœ‹äº†ã€Šé˜¿ç”˜æ­£ä¼ ã€‹ï¼Œâ€œç”Ÿæ´»å°±åƒä¸€ç›’å·§å…‹åŠ›ï¼Œä½ æ°¸è¿œä¸çŸ¥é“ä¸‹ä¸€é¢—æ˜¯ä»€ä¹ˆå‘³é“ã€‚â€œè¿™æ˜¯é˜¿ç”˜å¯¹ç”Ÿæ´»æœ€å¥½çš„è¯ é‡Šã€‚å°æ—¶å€™ï¼Œæœ‰äººéª‘ç€è‡ªè¡Œè½¦ç¾è¾±ä»–ï¼Œä»–åªä¼šè·‘ï¼Œæ‹¼å‘½çš„è·‘ï¼Œåªä¼šå†å…¬è·¯ä¸Šè·‘ã€‚é•¿å¤§åï¼Œåˆ«äººéª‘ç€è½¦æƒ³æ‰“ä»–ï¼Œé˜¿ç”˜è¿˜æ˜¯è·‘ï¼Œä½†æ˜¯è¿™æ¬¡é˜¿ç”˜å­¦ä¼šäº†ç½‘è‰åªä¸Šè·‘ï¼å°±è¢«å¤§å­¦çœ‹ä¸Šï¼Œè¿›å…¥è¿åŠ¨å¤§å­¦ï¼Œè¿˜é€šè¿‡å‚åŠ æ¯”èµ›èµ¢å¾—äº†å† å†›ï¼Œç„¶åï¼Œé˜¿ç”˜å½“å…µäº†ï¼Œå†åæ¥ï¼Œæ‰“ä¹’ä¹“çƒå¾ˆå‡ºè‰²ã€‚é˜¿ç”˜ä¼¼ä¹åšä»€ä¹ˆéƒ½èƒ½æˆåŠŸï¼Œä¹Ÿè®¸å¿ƒæ— æ—éª›ï¼Œæœ€ç¬¨çš„æ–¹æ³•+æ—¶é—´=æ”¶è·ã€‚ æˆ‘è§‰å¾—å¾ˆå¿ƒé…¸çš„æ˜¯ï¼Œå½“çå¦®å‘Šè¯‰ä»–æœ‰å„¿å­æ—¶å€™ï¼Œé˜¿ç”˜é—®ï¼Œâ€ä»–èªæ˜å—â€œï¼Ÿ Â¶2019ç¬¬å››å‘¨å®‰æ’ æ”¹è®ºæ–‡ï¼Œæ”¹å˜è‡ªå·±çš„åŠäº‹æ•ˆç‡å–”ï¼Œæ‹’ç»é‡å¤å·¥ä½œ ç¼–ç¨‹èƒ½åŠ› æ…¢æ…¢çš„åšäº‹æƒ…ï¼Œå…ˆæ…¢åå¿«ï¼Œ ç”Ÿæ´»ã€å­¦ä¹ ã€äº¤å‹ã€æ–‡é‡‡ 2019-ç¬¬ä¸‰å‘¨è¯»ä¹¦ç¬”è®° è¿™æ¬¡è¯»äº†ã€Šæç®€æ€ç»´ï¼šé¢ è¦†ä¼ ç»Ÿæ€ç»´æ¨¡å¼çš„æç®€æ³•åˆ™ã€‹ä½œè€…ï¼šS.Jæ–¯ç§‘ç‰¹ å·´é‡Œ.è¾¾æ–‡æ³¢ç‰¹ æˆ‘ä»¬ç”Ÿæ´»å……æ»¡äº†å„ç§è¯±æƒ‘ã€æ‚ä¹±ä¿¡æ¯ã€å¯¼è‡´äº†ç”Ÿæ´»çš„æ··ä¹±ï¼Œäº§ç”ŸçŸ¥è¯†ç„¦è™‘ã€å¹´é¾„å±æœºã€äººé™…å…³ç³»çš„æ·¡åŒ–ã€‚ä½œè€…ç»™æˆ‘ä»¬ä»‹ç»äº†è®¸å¤šé—®é¢˜ã€è®¸å¤šçš„è§£å†³æ–¹æ³•ï¼Œè®©æˆ‘ä»¬è¿™ä¸ªä¿¡æ¯çˆ†ç‚¸çš„æ—¶ä»£å¯ä»¥è¿‡çš„å……å®äº›ã€‚ æ¯å¤©ç¡8ä¸ªå°æ—¶ã€è¿˜å‰©ä¸‹16ä¸ªå°æ—¶ï¼Œåœ¨å‡å»2ä¸ªå°æ—¶è§£å†³ä¸ªäººå«ç”Ÿå’Œé¥®é£Ÿï¼Œé‚£ä¹ˆè¿˜æœ‰14ä¸ªå°æ—¶ï¼Œä¸€ä¸ªæ˜ŸæœŸ98ä¸ªå°æ—¶ã€‚é‚£ä¹ˆ98ä¸ªå°æ—¶ï¼Œä½ æŠ•å…¥åœ¨å“ªé‡Œå‘¢ï¼Ÿ æ€»çš„æ¥è¯´ï¼Œè¿™æœ¬ä¹¦ä¼ è¾¾çš„ä¸œè¥¿ï¼Œæˆ‘è¿˜æ˜¯å¾ˆå–œæ¬¢çš„ï¼Œæç®€ä¸»ä¹‰è€…ï¼Œå°‘ä¸å¾—ä¹Ÿå¤šä¸å¾—ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ Â¶è¯»ã€Šæ‹†æ‰æ€ç»´é‡Œçš„å¢™ã€‹ æ‘˜å½• ï¼šæˆ‘ä»¬çš„ç”Ÿæ´»ä¹Ÿç”±ä¸‰ä¸ªæ”¯æ¶ç»„æˆï¼šè‡ªæˆ‘ã€å®¶åº­ä¸å›¢ä½“å’ŒèŒä¸šã€‚è¿™æ ·çš„æ”¯æ¶æ”¯æ’‘ç€æˆ‘ä»¬çš„çµé­‚ï¼Œå®ƒåœ¨è®°å½•æˆ‘ä»¬çš„ç”Ÿå‘½ã€‚æˆ‘ä»¬ä¸€ç›´éƒ½åœ¨è°ƒæ•´ç€ä¸‰ä¸ªä½ç½®çš„å¹³ç¨³ï¼Œä½¿ä¹‹æˆä¸ºæœ€ç¨³å›ºçš„è”åŠ¨ä¸‰è„šæ¶ã€‚ è¿™å¥å¤§æ¦‚æ˜¯ç»“åˆæˆ‘çš„ç»å†ï¼Œæœ€å…·æœ‰æ„Ÿæ‚Ÿçš„ã€‚å› ä¸ºä¸€æ—¦èµ°å‡ºå¤§å­¦ï¼Œè¿™ä¸‰è€…æ‰å¼€å§‹çœŸæ­£çš„ç»„æˆæˆ‘ä»¬çš„ç”Ÿæ´»ã€‚ å¤å…¸è€å¸ˆï¼Œä»èŒä¸šã€æˆåŠŸå­¦ã€çˆ±æƒ…ã€å®¶åº­ç­‰ç­‰ä¸åŒçš„æ¡ˆä¾‹ï¼Œç»™æˆ‘åˆ†æäº†å¤§å¤šæ•°äººä¼šé¢ä¸´çš„æ— å½¢çš„â€å¢™â€œï¼Œç»™äº†æˆ‘ä»¬å¦‚ä½•æ‹†æ‰è¿™äº›å¢™çš„æ–¹æ³•ã€‚ä½†æ˜¯å‘¢ï¼Œå¯¹äºå¤å…¸è€å¸ˆçš„çˆ±æƒ…è§‚ç‚¹ï¼Œæˆ‘å¹¶ä¸æ˜¯å¾ˆèµåŒï¼Œå› ä¸ºå‘¢ï¼Œé‚£äº›æ„¿æ„é™ªä½ åº¦è¿‡ä½™ç”Ÿçš„äººä»˜å‡ºçš„æ„Ÿæƒ…ï¼Œæ˜¯å¦‚æ­¤çš„å»‰ä»·å—ï¼Ÿæœ‰çš„äººæ—¢å¯ä»¥æ˜¯ç™½ç«ç‘°ï¼Œä¹Ÿå¯ä»¥çº¢ç«ç‘°å•Šï¼ Â¶2019å¹´ç¬¬äºŒå‘¨å®‰Tæ’ æ¯å¤©ä¸¤ä¸ªå°æ—¶é˜…è¯»è®ºæ–‡æˆ–è€…ä¸“ä¸šä¹¦ç±çš„é˜…è¯» å¼€é¢˜æŠ¥å‘Šä¿®æ”¹å’ŒPPTåˆ¶ä½œï¼ˆ3h) ã€Šæ‹†æ‰æ€ç»´é‡Œé¢çš„å¢™ã€‹ï¼ˆ3h) çœ‹å“ˆåˆ©æ³¢ç‰¹ï¼ˆä¸€é›†ï¼‰ 2018å¹´çš„æ€»ç»“ å°å°çš„æ‚”æ¨ä¸é—æ†¾ å¤§ä¸‰ä¸‹ï¼Œåœ¨è¯¾å ‚ä¸Šï¼Œæ‰“äº†åŠå­¦æœŸçš„æ¸¸æˆ ç”Ÿæ´»è¿˜æ˜¯ä¸è§„å¾‹ï¼Œè¶…å–œæ¬¢æ·±å¤œé€›çŸ¥ä¹ã€åˆ·Bç«™ é¢å¤´ä¸Šï¼Œä¸åœçš„å†’ç€ç—˜ç—˜å•Š è‹±è¯­å•è¯é‡åœ¨ä¸‹é™ing è¿åŠ¨é‡åœ¨é™ä½å–” å¾ˆè®¨åŒæ´—è¡£æœ ç›¸æ¯”äºä¸Šä¸€å¹´è¿›æ­¥çš„æ–¹é¢ æ„¿æ„å»æ‰¿æ‹…æ›´å¤šçš„è´£ä»» æ›´ä¹æ„å»äº¤æµ è¶Šæ¥è¶Šé‡è§†å¥åº·style ä¸ä¼šéšæ„å‘æ³„è‡ªå·±çš„æƒ…ç»ªäº† æ›´åŠ è®¤è¯†åˆ°è‡ªèº«çš„ä¼˜åŠ¿ä¸åŠ£åŠ¿äº† æ„Ÿåˆ°æ„‰å¿«çš„äº‹æƒ… çŸ¥é“è‡ªå·±æƒ³è¦ä»€ä¹ˆï¼ŒçŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆ æ•´ç†å®Œäº†å¤§å­¦æœŸé—´æ‰€æœ‰çš„ä¸œè¥¿ï¼Œå¾€äº‹ä¸å ªå›é¦–ï¼Œ ä½†ä¹Ÿåªèƒ½æ˜¯æŸ³æš—èŠ±æ˜åˆä¸€æ‘ã€‚ èƒ½è¯»ç ”ç©¶ç”Ÿäº† èŠèŠ2019å¹´çš„ç‚¹ç‚¹æœŸè®¸ å­¦ä¹ ä¸Š å¤šçœ‹19åœºçŸ¥ä¹live é˜…è¯»10æœ¬ä¹¦ç±ï¼Œä¹¦å•ä¹Ÿæœ‰äº† åœ¨ä¸“ä¸šå­¦ä¹ ä¸Šï¼Œå¸Œæœ›æœ‰æ‰€æå‡å’¯ ç”Ÿæ´»ä¸­ æ—©ç¡æ—©èµ·èº«ä½“å¥½ çœ‹åéƒ¨ç¾å‰§ï¼Œå°½ç®¡æˆ‘æœ€å¤§çš„å…´è¶£æ˜¯ç¡è§‰ æ—¶å¸¸æ›´æ–°æ­Œå•ï¼Œä¸æƒ³åœ¨ä¸€å¹´é‡Œé¢éƒ½æ˜¯ç›¸åŒçš„æ—‹å¾‹ é™é™é™é™é™é™é™ åˆç†å®‰æ’ æŠ˜æ˜Ÿæ˜Ÿ ç•ªèŒ„é—¹é’Ÿ å¶å°”å¬å¬ TED æŠ€æœ¯ æ¸…ç†ä¸‹äº†github ä»“åº“ é‡æ–°æ›´æ–°äº† github page å¤šè¯»ã€å¤šå†™ã€å¤šæƒ³]]></content>
      <categories>
        <category>è¯»ä¹¦æ—¥å¸¸</category>
      </categories>
      <tags>
        <tag>è¯»ä¹¦</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MayMay]]></title>
    <url>%2F2019%2F02%2F22%2FMayMay%2F</url>
    <content type="text"><![CDATA[https://www.kaggle.com/dgawlik/house-prices-eda/data https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python]]></content>
      <tags>
        <tag>wan</tag>
      </tags>
  </entry>
</search>
