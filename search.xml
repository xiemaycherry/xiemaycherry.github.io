<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[向着永恒出发]]></title>
    <url>%2F2020%2F07%2F28%2Fprepare-for-work%2F</url>
    <content type="text"><![CDATA[随从自己的心出发吧！只有是自己喜欢的，不管是对还是错，我肯定是都要坚持的。不喜欢，怎么都地久天长不了。 我还是希望以我喜欢的方式度过余生。 第一周：Python基础知识https://github.com/jackfrued/Python-100-Days/tree/master/Day01-15 第二周：numpy, pandas，matplotlib, seabornhttps://cloudxlab.com/blog/numpy-pandas-introduction/ 第三周：sklearn第五周：sql第六周： Linux第七周和八周：机器学习算法和数据挖掘]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习]]></title>
    <url>%2F2020%2F07%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Data-Analysis]]></title>
    <url>%2F2020%2F07%2F25%2FData-Analysis%2F</url>
    <content type="text"></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活心得]]></title>
    <url>%2F2020%2F07%2F20%2F%E7%94%9F%E6%B4%BB%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[提升自己 20200728 陌上花开，慢慢归矣5w2h是最好的精细化方法。 对不起，我是不是欺骗别人的感情了。从长远看，我是不是不成熟啊！老是给别人造成麻烦。没有跟别人沟通我的真实想法啊。浪费别人的时间和精力啊！ 我现在是很清楚自己的职业规划了。其实，我一直清楚自己的职业规划，但是就是没有沟通好。 既然知道自己内心需要什么，那就去行动啊！我要的不过是这个学历罢了，这个敲门砖。 from 201509-201909 美好的大学时光，我亲爱的西科。这里的四年，是我最快乐和幸福的时光了。如果可以我愿意放弃很多，只求多在那里多呆一天。我永远把在西南科技大学的时光视为我最宝贵的经历。每次想起的时候，泪水感觉就要出来了，就像离开西科大去电子科技大学的车上。看着模糊的西科大，我哭了。四年对我太重要了。在这里，我学会了科研。 from 201909-202007 研一时光。上课，科研，实验室还是三点一线。科研做的还是顺利，只是自己没有沟通自己的需求。我就想做两年的科研，合作一篇论文，然后去工作。当然有点想读博，仔细想想，读博完全没有任何意义的。我努力的意义只是想不辜负自己，并不是要读博。这个独立的科研过程，自己学到了很多东西，这也是增长见识，知识和阅历的事情。如果是因为缺钱读博，怎么可能呢？完全是因为没有意义，如果读了四年，还跟现在差不多，就是增长了点知识，对我来说，完全没有意义。读研的经历美好在于认识了我亲爱的室友，杉杉；和实验室的老师和同学，他们都很厉害和优秀。增吃增喝了不少。啦啦啦啦啦！我自己也没有想到，想想自己当初的简历，要花三年达成的目标，居然一年就完成了。啦啦啦啦啦！在这里，我独立了科研。 学生生涯结束了！ 往后，准备去求职了！ 人生没有白走的路，每一段旅程都是收获慢慢！ 刷题准备哈。 20200727向着永恒出发。继续加油， 把内心许多的执念抛弃以后，你就会发现你喜欢的东西，自然而然的流露出了！从长远看，自己想要的，已经在路上了！你想得到的东西也在心里了。 不过就是生活中的小插曲罢了，惊喜罢了 。就像放弃保研的，放弃研一的硕博连读，我一直的看清自己的内心，之所以还要留恋，我也不知道为什么，可能就是想合作一两次吧！对不起，我是不是老是欺骗别人的感情啊！ 精通一个领域其实很简单， 20200725努力修炼自己！突然发现什么都很垃圾了！ 努力提升自己一、自我投资 阅读／影音学习／写日记／认识新朋友／学习专业知识／考资格证／参加研讨会、读书会／把在路上的时间转变为学习时间／利用博客、电子报发布信息／订阅刊物／重新检视人生计划／把一年的目标写在纸上 二、金钱 存钱／节约／投资／填写家庭收支簿／不赌博／请他人吃饭／捐款 三、心灵成长（压力、动力） 每天要说积极向上的话／冥想／每天写一件感恩的事／早上泡澡／每天都有一件期待的事／每周做一件有趣的事／整理／一天做三次深呼吸／听喜欢的音乐／问有建设性的问题／一天少做一件事（工作清单或备忘录） 四、运用时间 不看电视／拟定第二天的计划／限定看电子邮件的次数／拒绝聚餐的邀约／杂事统一处理／先处理最重要的三件事／列工作清单／严守下班时间／提早进公司／一次只集中在一件事上／不断改善对时间的运用 五、人际关系 经常称呼对方的名字／每天都要称赞他人／一天有40％的时间保持笑容／大声地与人打招呼／成为倾听的人／原谅他人／写交换日记／每天与重要的人交谈10分钟以上／不说抱怨、不满的话／先说结论／以双赢的目标思考 六、健康、美 吃健康食品／把白米换成糙米／每天刷3次牙／吃天然食物／每天睡满7个小时／一天喝2升水／每天晒太阳30分钟／不喝酒／讲究穿着／均衡摄取营养★／饮食以蔬果为主★／戒烟★／肌力训练★／做有氧运动★／限制热量摄取★／按摩／做伸展运动 列表中出现★者，为需要三个月时间培养的身体习惯。 原则没什么办法，我就是这样对未来没有要求，还在积极努力的自己！ 要拒绝社交。我现在觉得还是个个人生活好，我再也不想去认识我的人群中了，我只想去不认识的人群中。如果没有人认识我多好啊！感觉得了人群密集症。 要知世故。我也是最近才发现，我一点也和别人合不来，虽然我现在觉得我没有什么错，也不在意什么。但是感觉这样不好，那是被人猜忌，这是为什么呢？有时候太过于做自己了，好像不太好啊！ 也不知道是别人改变了我，还是我刻意的，重塑自己的过程，很难。]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>生活日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学]]></title>
    <url>%2F2020%2F07%2F17%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[统计学 统计学 day Ox00day Ox00 首先介绍随机实验的基本概念，包括随机实验，样本点，样本空间，基本事件，随机事件；其次介绍概率论的基本概念，包括概率的公理化定义，古典概率，条件概率，全概率，贝叶斯公式等等。特别注意两个容易混淆的概念：事件的独立性和互斥。 day Ox01 首先引 出随机变量的定义，从离散随机变量和连续随机变量两个维度，介绍典型的分布函数。其中概率函数和分布函数是非常重要的概念。 基本概念随机试验：记作$E$ 样本点： 随机试验中出现的可能结果称为样本点，记作 $\omega$ 样本空间： 所有样本点组成的集合称为样本空间，随机实验所有的结果的集合，记作$\Omega$ 事件： 样本空间的子集，叫做随机事件，记作A,B,C。 ​ 分类：基本事件（由一个样本点构成），不可能事件（不包含任何样本点），必然事件（样本空间的所有样本点组成） 事件的关系和运算 ​ A与B互斥（互不相容），并为空集。不可能同时发生。 ​ 对立（互逆）：A,B在一次实验中有且仅有一个发生。 事件间的关系包含 相等 互不相容性：不可能同时发生，没有交集 事件的概率概率的公理化定义计算方法古典方法随机事件的要求：(1). 涉及的随机现象只有有限个基本结果（2). 每个基本结果出现的可能性是相同的（等可能性） 事件的基本结果： P(A) = \frac{k}{n} = \frac{事件包含的基本事件的个数}{全空间包含的基本结果总数}事件的独立性两个事件的独立性是指一个事件的发生不影响另一个事件的发生， P(AB) = P(A)P(B)多个事件的独立性 P(A_iA_j) = P(A_i)P(A_j)\\ P(A_iA_jA_k) = P(A_i)P(A_j)P(A_k)\\ \vdots P(A_1A_2\cdots A_n) = P(A_1)P(A_2)\cdots P(A_n)实验的独立性实验$E_1$的任意一个结果（事件）与实验$E_2$的任一个结果都是相互独立的事件，则称实验相互独立 条件概率 P(A|B) = \frac{P(AB)}{P(B)}乘法公式 P(A_1A_2A_3) = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)全概率公式 P(A) = P(A|B)P(B)+P(A|\hat{B})P(\hat{B}) P(A) = \sum_{i = 1}^nP(A|B_i)P(B_i)贝叶斯公式 P(B_k|A) = \frac{P(A|B_k)P(B_k)}{\sum_{i = 1}^nP(A|B_k)P(B_k)}随机变量 day Ox01随机变量表示随机现象结果，一般大写字母X,Y,Z, 随机变量取值用小写字母x,y,z等表示。 用等号或者不等号把X与x联系起来就很多有趣的事件，X=x,Y&lt;y,等等构成了事件。 随机变量定义在基本空间$\Omega$上的实值函数$X = X(w)$成为随机空间 X: w->实数域（映射)随机变量的分布函数分布函数的定义 F(x) = P(X]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西瓜书]]></title>
    <url>%2F2020%2F07%2F17%2F%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%91%A8%E5%BF%97%E5%8D%8E%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[阅读目录[TOC] 第一章 What is the machine learning?非常官方的定义： Tom mitchell(1998) Well-posed LearningProblem:A compute program is said to learn from experience E with respect to same task T and some performance measure P,if its performance on T,as measured by P, improves with experience E。（这个我莫法翻译喔）大概意思是强大的计算机能够事先地完成人为非显示编程好的任务，怎么完成呢？对于某个任务T,给定一个性能度量方法P,在经验E的影响下，如果P对T的测量结果得到了改进，则说明该程序从E中学习了机器学习的过程大致如此：让计算机从数据中产生模型(model)，首先提供经验数据，给定学习算法(learning algorithm)和性能测量方法，它就能根据数据产生模型。模型： 泛指从数据中学得的结果模式： 局部性的结果 基本术语数据集: data set样本： sample属性（特征）： attribute（feature)属性值： attribute value属性空间（特征空间）： attribute space （ sample space）特征向量： feature vector学习（训练）：learning（training）训练数据： training data训练集： training set假设：hypothesis 学得模型对应了关于数据的某种潜在规律泛函能力: generalization 假设空间归纳（induction）： 从特殊到一般的“泛化”(generalization)过程演绎（deduction)： 从一般到特殊的“特化”(specialization)过程机器学习显然是归纳学习（inductive learning)归纳学习分狭义与广义，狭义是指要求从training set 中学得概念，广义是指从sample中学习 学习过程（训练过程）看作是在所以假设组成的空间中进行搜索的过程，搜索目标是找到与training set匹配的假设。如果假设的表示一旦确定，假设空间与其规模就确定了。想更详细了解假设空间，戳我啦5.2现实问题中常面临很大的假设空间，我们可以寻找一个与训练集一致的假设集合，称之为版本空间。版本空间从假设空间剔除了与正例不一致和与反例一致的假设，它可以看成是对正例的最大泛化。归纳偏好机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias),也就是学习算法在一个可能很庞大的假设空间中对假设进行选择的启发式或者“价值观”奥卡姆剃刀定律： 若有多个假设与观测一致，则选择做简单的哪个。没有免费的无餐定理（No Free Lunch Theorem[NFL]) 在所以问题出现的机会相同，或者所以问题同等重要下，所有算法的期望一样。但在实际问题中，针对具体的问题，不同的算法才会出现相对优劣。 发展历程推理期：二十世纪五十年代到七十年代初，AI处于推理区，代表性工作主要是A.Newell 和H.Simon的“逻辑理论家”程序和此后的“通用问题求解”程序等。“逻辑理论家”程序证明了数学家罗素和怀特海的《数学原理》里面的某些定理，获得图灵奖。知识期：从二十世纪七十年代中期开始，AI的研究进入了“知识期”，大量的专家系统出现，E.A.Feigenbaum（知识工程之父）在1994获得图灵奖。人们意识到，专家系统面临“知识工程瓶颈”,在那个时候，有人把知识总结出来再教给计算机是相当困难的。1950年，图灵再关于图灵测试的文章中，曾提到机器学习的可能二十世纪五十年代初，A.Samuel著名跳棋程序。五十年代中后期，基于神经网络的”连接主义“学习，如F.Rosenblatt的感知器（Perceptro），B.Widrem的Adaline,六七十年代，基于逻辑表示的”符号主义学习技术蓬勃发展学习期：二十世纪八十年代是机器学习百花初放的时期。一大主流是符号主义学习，代表决策树（decision tree).二十世纪九十年代中期之前，另外一大主流技术是基于神经网络的连接主义学习。二十世纪九十年代中期，”统计学习“占据主流，代表支持向量机。二十一世纪初，连接主义学习掀起了”深度学习“为名的热潮。 第二章 ： 模型评估与选择经验误差与过拟合、欠拟合训练误差（training error) or 经验误差（empirical error): 学习器在训练集上的输出与训练集之间的差异过拟合（over fitting）：在训练集上表现非常好，泛化能力太差，最常见的情况是学习能力太强学习到不太一般的特性，无法彻底避免，只能“缓解”欠拟合（under fitting）：这种情况容易克服模型选择(model selection): 不同的参数配置，产生不同的模型。理论上最好的模型是对泛化能力进行评估，最好的就是泛化误差最小的，泛化误差是无法直接获取的 评估方法设置一个”测试集（testing set)”来测试学习器在新样本的判断能力，用测试误差近似泛化误差要求： 测试样本与训练样本独立同分布的 测试集应该尽可能与训练集互斥，测试样本尽量不出现在训练集中如何产生training set 和 testing set 留出法（hold-out)要求：数据集($D$)划分成两个互斥的集合（训练集($S$,测试集$T$),需要注意的是，划分后，尽量可能的保持数据分布的一致性。不同的划分结果，得到不同的测试误差。单次使用留出法得到的结果是不够稳定的，所以一般采用若干次的随机划分，重复进行实验评估后去平均值 交叉验证法（cross validation)I. 将数据($D$)划分成$k$个大小相似的互斥子集，每个子集$D_i$都尽可能保持数据分布的一致性II. 每次都用$k-1$作为训练集，余下的哪个子集作为测试集，于是乎都到了k个测试结果的均值值得注意的是，$k$的取值对结果的稳定性和保真性有很大的影响，因此也叫k者交叉验证（k-flold cross validation) k的通常取值是10同样的，数据集$D$划分为$k$个子集有很多的划分方式，可重复$P$次$k$折交叉验证。 自助法 (bootstrapping)注意的是我们希望通过所以的训练集（$D$)训练出模型，但是流出法和交叉验证的方法，都保留一部分作为测试集，因此实际评估的模型所使用的训练集更下，这也许会导致估计偏差。自助法： 可重复采样或者有放回采样 记采样产生的数据集（$D’$),每次从$D$中挑选应该样本，将其拷贝至($D’$),并再将采样的样本放回数据集($D$),重复($m$)次以后，得到了包含($m$)个样本的数据集($D’$) 对于可重复采样，样本始终不采到的概率是$(1-\frac{1}{m})^m$,取极限得到：初式数据集中$36.8%$为出现在采样数据集中，因此可将($D$)作为训练集，($D\D’$)作为测试集，又称外包估计(out-of-bag estimate)自助法适用于数据量少，难区别测试集和训练集时，自助法会改变初始数据的分布，在初始数据足够的情况下，流出法和交叉验证更常用一些 调参和最终的模型学习算法都有参数(parameter),不同的参数配置，学得模型的性能也往往不同验证集(validation set): 模型评估和选择中用于估计测试的数据集称为的数据集往往将训练集划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参 性能度量(performance measure)假设检验（其实我一直都并不是特别了解） 假设检验的基本原理是重要的统计推断问题之一，根据样本提供的信息，检验关于总体某个假设是否正确。包括参数的假设检验（均值、方差等）和非参数（分布啊）的假设检验。 参数检验： 提出假设H—-&gt;在构造统计量，确定统计量的分布—-&gt; 确定拒绝域和接受域的分界线—-&gt; 在根据样本计算统计量的值u —-&gt; 推断 分布拟合检验 偏差和方差通过概率论分析对学习算法的期望泛化错误率进行拆解$x$: 测试样本$y_D$： $x$在数据集中的标记$y$: $x$的真实标记$f(x:D)$: 在训练集上学得的模型$f$在$x$上预测输出以回归任务为例子：学习算法的期望预测为： \hat{f}(x) = E_D[f(x;D)]方差：度量同样的样本大小的训练集的变动所导致的学习性能的变化，即刻画数据扰动所造成的影响 var(x)= E_D[(f(x;D)-\hat{f}(x))^2]噪声： 表达了当前任务上任务学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。 \epsilon^2=E_D[(y_D-y)^2]期望输出和真实标记的差别称为偏差(bias): 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力 bias^2(x)=(f(x)-y)^2若假设噪声期望为零，那么算法的期望泛化误差： E(f;D)=E_D[(f(x;D)-y)^2]\\ =....=E_D[(f(x;D)-\hat{f}(x))^2]+(\hat{f}(x)-y)^2+E_D[(y_D-y)^2]E(f;D)=bias^2(x)+var(x)+\epsilon^2由上式可知，泛化能力由学习算法的能力、数据的充分性、学习任务本身的难度共同决定的。underfitting: 偏差主导泛化误差over fitting： 训练数据发生的扰动渐渐被学习到，方差主导了泛化误差 第三章 线性模型我自己其实是一直停留在线性模型学习过程，因为每次开头都是这一张，所以我就学习了很多次。这次不准备再细看了。 线性判别分析 Linear Discriminant Analysis (LDA)基本思想： 在训练样例集上，设法将样本例子投影到一条直线上使得同类样例的投影尽可能接近、异类投影点尽可能远离。数学表达：$D={(x_i,y_i)}_{i=1}^{m}$: data set$X_i$: 第$i$类集合$u_i$: 第$i$类集合均值向量$\sum{i}$: 第$i$类集合协方差矩阵$ w^Tu_i$： 第$i$类集合在直线上的投影$ w^T\sum_{i}w$: 样本点的在直线上的投影学习算法：同类更近：$\min \sum_{i=1}^{n}(w^T\sum_{i}w)$类中心越大：$\max ||w^{T}u_1-(\sum_{i=2}(w^{T}u_i))||_2^2$因此，想最大化的目标考虑$i = 2$的情况 J = \frac{||w^Tu_0-w^Tu_1||_2^2}{w^T\sum_{i=1}w+w^T\sum_{i=2}w} =\frac{w^T(u_0-u_1)(u_0-u_1)^Tw}{w^T(\sum_1+\sum_2)w} 应用空间几何和矩阵的关系描述 类内散度矩阵($S_W$)\sum_1+\sum_2 类间散度矩阵：(u_0-u_1)(u_0-u_1)^T 所以，我们想优化目标如下：J = \frac{w^T_Sbw}{w^TS_ww}如何确定$w$呢？注意到分子分母都是关于$w$的二次型，因此解这和w的方向有关系，因此，可令 $w^TS_ww=1$,优化问题可是如下： \min -w^TS_bw \\ s.t. w^TS_ww = 1构造lagrange 函数 L = -w^TS_bw+r(w^TS_ww-1)对$w$求导可得： S_bw =rS_ww$S_b w$和$ u_0 - u_1 $ 方向是$u_0-u_1$,不妨设 S_nw=r(u_0-u_1)so,w = s_w^{-1}(u_0-u_1)这里考虑到数值解的稳定性，因此往往把$S_w$进行奇异值分解 第四章 决策树决策树是一种特别普通的符合生活做决策的过程。 第五章 神经网络神经网络最开始出现是根据生物神经网络来的。 最简单的神经网络：神经元模型(neuron|unit)McCulloch and Pitts抽象出“M-P神经元模型” 感知器（Perceptron)输入层和输出层，输出层：M-P神经元感知器的学习过程一定是收敛的 多层前馈神经网络 （multi-layer feddforward neural networks)前馈：网络的拓扑结构不存在环或者回路神经元的学习过程：就是根据训练数据来调整神经元之间的”连接权”(connection weight),以及每个功能神经元的阙值 误差逆传播算法： error BackPropagation (BP)全局最小和局部最小神经网络的训练过程其实也就是参数寻优的过程，基于梯度的搜素是使用最为广泛的参数寻优方法，但是如果误差函数在当前点的梯度为零，则很有可能达到局部极小。 第六章 支持向量机支持向量机的学习原理很简单也很有趣，从分类问题，怎么一步一步建立的优化问题，一步一步的完善优化问题以及求解，从硬间隔到软间隔，分类问题是考虑分对，而回归问题希望预测值和原始值尽可能的接近，这样就造成了约束条件，目标性的不同。 最重要的是引入了核方法，低维空间的非线性关系映射成了高维空间线性关系，这是特别重要的思想 第八章 集成学习基本思想构建一组基学习器（base learner)，在结合 a. 如果集成中是相同类型的个体学习器，如决策树，全是神经网络的集成“同质”（homogeneous),个体学习器叫基学习器 b. 不同的学习器，异质（heterogeneous)，个体学习器叫组件学习器 为什么有效 多样性的基学习器 不同的模型取长补短 每个基学习器都犯错误，综合起来可能性不大 举个栗子 也许一个线性模型不能简单分类，但是多个线性模型综合，可将数据集成功分类 构建不同的机器学习Q 1: 如何建立基学习器 尽量满足多样性 M1: 不同的学习算法 M2: 相同学习算法、不同的参数 M3: 不同的数据集（不同的样本子集、数据集上不同的特征） homogenous ensemble 采用相同的学习算法、不同的训练集 Bagging Boosting 相同算法，不同的参数设置 相同的训练集，不同的学习算法 Q2: 如何综合呢？ t投票法：majority voting weighted voting 训练一个新模型确定如何综合 Stacking 偏好的简单模型 综合Bagging = Boostrap AGGregatING有放回采样，同质学习器 算法1234567891011Input : 训练集 D=&#123;(x1,y1)&#125; 基学习算法A 训练轮数 T过程 for t = 1,2,...,T do h_t= A(D,Dt) // Dt第t次采样的分布 end for输出 回归：Average 分类：投票法 优点没有用于建模的样本，可以用作验证集来对泛化能力进行包外估计，可以得出Bagging泛化误差的包外估计 random forest（RF)输入为样本集$D={(x,y1),(x2,y2),…(xm,ym)}$，弱分类器迭代次数T。 输出为最终的强分类器f(x)f(x) 1）对于t=1,2…,T: a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$Dt$ b)用采样集$Dt$训练第t个决策树模型$Gt(x)$，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分 2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。 参数设置 利用00B样本评估变量的重要性 Boosting 提高顺次建立学习器，就是先从训练集上训练一个基学习器，再根据学习器的表现对训练集分布进行调整，让先学习器错误训练的样本在后续收到更多的关注，然后基于调整的分布训练下一个学习器，最后，在将这T个学习器进行加权结合 基学习器的线性组合 H_N(x;P)=\sum_{t=1}^{N}\alpha_th_t(x;a_t)$a_t$是第$i$个弱学习器的最优参数，$\alpha_t$是在强分类器中的比重，$P$是$a_t$和$\alpha_t$的组合 最小化指数损失函数 l_{exp}(H|D)=E_{x~D}[e^{-f(x)H(x)}] H_n(x)=H_{n-1}(x)+\alpha_{n}h_{n}(x,a_n)l(h_i(x,a_t)|D)=E_{x~D}(exp(-f(x)h_i(x)))\\=p(f(x)=1)exp(-h_i(x))+p(f(x)=-1)exp(h_i(x))\frac{\partial l(h_i(x,a_t)|D)}{\partial h_i(x,a_t)}=\\ -p(f(x)=1)exp(-h_i(x))+p(f(x)=-1)exp(h_i(x))=0h(x)=\frac{1}{2}ln\frac{P(f(x)=1)}{P(f(x)=-1)}采取不同的损失函数，得到不同的类型 https://blog.csdn.net/luanpeng825485697/article/details/79383492 GBDTStacking 不同学习器，相同数据集 第一层 第二层：不用第一层的数据 可用交叉验证 注意事项： 过拟合问题：第二层线性回归 第一层尽可能的多样性： 综合好的模型 防止过拟合 1. 随机性 2. Bagging Boosting Stacking 极大似然估计似然： 相似的样子 对于一组数据，假设符合正态分布，希望已知点在这个正态分布的情况下，所有点对于的概率之和或者积最大， ，蓝色表示数据，红色就是做得正态分布 第十章 降维与度量学习k近邻学习k-Nearest Neighbor 原理： 基于某种距离度量找出训练集中与其最靠近的k个训练样本，根据k个邻居的信息进行预测。 给定测试样本$x$,如果最邻近样本$z$,最邻近分类器出错的概率就是$x$与$z$不再同一类 p(err) = 1-\sum_{c \in y}p(c|x)P(c|z)低维嵌入缓解维数灾难的重要途经之一是降维（dimension reduction）这样使得子空间中样本密度大幅度提高，距离计算变得更容易， 多维缩放（Multiple Dimensional,Scaling）MDS 假定m个样本在原始空间的距离矩阵$D$,在低维空间中，两个样本欧式距离等于原空间的距离，$||z_i-z_j|| = dist_{ij}$, 令$B=Z^TZ$为降维后样本的内积矩阵, dist_{ij}^2=||z_i||^2+||z_j||^2-2z_iz_j=b_{ii}+b_{jj}-2b_{ij}对降维后数据中心化，均值为0,$\sum_{i=1}^{m}z_i$,于是乎就有$\sum_{i=1}^{M}b_{ij}=z_j(z_1+z_2+…+z_m)=0=\sum_{j=1}^{m}x_{ij}$ ,可得 \sum_{i=1}^{m}dist_{ij}^2=\sum_{i=1}^{m}(b_{ii}+b_{jj}-2b_{ij})=tr(B)_mb_{jj}\\ \sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2 = 2m tr(B)\\ tr(B)=\sum_{i=1}^{m}||z_i||^2可得 b_{ij}=-\frac{1}{2}(dist_{ij}^2-dist_{i.}^2-dist_{.j}^2+dist{..}^2)对矩阵B做特征值分解(eigenvalue decomposition)，$B = V \land V$,则 Z = \land_{*}^{1/2}V_{*}欲获得低维子空间，最简单是对原始高维空间进行线性变换，$Z = W^TX$,特别的，$W$取正交变换，$W={w_1,w_2,…,w_{d’}}$W是d’个d维基向量， 主成分分析Principal Component Analysis ：PCA 在正交空间里面的样本，用一个超平面对样本进行恰当的表达，至少这个样本点满足 最近重构性： 样本点到这个超平面的距离足够近 最大可分性： 样本点在这超平面上的投影尽可能分开 对于最近重构性： 假设样本去中心化，再假设投影变换后得到欣的正交坐标系${w_1,w_2,…,w_d}$,d维空间里面的一组单位正交基，$||w_i||_2=0$,$||w_i^Tw_j||=0$,如果再新坐标系中丢掉一部分坐标，样本点在新坐标的投影是$z_i={w_1^Tx_{i1}},..,w_{d’}^Tx_{i}$,于是又$z_{ij} =w_{j}^Tx_i$,$\hat{x_i}=\sum_{j}^{d’}w_jx_i$ \sum_{i=1}^{m}||\sum_{j=1}^{d'}z_{ij}w_j-x_i||_2^2=\sum_{i=1}^{m}z_i^Tz_i-2\sum_{i=1}^{m}z_i^TW^Tx_i+x_i^Tx_i\\ =\sum_{i=1}^{m}x_i^TWW^Tx_i-2\sum_{i=1}^{m}x_i^TWW^Tx_i+x_i^Tx_i\\ min -\sum_{i=1}^{m}z_i^Tz_i=-tr(Z^TZ)\\ min -tr(\sum_{i=1}^{m}W^Tx_ix_i^TW)=-tr(W^T(\sum_{i=1}^{m}x_i^Tx_i)W）=-tr(W^TXX^TW)\\ s.t W^TW = I对于最大可分性$(W^T\hat{X}=0)$ max tr(W^TXX^TW)\\s.t W^TW = I根据lagrange L(W,\lambda)=-tr(W^TXX^TW)-\lambda(W^TW-I)\\ \frac{\partial L}{\partial w_i}=-2w_iXX^T-2\lambda_i w_i=0\\ XX^Tw_i = \lambda w_i$XX^T$是协方差矩阵,$\lambda$是特征值，$w_i$是特征向量 特别提示，$x$需要中心化 对于线性PCA降维方法是从高维空间映射到低维空间，$Z= W^TX$,然而不少情况，则需要非线性映射才能找到恰当的低维嵌入， $\phi(x)$ \max tr(\phi(X)\phi(X)^T)=tr( W^T\varphi(x)\varphi(x)^TW)\\ W^TW = I于是有 \varphi(x)^T\varphi(x)w_i=\lambda_iw_i\\ w_i=\frac{tr(\varphi(x)^T\varphi(x))}{\lambda_iw_i} z_j = \frac{\sum_{i=1}^{m}\varphi(x)^T\varphi(x)}{\lambda_iw_i}\varphi(x_i)\ =\frac{\sum_{i=1}^{m}\varphi(x_i)K(x_i,x)}{\lambda_iw_i}流形学习（表示学习有点困难)第十一章 特征选择与稀疏学习对于一个学习任务，对任务有用的特征,称为”relevant feature”，对于没有用的属性”irrelevant feature”,因此从给定特征集选择出相关特征子集的过程，特征选择（feature selection),原因一，降维；原因二：降低学习的任务。 无关特征，包括一类冗余特征（redundant feature），能够从其他特征里面推演出来。 特征搜索前向（forward)搜索对于特征集合$\{a_1,a_2,…,a_d \}$,每个特征看作一个候选集，对这$d$候选的单特征子集进行评价，可选出最优子集，然后，再下一轮子集中，构成了两个特征候选的子集， 后向 (backward) 搜索每次尝试去掉一个无关特征 双向(bidirectional)搜索上述操作只是贪心策略，仅仅考虑了本轮选定集合最优 ​ 子集评价（subset evaluation)已知一个数据集$D$,假定第$i$类样本所占比例$p_i$,对于属性子集$A$,假设根据取值D分成V个子集$\{D^1,D^2,…,D^V\}$,则子集A的信心 增益 Gain(A) = Ent(D)-\sum_{i=1}^V\frac{|D^i|}{|D|}Ent(D^i)\\ Ent(D)=\sum_{i=1}^{|y|}p_ilog^{-p_i}​ 信息增益Gain(A)越大，说明特征子集A包含的有助于分类的信息越多，特征子集A是对数据集D的一个划分，样本D的标记信息Y则对应着D的真实划分，就能对A进行评价，对Y对应的划分的差异越小，则说明A越好， 过滤式选择Relief （Relevant Feature） 设计一个“相关统计量”来描述度量特征的重要性，该统计量是一个向量，每个分量对应一个初式特征，而特征子集的重要性则是每个特征对应统计量分量之和来决定，最终只需指定一个阙值，根据阙值选择统计量分量对应的特征即可 如何确定相关统计量 给定训练集$(x_i,y_i)$,对于实例$x_i$,在其同类样本中找最近邻（near-hit),在从异类样本中寻找其最近邻$x_{x,nm}$称为“猜错近邻”， \delta^j =\sum_i-diff(x_i^j,x_{i.nh}^j)^2+diff(x_i^j,x_{i,nm}^j)^2分值越大，说明对应属性的分类能力越强 对于多分类问题 \delta^j = \sum_i-diff(x_i^j,x_{i,nh}^j)^2+\sum_{l \neq k}p_l\ diff(x_i^j,x_{i,l,nm}^j)这种方法看一个属性（特征）重不重要，先计算出每个属性的统计分量，按照公式，子集的评价就是对于分量的和 包裹式选择直接把最终将要使用的学习器的性能作为特征子集的评价准则，特征选择的目的就是为给定学习期选择有利其性能的特征子集。 LVW（Las Vegas Wrapper）是典型的包裹式特征选择方法，拉斯维加斯方法（Las Vegas method）框架下使用随机策略来进行子集搜索，并以最终分类器的误差为特征子集评价准则 算法 嵌入式选择学习器自动地进行特征选择 L-P范数 L_P = ||X||_P = p\sqrt{\sum_{i=1}^{n}x_i^p} L0范数 ||X||_0=向量中非零元素的个数L1范数 ||x||_1 = \sum|x_i|L2范数，最常用 ||X||_2=\sqrt{x_i^2}无穷范数 ||x||=max|x_i|对于线性回归模型，防止过拟合，如果使用L2,称为岭回归(ridge regression),如果采取L1范数，则有称为LASSO，L1比L2更易于稀疏解，可以看得出L1范数正则化的过程得到了仅采用一部分初始化特征的模型。 L1正则化求解可使用近端梯度下降法(Proximal Gradient Descent)PGD L-Lipschitz条件 设函数$Φ(x)$在有限 区间$[a,b]$上满足如下条件： (1) 当$x∈[a,b]$时，$Φ(x)∈[a,b]$，即$a≤Φ(x)≤b$. (2) 对任意的$x1，x2∈[a,b]$， 恒成立：$|Φ(x1)-Φ(x2)|≤L|x1-x2|$. 如果$f(x)$可导，并且$\nabla f$满足L-Lipschitz条件， ||\nabla f(x')-\nabla f(x)||_2^2]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PageRank算法]]></title>
    <url>%2F2020%2F07%2F15%2FPageRank%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[PageRank是一种网页排序算法，基于页面的质量和数量。可应用于评估网页节点重要性。 PageRank算法PageRank,即\网页排名**，又称网页级别、Google左侧排名或佩奇排名。PageRank是Google用于用来标识网页的等级/重要性的一种方法，是Google用来衡量一个网站的好坏的唯一标准。 假设 数量假设: 如果一个页面节点入链数量越多，则这个页码越重要。 质量假设：指向页面A的入链质量不同，考虑权重的影响，则这个页面越是重要。 算法求解 第一阶段：通过网页链接关系构建起Web图，初始每个页面相同的PageRank值，再通过若干轮得到每个页面的最终pagerank. 每一轮更新页面PageRank得分的计算方法 权重 PR(T)/L(T)\\ where PR(T)的PageRank值，L(T)为T的出链数目修正$L(T)$为0的情况，孤立网页，使得很多网页能被访问到。$q = 0.85$ PR(A) = (\frac{PR(B)}{L(B)}+\frac{PR(C)}{L(C)}+\dots)q+1-q其他网络属性度量方法Centrality indices: degree, betweenness, and closeness. reference提出者： The anatomy of a large-scale hypertextual Web search engine https://en.wikipedia.org/wiki/PageRank]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言]]></title>
    <url>%2F2020%2F07%2F03%2FR%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[https://bookdown.org/qiyuandong/intro_r/-r-basics-2.html#section-3.3 入门： https://rc2e.com/ http://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/intro.html 全面： https://github.com/harryprince/R-Tutor 视频： 中文： https://www.youtube.com/watch?v=rPj5FsTRboE 英文：https://www.youtube.com/watch?v=32o0DnuRjfg 这个教程好： https://sites.google.com/site/econometricsacademy/econometrics-models/linear-regression https://www.youtube.com/watch?v=YMt5K68ZvjQ&amp;list=PLRW9kMvtNZOh7Xt1m5Mlhhz2wtr0tCUEE]]></content>
      <categories>
        <category>R</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xgboost]]></title>
    <url>%2F2020%2F07%2F03%2FXgboost%2F</url>
    <content type="text"><![CDATA[理论部分该算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。 boosting: https://zhuanlan.zhihu.com/p/38329631 Xgboost 就是回归树的集成 https://www.csuldw.com/2019/07/20/2019-07-20-xgboost-theory/ https://blog.csdn.net/github_38414650/article/details/76061893?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare https://blog.csdn.net/qq_24519677/article/details/81809157 有空再推导了 调用库Python 提供了两种库 xgboost xgboost sklearn接口 搭建模型 参数设置 GridSearchCV 调参(网格法) 调参步骤，参数范围 https://blog.csdn.net/han_xiaoyang/article/details/52665396 12345678import xgboost as xgbfrom xgboost import XGBRegressorfrom sklearn.metrics import mean_absolute_error,make_scorerfrom sklearn.grid_search import GridSearchCVfrom sklearn.cross_validation import KFold, train_test_splitfrom sklearn.datasets import load_boston https://blog.csdn.net/s09094031/article/details/94871596?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare 1sklearn.model_selection.``train_test_split test_size train_size： ​ 三种类型。float，int，None。 float：0.0-1.0之间，代表训练数据集占总数据集的比例。 int：代表训练数据集具体的样本数量。 None：设置为test_size的补。 default：默认为None。 random_state：三种类型。int，randomstate instance，None。 int：是随机数生成器的种子。每次分配的数据相同。 randomstate：random_state是随机数生成器的种子。（这里没太理解） None：随机数生成器是使用了np.random的randomstate。 种子相同，产生的随机数就相同。种子不同，即使是不同的实例，产生的种子也不相同。 shuffle：布尔值，可选参数。默认是None。在划分数据之前先打乱数据。如果shuffle=FALSE，则stratify必须是None。 stratify：array-like或者None，默认是None。如果不是None，将会利用数据的标签将数据分层划分。 若为None时，划分出来的测试集或训练集中，其类标签的比例也是随机的。 若不为None时，划分出来的测试集或训练集中，其类标签的比例同输入的数组中类标签的比例相同，可以用于处理不均衡的数据集。 x_train, y_train, x_test, y_test = train_test_split(x, y, test_size=0.23, random_state=2) https://blog.csdn.net/qq_43288098/article/details/105407204?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare 参数：分开调 https://blog.csdn.net/zc02051126/article/details/46711047 https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn 模型保存https://www.fatrabbids.com/2018/10/19/xgboost%e7%9a%84%e4%bf%9d%e5%ad%98%e6%a8%a1%e5%9e%8b%e3%80%81%e5%8a%a0%e8%bd%bd%e6%a8%a1%e5%9e%8b%e3%80%81%e7%bb%a7%e7%bb%ad%e8%ae%ad%e7%bb%83/#more-235 XGBoost的特性重要性和特性选择 模型复杂度 特征数量衡量：特征重要性阙值的增加，选择特征数量减少，模型的准确率会下降。当然，特征数量的减少反而会是准确率升高，因为这些被剔除特征是噪声。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[English-Daily]]></title>
    <url>%2F2020%2F06%2F23%2FEnglish-Daily%2F</url>
    <content type="text"><![CDATA[2020-7-6coincide with v. 与…相符 stalk v. 潜近（猎物或人）；（非法）跟踪；怒冲冲地走；趾高气扬地走 n. 秆；柄；（叶）柄；（花）梗 verge Bella was on the verge of tears when she heard the news. 听到这个消息时，贝拉差点就要哭了。 resistant adj. 抵制的，反抗的，抗拒的；有抵抗力的；抵抗…的；不受……损害的 People are usually resistant to change. 人们通常抗拒改变。 liar The tall guy was a notorious liar. 那个高个子是个臭名昭著的骗子。 politics n. 政治；政治事物（活动）；政见；权术 oblige (以法律、义务等)强迫, 迫使; 帮忙, 效劳; [常用被动]使感激; 使(行为等)成为必要 phrase. (feel obliged to do sth.)觉得有义务做；不得不做 I felt obliged to leave after such an unpleasant quarrel. 发生了这样不愉快的争吵之后，我觉得有必要离开。 2020-7-1jelly n. 果冻；肉冻；果酱；胶状物，胶凝物；轻便塑料鞋 oval adj. 椭圆形的；卵形的 n. 椭圆形；卵形 rigorous /‘rɪɡərəs/ adj. 谨慎的，细致的；严格的，严厉的 He makes a rigorous study of the plants in the area. 他对该地的植物进行了缜密的研究。 ultimately UK/‘ʌltɪmətli/ adv. 最终, 最后, 归根结底, 终究 Everything will ultimately depend on what is said at the meeting. 一切将最终取决于会议的内容。 sturdy UK/‘stɜːdi/ adj. 结实的，坚固的；强壮的；健壮的；坚决的，顽强的 broaden UK/‘brɔːdn/ You should broaden your experience by travelling more. 你应该多到各地走走以增广见识. broaden the horizon 开拓视野 propel UK/prə’pel/ v. 推进，推动；驱使；迫使 voyage UK/‘vɔɪɪdʒ/ n. 航行, （尤指）航海 v. 航行, 远行, （尤指）远航 例句 The voyage from England to India used to take 3 weeks. 从英格兰到印度的航行曾经需要三周。 2020-6-28moist UK/mɔɪst/ adj. 微湿的, 湿润的 insult UK/ɪn’sʌlt/v. 侮辱，辱骂 n. 侮辱，辱骂 spontaneous UK/spɒn’teɪniəs/ They greeted him with spontaneous applause. 他们自发地鼓起掌来欢迎他。 slender UK/‘slendə(r)/ perimeter UK/pə’rɪmɪtə(r)/ n. 周长；外缘，边缘 blouse UK/blaʊz/ He pointed out a woman passing by who was wearing a skirt and blouse. 他指出了一个穿着裙子和衬衫的过路女子。 perfume UK/‘pɜːfjuːm/ n. 香水, 香料, 芳香 v. 使…发出香气, 洒香水 2020-6-272020-6-26Functional foods are food products that have a potentially positive effect on health beyond basic nutritional benefits. Functional foods aim to solve not only all the needs that regular foods provide, but also to address functional needs, which can range from maintaining and improving physical or mental health to adjusting energy levels and moods. Food has been historically used as preventive medicine in many cultures around the world, but the recent rise of functional foods can be directly linked to the rise of the wellness economy, which, in turn, is largely driven by influencer marketing and social media use. 2020-6-25IT IS A truth universally acknowledged that inequality（不平等）in the rich world（发达国家）is high and rising. Or, at least, it used to be. A growing band of economists are challenging the received（被公认的）wisdom, pointing out that trends in the distribution（分布，分配）of income and wealth may not be as bad as is often thought. 众所周知，富裕国家的不平等现象非常严重，而且还在加剧。或者说，至少曾经是这样的。越来越多的经济学家开始质疑既有的观点，他们指出收入和财富的分布趋势可能不是像通常被认为的那么糟糕。 2020-6-24imaginary adj. 想象中的, 幻想的, 虚构的 carriage n. 运输；运费，（旧时）马车；火车车厢；仪态，姿态，举止 message messenger n. 信使, 送信人, 通信员, 邮递员 pavement n. 人行道 postpone v. 延期, 延迟, 暂缓 We’ll have to postpone the meeting until next week. 我们将不得不把会议推迟到下周举行。 velocity n. 速度，速率；高速 reconcile v. 使和谐一致，调和；使和解；将就，妥协 It’s difficult to reconcile these two different points of view. 很难兼顾这两种不同的观点。 2020-6-23￼The success of the brand wasn’t built through big marketing campaigns, but through a savvy digital marketing strategy that increased brand awareness and generated high engagement, traffic, and conversions. 该品牌的成功并不建立于大型营销活动，而是建立于精准的数字营销策略，该策略提高了品牌的知名度，获得了很高的参与度、流量和转化率。 traffic: 信息流量，通信量 With only 40 physical stores, which are mostly used to drive consumers to e-commerce portals, Perfect Diary maintains momentum primarily through its digital footprint. Currently, it has a powerful presence on Little Red Book, Bilibili, Weibo, WeChat, Tmall, and Douyin. Thereafter she wrote articles for papers and magazines for a living. 此后她给报纸和杂志撰稿谋生。 adv. 此后, 之后, 以后 spur n. 刺激, 激励, 鞭策; 踢马刺, 靴刺; 骨刺; 山嘴, 尖坡 v. 刺激, 激励, 促进, 鞭策 stick adj. 黏（性）的, 一面带黏胶的, 闷热的, 感到热得难受的 n. 告事贴 I have to take a shower before going out because the sweat had made my skin sticky. 出门前我得冲个澡，因为汗水让我的皮肤黏乎乎的 devotion n. 关爱，关照；奉献；忠诚；宗教礼拜 The career needs our devotion for all our lives. 这项事业需要我们毕生的奉献。 reckless adj. 鲁莽的；不计后果的；无所顾忌的 wag v. 摇动；摆（尾巴），（尾巴）摇，摆动 n. 摇摆，摆动；老开玩笑的人，爱闹着玩的人 keen adj. 热衷的, 热情的; 渴望的; 敏捷的; 灵敏的; 锋利的; 强烈的 n. 恸哭; 挽歌 v. (为死者)恸哭 be keen on sth对 感兴趣 be keen to do 渴望做某事 offspring n. 子女，后代；幼崽；幼苗 receipt n. 收据，收入]]></content>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[day]]></title>
    <url>%2F2020%2F06%2F22%2Ftime-series-01%2F</url>
    <content type="text"><![CDATA[时间序列及其分解 时间序列分类平稳序列（stationary series)序列中的各观察值基本上在某个固定的水平上波动，在不同时F间段波动程度不同，但不存在某种规律。平稳性时间序列的均值和方差都是常数。 方法：a) 看原图。是否在某个常数附近波动，且波动范围有界。如果有明显的趋势性或者周期性，则不是。b) ADF单位根检测。p值。 非平稳序列（non-stationary series)涉及趋势、季节性和周期三种特性，包含其中一种或者多种成分。 趋势(trend)时间序列在长时期内呈现出来的某种上升或者下降的趋势。分为线性和非线性。 季节性（seasonality)是指时间序列在一年内重复出现的周期波动。因季节不同而发生变化，如旅游旺季，旅游淡季。 周期性（cyclicity）是指时间序列呈现出的长期趋势。周期性不同于趋势变动，它是涨落相间的交替波动。不同意季节变动，它无固定规律，变动周期多在一年以上，且周期长短不一。周期性通常是由经济环境的变化引起的。 偶然性因素其导致时间序列呈现出某种随机波动。 时间序列的成分可分为：趋势（T),季节性（S),周期性（C),随机性（I)。 平稳时间序列分析AR模型 自回归模型AR 自回归模型描述当前值与历史值之间的关系，用变量自身的历史时间数据对自身进行预测。自回归模型必须满足平稳性的要求。 移动平均模型MA 移动平均模型关注的是自回归模型中的误差项的累加 自回归移动平均模型ARMA 自回归模型AR和移动平均模型MA模型相结合，我们就得到了自回归移动平均模型ARMA(p,q) 差分自回归移动平均模型ARIMA 将自回归模型、移动平均模型和差分法结合，我们就得到了差分自回归移动平均模型ARIMA(p,d,q) 参数确定拖尾和截尾拖尾指序列以指数率单调递减或震荡衰减，而截尾指序列从某个时点变得非常小。 ARIMA建模过程 将序列平稳（差分法确定d） p和q阶数确定：ACF与PACF ARIMA（p,d,q） 模型 ACF PACF AR（p） 衰减趋于零（几何型或振荡型） p阶后截尾 MA（q） q阶后截尾 衰减趋于零（几何型或振荡型） ARMA（p,q） q阶后衰减趋于零（几何型或振荡型） p阶后衰减趋于零（几何型或振荡型） 参数 p,q 的自动确定方式信息准则在参数估计的时候，我们可以采用似然函数作为目标函数。可以通过加入模型复杂度的惩罚项避免过拟合问题。比如赤池信息准则（AIC)和贝叶斯信息准则(BIC) AIC=2k−2ln(L)一方面引入惩罚项，使得模型参数尽快少，减少过拟合。另一方面，也希望提高模型的拟合度（极大似然） BIC=kLn(n)−2ln(L)k为模型参数个数，n为样本数量，L为似然函数。引入$Kln(n)$惩罚项在维度过大且样本数据相对较少的情况下，可以有效避免出现维度灾难。 时间序列的分解加法模型 X_t = T_t + C_t+S_t + I_t ,t = 1,2,..,n每个时间序列看成是三个部分的叠加，分别是趋势项、循环项，季节项，随机项 乘法模型 X_t = T_t*C_t*S_t*I_t趋势分析趋势拟合法就是把时间作为自变量，相应的序列观察值作为因变量，建立序列值随时间变化的回归模型。可分为线性拟合和曲线拟合。 线性拟合如果长期趋势呈现出线性特征，可用线性模型拟合， \left\{\begin{array}{c} x_t = a+bt+I_t\\ E(I_t) = 0,Var(I_t) = \sigma^2 \end{array} \right.其中，$T_t = a+bt$就是消除随机波动影响后的该序列的长期趋势。 曲线拟合如果长期趋势呈现出线性特征，可用曲线模型来拟合 \left\{ \begin{array}{c|c|c} 二次型& T_t = a+bt+ct^2& 变换后，线性最小二乘法\\ 指数型&T_t = ab^t& 对数变化 & 最小二乘法\\ 修正指数型&T_t = a+bc^t& &迭代法\\ Gompertz型& T_t = e^{a+bc^t}& & 迭代法\\ Logistic & T_t = \frac{1}{a+bc^t}& 迭代法 \end{array} \right.平滑法移动平均法假设在比较短的时间间隔里，序列的取值是较稳定的，这种差异是由随机波动造成的。由此，可用一定时间间隔内的平均值作为某一期的估计值。 n期中心移动平均 \widetilde{x_t} = \frac{1}{n}(\frac{1}{2}x_{t-\frac{n}{2}}+x_{t-\frac{n}{2}+1}+\dots+x_{t+\frac{n}{2}-1}+\frac{1}{2}x_{t+\frac{n}{2}})n期移动平均 \widetilde{x_t} = \frac{1}{n}(x_t+x_{t-1}+\dots+x_{t-n+1})指数平滑法简单指数平滑 \widetilde{x_t} = \alpha x_t+\alpha (1-\alpha )x_{t-1}+\dots)季节效应季节性效应的存在，使得气温会在不同年份的相同月份呈现出相似的性质。 如果只是存在季节性和随机波动性 x_{ij} = \hat{x}S_j+I_{ij}其中$S_j$表示第j个月的季节指数，$\hat{x}$为各月平均气温。 季节指数的计算: Step1: 计算周期内各期的平均数 \hat{x}_k = \frac{\sum_{i= 1}^{n}x_{ik}}{n}（k = 1,2,...,m)其中，m表示周期，n表示周期的数量 Step2: 计算总平均数 \hat{x} = \frac{\sum_{i = 1}^{n}\sum_{k = 1}^{m}x_{ik}}{nm}Step3: 计算季节指数 S_k = \frac{\hat{x}_k}{\hat{x}}混合效应加法模型 x_t = T_t + S_t + I_t乘法模型 x_t = T_t*S_t*I_t混合模型 x_t = S_t*T_t+I_t\\ x_t = S_t*(T_t+I_t)如果季节波动的振幅不受趋势变动的影响，则说明季节性与趋势之间没有相互作用关系，可加。如果季节波动的振幅随趋势的变化而变化，是相互作用的关系，可尝试混合模型和乘法模型。 Tool in Python: xfresh特征提取官网： https://tsfresh.readthedocs.io/en/latest/text/quick_start.html 中文： https://github.com/SimaShanhe/tsfresh-feature-translation Data Formatscolumn_id: Features will be extracted individually for each entity(id); one row per id. column_sort: sorting the time series. 特征提取: 可以一次性提取完；也可以单独提取kind_to_parameters 设置参数；还可以提取 可分布式计算 the rolling mechanism 首先确定滑动窗口 Step1 : 实现单变量特征的提取 Step2 : 实现多变量特征的提取 Day Ox 01知识清单: 特征提取：大概上千种特征（几十种方法） tsfresh.feature_extraction.extraction.extract_features(timeseries_container,default_fc_parameters=None, kind_to_fc_parameters=None**, column_id=None, column_sort=None, column_kind=None, column_value=None, chunksize=None, n_jobs=1, show_warnings=False, disable_progressbar=False, impute_function=None, profile=False, profiling_filename=’profile.txt’, profiling_sorting=’cumulative’, distributor=None)** pandas.DataFrame containing the different time series column_id (str) – The name of the id column to group by. column_sort (str) – The name of the sort column. n_jobs (int) – The number of processes to use for parallelization. 时间序列的滑动窗口（单序列划分成多序列） tsfresh.utilities.dataframe_functions.``roll_time_series(*df_or_dict*, column_id**, column_sort=None, column_kind=None, rolling_direction=1, max_timeshift=None, min_timeshift=0, chunksize=None, n_jobs=1, show_warnings=False, disable_progressbar=False, distributor=None)** max_timeshift (int) – If not None, the cut-out window is at maximum max_timeshift large. If none, it grows infinitely. min_timeshift (int) – Throw away all extracted forecast windows smaller or equal than this. Must be larger than or equal 0. n_jobs (int) – The number of processes to use for parallelization. If zero, no parallelization is used. show_warnings=False （指定）特征提取 显著性检测 https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_selection.html?highlight=select_features#tsfresh.feature_selection.selection.select_features 相关性检测 https://tsfresh.readthedocs.io/en/latest/text/parallelization.html#parallelization-of-feature-selection 123456789101112131415161718192021222324252627282930313233from tsfresh import extract_features, select_features,extract_relevant_featuresfrom tsfresh.utilities.dataframe_functions import imputefrom tsfresh.utilities.dataframe_functions import roll_time_series, make_forecasting_frameimport pandas as pdimport tsfresh as tsf fc_parameters_value1 = &#123;"length": None, "sum_values": None&#125;fc_parameters_value2 = &#123;"maximum": None, "minimum": None&#125;kind_to_fc_parameters = &#123; "value1": fc_parameters_value1, "value2": fc_parameters_value2&#125;if __name__ == '__main__': # ceate data rawdata = &#123;'id1': [0,0,0,0,0,1,1,1,1,1],'time': [1,2,3,4,5,10,11,12,13,14],\ 'value1': [1,2,3,4,5,6,7,8,9,10], 'value2': [1,2,3,4,5,6,7,8,9,10] &#125; df = pd.DataFrame(rawdata)# 设置长度+1 = 真实长度,是当前编号往上数. df_rolled = roll_time_series(df, column_id="id1", column_sort="time", max_timeshift=1, min_timeshift=0)# roll_time_series的返回值 print(df_rolled) df_rolled = df_rolled.drop('id1',axis = 1)# column_id: 聚合列 column_sort:排序，一个column_id就对应一个特征 extracted_features = extract_features(df_rolled, column_id='id', column_sort='time', kind_to_fc_parameters = kind_to_fc_parameters, show_warnings=False) print(extracted_features) Day Ox 02 查看提取特征可根据此提取自动提取的特征，用于预测时候的提取特征 1kind_to_fc_parameters = tsf.feature_extraction.settings.from_columns(extracted_features) 1234# 5. 特征抽取与过滤同时进行（一步到位，省去多余计算）# column_id: group by #features_filtered_direct = extract_relevant_features(timeseries, y, column_id='id', column_sort='time')#print(features_filtered_direct.head()) 学习路径： 1. 数据格式 2. 滑动窗口设置 3. 特征提取 4. 特征选择 专题 时间序列的竞赛方案https://mp.weixin.qq.com/s?__biz=MzU1Nzc1NjI0Nw==&amp;mid=2247485604&amp;idx=1&amp;sn=6283ec080344665bfad90570bf1504a4&amp;chksm=fc31b29ccb463b8acac7acf4d89494aaad0c76620becb2b07c370ccbfaff850edc3c1ad4e0fd&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1593390448780&amp;sharer_shareid=fb5716a8ad12ea6329433df53d4cbf64#rd https://www.zhihu.com/question/21229371/answer/533770345 Prophet 工具]]></content>
  </entry>
  <entry>
    <title><![CDATA[回归分析]]></title>
    <url>%2F2020%2F06%2F20%2F%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[[TOC] 回归分析最简单的线性回归，避免多重共线性，过拟合，引入正则项的线性回归模型。涉及到的数学知识：一范数，二范数，多元函数求极值。模型的含义，参数求解算法，目标函数，以及各种模型的优缺点。 定义回归分析是寻找自变量和因变量之间的数量关系，用于预测建模的方法。其一，它可以揭示自变量和因变量之间的显著性检测。其二，揭示多个自变量对一个因变量的影响程度大小。 回归类型1）独立变量的数量 2）度量变量的类型 3）回归线的形状 1. 线性回归（Linear Regression)因变量：连续； 自变量：连续或者离散 模型的形式 Y = a+bX+𝜀\\ \left(\begin{array}{c} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{array}\right)=\left(\begin{array}{cccc} 1 & x_{11} & \cdots & x_{1(p-1)} \\ 1 & x_{21} & \cdots & x_{2(p-1)} \\ \vdots & \vdots & \vdots & \vdots \\ 1 & x_{n 1} & \cdots & x_{n(p-1)} \end{array}\right) \beta+\left(\begin{array}{c} e_{1} \\ e_{2} \\ \vdots \\ e_{n} \end{array}\right)\\ Y_{n*1} = X_{n*p}\beta+𝜀where $a$ and $b$ are the regression coefficients, and 𝜀 is the random error. 目标函数 min SSR = \sum_{i}(y_i-f(x_i))^2\\ min_{w}||Xw-y||_2^2参数估计最小二乘法（Lease Square Method)（OLS) This approach is called the method of ordinary least squares. 模型评估拟合优度 R-square , coefficient of determinationLarger $R^2$ indicates a better fit and means that the model can better explain the variation of the output with different inputs. https://realpython.com/linear-regression-in-python/ 要求 自变量和因变量之间必须满足线性关系。 多元回归存在多重共线性，自相关性和异方差性。 线性回归对异常值非常敏感。异常值会严重影响回归线和最终的预测值。 多重共线性会增加系数估计的方差，并且使得估计对模型中的微小变化非常敏感。结果是系数估计不稳定。 在多个自变量的情况下，我们可以采用正向选择、向后消除和逐步选择的方法来选择最重要的自变量。 逻辑回归（Logistic Regression)Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。 Logistic 分布 F(x) = P(X]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识清单]]></title>
    <url>%2F2020%2F06%2F19%2F%E7%9F%A5%E8%AF%86%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[主要是列出关于日常中遇到的很好的资料，自己不清楚的文章和资料。 2020-6-29 Z检测和T检测https://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&amp;mid=2247485455&amp;idx=1&amp;sn=857066158bf8c2de38939f3037416035&amp;chksm=eb9321b9dce4a8afd68d764c295f8bcc69c62f2b1d000f3e1c5e61a7d9b6e2ec3de8df068174&amp;mpshare=1&amp;scene=24&amp;srcid=&amp;sharer_sharetime=1593403964973&amp;sharer_shareid=0e2d0ffe45c3a6dfb66aa422c3a1381d#rd 2020-6-28视频： http://www.julyedu.com/video/play/58/405 2020-6-19SQL 中文: https://www.liaoxuefeng.com/wiki/1177760294764384 英文： https://www.codecademy.com/courses/learn-sql/lessons/manipulation/exercises/sql 视频： https://www.jikexueyuan.com/course/sql/ 基础 https://study.163.com/course/courseMain.htm?courseId=215012&amp;_trace_c_p_k2_=f68f3d2867a343789ac2d3cfa92dd308 https://www.nowcoder.com/discuss/95812?type=2 https://www.cnblogs.com/zsh-blogs/category/1413021.html]]></content>
      <categories>
        <category>规划</category>
      </categories>
      <tags>
        <tag>技能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data-Science]]></title>
    <url>%2F2020%2F06%2F11%2FData-Science%2F</url>
    <content type="text"><![CDATA[CourseTsinghua Dr. Yuan Data Mining: Theories and Algorithms for Tackling Big Data ToolsStata: https://www.stata.com/why-use-stata/ https://www.youtube.com/watch?v=AyXeh7iojuA BOOOOOOKhttps://www-users.cs.umn.edu/~kumar001/dmbook/index.php]]></content>
      <categories>
        <category>数据科学(Data Science)</category>
      </categories>
      <tags>
        <tag>Data Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密码学]]></title>
    <url>%2F2020%2F06%2F11%2F%E5%AF%86%E7%A0%81%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[科普 Day1现代信息安全的基本要求： 信息的保密性 Confidentiality：防止信息泄漏给未经授权的人（加密解密技术）机密性 信息的完整性 Integrity：防止信息被未经授权的篡改（消息认证码，数字签名） 认证性 Authentication：保证信息来自正确的发送者（消息认证码，数字签名）认为 其他 不可否认性 Non-repudiation：保证发送者不能否认他们已发送的消息（数字签名） 第一章 引言涉及的知识点包括信息安全的要求（主要四个方面），密码学基本概念，安全的定义，密码算法的设计要求，古典密码（替换，代替） o密码学基本概念**,**如密码编码学、密码分析学、明文、密文、加密、解密 o对称密码体制和非对称密码体制 o古典密码体制，如置换密码、单表代换密码、多表代换密码（要会计算） 现代信息安全的基本要求： 信息的保密性 Confidentiality：防止信息泄漏给未经授权的人（加密解密技术） 信息的完整性 Integrity：防止信息被未经授权的篡改（消息认证码，数字签名） 认证性 Authentication：保证信息来自正确的发送者（消息认证码，数字签名） 不可否认性 Non-repudiation：保证发送者不能否认他们已发送的消息（数字签名） http://yuqiangcoder.com/2019/10/07/%E5%AF%86%E7%A0%81%E5%AD%A6%E6%A6%82%E8%BF%B0.html 密码学就是要通过算法和协议实现相应的功能 凯撒密码：移动2位，H K 恺撒密码 安全p无条件安全的(不可破译的)： p无论截获多少密文，都没有足够信息来唯一确定明文，则该密码是无条件安全的，即对算法的破译不比猜测有优势 p计算上安全的： p使用有效资源对一个密码系统进行分析而未能破译，则该密码是强的或计算上安全的 密码算法要求密码算法只要满足以下两条准则之一就行： （1） 破译密文的代价超过被加密信息的价值。 （2 ) 破译密文所花的时间超过信息的有用期。 满足以上两个准则的密码算法在实际中是可用的。 单表代替密码单表代替密码可分为 • 加法密码 • 乘法密码 • 仿射密码 古典密码置换密码 单表代替密码算法 多表代替密码算法 第二章 流密码一次一密，流密码，密钥流三个概念。 o流密码基本概念、特点 o线性反馈移位寄存器 oRC4 一次一密（理想） •优点： •密钥随机产生，仅使用一次 •无条件安全 •加密和解密为加法运算，效率较高 •缺点： •密钥长度至少与明文长度一样长，密钥共享困难，不太实用 流密码密码体制 序列密码 •流密码的基本思想 •利用密钥k产生一个密钥流 •密钥流 •由密钥流发生器 f 产生： Ø内部记忆元件的状态σi独立于明文字符的叫做同步流密码，否则叫做自同步流密码。 密码分析学的目标在于破译（ BC ） A. 明文 B. 密文 C. 密钥 D. 算法结构 保密通信系统的安全威胁 保密通信的安全威胁： 被动攻击：窃听，嗅探流量分析等，主要是破坏消息的机密性； 主动攻击：中断，篡改，假冒等。 中断破坏了信息的可用性 篡改破坏了信息的完整性 假冒破坏了真实性（认证） 所以保密通信系统的安全需求有： 机密性——采用加密机制 完整性——采用完整性验证机制，如Hash函数，消息认证码 真实性——采用认证机制，如数字签名，认证协议 中断——用密码学的技术没有太好的办法（这是我个人的理解） 古典密码学 置换密码：又称换位密码，加密过程中明文的字母保持相同，但是顺序被打乱。只要把位置恢复，就能得到明文。 代换密码：明文中的每一个字符被替换成密文中的另一个字符。接收者对密文做反向替换就可以恢复明文。 多名或同音代替密码 多字母代替密码 多表代替密码 总结古典密码学的特点：加密对象；方法；保密内容；破解； 计算强度小 出现在 DES 之前 数据安全基于算法的保密。这和现代密码有很大的差距，只要知道加密方法，就能轻易的获取明文。现代的密码基于秘钥的加密，算法都是公开的，而且公开的密码算法安全性更高，能被更多人评论和使用，加强漏洞的修补。 以字母表为主要加密对象。古典密码大多数是对有意义的文字进行加密，而现代密码是对比特序列进行加密。这也是现代密码和古典密码的区别，而且古典密码的分析方法也是用字母频率分析表来破解的。 替换和置换技术 密码分析方法基于字母与字母组合的频率特性以及明文的可读性 现代密码学 1976：由 Diffie 和 Hellman 在《 密码学的新方向》（《New Directions in Cryptography》）提出了公钥密码学体制的思想 1977年：美国国家标准局颁布数据加密标准 DES（Data Encryption Standard） 1978年：第一个公钥算法 RSA 算法（由 Ron Rivest、Adi Shamir 和 Leonard Adleman 的姓氏首字母组成） 现代密码学主要有三个方向：私钥密码（对称密码）、公钥密码（非对称密码）、安全协议。 私钥密码也称对称密码，是对文字的加密转换成对比特序列的加密（相对于古典密码），用同一个密钥进行加密和解密操作，这个密钥发送方和接收方都是要保密的，所以称为私钥密码。它的两个基本操作就是代换和置换就是来源于古典密码学的。 对称密码有两个设计原则，一个是扩散（Diffusion）：明文的统计结构被扩散消失到密文的长程统计特性，使得明文和密文之间的统计关系尽量复杂。 另一个是混乱（confusion）：使得密文的统计特性与密钥的取值之间的关系尽量复杂。 对称密码的代表有 DES 算法和 AES 算法， 公钥密码 DH 密钥交换协议 RSA 算法是第一个公钥密码算法，也是第一个数字签名算法。 p q pi(n) =(p-1)(q-1);与n互质的书&lt;=n 选e 与pi(n)最大公约数1，互质， 找d，e*d/pi(n)=1 (n,e)共（n,d)私钥 a^emod n =b b^d mod n=c 根据以上密钥对的生成过程： 如果想知道 d 需要知道欧拉函数 φ(n) 如果想知道欧拉函数 φ(n) 需要知道 P 和 Q 要知道 P 和 Q 需要对 n 进行因数分解。 对于本例中的 4757 你可以轻松进行因数分解，但对于大整数的因数分解，是一件很困难的事情，目前除了暴力破解，还没有更好的办法，如果以目前的计算速度，破解需要50年以上，则这个算法就是安全的 椭圆曲线加密算法，简称ECC，是基于椭圆曲线数学理论实现的一种非对称加密算法。相比RSA，ECC优势是可以使用更短的密钥，来实现与RSA相当或更高的安全，RSA加密算法也是一种非对称加密算法 重合 四、同余运算同余就是有相同的余数，两个整数 a、 b，若它们除以正整数 m所得的余数相等，则称 a， b对于模m同余。 乘法逆元； 六、乘法逆元在模7乘法中： 1的逆元为1 (1*1)%7=1 2的逆元为4 (2*4)%7=1 3的逆元为5 (3*5)%7=1 4的逆元为2 (4*2)%7=1 5的逆元为3 (5*3)%7=1 6的逆元为6 (6*6)%7=1 https://zhuanlan.zhihu.com/p/101907402 第三章现代密码o分组密码基本概念、特点 oFeistel 网络 oDES，密钥长度、分组长度、S盒、多重DES o分组密码的四种运行模式 oAES，密钥长度、分组长度 太难记住了，原理也太难了 Day 2 现代密码一次性密码Frank Miller 在1882 年提出了一次性密码（One-time pad）的概念——加密：将消息和私钥进行异或运算得到密文；解密：将密钥和密文进行异或运算得到原消息，这个过程类似于前面提到的 a ⊕ b ⊕ a = b 。一次性密码的定义如下所示： 无条件安全 密钥随机产生的，只能用一次 异或 1+1 =0 ，0+1 = 1 共享密钥难 流密码体制密钥k,产生密钥流（发 同步流密码（状态无光） 一.加密方法的分类：按照不同的标准有不同的分类标准：1.按照密钥的特征不同，可以分为对称密码与非对称密码。2.按照加密方式的不同，可以分为流密码和分组密码。3.非对称密码均属于分组密码。 1.流密码。又名序列密码。明文称为明文流，以序列的方式表示。加密时候，先由种子密钥生成一个密钥流。然后利用加密算法把明文流和密钥流进行加密，产生密文流。流密码每次只针对明文流中的单个比特位进行加密变换，加密过程所需要的密钥流由种子密钥通过密钥流生成器产生。流密码的主要原理是通过随机数发生器产生性能优良的伪随机序列，使用该序列加密明文流（按比特位加密），得到密文流。由于每一个明文都对应一个随机的加密密钥，所以流密码在绝对理想的条件下应该是算一种无条件安全的一次一密密码。机密流程：种子密码-&gt;随机数发生器-&gt;密钥流明文流-&gt;(通过密钥流)-&gt;加密变换-&gt;密文流设明文流为：m=m1m2·····mi·····，密钥流由密钥流发生器f产生：zi=f（k，ai），ai指加密器存储器在i时刻的状态，f是由种子密钥k和ai产生的函数，设最终的密钥流为k=k1k2···ki·····，加密结果为c=c1c2····ci·····=Ek1（m1）.。。。Eki（mi），解密结果为m=Dk1（c1）Dk2（c2）···Dki（ci）=m1m2···mi，无论加密解密，其关键都是密钥流。 2.流密码的分类分为同步流密码和自同步流密码3.流密码的特性：极大的周期，良好的统计特性，抗线性分析。4.流密码的安全性取决于密钥流的安全性，要求密钥流序列有较好的随机性。5.不明密钥的人如何对流密码进行分析。这种密钥流一般都是周期的，做到完全随机是困难的，这样伪随机序列，理论上是可以分析出来的。举个例子。敌方截获了密文串：101101011110010明文串：011001111111001密钥流：110100100001011可以根据前10个比特建立如下方程 密钥流生成器： 高要求关键 要求： 游程：周期 0.1 发聩函数 产生密钥流的要求，方法、设计 反馈移位寄存器 ​ ：寄存器 ​ ： 返回函数 初始状态 线性反馈移位寄存器 快 周期“ 输出形状：发聩函数 算法 RC4 流密码是一次一密吗？不是 RC4没有实现的m-序列不可约《2^n-1 充要 本原多项式 反馈函数形式 伪随机性 求你12 Day 3 3_13 分组密码应用设计结构原理安全性原则混淆原则 扩散原则 算法要求分组长度足够大 密钥量足够大 DES算法56-64 IBM第一个商业 后面出现了AES 算法框图 IP 初始置换 论函数 16论 分左右32bit 公式：函数（R,轮密钥） S盒 输入六位，8个盒子 输出32bit step1; 32bit-48bit() 选择扩展运算 E 8*4-》两端 置换 S盒 4*16 选择压缩运算 ​ 输入输出 ​ 输入：6bit 二进制-》十进制 确定位置 P盒置换 32 -32 密钥编排 置换-》两组-》循环左移》16轮密钥 性质：互补性和弱密钥性 2DES 56+1 = 57 中间人相遇工具 3DES 分组密码的工作模式 为什么？分组长度是固定，而数据长度和格式是不同的， 电码本模式 密码分组链接模式 ​ CBC加密 完整性（认证码生成）加密，对比 明文校验码-》CBC(M.r)&gt;对比 解决：明文统计规律隐藏 工作模式 2 数据格式： ​ 字节、比特、等等故事 分组密码概述共享密钥 IV 有限域的基本概念单位元：加法 逆元：乘法 AES字节为处理单元 8bits 加法：mod 2 多项式除法 8 4 3 1 0 的末多项式取模 多项式运算 128 128，192，256 S-按列 四个基本做出 10 12 14 s:16 字节代换 She 16*16 S里面查表代换 二进制 十六进展 求逆 混淆效应 乱了 行移位 ​ 循环左移 列混淆 ​ 每一列矩阵现场 ​ 看成多项式 ​ 矩阵选择 轮密钥加 异或：子密钥 居住 ​ 初始密钥， AES ​ 四个位-》一个字节-》16进制 一个字节=》两个十六进数 密钥扩展算法 逆S盒 Day 4 现代密码学 3-27公钥：密钥管理， 非对称密码体制 密钥对 pk sk 加密：公钥 优势： ​ 密钥分发 ​ 密钥管理 ：1 N-1 ​ 开放系统 RSA加密算法数学知识 ​ 算法 大数据分解 密钥生成 最大公因子和乘法逆元的计算方法。 https://blog.csdn.net/boksic/article/details/7014386 https://blog.csdn.net/a745233700/article/details/102341542?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-5&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-5 https://blog.csdn.net/weixin_34138377/article/details/92199465?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-4&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-4 https://blog.csdn.net/weixin_41482303/article/details/85417302?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3 签名 证书 https://blog.csdn.net/weixin_34007879/article/details/85528967?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2]]></content>
      <categories>
        <category>科普</category>
      </categories>
      <tags>
        <tag>日常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天了解多一点]]></title>
    <url>%2F2020%2F06%2F11%2F%E6%AF%8F%E5%A4%A9%E4%BA%86%E8%A7%A3%E5%A4%9A%E4%B8%80%E7%82%B9%2F</url>
    <content type="text"><![CDATA[Hey, password is required here. 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]]></content>
      <categories>
        <category>科普</category>
      </categories>
      <tags>
        <tag>日常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F04%2F13%2Fmachine%20learning%20test%2F</url>
    <content type="text"><![CDATA[Logistics Regression 如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。 ​ 这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。 逻辑回归的基本假设 任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的 第一个 基本假设是 假设数据服从伯努利分布。 伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是p,抛中为负面的概率是 1-p,在逻辑回归这个模型里面是假设 $h_θ(x)$为样本为正的概率， $1−h_θ(x)$为样本为负的概率。那么整个模型可以描述为$h_θ(x;θ)=p$ 逻辑回归的第二个假设是假设样本为正的概率是 $p=\frac{1}{1+e^{w^Tx}}$ 所以逻辑回归的最终形式 $h_θ(x;θ)=\frac{1}{1+e^{w^Tx}}$ 逻辑回归的损失函数 逻辑回归的损失函数是它的极大似然函数 $Lθ(x)=\pi_{i=1}^{m}h_θ(xi;θ)^y_i∗(1−h_θ(xi;θ))^{1−y_i}$ 逻辑回归的求解方法 由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。 小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。 其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。 第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。 第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。这里我们不展开，有空我会专门出一个专题介绍。 逻辑回归的目的 该函数的目的便是将数据二分类，提高准确率。 逻辑回归如何分类 逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。 3.对逻辑回归的进一步提问​ 逻辑回归虽然从形式上非常的简单，但是其内涵是非常的丰富。有很多问题是可以进行思考的 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？ 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新 $w_j=w_j−(y^i−h_w(x^i;w))∗x_j^i\theta$ 这个式子的更新速度只和$x_j,y_j 相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。 为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？ 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。 但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。 如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。 为什么我们还是会在训练的过程当中将高度相关的特征去掉？ 去掉高度相关的特征会让模型的可解释性更好 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。 4.逻辑回归的优缺点总结​ 面试的时候，别人也经常会问到，你在使用逻辑回归的时候有哪些感受。觉得它有哪些优缺点。 ​ 在这里我们总结了逻辑回归应用到工业界当中一些优点： 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。 资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。 ​ 但是逻辑回归本身也有许多的缺点: 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。 模型、策略、算法Codings]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F04%2F13%2Fmachine%20learning%2F</url>
    <content type="text"><![CDATA[英伟达:芯片，GPU 开发框架：tensorflow，pytorch caffe 监督学习学习目的是学习一个输入到输出的映射，称为模型。模型的集合就是假设空间。 模型：概率模型；非概率模型； 学习过程：搜索过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow]]></title>
    <url>%2F2019%2F04%2F11%2Ftensorflow%2F</url>
    <content type="text"><![CDATA[official definition What is tensorflowflow of tensors “TensorFlow is an open source software library for numerical computation using dataflow graphs. Nodes in the graph represents mathematical operations, while graph edges represent multi-dimensional data arrays (aka tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.”* A major difference between numpy and TensorFlow is that TensorFlow follows a lazy programming paradigm. It first builds a graph of all the operation to be done, and then when a “session” is called, it “runs” the graph. It’s built to be scalable, by changing internal data representation to tensors (aka multi-dimensional arrays). Building a computational graph can be considered as the main ingredient of TensorFlow. It’s easy to classify TensorFlow as a neural network library, but it’s not just that. Yes, it was designed to be a powerful neural network library. But it has the power to do much more than that. You can build other machine learning algorithms on it such as decision trees or k-Nearest Neighbors. You can literally do everything you normally would do in numpy! It’s aptly called “numpy on steroids” The advantages of using TensorFlow are: It has an intuitive construct, because as the name suggests it has “flow of tensors”. You can easily visualize each and every part of the graph. Easily train on cpu/gpu for distributed computing Platform flexibility. You can run the models wherever you want, whether it is on mobile, server or PC. scikit-learn 123456# define hyperparamters of ML algorithmclf = svm.SVC(gamma=0.001, C=100.)# train clf.fit(X, y)# test clf.predict(X_test) The usual workflow of running a program in TensorFlow is as follows: Build a computational graph, this can be any mathematical operation TensorFlow supports. Initialize variables, to compile the variables defined previously Create session(会话）, this is where the magic starts! Run graph in session, the compiled graph is passed to the session, which starts its execution. Close session, shutdown the session. Lets write a small program to add two numbers! 12345678910111213141516171819# import tensorflowimport tensorflow as tf# build computational grapha = tf.placeholder(tf.int16)b = tf.placeholder(tf.int16)addition = tf.add(a, b)# initialize variablesinit = tf.initialize_all_variables()# create session and run the graphwith tf.Session() as sess: sess.run(init) print "Addition: %i" % sess.run(addition, feed_dict=&#123;a: 2, b: 3&#125;)# close sessionsess.close() A typical implementation of Neural Network would be as follows: Define Neural Network architecture to be compiled Transfer data to your model Under the hood, the data is first divided into batches, so that it can be ingested. The batches are first preprocessed, augmented and then fed into Neural Network for training The model then gets trained incrementally Display the accuracy for a specific number of timesteps After training save the model for future use Test the model on a new data and check how it performs 三类非常重要的变量占位符tensorFlow中接收值的方式为占位符(placeholder)，创建placeholder 123- # b = tf.placeholder(tf.float32, [None, 1], name='b')第二个参数值为[None, 1]，其中None表示不确定，即不确定第一个维度的大小，第一维可以是任意大小。特别对应tensor数量(或者样本数量)，输入的tensor数目可以是32、64… placeholder: A way to feed data into the graphsfeed_dict: A dictionary to pass numeric values to computational graph 常量tf.constant()`定义常量 1const = tf.constant(2.0, name='const') 变量 ​ 使用tf.Variable()定义变量 1c = tf.Variable(1.0, dtype=tf.float32, name='c') TensorFlow中所有的变量必须经过初始化才能使用，**初始化方式分两步： 定义初始化operation 12# 1. 定义init operationinit_op = tf.global_variables_initializer() 运行初始化operation 12# 2. 运行init operation sess.run(init_op) reference https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/ https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial https://github.com/aymericdamien/TensorFlow-Examples video:https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/ course: https://classroom.udacity.com/courses/ud187 tensorflow: GOOGLE 开源、Deep learning 练数成金C1 tensorboard ：a tool;visual network;debug alter dir of jupyter 顺便改了下新下载的路径（GOOD） CPU or GPU C2graphs 代表计算任务，节点（op)，一个op可以获得o个或者多个tensor,输出1个或者多个tensor Session(会话)的上下文（context)中执行 tensor表示数据,n维数组 C3 简单的回归神经网络（拟合二次函数），貌似学了理论没有实践，还真是忘得快啊 手写体分类、Softmax函数 softmax函数可以给不同的对象分配概率，softmax($x_i$)=$\frac{exp(x_i)}{\sum_j{exp(x_j)}}$ 如输出[1,2,5] ,$p1=\frac{exp(1)}{exp(1)+exp(2)+exp(5)}$,$p2=\frac{exp(2)}{exp(1)+exp(2)+exp(5)}$,$p1=\frac{exp(5)}{exp(1)+exp(2)+exp(5)}$ Keras 安装 backend 基于什么做运算（tensorflow or theano) import keras 查看 底层搭建 a） /.keras/keras.json 相关的配置信息 b) 终端改，单次 import os os.environ[‘KERAS_BACKEND’]= ‘tensorflow’ import keras For example model :Sequential layer : Dense activation 训练算法：model.compile(参数optimizer=’梯度下降法的变种’ , loss=’rms/‘) 训练：model. fit (x,y) model.train_on_batch evaluate:model.evaluate prediction: model.predict(x_test, batch_size=128) https://github.com/MorvanZhou/tutorials/blob/master/kerasTUT/5-classifier_example.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 4 - Regressor exampleimport numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.models import Sequential # 按顺序建立from keras.layers import Dense # 全连接层import matplotlib.pyplot as plt# create some dataX = np.linspace(-1, 1, 200)np.random.shuffle(X) # randomize the dataY = 0.5 * X + 2 + np.random.normal(0, 0.05, (200, ))# plot dataplt.scatter(X, Y)plt.show()X_train, Y_train = X[:160], Y[:160] # first 160 data pointsX_test, Y_test = X[160:], Y[160:] # last 40 data points# build a neural network from the 1st layer to the last layermodel = Sequential()model.add(Dense(units=1, input_dim=1)) # choose loss function and optimizing methodmodel.compile(loss='mse', optimizer='sgd')# trainingprint('Training -----------')for step in range(301): cost = model.train_on_batch(X_train, Y_train) if step % 100 == 0: print('train cost: ', cost)# testprint('\nTesting ------------')cost = model.evaluate(X_test, Y_test, batch_size=40)print('test cost:', cost)W, b = model.layers[0].get_weights()print('Weights=', W, '\nbiases=', b)# plotting the predictionY_pred = model.predict(X_test)plt.scatter(X_test, Y_test)plt.plot(X_test, Y_pred)plt.show() 51234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556"""To know more or get code samples, please visit my website:https://morvanzhou.github.io/tutorials/Or search: 莫烦PythonThank you for supporting!"""# please note, all tutorial code are running under python3.5.# If you use the version like python2.7, please modify the code accordingly# 5 - Classifier exampleimport numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Activationfrom keras.optimizers import RMSprop# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called# X shape (60,000 28x28), y shape (10,000, )(X_train, y_train), (X_test, y_test) = mnist.load_data()# data pre-processingX_train = X_train.reshape(X_train.shape[0], -1) / 255. # normalizeX_test = X_test.reshape(X_test.shape[0], -1) / 255. # normalizey_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10)# Another way to build your neural netmodel = Sequential([ Dense(32, input_dim=784), Activation('relu'), Dense(10), Activation('softmax'),])# Another way to define your optimizerrmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)# We add metrics to get more results you want to seemodel.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy'])print('Training ------------')# Another way to train the modelmodel.fit(X_train, y_train, epochs=2, batch_size=32)print('\nTesting ------------')# Evaluate the model with the metrics we defined earlierloss, accuracy = model.evaluate(X_test, y_test)print('test loss: ', loss)print('test accuracy: ', accuracy) 6 CNN卷积神经网络不是对 https://www.cnblogs.com/skyfsm/p/6790245.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990"""To know more or get code samples, please visit my website:https://morvanzhou.github.io/tutorials/Or search: 莫烦PythonThank you for supporting!"""# please note, all tutorial code are running under python3.5.# If you use the version like python2.7, please modify the code accordingly# 6 - CNN example# to try tensorflow, un-comment following two lines# import os# os.environ['KERAS_BACKEND']='tensorflow'import numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flattenfrom keras.optimizers import Adam# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called# training X shape (60000, 28x28), Y shape (60000, ). test X shape (10000, 28x28), Y shape (10000, )(X_train, y_train), (X_test, y_test) = mnist.load_data()# data pre-processingX_train = X_train.reshape(-1, 1,28, 28)/255.X_test = X_test.reshape(-1, 1,28, 28)/255.y_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10)# Another way to build your CNNmodel = Sequential()# Conv layer 1 output shape (32, 28, 28)model.add(Convolution2D( batch_input_shape=(None, 1, 28, 28), filters=32, kernel_size=5, strides=1, padding='same', # Padding method data_format='channels_first',))model.add(Activation('relu'))# Pooling layer 1 (max pooling) output shape (32, 14, 14)model.add(MaxPooling2D( pool_size=2, strides=2, padding='same', # Padding method data_format='channels_first',))# Conv layer 2 output shape (64, 14, 14)model.add(Convolution2D(64, 5, strides=1, padding='same', data_format='channels_first'))model.add(Activation('relu'))# Pooling layer 2 (max pooling) output shape (64, 7, 7)model.add(MaxPooling2D(2, 2, 'same', data_format='channels_first'))# Fully connected layer 1 input shape (64 * 7 * 7) = (3136), output shape (1024)model.add(Flatten())model.add(Dense(1024))model.add(Activation('relu'))# Fully connected layer 2 to shape (10) for 10 classesmodel.add(Dense(10))model.add(Activation('softmax'))# Another way to define your optimizeradam = Adam(lr=1e-4)# We add metrics to get more results you want to seemodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])print('Training ------------')# Another way to train the modelmodel.fit(X_train, y_train, epochs=1, batch_size=64,)print('\nTesting ------------')# Evaluate the model with the metrics we defined earlierloss, accuracy = model.evaluate(X_test, y_test)print('\ntest loss: ', loss)print('\ntest accuracy: ', accuracy)]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>tensorlow</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[: [TOC] 利用听力材料学英语最高效有效输出应该是稍高于现有水平的、更准确、更得体的表达输出 先记住以下两点，下面我们再一一解析应该要怎么做到。 1. 学习需要反馈，来告诉我们这输出是正确的，可以继续；或者是错误的，需要修正。 2. 将输入内化，变成自己的知识。 我一般会听四遍，但不是全听原文。 第一遍：泛听原文，不要看字幕或脚本，清楚录音的内容。听第一遍的时候我通常连笔记都不做，目的是为了让自己流畅的听完，对听力的内容有一个整体的掌握。 第二遍：先听原文，根据内容段落，开始复述内容，并录音。这一步的目的是强迫自己调用已经学过的知识，组织语言和进行练习。 第三遍，听自己的录音，然后做出修正。这一步很重要，可以让你了解自己的发音问题和语法问题，并把可听出来的语法问题进行修改。通过这一步，我们就可以把简单重复输入的语言材料，转化为有效输出。 第四遍，听原文看字幕和脚本，看把听不懂的地方标注，说明为什么听不懂（比如是因为自己发音不准导致的听不出，或者就是因为这个词没背过、不熟悉）。不熟悉的用法和自己用错的地方总结，背下来，下次试着用。 Tips： 不要选择太难的材料，太难的材料容易使自己丧失学习兴趣。 一开始，不要选择太长的听力材料。10分钟左右最佳。在这里推荐ted，可以选择有字幕或关闭字幕。 在一个相近的时间段内，选择相近题材的材料。比如我会在两个星期内选择“心理”题材的录音。这样我就会有更大的几率用上刚学过的结构和词汇。 及时总结，及时复习已背过的材料，复习的重要性大家都懂，这里就不多说了。 真题听写 材料选择 能够听得懂 70%的材料 2 具体执行方法 先泛听一篇 再循环听几遍 再逐句逐句的听 美剧精听 先看中文听 英文，查 听找 台词，跟读 重复三四至少10]]></content>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯分类器]]></title>
    <url>%2F2019%2F03%2F28%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[[TOC] 概率论的知识 条件概率 P(A|B)=P(A\cap B)/P(B)已知B发生的概率，求A发生的概率 全概率 P(B) = \sum_{i=1}^{N}P(B \cap A_i)P(A_i)贝叶斯推断 P(A|B)=P(A)\frac{P(B|A)}{P(B)} P(A_i|B)=P(A_i)\frac{P(B|A_i)}{\sum P(A_i)P(B|A_i)}$P(A)$：Prior probability 先验概率，在B事件发生之前，对A事件做一个判断 $P(A|B)$:Posterior probability 后验概率，在B事件发生之后，对A事件的概率重新评估 $P(B|A)/P(B)$:称为可能性函数，一个调整因子 后验概率=先验概率*调整因子 （可知，调整因此&gt;1,发生概率增大了， 贝叶斯决策论英文：Bayesian decision theory 设有$N$种可能的类别, 即γ=${c_1,c_2,…,c_N}$. $λ_ij$是将一个真实类别为$c_j$的样本判为$c_x$的损失。 基于后验概率可得将样本分类所产生的期望损失, 或者成为条件风险(Conditional Risk) R(C_i|x)=∑_{j=1}^Nλ_{ij}P(c_j|x)于是， 我们的任务就是寻找判定准则h， 令$χ→γ$ 使得最小化总体风险，$R(h)=E_x[R(h(x)|x]$最小. 对于每一个$x$，若$h$都能最小化条件风险，那么总体也被最小化了。 可以简化为对每个样本选择其条件风险最小的分类, 即: h(x)=arg \min_{c⊂λ}R(c|x)此$h(x)$就是贝叶斯最优分类器。 $R(h)$为贝叶斯风险(Bayes Risk), $1−R(h)$反映了分类器的最优性能. 具体来说，如果目标是最小化分类错误率， \lambda_{ij}=\begin{cases} 0\ \ i==j\\1 \ \ \ i!=j \end{cases}则$R(c|x)=1-p(c|x)$，因此可知，$h(x)=\max_{c\in C} p(c|x)$ 对于样本$x$,选择后验概率$P(c|X)$最大的类别为标记。 问题转换为 P(c_i|x)=\frac{P(c_i)P(x|c_i)}{\sum P(x)}求先验概率和似然($P(x|c)$) 其中 $P(c)$表达了样本空间种各类样本所占的比列，根据大数定律，当样本足够充分的独立同分布样本是，可以频率估计 $P(x|c)$,涉及关于x所以属性的联合概率，用频率估计概率可能不太好，对于估计类条件概率的一种宠用策略是先假设具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。 $P(x|c)$是类条件概率，由某个分布决定，$P(x|\theta_c)$来表示了 频率注意派认为可以通过优化似然函数估计参数。$D_c$类别c的样本集合，独立同分布 P(D_c|\theta_c)=\Pi_{x \in D_c}P(x|\theta_c) LL(\theta_c)=log P(D_c|\theta_c)朴素贝叶斯分类器英文：naive Bayes classifier 假设：属性条件独立性假设，每个属性独立性对分类结果发生影响 P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)\Pi_{i=1}^{d}P(x_i|c)}{P(x)}对于一个$x$，$P(x)$都是相同的，因此贝叶斯模型可写为 h_{nb}(x)=arg max_{c\in y}P(c)\Pi_{i=1}^{d}P(x_i|c)计算过程假设$D_{c_i}$表示第i类的样本集合， $P(c_i)=\frac{|D_{c_i}|}{|D|}$ 如果是离散属性 P(x_i|c_i)=\frac{|D_{c,x_i}|}{|D_{c_i}|}如果是连续属性，$P(x_i|c_i)$服从$N(u_{c,i},\theta_{c,i}^2)$的分布 P(x_i|c)=\frac{1}{\sqrt{2\pi}\theta_{c,i}}exp(-\frac{(x_i-u_i)^2}{2\theta_{c,i}^2}) $P(c_i)\Pi_{i=1}^{N}P(x_i|c_i)$ 注意为了避免其他属性携带的信息被训练集中未出现的属性值抹去，因此用拉普拉斯修正（Laplacian correction) P(c)=\frac{|D_{c_i}|+1}{|D|+N}\\ P(x_i|c)=\frac{|D_{x_i,c}|+1}{|D_c|+N_i}$N$:训练集可能出现的类别数 $N_i$:第i个属性可能的取值数 显然，拉普拉斯修正避免因训练集不充分导出的概率估值为0的情况 朴素贝叶斯的种类再scikit-learn中，一共有三个朴素贝叶斯，分别是 GaussianNB P(x_i|C_i)=\frac{1}{\sqrt{2\pi}\theta_{c,i}}exp(-\frac{(x_i-u_i)^2}{2\theta_{c,i}^2})12345678910111213141516171819202122#导入包import pandas as pdfrom sklearn.naive_bayes import GaussianNBfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score#导入数据集from sklearn import datasetsiris=datasets.load_iris()#切分数据集Xtrain, Xtest, ytrain, ytest = train_test_split(iris.data, iris.target, random_state=42)#建模clf = GaussianNB()clf.fit(Xtrain, ytrain)#在测试集上执行预测，proba导出的是每个样本属于某类的概率clf.predict(Xtest)clf.predict_proba(Xtest) #每一类计算结果都输出#测试准确率accuracy_score(ytest, clf.predict(Xtest)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import numpy as npimport pandas as pdimport randomdataSet =pd.read_csv('iris.txt',header = None)dataSet.head()def randSplit(dataSet, rate): l = list(dataSet.index) #提取出索引 random.shuffle(l) #随机打乱索引 dataSet.index = l #将打乱后的索引重新赋值给原数据集 n = dataSet.shape[0] #总行数 m = int(n * rate) #训练集的数量 train = dataSet.loc[range(m), :] #提取前m个记录作为训练集 test = dataSet.loc[range(m, n), :] #剩下的作为测试集 dataSet.index = range(dataSet.shape[0]) #更新原数据集的索引 test.index = range(test.shape[0]) #更新测试集的索引train,test=randSplit(dataSet, 0.8)def gnb_classify(train,test): labels = train.iloc[:,-1].value_counts().index #提取训练集的标签种类 mean =[] #存放每个类别的均值 std =[] #存放每个类别的方差 result = [] #存放测试集的预测结果 for i in labels: item = train.loc[train.iloc[:,-1]==i,:] #分别提取出每一种类别 m = item.iloc[:,:-1].mean() #当前类别的平均值 s = np.sum((item.iloc[:,:-1]-m)**2)/(item.shape[0]) #当前类别的方差 mean.append(m) #将当前类别的平均值追加至列表 std.append(s) #将当前类别的方差追加至列表 means = pd.DataFrame(mean,index=labels) #变成DF格式，索引为类标签 stds = pd.DataFrame(std,index=labels) #变成DF格式，索引为类标签 for j in range(test.shape[0]): iset = test.iloc[j,:-1].tolist() #当前测试实例 iprob = np.exp(-1*(iset-means)**2/(stds*2))/(np.sqrt(2*np.pi*stds)) #正态分布公式 prob = train.iloc[:,-1].value_counts()/len(train.iloc[:,-1]) #初始化当前实例总概率 for k in range(test.shape[1]-1): #遍历每个特征 prob *= iprob[k] #特征概率之积即为当前实例概率 cla = prob.index[np.argmax(prob.values)] #返回最大概率的类别 result.append(cla) test['predict']=result acc = (test.iloc[:,-1]==test.iloc[:,-2]).mean() #计算预测准确率 print(f'模型预测准确率为&#123;acc&#125;') return testgnb_classify(train,test)for i in range(20): train,test= randSplit(dataSet, 0.8) gnb_classify(train,test) MultinomialNB先验概率多项式分布的朴素贝叶斯，假设特征是由一共简单多项式分布生成，多项分布可以描述各种类型样本出现的频率，该模型常用于文本分类，特别表示次数。$\lambda$常取值1 P(x_{il}|c)=\frac{x_{il}+\lambda}{m_k+n\lambda}12345678910def loadDataSet(): dataSet=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] #切分好的词条 classVec = [0,1,0,1,0,1] #类别标签向量，1代表侮辱性词汇，0代表非侮辱性词汇 return dataSet,classVecdataSet,classVec = loadDataSet() 12345678def createVocabList(dataSet): vocabSet = set() #创建一个空的集合 for doc in dataSet: #遍历dataSet中的每一条言论 vocabSet = vocabSet | set(doc) #取并集 vocabList = list(vocabSet) return vocabListvocabList = createVocabList(dataSet) 12345678def setOfWords2Vec(vocabList, inputSet): returnVec = [0] * len(vocabList) #创建一个其中所含元素都为0的向量 for word in inputSet: #遍历每个词条 if word in vocabList: #如果词条存在于词汇表中，则变为1 returnVec[vocabList.index(word)] = 1 else: print(f" &#123;word&#125; is not in my Vocabulary!" ) return returnVec #返回文档向量 12345678def get_trainMat(dataSet): trainMat = [] #初始化向量列表 vocabList = createVocabList(dataSet) #生成词汇表 for inputSet in dataSet: #遍历样本词条中的每一条样本 returnVec=setOfWords2Vec(vocabList, inputSet) #将当前词条向量化 trainMat.append(returnVec) #追加到向量列表中 return trainMattrainMat = get_trainMat(dataSet) 1234567891011121314151617181920def trainNB(trainMat,classVec): n = len(trainMat) #计算训练的文档数目 m = len(trainMat[0]) #计算每篇文档的词条数 pAb = sum(classVec)/n #文档属于侮辱类的概率 p0Num = np.zeros(m) #词条出现数初始化为0 p1Num = np.zeros(m) #词条出现数初始化为0 p0Denom = 0 #分母初始化为0 p1Denom = 0 #分母初始化为0 for i in range(n): #遍历每一个文档 if classVec[i] == 1: #统计属于侮辱类的条件概率所需的数据 p1Num += trainMat[i] p1Denom += sum(trainMat[i]) else: #统计属于非侮辱类的条件概率所需的数据 p0Num += trainMat[i] p0Denom += sum(trainMat[i]) p1V = p1Num/p1Denom p0V = p0Num/p0Denom return p0V,p1V,pAb #返回属于非侮辱类,侮辱类和文档属于侮辱类的概率p0V,p1V,pAb=trainNB(trainMat,classVec) 1234567891011121314151617181920212223from functools import reducedef classifyNB(vec2Classify, p0V, p1V, pAb): p1 = reduce(lambda x,y:x*y, vec2Classify * p1V) * pAb #对应元素相乘 p0 = reduce(lambda x,y:x*y, vec2Classify * p0V) * (1 - pAb) print('p0:',p0) print('p1:',p1) if p1 &gt; p0: return 1 else: return 0def testingNB(testVec): dataSet,classVec = loadDataSet() #创建实验样本 vocabList = createVocabList(dataSet) #创建词汇表 trainMat= get_trainMat(dataSet) #将实验样本向量化 p0V,p1V,pAb = trainNB(trainMat,classVec) #训练朴素贝叶斯分类器 thisone = setOfWords2Vec(vocabList, testVec) #测试样本向量化 if classifyNB(thisone,p0V,p1V,pAb): print(testVec,'属于侮辱类') #执行分类并打印分类结果 else: print(testVec,'属于非侮辱类') #执行分类并打印分类结果 testVec1 = ['love', 'my', 'dalmation']testingNB(testVec1) BernoulliNB伯努利分布，如果是二元伯努利分布 P(x_{il}|C_i)=P(i|Y=C_i)x_{il}+(1-P(i|Y=C_i))(1-x_{il})如果样本属性大多数属于连续，GaussionNB 如果是离散值，使用MultinomialNB 如果样本特征是二元离散值或者稀疏离散值，BernoulliNB 半朴素贝叶斯信息量、熵、联合熵、条件熵、互信息信息量反应了随机变量取某个值含的可能性大小，或者是含有的信息多少 I(X=x)=-log_2^{p(x）}熵(entropy)反应了信源平均每个符号的信息量,或者是随机变量不确定性的衡量 H(X)=E(I(X))=\sum p(X=x)(-log_2^{p(x)})联合熵反应了多个随机变量的平均信息量 H(X,Y)=\sum p(x,y)(-log_2^{p(x,y)})条件熵（Conditional entropy）反应了已知一个随机变量下，另一个随机变量的不确定性 H(X|Y)=-\sum p(y)H(X|Y=y)=-\sum p(x,y)log_2^{p(x|y)}互信息(mutual information)反应了已知一个随机变量的情况下，另外一个随机变量不确定性减少了多少,可以把互信息看成由于知道 y 值而造成的 x 的不确定性的减小 I(X;Y)=\sum \sum p(x,y)log(\frac{p(x,y)}{p(x)p(y)})\\ =H(X)-H(X|Y)=H(Y)-H(Y|X)如果两个随机变量独立，则互信息为0,因此，互信息可以衡量两个随机变量的相关程度 条件互信息在条件z发生时的条件互信息 I(X;Y|Z) = \sum\sum p(x,y|z)log_2^{\frac{p(x,y|z)}{p(x|z)p(y|z)}} 半朴素贝叶斯适当的考虑一部分属性间的相互依赖关系，这个关系可以用互信息描述 独依赖假设每个属性只有一个其他 的属性.则计算公式改下如下 p(C)\Pi_{i=1}^{d} P(x_i|C_i,pa_i)$pa_i$是属性$x_i$所依赖的属性，被称为$x_i$的父属性 1) SPODE 最简单的方法是：都选一个属性作为父属性 可以通过交叉验证的方法 2) TAN :最大带权生成树 权重：当y划分为$c_k$类时条件熵 I(x_i;y_i|y)=\sum_{x_i,y_i,c_k}p(x_i,y_j|c_k)log^{\frac{p(x_i;y_j|c_k)}{p(x_i|c_k)p(y_i|c_k)}}step 1: 计算任意两个属性之间条件互信息 I(X;Y|Y)=\sum_{i}I(X;Y|c_i)step 2: 以属性为结点构建完全图 step 3: 最大带权生成树，挑选根变量 step 4: 加入类别结点y,增加到每个属性的有向边 条件互信息反应了属性在已知类别下的相关性大小 集成学习AODE选择模型尝试将每个属性作为超父构建SPODE P(c_i|X)正比于 \sum_{i=1,|D_{x_i}>=m}p(c,x_i)\Pi_{j=1}^{d}p(x_j|c_i,x_i)$m$通常取30, P(c,x_i)=\frac{|D_{c,x_i}|+1}{|{D}|+N*N_i}\\ P(x_j|c,x_i)=\frac{|D_{c,x_i,x_j}+1|}{|D_{c,xi}|+N_j}贝叶斯网(Bayesian network)借助有向无环图来刻画属性之间的依赖关系，条件概率表来描述属性的联合概率分布。 一个贝叶斯网络$B$,包括结构$G$和参数$\Theta$ ,$B(G,\Theta)$,如果两个属性有直接依赖关系，用边连接，对于属性$x_i$,其父节点集合$G_i$,则$\Theta$包括每个属性条件概率$\Theta_{x_i|\pi_i}=P_B(x_i|\pi_i)$ 结构 p(x_1,x_2,...,x_n)=\Pi_{i=1}^{n}p_{B}(x_i|\pi_i)=\Pi_{i=1}^{d}\Theta_{xi|\pi_i}\\ =\Pi_{i=1}^{d}P(x_i|Parents(x_i))推断一旦训练好贝叶斯网后，就能回答query,通过一些属性的观测者来推断其他属性变量的取值，其中，已知变量的值观测推测待查询的过程“推断”,已知变量的观测者”证据“]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯分类器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二次规划]]></title>
    <url>%2F2019%2F03%2F25%2F%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[[TOC] KKT(Karush-Kuhn-Tucher)条件 给定优化问题 \min f(x)\\ subject\ to \begin{cases} g_i(x) = 0 (i=1,,,,m\\ h_i(x) =0 (i=m+1,...,n)\\ \lambda_i h_i(x)=0(i=m+1,..,n)二次规划问题问题的数学表达 \min Q(x) = \frac{1}{2}x^THx+g^Tx\\ s.t. a_i^Tx = b_i (i=1,..,m)\\ \ \ \ \ \ \ \ a_i^Tx =x^{*T}H(x-x^{*})+g^T(x-x^{*})=\lambda^TA(x-x^{*})http://www.hankcs.com/ml/lagrange-duality.html#h3-7 SMO ：Sequential minimal optimization支持向量机的对偶问题 \min \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^{m}\alpha_i\\ s.t. \sum_{i=1}^{m}\alpha_iy_i=0\\ 0]]></content>
      <categories>
        <category>机器学习</category>
        <category>数学</category>
      </categories>
      <tags>
        <tag>二次规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Boosting]]></title>
    <url>%2F2019%2F03%2F22%2FBoosting%2F</url>
    <content type="text"><![CDATA[[TOC] Boosting原理Boosting算法是将“弱学习算法“提升为“强学习算法”的过程。 加法模型 F_n(x;P) = \sum_{t=1}^{n}\alpha_th_t(x;a_t) 前向分步 F_m(x) = F_{m-1}(x)+\alpha_mh_m(x,a_m)如果选取不同损失函数，则产生不同的类型 AdaBoostAdaBoost就是损失函数为指数损失的Boosting算法。 每一次迭代的弱学习$h(x;a_m)$有何不一样，如何学习？ AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。 弱分类器权值$β_m$如何确定？ AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。 原理理解基于Boosting的理解，对于AdaBoost，我们要搞清楚两点： 每一次迭代的弱学习h(x;am)有何不一样，如何学习？弱分类器权值βm如何确定？对于第一个问题，AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。然后，再根据所采用的一些基本机器学习算法进行学习，比如逻辑回归。 对于第二个问题，AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。 公式推导指数损失函数 L(Y,f(x))=exp(-Yf(x))权重更新公式: 采用的指数误差函数 l_{exp}(a_th_t|D_t)=E(exp(-f(x)a_th_t(x)))\\ =p(f(x)=h_t(x))e^{-at}+p(f(x)!=h_t(x))e^{at}\\ =e^{-at}(1-\xi)+e^{at}\xi a_t=\frac{1}{2}ln \frac{1-\xi}{\xi}分布更新公式 \begin{aligned} l\left(H_{t-1}(x)+\alpha h_{t}(x) | D\right) &=E_{X \sim D}\left(\exp \left(-y(x)\left(H_{t-1}(x)+\alpha h_{t}(x)\right)\right)\right) \\ &=E_{x \sim D}\left(\exp \left(-y(x) H_{t-1}(x)\right) \exp \left(-y(x) \alpha h_{t}(x)\right)\right) \end{aligned}在泰勒展开$exp(-y(x)h_t(x))$ \begin{aligned} l\left(H_{t-1}(x)+h_{t}(x) | D\right) & \approx E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-\alpha y(x) h_{t}(x)+\frac{\alpha^{2} y^{2}(x) h_{t}^{2}(x)}{2}\right)\right] \\ &=E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-y(x) h_{t}(x)+0.5 \alpha^{2}\right)\right] \end{aligned} \begin{aligned} h(x) &=\arg \min _{h} l\left(H_{t-1}(x)+\alpha h_{t} | D\right) \\ &=\arg \max _{h} E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right) \alpha y(x) h_{t}(x)\right] \\ &=\arg \max _{h}\left[\frac{\exp \left(-y(x) H_{t-1}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]} y(x) h(x)\right] \end{aligned} 令一个新分布,注意分子是常数 D_{t}(x)=\frac{D(x) \exp \left(-y(x) H_{t-1}(x)\right)^{L}}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]} \begin{aligned} h(x) &=\arg \max _{h} E_{x \sim D,}(y(x) h(x)) \\ &=\arg \max _{h} E_{x \sim D_{t}}(1-2 \mathcal{I}(y(x) \neq h(x))) \\ &=\arg \min _{h} E_{x \sim D_{i}}(\mathcal{I}(y(x) \neq h(x))) \end{aligned}同理可得 \begin{aligned} D_{t+1} &=\frac{D(x) \exp \left(-y(x) H_{t}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \\ &=\frac{D_{t}(x) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right] \cdot \exp \left(-y(x) H_{t}(x)\right)}{\exp \left(-y(x) H_{t-1}(x)\right) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \\ &=D_{t}(x) \exp \left(-y(x) \alpha h_{t}(x)\right) \cdot C . \quad(C i s a \text {constant}) \end{aligned} Z_{t}=\sum_{i}^{m} D_{t}(x) \exp \left(-y(x) \alpha_{t} h_{y}(x)\right)指数误差函数 \begin{aligned} l(H(x) | D) &=\frac{1}{m} \sum_{i}^{m} \exp \left(-y_{i} H\left(x_{i}\right)\right) \\ &=\frac{1}{m} \sum_{i}^{m} \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ &=\sum_{i}^{m} D_{1}\left(x_{i}\right) \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ &=Z_{1} Z_{2}\left(x_{i}\right) \exp \left(-\sum_{j=2}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ & \vdots \\ &=\prod_{i=1}^{T} Z_{i} \end{aligned}算法描述总结一下，得到AdaBoost的算法流程： 输入：训练数据集$T={(x1,y1),(x2,y2),(xN,yN)}T={(x1,y1),(x2,y2),(xN,yN)}$，其中，$xi∈X⊆Rnxi∈X⊆Rn，yi∈Y=−1,1yi∈Y=−1,1，$迭代次数M 初始化训练样本的权值分布：$D1=(w1,1,w1,2,…,w1,i),w,i=1,2,…,N$。 对于$m=1,2,…,M$ (a) 使用具有权值分布$D_m$的训练数据集进行学习，得到弱分类器$h_m(x)$ (b) 计算$h_m(x)$在训练数据集上的分类误差率： $e_m=∑_{i=1}^{N}w_m,iI(h_m(xi)≠y_i)$ (c) 计算$h_m(x)$在强分类器中所占的权重： $\alpha_m=\frac{1}{2}log(\frac{1−e_m}{e_m})$ (d) 更新训练数据集的权值分布（这里，$z_m是归一化因子，为了使样本的概率分布和为1）： w_{m+1,i}=\frac{w_{m,i}}exp(−α_my_ih_m(xi))，i=1,2,…,10z_m=∑_{i=1}^{N}w_{m,i}exp(−α_my_ih_m(xi)) 得到最终分类器： F(x)=sign(∑_{i=1}^{N}α_mh_m(x))面经今年8月开始找工作，参加大厂面试问到的相关问题有如下几点： 手推AdaBoost 与GBDT比较 AdaBoost几种基本机器学习算法哪个抗噪能力最强，哪个对重采样不敏感？ 算法流程实例计算Python实现https://www.cnblogs.com/davidwang456/articles/8927029.html 集成学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Boosting, AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量回归]]></title>
    <url>%2F2019%2F03%2F19%2FSVR%2F</url>
    <content type="text"><![CDATA[[TOC] 支持向量机用于分类:硬间隔和软件间隔支持向量机。尽可能分对 支持向量机回归： 希望$f(x)$与$y$尽可能的接近。 支持向量机基本思想英文名:support vector regression 简记：SVR 标准的线性支持向量回归模型学习的模型: f(x)=w^Tx+b假设能容忍$f(x)$与$y$之间差别绝对值$\xi$,这就以$f(x)=w^Tx+b$形成了一个$2\xi$的间隔带，因此模型 \min \frac{1}{2}w^Tw\\ s.t -\xi]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>支持向量机回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机(SVM) ----- 分类器]]></title>
    <url>%2F2019%2F03%2F17%2FSVMClassifiar%2F</url>
    <content type="text"><![CDATA[[TOC] 预备的数学知识约束优化问题原问题,带等式约束，也带不等式约束的一般约束问题 \begin{cases} \min_{x}f(x)\\ s.t \begin{cases} m_i(x)>=0, i=1,..,m\\ n_j(x)=0，j=1,..,m\\ \end{cases} \end{cases}\tag{1}构造lagrange乘子法 L(x,\lambda_i,\eta_j)= f(x)-\sum_{i=1}^{m}\lambda_im_i(x)-\sum_{j=1}^{n}\eta_j \tag{2} \begin{cases} \min_{x} max_{\lambda_i,\eta_j} L(R^p)\\ s.t \lambda_i>=0 \end{cases}上述两个问题的等价性证明 如果x不满足约束$m_i(x)$,则$\lambda_i&gt;=0$,同时$m_i(x)&lt;$,则$L(R^{p},\lambda,\eta)$趋近无穷，反之，则存在最大值 min_{x} max_{\lambda,\eta}=min_{x}(max f满足条件,max f不满足约束)\\=min_{x} max_{\lambda,\eta}{f满足条件}对偶问题: 关于$\lambda,\eta$的最大化问题 max min L(x,\lambda,\eta)\\ s.t \lambda_i>=0弱对偶问题：对偶问题&lt;=原问题 证明: $max_{x} min(\lambda \eta ) L&lt;=min_{\eta,\lambda } max_{x} L$ \underbrace{\min_{x}L(x,\lambda,\eta)}_{A(\lambda,\eta)}0\end{cases}注意，$y_i(w^Tx_i+b)&gt;0$,所以$\exists r&gt;0, min(y_i(w^Tx_i+b))=r$,可令$r=1$,这是对超平面范数的固定作用，因为$y=w^Tx+b$和$y=2w^T+2b$是同一个超平面，总能找到缩放$w,b$使得，可以将$r$缩放到1 \Longrightarrow\begin{cases} max \frac{1}{||w||}\\ st. y_i(w^Tx_i+b)>=1\end{cases}\Longrightarrow\begin{cases} \min \frac{1}{2}w^Tw\\ st. y_i(w^Tx_i+b)>=1\end{cases}这是一个土二次规划问题 第二宝 对偶利用lagrange乘子法得出对偶问题 带约束 \begin{cases} \min \frac{1}{2}w^Tw\\ st. y_i(w^Tx_i+b)-1>=0\end{cases}\Longrightarrow L(w,b,\lambda）=\frac{1}{2}w^Tw-\sum_{i=1}^{N}\lambda_i(1-y_i(w^Tx_i+b)无约束 \begin{cases}min_{w,b} max_{\lambda}L(w,b,\lambda) \\ s.t \lambda_i>=0\end{cases}此时关于$w,b$无约束的。 对$(L(w,b,\lambda))$ 对$w$,$b$求偏导 \frac{\partial L}{\partial w}=w+\sum_{i=1}^{N}y_ix_i\lambda_i=0 \Longrightarrow w=-\sum_{i=1}^{N}y_ix_i\lambda_i\\ \frac{\partial L}{\partial b}=-\sum_{i=1}^{N}\lambda_iy_i=0带回$L(w,b,\lambda)$,可得对偶问题 \begin{cases} max_{\lambda}L(w,b,\lambda ) =-\frac{1}{2}\sum_i^N\sum_j^N\lambda_i \lambda_jy_iy_jx_i^Tx_j +\sum_i^N\lambda_i \\ s .t. \sum_{i=1}^N\lambda_iy_i,\lambda_i>=0\end{cases} \Longrightarrow\\\begin{cases} min_{\lambda}L(w,b,\lambda ) =\frac{1}{2}\sum_i^N\sum_j^N\lambda_i \lambda_jy_iy_jx_i^Tx_j -\sum_i^N\lambda_i \\ s .t. \sum_{i=1}^N\lambda_iy_i,\lambda_i>=0\end{cases}原问题和对偶问题有相同解的充要条件满足 KKT \begin{cases} \frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0,\frac{\partial L}{\partial \lambda}=0\\ \lambda_i(y_i(w^Tx_i+b)-1)=0\\ \lambda_i>=0\\ y_i(w^Tx_i+b)-1>=0 \end{cases}如果存在$(x_k,y_k)=+1or -1$使得$y_i(w^Tx_i+b)-1=0$即可求解$b=y_k-\sum_{i=0}^{N}\lambda_ix_i^Tx_k$ 代入模型 f(x)=sign(\sum_i^Na_iy_ix_i^Tx+y_k-\sum_{i=0}^{N}\lambda_ix_i^Tx_k)注意，对于任意的训练样本，总有$\lambda_i=0$或者$y_if(x_i)=1$,如果$\lambda_i&gt;0$,说明样本点落在最大间隔的边界上，这些点就是支持向量，这条边界$w^Tx+b=1or-1$ soft-marign 软间隔 想法：允许一部分样本可以不被正确分类 优化目标 \min_{w,b} \frac{1}{2}w^Tw+loss一些损失函数 0-1损失 个数 loss=\sum_{i=1}^NI\{y_i(w^Tx+b)=0,\\ 1-y_i(w^tx_i+b), y_i(w^Tx_i+b)=1-\xi_i\\ \xi_i>=0 \end{cases} 指数损失（exponential loss ) l_{exp}(z)=exp(-z) 对率损失logistic loss l_{log}(z)=log(1+exp(-z)） 核方法核函数的定义设 $\chi$为输入空间（Input Space）， $\mathrm{H}$为特征空间(Feature Space,一定是希尔伯特空间），存在一个映射 \varphi : \chi \rightarrow \mathrm{H}对任意的 $x, y \in \mathrm{X}$，函数 $K(x, y)$，满足 K(x, y)=则称 $K(x, y)$为核函数。可以看出，我们并不需要知道输入空间和特征空间满足的映射关系 ，只需要知道核函数就可以算出，输入空间中任意两点映射到特征空间的内积。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归树]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%9B%9E%E5%BD%92%E6%A0%91%2F</url>
    <content type="text"><![CDATA[[TOC] 分类树与回归树分类树用于分类问题。分类决策树在选取划分点，用信息熵、信息增益、或者信息增益率、或者基尼系数为标准。Classification tree analysis is when the predicted outcome is the class to which the data belongs. 回归决策树用于处理输出为连续型的数据。回归决策树在选取划分点，就希望划分的两个分支的误差越小越好。 Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient’s length of stay in a hospital)。 回归树英文名字：Regression Tree 原理介绍决策树最直观的理解其实就是，输入特征空间($R^n$)，然后对特征空间做划分，每一个划分属于同一类或者对于一个输出的预测值。那么这个算法需要解决的问题是1. 如何决策边界(划分点)？2. 尽可能少的比较次数(决策树的形状) 如上图，每一个非叶子对于某个特征的划分。 最小二乘回归树生成算法Q1: 选择划分点？遍历所有的特征($n$),对于每一个特征对应$s_i$个取值，尝试完所有特征，以及特征所以有划分，选择使得损失函数最小的那组特征以及特征的划分取值。 Q2: 叶节点的输出？取每个区域所以结果的平均数作为输出 节点的损失函数的形式 \min _{j, s}\left[\min _{c_{1}} Loss(y_i,c_1)+\min _{c_{2}} Loss(y_i,c_2)\right]节点有两条分支，$c1$是左节点的平均值，$c2$是右节点的平均值，换句话说，分一次划分都是使得划分出的两个分支的误差和最小。最终得到函数是分段函数 CART算法输入： 训练数据集 输出：回归树$f(x)$ 选择最优的特征$j$和分切点$s$ \min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right] 对于选定的$(j,s)$划分区域，并确定该区域的预测值 对两个区域递归1. 2. 直到满足停止条件 返回生成树 注：分切点选择：先排序，二分。 Python代码节点类属性：左右节点、loss、特征编号或者特征、分割点 12345678class Node(object): def __init__(self, score=None): # 构造函数 self.score = score self.left = None self.right = None self.feature = None self.split = None 回归树类构造方法 1234class RegressionTree(object): def __init__(self): self.root = Node() self.height = 0 给定特征、划分点，返回计算MAPE 12345678910111213141516def _get_split_mse(self, X, y, idx, feature, split): ''' X:训练样本输入 y:训练样本输出 idx:该分支对应的样本编号 feaure: 特征 split: 划分点 ''' split_x1=X[X[idex,feature]&lt;split] split_y1=y[X[idex,feature]&lt;split] split_x2=X[X[idex,feature]&gt;=split] split_y2=y[X[idex,feature]&gt;=split] split_avg = [np.mean(split_y1), np.mean(split_y2)] split_mape = [np.sum((split_y1-split_avg[0])**2),np.sum((split_y2-split_avg[1])**2)] return split_mse, split, split_avg 计算给定特征的最佳分割点 遍历特征某一列的所有的不重复的点，找出MAPE最小的点作为最佳分割点。如果特征中没有不重复的元素则返回None。 12345678910def _choose_split_point(self, X, y, idx, feature): feature_x = X[idx,feature] uniques = np.unique(feature_x) if len(uniques)==1: return Noe mape, split, split_avg = min( (self._get_split_mse(X, y, idx, feature, split) for split in unique[1:]), key=lambda x: x[0]) return mape, feature, split, split_avg 选择特征遍历全部特征，计算mape,然后确定特征和对应的切割点，注意如果某个特征的值是一样的，则返回None12345678910111213141516171819def _choose_feature(self, X, y, idx): m = len(X[0]) split_rets = [x for x in map(lambda x: self._choose_split_point( X, y, idx, x), range(m)) if x is not None] if split_rets == []: return None _, feature, split, split_avg = min( split_rets, key=lambda x: x[0]) idx_split = [[], []] while idx: i = idx.pop() xi = X[i][feature] if xi &lt; split: idx_split[0].append(i) else: idx_split[1].append(i) return feature, split, split_avg, idx_split 对应叶子节点，打印相关的信息1234def _expr2literal(self, expr): feature, op, split = expr op = "&gt;=" if op == 1 else "&lt;" return "Feature%d %s %.4f" % (feature, op, split) 建立好二叉树以后，遍历操作12345678910111213141516171819def _get_rules(self): que = [[self.root, []]] self.rules = [] while que: nd, exprs = que.pop(0) if not(nd.left or nd.right): literals = list(map(self._expr2literal, exprs)) self.rules.append([literals, nd.score]) if nd.left: rule_left = [] rule_left.append([nd.feature, -1, nd.split]) que.append([nd.left, rule_left]) if nd.right: rule_right =[] rule_right.append([nd.feature, 1, nd.split]) que.append([nd.right, rule_right]) 建立二叉树的过程，也就是训练的过程 控制深度 控制节叶子节点的最少样本数量 至少有一个特征是不重复的12345678910111213141516171819202122232425def fit(self, X, y, max_depth=5, min_samples_split=2): self.root = Node() que = [[0, self.root, list(range(len(y)))]] while que: depth, nd, idx = que.pop(0) if depth == max_depth: break if len(idx) &lt; min_samples_split or set(map(lambda i: y[i,0], idx)) == 1: continue feature_rets = self._choose_feature(X, y, idx) if feature_rets is None: continue nd.feature, nd.split, split_avg, idx_split = feature_rets nd.left = Node(split_avg[0]) nd.right = Node(split_avg[1]) que.append([depth+1, nd.left, idx_split[0]]) que.append([depth+1, nd.right, idx_split[1]]) self.height = depth self._get_rules() 打印叶子节点12345def print_rules(self): for i, rule in enumerate(self.rules): literals, score = rule print("Rule %d: " % i, ' | '.join( literals) + ' =&gt; split_hat %.4f' % score) 预测单样本 123456789101112def _predict(self, row): nd = self.root while nd.left and nd.right: if row[nd.feature] &lt; nd.split: nd = nd.left else: nd = nd.right return nd.score # 预测多条样本def predict(self, X): return [self._predict(Xi) for Xi in X] 1234567891011121314 def main(): print("Tesing the accuracy of RegressionTree...") X_train=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]]) y_train=np.array([[5.56 ],[5.7],[5.91],[6.4 ],[6.8],[7.05],[8.9],[8.7 ],[9 ],[9.05]]) reg = RegressionTree() print(reg) reg.fit(X=X_train, y=y_train, max_depth=3) reg.print_rules()main() 简单的例子训练数据 x 1 2 3 4 5 6 7 8 9 10 y 5.56 5.7 5.91 6.4 6.8 7.05 8.9 8.7 9 9.05 根据上表，只有一个特征$x$. 选择最优的特征$j$和分切点$s$ | 分切点(s) | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 6.5 | 7.5 | 8.5 | 9.5 || ————- | ——- | ——- | —— | —— | —— | —— | —— | ——- | ——- || $c_1$ | 5.56 | 5.63 | 5.72 | 5.89 | 6.07 | 6.24 | 6.62 | 6.88 | 7.11 || $c_2$ | 7.5 | 7.73 | 7.99 | 8.25 | 8.54 | 8.91 | 8.92 | 9.03 | 9.05 || loss | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 | 当分切点取$s=6.5$,损失最小$l(s=6.5)=1.93$,此时划分出两个分支，分别是$R_1=\{1,2,3,4,5,6\}$,$c_1=6.42$,$R_2=\{7,8,9,10\}$,$c_2=8.91$ a) 对R1继续划分 | x | 1 | 2 | 3 | 4 | 5 | 6 || —— | —— | —— | —— | —— | —— | —— || y | 5.56 | 5.7 | 5.91 | 6.4 | 6.8 | 7.05 | | 分切点(s) | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 || ————- | ——— | ——- | ——— | ——— | ——— || $c_1$ | 5.56 | 5.63 | 5.72 | 5.89 | 6.07 || $c_2$ | 6.37 | 6.54 | 6.75 | 6.93 | 7.05 || loss | 1.3087 | 0.754 | 0.2771 | 0.4368 | 1.0644 | 当分切点取$s=3.5$,损失函数$l(s=3.6)=0.2771$(假设此时满足停止条件）,此时得到两个分支，分别是$R_1=\{1,2,3\}$，$c_1=5.72$,$R_2={4,,5,6}$,$c_2=6.75$ b) 对R2继续划分 | x | 7 | 8 | 9 | 10 || —— | —— | —— | —— | —— || y | 8.9 | 8.7 | 9 | 9.05 | | 分切点(s) | 7.5 | 8.5 | 9.5 || ————- | ——— | ——— | ——— || $c_1$ | 8.9 | 8.8 | 8.87 || $c_2$ | 8.92 | 9.03 | 9.05 || loss | 0.0717 | 0.0213 | 0.0467 | 当分切点取$s=8.5$,损失函数$l(s=8,5)=0.0213$(假设此时满足停止条件）,此时得到两个分支，分别是$R_1=\{7,8\}$，$c_1=8.8$,$R_2=\{9,10\}$,$c_2=9.03$ 函数表达式 $$ \begin{equation} f(x)=\left\{ \begin{aligned} 5.72 &amp; &amp; x&lt;3.5\\ 6.7 5&amp; &amp;3.5&lt;=x&lt;6.5\\ 8.8&amp; &amp;6.5&lt;=x&lt;8.5\\ 9.03&amp; &amp;8.5&lt;=x&lt;10\\ \end{aligned} \right. \end{equation} $$ Python库1class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False) 12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-"""Created on Wed Mar 13 19:59:53 2019@author: 23230"""import numpy as npfrom sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as pltX=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]])y=np.array([[5.56 ],[5.7],[5.91],[6.4],[6.8],[7.05],[8.9],[8.7],[9 ],[9.05]])# Fit regression modelregr_1 = DecisionTreeRegressor(max_depth=2)regr_2 = DecisionTreeRegressor(max_depth=3)regr_3 = DecisionTreeRegressor(max_depth=4)regr_1.fit(X, y)regr_2.fit(X, y)regr_3.fit(X, y)X_test = np.copy(X)y_1 = regr_1.predict(X_test)y_2 = regr_2.predict(X_test)y_3 = regr_3.predict(X_test) # Plot the resultsplt.figure()plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data")plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2)plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=4", linewidth=2)plt.plot(X_test, y_3, color="r", label="max_depth=8", linewidth=2)plt.xlabel("data")plt.ylabel("target")plt.title("Decision Tree Regression")plt.legend()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>回归树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BP算法]]></title>
    <url>%2F2019%2F03%2F05%2FBP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[[TOC] 1. 需要的微积分知识1.1 导数对于一元函数，在导数存在的情况下，在某一点的导数，也就是该点的斜率。对于多元函数，对于某一点求导，则需要指明方向，两个特殊的方向，1. 偏导：在坐标轴方向的导数 2. 梯度的方向:总有一个方向是变化最快的。 1.2 求导的链式法则 $x \in R$, $z=g(f(x))$, $y=f(x)$ \frac{\partial z}{\partial x}=\frac{\partial z}{\partial y} \frac{\partial y}{\partial x} $ x \in R^m $, $f(x)$是$R^M$到$R^n$的映射，$g(f)$是$R^n$到R的映射 \frac{\partial g}{\partial x_i}=\sum_j^n \frac{\partial g}{\partial f_i} \frac{\partial f_i}{\partial x_i} 如果使用向量表示 \nabla_x^z=(\frac{\partial f}{\partial x})^T \nabla_y^z2. 梯度下降法2.1 梯度梯度其实本质也是一个向量，对于函数$f(X,y)$在$(W,y)$这一点的梯度 $(\frac{\partial f}{\partial X},\frac{\partial f}{\partial y})$梯度的几何意义：在该店变化增加最快的地方 2.2 梯度算法的解释图来自吴恩达的机器学习课程颜色偏红(A)的地方开始，根据梯度的负方向通过9次更新，达到了最小值(B)。现在给定一个点$A(\theta_0,\theta_1)$,干嘛呢，我们想从A到B点（最小值点),类似人类下山，需要知道往那个方向吧、走大多一步呢？方向：梯度的负方向 $ \delta=(\frac{\partial L}{\partial \theta_0},\frac{\partial L}{\partial \theta_1})$)步长：学习率（$\alpha$)因此，计算一次里目标更近了 $(\theta_0,\theta_1)=(\theta_0,\theta_1)-\alpha \dot (\delta)$在重复上两步，直到满意为止。 3.误差反向传播算法3.1 理论推导 3.1.1 符号说明上图是一个L层的神经网络，输入层为第一层，隐藏层：2至$L-1$层，输出层L 令 输入向量 $\vec{X}$ \vec{X} = (x_1,x_2,...,x_{m-1},x_m)输出向量 $\vec{Y}$ \vec{Y}=(y_1,y_2,...,y_{n-1},y_n)$$a 第j层隐藏层的输出向量 $\vec{h^{(j)}}$ $$\vec{h^{(j)}}=(h_1^{(j)},h_2^,...,h_{t-1}^{(j)},h_tj^{(j)})其中，$tj$:表示第j的隐藏层个数第$(l-1)$层的第i个神经元到第$l$层的第j个神经元的连接权重：$w_{ij}^{(l)}$，则第$(l-1)$层神经元到第$l$层神经元的连接权重矩阵 W^{(l)}=\left( \begin{matrix}w_{11}^{(l)}& \cdots & w_{1(tj)}\\ & \dots &\\ w_{s(l-1)}^{l}&\cdots&w_{s(l-1)s(l)}^{l} \end{matrix}\right)3.1.2 推导过程3.1.2.1 误差定义的误差函数,常见的衡量性指标见 戳我,这里选择的误差平方和最小第$i$个输出的误差,假设实际输出$(d(1),d(2),…,d(n))$：,一个输入样本对应的误差 E(i)=\frac{1}{2}\sum_{k=1}^n(y(i)-d(i))^2=\frac{1}{2}||y-d||^2所有训练样本($N$)的误差： E(i)=\frac{1}{2}\sum_{j=1}^{N}(\sum_{k=1}^n(y(i)-d(i))^2)=\frac{1}{2N}\sum_{j=1}^{N}(||y(i)-d(i)||^2)因此， E = \frac{1}{2N}\sum_{i=1}^N(||y(i)-d(i)||^2)其实，神经网络的输出是关于节点的复合函数。代价函数是关于$W$和$b$的函数。 3.1.2.2 正向传播输入层$\hat{X}$： X =(x_1,x_2,x_3,...,x_m)当有$N$个训练样本时，可用矩阵表示 X=\left( \begin{matrix} x_{11} &x_{12}&...&x_{1m}\\ x_{21} & x_{22}&...&x_{2m}\\ \vdots & \vdots&\dots&\vdots\\ x_{N1} & \vdots&\vdots&x_{Nm}\\ \end{matrix} \right)第二层 $h^{(2)}$,一共$s2$个节点:第i个节点的计算 h^{(2)}(i)=f(\sum_{j=1}^{s2}x(j)*w_{ji}^{(l)}+b_i)=f(x*w(:,i)+b_i)矩阵表示 h^{(2)}=f(x*W^{(l)}+b^{(2)})第i层 矩阵形式 h^{(l)}=f(h^{(l-1)}*W^{(l)}+b)3.1.2.3 反向传播梯度下降法更新权重，不断迭代到最优解。对$w_{ij}$求导数可得,可更新$w_{ij}$更新公式： w_{ij}=w_{ij}-\alpha \frac{\partial E}{\partial w_{ij}}当然简单的情况下，可直接写出公式，当太复杂的时候，引入BP简化求导 方便书写公式，对于第i的输入$h^{(i-1)}*W^{(i)}+b^{(i)}$记作$net^{(i)}$,其中，第$i$的输入和输出的关系，$输入=f(输出)$下面开始推导 首先，对于$L$层， 对于$W^{(L)}$，先看对$W_{ij}^{(L)}$求导， \frac{\partial E}{\partial W_{ij}^{(L)}} =\frac{\partial E}{\partial y(j)} * \frac{\partial y(i)}{\partial net_{j}^{L}} * \frac{\partial net_{j}^{L}}{\partial W_{ij}^{(L)}}\\ =(y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}h_i^{(L-1)}令$\delta_i^{(L)}=y(i)-d(i)$ 上述给出了单个分量的求偏导的结果，对于$W^{(L)}$ \frac{\partial E}{\partial W^{(L)}} =\left[\begin{matrix} \frac{\partial E}{\partial W_{11}^{(L)}} & \frac{\partial E}{\partial W_{12}^{(L)}}&\dots & \frac{\partial E}{\partial W_{1n}^{(L)}}\\ \frac{\partial E}{\partial W_{21}^{(L)}} & \frac{\partial E}{\partial W_{22}^{(L)}}&\dots& \frac{\partial E}{\partial W_{2n}^{(L)}}\\ \vdots& \dots& \dots& \dots\\ \frac{\partial E}{\partial W_{sL,1}^{(L)}} & \frac{\partial E}{\partial W_{sL,2}^{(L)}}&\dots& \frac{\partial E}{\partial W_{sL,n}^{(L)}} \end{matrix}\right] \\= \left[ \begin{matrix} h^{(L-1)}_1\\h^{(L-1)}_2\\ \dots\\h^{(L-1)}_n \end{matrix} \right] *\left[\begin{matrix} \delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\ \delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\ \dots\\ \delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}} \end{matrix}\right] ^T =h^{(L-1)}S^{(L)}其中， S^{(L)}=\left[\begin{matrix} \delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\ \delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\ \dots\\ \delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}} \end{matrix}\right]^T同理可得， \frac{\partial E}{\partial b_k^{(L)}}=(y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}其次，对于隐含层$L-1$层，对$W_{ij}^{(L)}$求导 \frac{\partial E}{\partial W_{ij}^{(L-1)}} =\sum_{k=1}^{n}\frac{\partial E}{\partial y(k)} * \frac{\partial y(k)}{\partial net_{k}^{L}} * \frac{\partial net_{k}^{L}}{\partial f(net_j^{(L-1)})}*\frac{\partial f(net_j^{(L-1)})}{\partial net_j^{(L-1)}}*\frac{\partial net_j^{(L-1)}}{\partial W_{ij}^{(L-1)}}\\ =\sum_{k=1}^{n} (y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}W_{kj}^{(L)}f(x)^{'}|_{x=net_j^{L-1}}h_i^{L-2}\\ =\sum_{k=1}^{n}S_i^{(L)}W_{kj}^{(L)}f(x)^{'}|_{x=net_j^{L-1}}h_i^{L-2}\\写出矩阵形式,对$W^{(L-1)}$ \frac{\partial E}{\partial W^{(L-1)}}=\left[\begin{matrix} h^{(L-2)}_1\\h^{(L-2)}_2\\\vdots\\h^{(L-2)}_{s(L-2)}\end{matrix}\right] \left[\begin{matrix} \delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\ \delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\ \dots\\ \delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}} \end{matrix}\right]^T \left[\begin{matrix} W_{11}^{(L)} & W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\ W_{21}^{(L)} & W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\ \vdots& \dots& \dots& \dots\\ W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)} \end{matrix}\right]^T \\ \left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\ 0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]\\ =h^{(L-2)}S^{(L-1)} S^{(L-1)}=\left(\left[\begin{matrix} f(x)^{'(L)}|_{x=net_1^{(L)}}&0& \dots& 0\\ 0&f(x)^{'}|_{x=net_2^{(L)}}0& \dots& 0\\ 0&\dots&\dots&0\\ 0&0&0&f(x)^{'(L)}|_{x=net_n^{(L)}} \end{matrix}\right]\left[\begin{matrix} \delta_1^{(L)}\\\delta_2^{(L)}\\\vdots\\\delta_n^{(L)}\end{matrix}\right] \right)^T\\ \left[\begin{matrix} W_{11}^{(L)} & W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\ W_{21}^{(L)} & W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\ \vdots& \dots& \dots& \dots\\ W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)}* \end{matrix}\right]^T \left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\ 0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]\\ =S^{(L)}\left[\begin{matrix} W_{11}^{(L)} & W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\ W_{21}^{(L)} & W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\ \vdots& \dots& \dots& \dots\\ W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)}* \end{matrix}\right]^T\left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\ 0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]*\\对$1&lt;l&lt;L$,求$W^{(l)}$的偏导, 最后，根据上述的推导喔，很容易得出$S^{(l)}$和$S^{(l+1)}$, S^{(l)}=S^{(l+1)}W^{(l+1)^T}F^{'(l)}(net^{(l)})\\ S^{(L)}=(Y-\hat{Y})F^{'(L)}(net^{(L)}) \frac{\partial E}{\part W^{(l)}}=\left[\begin{matrix}h^{(l-1)}_1\\h^{(l-1)}_2 \\\dots \\h^{(l-1)}_{sl}\end{matrix}\right]S^{(l+1)} \left[\begin{matrix}W_{11}^{(l+1)}&W_{12}^{(l+1)} &\dots& W_{2(sl+1)}^{(l+1)}\\ W_{21}^{(l+1)}&W_{22}^{(l+1)} &\dots& W_{2(sl+1)}^{(l+1)}\\ \dots&\dots&\dots&\dots\\ W_{sl1}^{(l+1)}&W_{sl2}^{(l+1)} &\dots& W_{sl(sl+1)}^{(l+1)}\\ \end{matrix} \right]^T\left[\begin{matrix} \part f^{'(l)}(net_1^{l})&0&\dots & 0\\ 0\\0 &\part f^{'(l)}(net_2^{l})&\dots&0\\ 0 & 0&\dots&0\\ 0&0&\dots&\part f^{'(l)}(net_l^{l})\end{matrix}\right]3.2 BP算法的小结算法分为两个阶段：前向阶段和后向传播阶段 后向阶段算法： Step 1: 计算$\hat{y}^{(L)}$ Step 2: for l =L:2 ​ 计算$S^{(l)}=S^{(l+1)}W^{(l+1)}F’(net^{(l)})$ ​ 计算 $\Delta W^{(l)}=h^{(l-1)}S^{(l)} $ ​ 计算$W^{(l)}=W^{(l)}-\delta \Delta W^{(l)}$ 3.3 Python实现3.3.1 最简单三层网络1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071'''不用任何框架，自己写一个三层的神经网络# input-3,hidden-4 output-1'''import numpy as npnp.random.seed(1)# Input MatrixX = np.array([[0, 0, 1], [0, 1, 1], [1, 0 ,1], [1, 1, 1],])# Output Matrixy = np.array([[0], [1], [1], [0]])# Nonlinear functiondef sigmoid(X,derive=False): if not derive: return 1 / (1 + np.exp(-X)) else: return X*(1-X)# reludef relu(X,derive = False): if not derive: return np.maximum(0,X) else: return (X&gt;0).astype(float) # Weight biasW1 = 2 * np.random.random((3, 4))-1b1 = 0.1 * np.ones((4,)) W2 = 2 * np.random.random((4,1))-1b2 = 0.1 * np.ones((1,)) rate = 0.1noline = relu# Trainingtrain_times = 200 for time in range(train_times): # Layer one A1 = np.dot(X,W1)+b1 Z1 = noline(A1) # Layer two A2 = np.dot(Z1, W2)+b2 Z2 = noline(A2) cost = -y+Z2 # Calc deltas S2= cost*noline(A2,True) delta_W2 = np.dot(Z1.T,S2) bias2 = S2.sum(axis=0) S1 = np.dot(S2, W2.T)*noline(A1,True) delta_W1= np.dot(X.T, S1) bias1 = S1.sum(axis=0) # update W1 = W1-rate*delta_W1 b1 = b1-rate*bias1 W2 = W2-rate*delta_W2 b2 = b2-rate*bias2 print('error',np.mean(((y-Z2)*(y-Z2))**2))print("prediction",Z2) 3.4 附录： Name Abbreviation Mean absolute percentage error MAPE Root mean squares percentage error RMSPE Mean absolute percentage error MAE Mean squares error MSE Index of agreement IA Theil U statistic 1 U1 Theil U statistic 2 U2 Correlation coefficient R MAPE = $\frac{1}{n} \sum_{k=1}^{n}\left|\frac{x^{(0)}(k)-\hat{x}^{(0)}(k)}{x^{(0)}(k)}\right| \times 100$RMSPE = $\sqrt{\frac{1}{n} \sum_{k=1}^{n}\left(\frac{\hat{x}^{(0)}(k)-x^{(0)}(k)}{x^{(0)}(k)}\right)^{2}} \times 100$MAE = $\frac{1}{n} \sum_{k=1}^{n}\left|\hat{x}^{(0)}(k)-x^{(0)}(k)\right|$MSE = $\frac{1}{n} \sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}$IA = $1-\frac{\sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}}{\sum_{k=1}^{n} \left( \left| \hat{x}^{(0)}(k)-\overline{x} \right|+\left| x^{(0)}(k)-\overline{x}\right| \right)^{2}}$U1 = $\frac{\sqrt{\frac{1}{n} \sum_{k=1}^{n}\left(x^{(0)}(k)-x^{(0)}(k)\right)^{2}}}{\sqrt{\frac{1}{n} \sum_{k=1}^{n} x^{(0)}(k)^{2}}+\sqrt{\frac{1}{n} \sum_{k=1}^{n} x^{(0)}(k)^{2}}}$U2 = $\frac{\left[\sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}\right]^{1 / 2}}{\left[\sum_{k=1}^{n} x^{(0)}(k)^{2}\right]^{1 / 2}}$R = $\frac{\operatorname{Cov}(\hat{x}^{(0)}, x^{(0)})}{\sqrt{\operatorname{Var}[\hat{x}^{(0)}] \operatorname{Var}[x^{(0)}]}}$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2019%2F03%2F03%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[主要是分享决策的基本知识点，重点在分类决策树上，对于回归的决策树后面在给出。希望大家和我一起做知识的传播者啦！:smile: :smiley: :grin: :open_mouth: [TOC] 决策树英文名字：Descision Tree 什么是决策树举个校园相亲的例子，今天校园的小猫(女)和小狗(男)准备配对，小猫如何才能在众多的优质🐶的心仪的狗呢？于是呢？有一只特乖巧的小猫找到了你，你正在学习机器学习，刚好学习了决策树，准备给这只猫猫挑选优质狗，当然，你不仅仅是直接告诉猫哪些狗是合适你的？你更应该详细的给猫讲解决策树是如何根据它提出的标准选出的符合要求的狗呢？猫给出如下信息：年龄=0.5 6.5&lt;=体重&lt;=8.5;心仪; 年龄&gt;=0.5 体重&gt;8.5 长相好 心仪;其余情况不心仪; 根据上述条件可以构造一颗树：上面的图就是决策树，最终的结果是心仪或者不心仪。决策树算法以树形结构表示数据分类的结果 基本概念决策树属于也只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。根节点：决策树具有数据结构里面的二叉树、树的全部属性非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试叶子节点 ：分类后获得分类标记分支： 测试的结果 数学问题-熵-Gini系数什么是熵：熵的概念源于物理学，用于度量一个热力学系统的无序程度。信息熵：不得不提香农这个大写的人啦！信息论里面的知识。在信息论里面，信息熵衡量信息量的大小，也就是对随机变量不确定度的一个衡量。熵越大，不确定性越大；对于某个单符号无记忆信源，发出符号($x_i$)的概率是$p_i$,概率越大，符号的信息量就越小，香农公式 $I(x_i)=-log_{p_i}$。信源所含的信息熵就是信息量的期望]$H(x)=-\sum p_i*log_{p_i}$Gini系数： $Gimi(p) = 1-\sum_{k=1}^{K}p_k^2$ 决策树如何构建的问题自我提问阶段： 每个节点的位置如何确定？特征的选择：每次选入的特征作为分裂的标准，都是使得决策树在这个节点的根据你自己选择的标准（信息熵最小、信息增益最大、gini系数最小）. 每个节点在哪个值上做划分，确定分支结构呢？遍历划分的节点的分界值操作来解决这个问题 可以想象，我们构造的决策树足够庞大，决策树可以把每一个样本都分对，那么决策树的泛化能力就可以很差了为了解决这个问题，就需要剪枝操作了 训练算法基于信息熵的构造当选择某个特征作为节点时，我们就希望这个特征的信息熵越小越好，那么不确定性越小。计算特征的信息熵公式如下： H(x) = -p_i(x)log^{p_i(x)} = -\frac{n_j}{S}log^{\frac{n_j}{S}}$n_j$: 第j个类别，在样本中出现的频数$S$: 样本个数对于离散属性，直接计算信息熵，连续属性，就需要划分区间，按区间计算信息熵。 基于某一层的数据集 a. 遍历计算所有属性，遍历相应属性以不同值为分截点的信息熵 b. 选择信息熵最小的作为节点 如果到达终止条件，返回相应信息，否则，按照分支重复步骤1ID3算法： 信息增益最大化C:类别H(C)=-\sum_{i=1}^{m}p_i log _2^{p_i}按照D组划分CH(C/D)=\sum_{i=1}^{v}\frac{|C_i|}{|C|}H(C_i)信息增益gain(D) = gain(C)-H(C/D)这里我就以网上给出的数据为例，给出根据信息熵构成决策树的计算过程。 确定特征，统计属性值和分解结果，总共四个特征，四种特征的统计结果如下图： 根据历史数据，在不知到任何情况下，计算数据本身的熵为 - \frac{9}{14}log_2 \frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940 计算每个特征做为节点的信息熵以天气为例，天气三种属性，当Outlook = sunny时，H(x) = $-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5}$; 当Outlook= overcast,$H(x)=0$,当Outlook = rainy ,$H(x) = 0.971$所以，当选天气作为节点时，此时$H(x)=\frac{5}{14}0.971+\frac{4}{14}0+\frac{5}{14}*0.971 = 0.693$,gain(天气) = 0.247同理，可得gain(温度) =0.029 gain(湿度)=0.152，gain(风)=0.048因此选择天气节点，在递归实现其他节点的选择。信息增益的方法偏向选择具有大量值的属性，也就是说某个属性特征索取的不同值越多，那么越有可能作为分裂属性，这样是不合理的； C4.5: 信息增益率如果这里考虑了一列ID,每个ID出现一次，所以算出的信息增益大。$ H(x) = 0$,信息增益最大化了，可以引入信息增益率 C(T) = \frac{信息增益}{H(T)} =\frac{H(C)-H(C/T)}{H(T)}CART:基尼(Gini)系数G = 1-\sum_{i=l_k}^{k}p_i^2$$,也是对随机变量不确定性的一个衡量，gini越大，不确定性越大 ### 连续属性的处理方法 选取分解点的问题： 分成不同的区间（二分、三分....)，分别计算增益值，然后比较选择。 将需要处理的样本（对应根节点）或样本子集（对应子树）按照连续变量的大小从小到大进行排序 假设该属性对应不同的属性值共N个，那么总共有N-1个可能的候选分割值点，每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点 ## 评价 评价函数： $$C(T) = \sum_{releaf} N_t*H(T)$ N_t$：每个叶子节点里面含有的样本个数$H(T)$:叶子节点含有的信息熵 过拟合如果决策树过于庞大，分支太多，可能造成过拟合。对应训练样本都尽可能的分对，也许样本本身就存在异常点呢？I. 预剪枝：边构建，边剪枝 指定深度d 节点的min_sample 节点熵值或者gini值小于阙值熵和基尼值的大小表示数据的复杂程度，当熵或者基尼值过小时，表示数据的纯度比较大，如果熵或者基尼值小于一定程度数，节点停止分裂。 当所以特征都用完了 指定节点个数当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。 II. 后剪枝： 构建好后，然后才开始裁剪 C_\alpha(T) = C(T)+\alpha|T_{leaf}|在构造含一棵树后，选一些节点做计算，看是否需要剪枝 决策树单个节点选择的代码实现简单实现了单个节点决策构造过程12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182def split(X,y,d,value):'''在d纬度上，按照value进行划分''' index_a =(X[:,d]&lt;=value) index_b =(X[:,d]&gt;value) return X[index_a],X[index_b],y[index_a],y[index_b]from collections import Counterfrom math import log from numpy as npdef entropy(y): counter = Counter(y) # 字典 res = 0.0 for num in counter.values(): p = num/len(y) res+=-p*log(p) return resdef gain(X,y,d,v): X_l,X_r,y_l,y_r = split(X,y,d,v) e = len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r) return (entropy(y)-e)def gainratio(X,y,d,v): X_l,X_r,y_l,y_r = split(X,y,d,v) gain =entropy(y) - len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r) return gain/(entropy(y_l)+entropy(y_r))def gini(y): counter = Counter(y) res = 1.0 for num in counter.values(): p = num / len(y) res += -p**2 return res #X_l,X_r,y_l,y_r = split(X,y,d,v) #return 1-(len(y_l)/len(y))**2-(len(y_r)/len(y))**2def try_split(X,y): best_entropy = float('inf') best_d,best_v=-1,-1 for d in range(X.shape[1]): sorted_index = np.argsort(X[:,d]) for i in range(1, len(X)): if (X[sorted_index[i],d] != X[sorted_index[i-1],d]): v = (X[sorted_index[i-1],d]+X[sorted_index[i],d])/2 X_l,X_r,y_l,y_r = split(X,y,d,v) # 信息熵 e = entropy(y_l)+entropy(y_r) #gini e = gini(y_l) + gini(y_r) # 信息增益 e = -gain(X,y,d,v) if e &lt; best_entropy: best_entropy, best_d,best_v = e,d,v return best_entropy, best_d, best_v# 手动来划分data =np.array([[ 0.3 , 5 , 2 , 0 ],[ 0.4 , 6 , 0 , 0 ],[ 0.5 , 6.5 , 1 , 1 ],[ 0.6 , 6 , 0 , 0 ],[ 0.7 , 9 , 2 , 1 ],[ 0.5 , 7 , 1 , 0 ],[ 0.4 , 6 , 0 , 0 ],[ 0.6 , 8.5 , 0 , 1 ],[ 0.3 , 5.5 , 2 , 0 ],[ 0.9 , 10 , 0 , 1 ],[ 1 , 12 , 1 , 0 ],[ 0.6 , 9 , 1 , 0 ],])X =data[:,0:3]y = data[:,-1]# 手动来划分best_entropy, best_d, best_v = try_split(X, y)print(best_entropy, best_d, best_v)X1_l, X1_r, y1_l, y1_r = split(X,y,best_d,best_v)print(X1_l, X1_r, y1_l, y1_r)best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)X2_l, X2_r, y2_l, y2_r = split(X1_r,y1_r,best_d2,best_v2)entropy(y2_l) Python sklean里面tree模块里面的DecisionTreeClassifier1234from sklearn import treeclf =tree.DecisionTreeClassifier(max_depth=1,criterion ='gini') # criterion='entropy|gini'clf = clf.fit(X,y) 训练好一颗决策树之后，我们可以使用export_graphviz导出器以Graphviz格式导出树。1234import graphviz dot_data = tree.export_graphviz(clf, out_file=None,) graph = graphviz.Source(dot_data) graph.render("data") 在运行时可以出错：ExecutableNotFound: failed to execute [‘dot’, ‘-Tpdf’, ‘-O’, ‘data’], make sure the Graphviz executables are on your systems’ PATH原因：graphviz本身是一个软件，需要额外下载，并将其bin加入环境变量之中。下载]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的读书笔记]]></title>
    <url>%2F2019%2F02%2F28%2FSVD%2F</url>
    <content type="text"><![CDATA[目录 :smile::one: 简单说一下特征值、特征向量与特征分解&nbsp;&nbsp; I. 特征值、特征向量与特征分解&nbsp;&nbsp; II. 几何意义&nbsp;&nbsp; III. 如何实现通过Matlab、Python实现:two:详细解说SVD&nbsp;&nbsp; I. 几何意义&nbsp;&nbsp; I. 奇异值分解的推导过程&nbsp;&nbsp; I. SVD算例&nbsp;&nbsp; I. 如何通过Matlab和Python:three:应用举例&nbsp;&nbsp; I. 特征值、特征向量与特征分解:four:特征分解、奇异值分解的区别&nbsp;&nbsp; I. 特征分解、奇异值分解的区别 简单说一下特征值、特征向量与特征分解 特征值、特征向量与特征分解Theory:对于一个正阵$M$，满足如下： Mx=\lambda x其中$\lambda$被成为特征值，满足$||M-\lambda E||=0$再有$(M-\lambda E)x=0$，可计算其特征向量。如果有了特征值和特征向量后呢，则可以将矩阵$M$用特征分解： M=W\sum W^{-1}$W={w_1,w_2,…,w_n}$分别是特征值$\lambda_1,\lambda_2,…,\lambda_n$对应的特征向量构成的方阵 几何意义 对应矩阵M,其对应的线性变化 Mx = x'上面这个式子，$Mx，x’$是一个向量，$x,x’$可能是不共线的(如图(b))，如果向量$Mx,x’$满足$Mx=x’=\lambda x$,则如图(b)，这说明了这个变换就是对向量x做一个拉伸或者压缩。 如何实现通过Matlab、Python实现数学推导： Mx = \lambda xMx-\lambda x=(M-\lambda E)x=0齐次线性方程组有非零解，则$||M-\lambda E||=0$可求得特征向量再带回，可得特征向量。Matlab:123d = eig(M) % 求取矩阵M的特征值，向量形式存储[V,D] = eig(M) % 计算M的特征值对角阵D和特征向量V，使得MV = VD成立[V,D] = eig(M,'nobalance') %当矩阵M中有与截断误差数量级相差不远的值时，该指令可能更精确。'nobalance'起误差调节作用 Pythonnumpy科学计算库提供相应的方法1234import numpy as npx = np.diag((1,2,3)) # 这是你想要求取特征值的数组a,b = numpy.linalg.elg(x) # 特征值赋值给a,对应的特征向量赋值给b 详细解说SVDSVD的英文全称： Singular Value Decomposition，中文名字：奇异值分解 几何意义图来源以二维空间为例几何意义就是把一个单位正交的网格，转换为另外一个单位正交的网格 假如选取了一组单位正交基{$\vec{v}_1$,$\vec{v}_2$},刚好矩阵$M$的线性变化$M\vec{v}_1 $,$M\vec{v}_2 $ 也正交，用$\vec{u}_1,\vec{u}_2 $分别表示$M\vec{v}_1 $,$M\vec{v}_2 $ 的单位向量，用$\lambda_1,\lambda_2 $表示$M\vec{v}_1 $,$M\vec{v}_2$的长度，描述网格在这些特定方向上的拉伸量，也被称作矩阵M的奇异值。$M\vec{v}_1 =\lambda_1\vec{u}_1 $$M\vec{v}_2 =\lambda_2\vec{u}_2 $对任意给定的向量 $\vec{x}$ ,则有 \mathbf{x}=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) \mathbf{v}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) \mathbf{v}_{2} 再将M的线性变换 \begin{aligned} M \mathbf{x} &=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) M \mathbf{N}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) M \mathbf{v}_{2} \\ M \mathbf{x} &=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) \sigma_{1} \mathbf{u}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) \sigma_{2} \mathbf{u}_{2} \end{aligned} \begin{array}{c}{M \mathbf{x}=\mathbf{u}_{1} \sigma_{1} \mathbf{v}_{1}^{\top} \mathbf{x}+\mathbf{u}_{2} \sigma_{2} \mathbf{v}_{2}^{\top} \mathbf{x}} \\ {M=\mathbf{u}_{1} \sigma_{1} \mathbf{v}_{1}^{\top}+\mathbf{u}_{2} \sigma_{2} \mathbf{v}_{2}^{\top}}\end{array} so M=U \Sigma V^{T}奇异值分解的推导过程$u=(u_1,u_2,…,u_m)$$v=(v_1,v_2,…,v_n)$$u,v$都是空间的基,是正交矩阵 $u^Tu=E,v^Tv = E$任何一个矩阵$M_{m*n}$，$rank(M)=k$，一定存在ＳＶＤ,换句话说，M可以将一组单位正交基映射到另一组单位正交基。答案是肯定的证明如下：在n为空间中，有一组单位正交基{$\vec{v}_1,\vec{v}_2,…,\vec{v}_n$},线性变化作用以后 {M\vec{v}_1,M\vec{v}_2,...,M\vec{v}_n}也是正交的，则有 (M\vec{v}_i,M\vec{v}_j) = (M\vec{x}_i)^TM\vec{v}_j=\vec{v}_i^TM^TM\vec{v}_j=0注意喔，$M^TM$是矩阵喔，则会有$M^TM\vec{v}_j=\lambda \vec{v}_j$接下去， \begin{aligned} v_{i}^{T} M^{T} \mathrm{M} v_{j}=& v_{i}^{T} \lambda_{j} v_{j} \\ &=\lambda_{j} v_{i}^{T} v_{j} \\ &=\lambda_{j} v_{i}\dot v_{j}=0 \end{aligned} 上述就证明了是有的：任何一个矩阵，都可以将一组单位正交基转换成另外一组正交基。 当$i=j$,$=\lambda_i \vec{v}_i \vec{v}_i=\lambda_i$ 进行一些单位化，记$u_i=\frac{A\vec{v}_i}{|M\vec{v}_i|}=\frac{1}{\sqrt{\lambda_i}}M\vec{v}_i$则 A v_{i}=\sigma_{i} u_{i}, \sigma_{i}(\operatorname{奇异值})=\sqrt{\lambda_{i}}, 0 \leq i \leq \mathrm{k}, \mathrm{k}=\operatorname{Rank}(\mathrm{A}) 当$k &lt; i &lt;= m$时，对$u1，u2，…，uk$进行扩展$u(k+1),…,um$，使得$u1，u2，…，um$为$m$维空间中的一组正交基.也可对$\vec{v}_1,\vec{v}_2,…,\vec{v}_k$进行扩展，扩展的$\vec{v}_{k+1},…,\vec{v}_{n}$存在零子空间里面。 M\left[ \begin{array}{lll}{\vec{v}_{1}} & {\cdots} & {\vec{v}_{k}}\end{array}\right| \vec{v}_{k+1} \quad \cdots \quad \vec{v}_{m} ]= \left[ \begin{array}{c}{\vec{u}_{1}^{T}} \\ {\vdots} \\ {\frac{\vec{u}_{k}^{T}}{\vec{u}_{k+1}}} \\ {\vdots} \\ {\vec{u}_{n}^{T}}\end{array}\right] \left[ \begin{array}{ccc|c}\sigma_{1} & & 0 & 0\\ & {\ddots} & \sigma_{k} & 0 \\ \hline 0 & & 0 &0\end{array}\right] M=\left[ \begin{array}{lll}{\vec{u}_{1}} & {\cdots} & {\vec{u}_{k}}\end{array}\right] \left [ \begin{array}{ccc}\sigma_{1} & & \\ & {\ddots} & \\ & & {\sigma_{k}}\end{array}\right] \left[ \begin{array}{c}{\vec{v}_{1}^{T}} \\ {\vdots} \\ {\vec{v}_{k}^{T}}\end{array}\right]+ \left[ \begin{array}{ccc}{\vec{u}_{k+1}} & {\cdots} & {\vec{u}_{m}}\end{array}\right] \left[\begin{array}{c} 0 \end{array} \right] \left[ \begin{array}{c}{\vec{v}_{k+1}^{T}} \\ {\vdots} \\ {\vec{v}_{n}^{T}}\end{array}\right]SVD算例U：$AA^T$的特征值和特征向量，用单位化的特征向量构成 UV: $A^TA$ 的特征值和特征向量，用单位化的特征向量构成 V$\sum_{mn} $ :将$ AA^{T} $或者 A^{T}A 的特征值求平方根，然后构成 Σ以矩阵$A = \left[\begin{matrix} 1 &amp; 1\\1 &amp;1\\ 0 &amp;0\\\end{matrix} \right]$第一步 U ，下面是一种计算方法对矩阵 A A^{T}=\left[ \begin{array}{lll}{2} & {2} & {0} \\ {2} & {2} & {0} \\ {0} & {0} & {0}\end{array}\right] 特征分解， 特征是4，0，0 特征向量是 $\left[\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0\right]^{T},\left[-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0\right]^{T},[0,0,1]^{T}$,可得到 U=\left[ \begin{array}{ccc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} & {0} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}} & {0} \\ {0} & {0} & {1}\end{array}\right] 第二步 计算矩阵$A^TA$的特征分解，可得 特征值4，0， V=\left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}}\end{array}\right]第三步计算$\sum_{mn}$ \Sigma=\left[ \begin{array}{ll}{2} & {0} \\ {0} & {0} \\ {0} & {0}\end{array}\right] 最后， A=U \Sigma V^{T}=\left[ \begin{array}{ccc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} & {0} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}} & {0} \\ {0} & {0} & {1}\end{array}\right] \left[ \begin{array}{ll}{2} & {0} \\ {0} & {0} \\ {0} & {0}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}}\end{array}\right]^{T}=\left[ \begin{array}{cc}{1} & {1} \\ {1} & {1} \\ {0} & {0}\end{array}\right]如何通过Matlab和PythonMatlab：1234567891011s = svd(A)[U,S,V] = svd(A)[U,S,V] = svd(A,'econ')[U,S,V] = svd(A,0)input: A 矩阵output: s:奇异值，以列向量形式返回。奇异值是以降序顺序列出的非负实数 S： U:左奇异向量，以矩阵的列形式返回。 V:奇异值，以对角矩阵形式返回。S 的对角元素是以降序排列的非负奇异值。 右奇异向量，以矩阵的列形式返回。 Python123import numpy as npM = np.array([ [1,1,2],[0,0,1]])U,S,V = np.linalg.svd(M) 应用举例应用 2.1 信息检索 2.2 推荐系统 2.3 基于协同过滤的推荐系统 2.4 图像压缩 特征值分解和奇异值分解的区别 特征值分解只能是方阵，而奇异值分解是矩阵就可以 特征值分解只考虑了对矩阵缩放效果，奇异值分解对矩阵有选择、收缩、投影的效果]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python库]]></title>
    <url>%2F2019%2F02%2F24%2Fpython%E5%BA%93%2F</url>
    <content type="text"><![CDATA[开始接触Python是大二结束的时候，到现在都快两年了，其实一直并不是很细节的学习，只是希望能够跑个结果。不过呢？，以后肯定是会经常用Python，所以呢？我接下来会认真学习Python Python 高级用法总结基本数据类型：整型、浮点型、布尔类型 容器： Containers容器是一种把多个元素组织在一起的数据结构，容器中的元素可以逐个地迭代获取，可以用in, not in关键字判断元素是否包含在容器中。通常这类数据结构把所有的元素存储在内存中（也有一些特例，并不是所有的元素都放在内存，比如迭代器和生成器对象）在Python中，常见的容器对象有：list, dequeset, frozensetsdict, defaultdict, OrderedDict, Countertuple, namedtuplestr list推导（list comprehensions)官方解释：列表解析式是Python内置的非常简单却强大的可以用来创建list的生成式。 1对于一个列表，既要遍历索引又要遍历元素。 123array = ['I', 'love', 'Python']for i, element in enumerate(array): array[i] = '%d: %s' % (i, seq[i]) 12345def getitem(index, element): return '%d: %s' % (index, element)array = ['I', 'love', 'Python']arrayIndex = [getitem(index, element) for index, element in enumerate(array)] 迭代器和生成器可迭代对象：凡是可以返回一个迭代器的对象都可称之为可迭代对象例如：list dic str set tuple range() enumerate(枚举) f=open()（文件句柄）123456789### 迭代器(iterator)是一个带状态的对象，他能在你调用next()方法的时候返回容器中的下一个值，任何实现了__iter__和__next__()（python2中实现next()）方法的对象都是迭代器，__iter__返回迭代器自身，__next__返回容器中的下一个值，如果容器中没有更多元素了，则抛出StopIteration异常### 生成器(generator)生成器其实是一种特殊的迭代器，不过这种迭代器更加优雅。它不需要再像上面的类一样写__iter__()和__next__()方法了，只需要一个yiled关键字。 生成器一定是迭代器（反之不成立）#列表生成式lis = [x*x for x in range(10)]# 受到内存限制，列表容量肯定是有限的#生成器表达式generator_ex = (x*x for x in range(10)) 生成器： 不用创建完整的list，为节省大量的空间，在Python中，这种一边循环一边计算的机制，称为生成器：generatorTuples:() 字典：{：，} Sets: {,}函数类 Python库——numpyWhatNumPy=Numerical+Python主要是提供了高性能多维数组这个对象，以及处理相关的方法 How 自定义一个（1D or MD)数组或者特殊的数组,一维，二维 数组切片（也就是提取数组元素），注意 a[:,0]和a[:,0:1]是不同的喔 关于数组属性的方法 数组运算 索引 where 函数 索引的布尔数组 广播（Broadcasting）用于处理不同性状的 数组。 Broadcasting提供了一种矢量化数组操作的方法，使得循环发生在C而不是Python。标量乘以一个矢量的时候，用Boradcasting更快，因为 broadcasting在乘法期间移动较少的内存 array 和 matrix 选择哪个? 戳我 矢量化和广播、索引在Python中循环数组或任何数据结构时，会涉及很多开销。 NumPy中的向量化操作将内部循环委托给高度优化的C和Fortran函数，从而实现更清晰，更快速的Python代码。stack|vstack|hstack 1234567891011121314151617181920212223242526272829303132a = np.array([1, 2, 3])b = np.array([2, 3, 4])np.stack((a, b))array([[1, 2, 3], [2, 3, 4]])% hstacka = np.array((1,2,3))b = np.array((2,3,4))np.hstack((a,b))array([1, 2, 3, 2, 3, 4])a = np.array([[1],[2],[3]])b = np.array([[2],[3],[4]])np.hstack((a,b))array([[1, 2], [2, 3], [3, 4]])% vstacka = np.array([1, 2, 3])b = np.array([2, 3, 4])np.vstack((a,b))array([[1, 2, 3], [2, 3, 4]])a = np.array([[1], [2], [3]])b = np.array([[2], [3], [4]])np.vstack((a,b))array([[1], [2], [3], [2], [3], [4]]) mean123456a = np.array([[1, 2], [3, 4]])np.mean(a)np.mean(a, axis=0)np.mean(a, axis=1) reshapereshape(x, y)，其中x表示转换后数组的行数，y表示转换后数组的列数。当x或者y为-1时，表示该元素随机分配，如reshape(2, -1)表示列数随机，行数为两行。 123456789格式：np.reshape((x, y, z))参数的含义：x：表示生成的三维数组中二维数组的个数y：表示单个二维数组中一维数组的个数z：表示三维数组的列数 numpy数组去掉冗余的维度——-squeeze()函数import numpy as np a = [[[10, 2, 3]]] a = np.array(a) a_sque = np.squeeze(a) print(a) print(a_sque) Python库——pandas记得学习pandas是在大三时候的美赛，花了一天多时间学习pandas，然后预处理数据，当时三个队友都是各自的家，是非常愉快的！！！ whatPython Data Analysis Library 三种数据结构序列： Series 1D数据帧： DataFrame 2D面板： Panel &gt;2D 自定义创建 可以通过字段、数据、series、列表 列表传入的时候，主要行列，如果单个列表：列；如果是[[],[]]是按行[] 如果位置不对可转置 创建空 pd.DataFrame() 选择区块 a) Series [] b) DataFrame 列选择 [‘colums的名字’] 行列选择：.loc[列名,行名]名称 .iloc[列索引,行索引]整数 array.value 统计描述 .descibe(include = ‘all’) .head() .tail() .select_dtype(include=[]) .columns .dtype 缺少数据 查看缺失值isnull() notnull() 也可以 做一些统计，sum, any,all 清理缺失值 dropna(axis=0)：axis = 0:index axis=1,columns 填充缺少指 fillna() 标量替换 替换 统计函数 Pandas 函数应用表合理函数应用：pipe()行或列函数应用：apply()元素函数应用：applymap()eg： pd.pipe(lambda x: x*100) 类别变量向量化非数值类型的处理方法 时间序列生成 data_range pandas.date_range(“11:00”, “21:30”, freq=”30min”) 参数1Return a fixed frequency DatetimeIndex. Parametersstartstr or datetime-like, optionalLeft bound for generating dates. endstr or datetime-like, optionalRight bound for generating dates. periodsint, optionalNumber of periods to generate. freqstr or DateOffset, default ‘D’Frequency strings can have multiples, e.g. ‘5H’. See here for a list of frequency aliases. tzstr or tzinfo, optionalTime zone name for returning localized DatetimeIndex, for example ‘Asia/Hong_Kong’. By default, the resulting DatetimeIndex is timezone-naive. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. namestr, default NoneName of the resulting DatetimeIndex. closed{None, ‘left’, ‘right’}, optionalMake the interval closed with respect to the given frequency to the ‘left’, ‘right’, or both sides (None, the default). **kwargsFor compatibility. Has no effect on the result. ReturnsrngDatetimeIndex12345678910111213141516171819202111. DataFrame.stackParameterslevelint, str, list, default -1Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels.dropnabool, default TrueWhether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section.ReturnsDataFrame or SeriesStacked dataframe or series.​```pythondf_single_level_cols weight heightcat 0 1dog 2 3df_single_level_cols.stack()cat weight 0 height 1dog weight 2 height DataFrame.value_connts()返回序列，index=统计值，值：统计个数 Matplotlibmatplotlib.pyplot as plt 窗口：figure: 一个窗口，plt.figure(num=,figsize=(h,w))下面数据都属于当前的figure,有一定的顺序喔 画图：plt.plot(x,y,color=,linewidth=,linestyle,label=) 标注信息： plt.xlim((,)), plt.yxlim((,)),plt.xlabel(),plt.ylabel(),ticks:图像的小标，plt.xticks(),plt.yticks([值1，值2],[r’$值1\ 对应的文字$’,r’值2的文字 \alpha]) 坐标轴：axis gac=’get current axis’ax = plt.gca() # 轴# 获取四个轴ax.spines[‘right|left|top|’].set_color(‘none’)ax.xaxis.set_ticks_position(‘bottom’)ax.spines[‘bottom’].set_position((‘data’,-1)) 图例：legend: a. plt.plot(,label=), plt.legend() b. l1, = plt.plot() plt.legend(handles=[l1,],labels=[,],loc=’best|upper right|’) 注解 annotationa. 点的位置(x0，y0) plt.scatter(). plt.plot([x0,y0],[y0,0],’k—‘,lw=)b . method 1:plt.annotate(r’name’,xy=(,)起始点，xycoords=’data’//基于xy,xytext=(+30,30),textcoords=’offseet points’//文本基于xy,arrowprops=dict(arrowstyle=’-&gt;’箭头,connectionstyle=’arc3,rad=.2’)弧度) Bar 柱状图plt.bar(x,+|-y,facecolor=””,edgecolor,)|# ha horizontal alignment 对齐方式for x,y in zip(x,y): plt.text(x+0,4,y+0.05,’%.2f’%y,ha=’center’,va=’bottom’) 很多自动 subplot(总行，当前行的列，总的按最小分的第几个)subplot(,,)index reset_index:限于DataFrame set_index index scikit-learn官方教程绝对是最好最棒的选择，有简单数学推导、直观立马就能上手的案例，还能提阅读英文的能力喔，实在是一举多得啊！！！！ scikit-learn.org regressionFeature selectionMethod from sklearn.feature_selection import VarianceThresholdsklearn.feature_selection.SelectFromModelclass sklearn.feature_selection.SelectFromModel(estimator, , threshold=None, prefit=False, norm_order=1, max_features=None) seabornseaborn.jointplot(x, y, data=None, kind=’scatter’, stat_func=None, color=None, height=6, ratio=5, space=0.2, dropna=True, xlim=None, ylim=None, joint_kws=None, marginal_kws=None, annot_kws=None, **kwargs) Parameters x, ystrings or vectorsData or names of variables in data.dataDataFrame, optionalDataFrame when x and y are variable names.kind{ “scatter” | “reg” | “resid” | “kde” | “hex” }, optionalKind of plot to draw.stat_funccallable or None, optionalDeprecated**colormatplotlib color, optionalColor used for the plot elements.heightnumeric, optionalSize of the figure (it will be square).rationumeric, optionalRatio of joint axes height to marginal axes height.spacenumeric, optionalSpace between the joint and marginal axesdropnabool, optionalIf True, remove observations that are missing from x and y.{x, y}limtwo-tuples, optionalAxis limits to set before plotting.{joint, marginal, annot}_kwsdicts, optionalAdditional keyword arguments for the plot components.kwargs**key, value pairingsAdditional keyword arguments are passed to the function used to draw the plot on the joint Axes, superseding items in the joint_kws dictionary. Returns gridJointGridJointGrid object with the plot on it. http://seaborn.pydata.org/generated/seaborn.JointGrid.html#seaborn.JointGrid g = sns.jointplot(x=”x”, y=”y”, kind = ‘reg’ , space=0,color = ‘g’, data=df11,stat_func=sci.pearsonr) sns.set() sns.axes_style(“darkgrid”) sns.set_context(“paper”) https://blog.mazhangjing.com/2018/03/29/learn_seaborn/ https://blog.csdn.net/weiyudang11/article/details/51549672 123456789101112131415#初始化类g=sns.JointGrid(x=&apos;v_ma5&apos;,y=&apos;price_change&apos;,data=stock,space=0.5,ratio=5)g=sns.JointGrid(x=&apos;v_ma5&apos;,y=&apos;price_change&apos;,data=stock,space=0.5,ratio=5)g=g.plot_joint(plt.scatter,color=&apos;.3&apos;,edgecolor=&apos;r&apos;)g=g.plot_marginals(sns.distplot,kde=False)from scipy import statsg=sns.JointGrid(x=&apos;v_ma5&apos;,y=&apos;price_change&apos;,data=stock,space=0.5,ratio=5)g=g.plot_joint(plt.scatter,color=&apos;.3&apos;,edgecolor=&apos;r&apos;)_=g.ax_marg_x.hist(stock.v_ma10,color=&apos;r&apos;,alpha=.6,bins=50)_=g.ax_marg_y.hist(stock.low,color=&apos;y&apos;,orientation=&quot;horizontal&quot;,bins=20)rquare=lambda a,b:stats.pearsonr(a,b)[0]**2g=g.annotate(rquare,template=&apos;&#123;stat&#125;:&#123;val:.2f&#125;&apos;,stat=&apos;$R^2$&apos;,loc=&apos;upper left&apos;,fontsize=12) 颜色和风格设置调色板主要使用以下几个函数设置颜色：color_palette() 能传入任何Matplotlib所有支持的颜色color_palette() 不写参数则默认颜色 current_palette = sns.color_palette() sns.palplot(current_palette) plt.show() set_palette() 设置所有图的颜色 sns.palplot(sns.color_palette(“hls”,8)) plt.show() 颜色的亮度及饱和度l-光度 lightnesss-饱和 saturation sns.palplot(sns.hls_palette(8,l=.7,s=.9)) plt.show() xkcd选取颜色xkcd包含了一套众包努力的针对随机RGB色的命名。产生了954个可以随时通过xkcd_rgb字典中调用的命名颜色 plt.plot([0,1],[0,1],sns.xkcd_rgb[‘pale red’],lw = 3) #lw = 线宽度plt.plot([0,1],[0,2],sns.xkcd_rgb[‘medium green’],lw = 3)plt.plot([0,1],[0,3],sns.xkcd_rgb[‘denim blue’],lw = 3)plt.show() 汇总http://seaborn.pydata.org/api.html# https://github.com/mwaskom/seaborn/blob/master/seaborn/rcmod.py https://xkcd.com/color/rgb/]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的读书笔记]]></title>
    <url>%2F2019%2F02%2F22%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[2019 第十五周三月份至2019.4.9这段时间，才发现我是如此没有自律的人，充分体现了我是人的特性，那就是我是群体动物，苦笑.jpg,苦笑.jpg, en, 最近突然想给自己打上厨娘的身份，如果可以每天花两个小时做饭就好了 愿你被世界温柔的相待 接触的东西越多，越深入，就会发现我是如此的菜，开始有些知识焦虑了，知识那么多~，可是我只有一个头脑啊~ 开始不想写一些特别低俗的博客了，一是觉得浪费时间，二是输出效果太差，引不起特别大的关注，虽然我写博客，完全是站在自己的角度，没有考虑读者的意愿，（滑稽.jpg)。 现在的自己，不是停留在基本的问题上，更应该去探索未知 的知识世界，虽然离这个flag可能还有几年的时间，能够给世界的知识创造一点点价值，哪怕只是一小点点。离这个目标还需要努力啊！！！！！ 我想我应该去记录学习知识的过程，突破更大的更困难的问题。 2019-第四周读书笔记 这周读了一本小说，是张爱玲的《倾城之恋》，原来和电视剧的何晟铭主演《倾城之恋》不是同一个事情啊！ 看了《阿甘正传》，“生活就像一盒巧克力，你永远不知道下一颗是什么味道。“这是阿甘对生活最好的诠释。小时候，有人骑着自行车羞辱他，他只会跑，拼命的跑，只会再公路上跑。长大后，别人骑着车想打他，阿甘还是跑，但是这次阿甘学会了网草坪上跑！就被大学看上，进入运动大学，还通过参加比赛赢得了冠军，然后，阿甘当兵了，再后来，打乒乓球很出色。阿甘似乎做什么都能成功，也许心无旁骛，最笨的方法+时间=收获。 我觉得很心酸的是，当珍妮告诉他有儿子时候，阿甘问，”他聪明吗“？ 2019第四周安排 改论文，改变自己的办事效率喔，拒绝重复工作 编程能力 慢慢的做事情，先慢后快， 生活、学习、交友、文采2019-第三周读书笔记这次读了《极简思维：颠覆传统思维模式的极简法则》作者：S.J斯科特 巴里.达文波特 我们生活充满了各种诱惑、杂乱信息、导致了生活的混乱，产生知识焦虑、年龄危机、人际关系的淡化。作者给我们介绍了许多问题、许多的解决方法，让我们这个信息爆炸的时代可以过的充实些。 每天睡8个小时、还剩下16个小时，在减去2个小时解决个人卫生和饮食，那么还有14个小时，一个星期98个小时。那么98个小时，你投入在哪里呢？ 总的来说，这本书传达的东西，我还是很喜欢的，极简主义者，少不得也多不得！！！！！！！！！！！！！！！ 读《拆掉思维里的墙》摘录 ：我们的生活也由三个支架组成：自我、家庭与团体和职业。这样的支架支撑着我们的灵魂，它在记录我们的生命。我们一直都在调整着三个位置的平稳，使之成为最稳固的联动三脚架。 这句大概是结合我的经历，最具有感悟的。因为一旦走出大学，这三者才开始真正的组成我们的生活。 古典老师，从职业、成功学、爱情、家庭等等不同的案例，给我分析了大多数人会面临的无形的”墙“，给了我们如何拆掉这些墙的方法。但是呢，对于古典老师的爱情观点，我并不是很赞同，因为呢，那些愿意陪你度过余生的人付出的感情，是如此的廉价吗？有的人既可以是白玫瑰，也可以红玫瑰啊！ 2019年第二周安T排每天两个小时阅读论文或者专业书籍的阅读开题报告修改和PPT制作（3h)《拆掉思维里面的墙》（3h)看哈利波特（一集） —-2018年的总结 小小的悔恨与遗憾 大三下，在课堂上，打了半学期的游戏 生活还是不规律，超喜欢深夜逛知乎、刷B站 额头上，不停的冒着痘痘啊 英语单词量在下降ing 运动量在降低喔 很讨厌洗衣服 相比于上一年进步的方面 愿意去承担更多的责任 更乐意去交流 越来越重视健康style 不会随意发泄自己的情绪了 更加认识到自身的优势与劣势了感到愉快的事情 知道自己想要什么，知道自己在做什么 整理完了大学期间所有的东西，往事不堪回首， 但也只能是柳暗花明又一村。 能读研究生了 聊聊2019年的点点期许学习上 多看19场知乎live 阅读10本书籍，书单也有了 在专业学习上，希望有所提升咯生活中 早睡早起身体好 看十部美剧，尽管我最大的兴趣是睡觉 时常更新歌单，不想在一年里面都是相同的旋律 静静静静静静静 合理安排 折星星 番茄闹钟 偶尔听听 TED技术 清理下了github 仓库 重新更新了 github page 多读、多写、多想]]></content>
      <categories>
        <category>读书日常</category>
      </categories>
      <tags>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习の历程]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%AD%A6%E4%B9%A0Daily%2F</url>
    <content type="text"><![CDATA[贴张伏魔咒 八月要准备考试了 现代密码理论（李发根老师授课）考试时间：2020-08-28，09:30-11:30 软件安全性分析（陈厅老师授课）考试时间：2020-09-03，09:30-11:30 20200728 挖掘内心需要的东西 今天早上，逛了知乎 今天下午，跑时间序列。 今天晚上，看python，规划自己的职业规划。（能不能像个成年人啊） 离 数据分析 + 机器学习 的目标越来越近（差不多是时候可以出去实习了） 编程。python各类库。 办公软件。 机器学习算法。 好好工作，健身，看剧，旅行，独居，美食，追剧，看书。我非常渴望的生活日常。我不喜欢工作压力太的环境，喜欢稍微轻松点的环境。 对不起啊，对不起啊，对不起啊。 20200727 放下许多不确定因素以后，就会轻松很多。 今天早上睡觉，刷视频 aaaaaaaa 今天下午，继续码字。 今天晚上，sklearn太好玩了呀！ 问题就在于你想太多，而读的书太少了！！！！！阅读的确给人很多启发，尤其是人物传记类。 帮别人填个志愿，还得到一个瓜瓜瓜！ 在这么追剧下去，爱奇艺都要看完了啊！ 2020-7-26 犹豫的自己，职场生涯！ 今天早上睡觉 今天下午数据分析 今天晚上时间序列。跑程序 热死人了！塞翁失马焉知非福！不过相对于把毕业设计提前做好了。也不是什么损失。啦啦啦啦啦啦啦啦啦啦啦！ 酸死了。我现在知道了。开学好好沟通一下，我的想法吧！ 2020-7-25 今天早上去外婆家吃饭。好吃呢 今天下午看了一下建模。建模果然很厉害啊！看了纪晓岚和和珅(太好看了) 今天晚上看回顾了数据分析 2020-7-24 今天早上学绘图。 今天下午跑时间序列 今天晚上总结绘图模板（seaborn ,matplotlib）看剧 弟弟太厉害了，文课考的太好了 2020-7-23 真正的英雄主义者是看清生活的真相，依然热爱生活。 今天早上睡觉，看剧。舒服 今天下午时间序列 今天晚上写分析报告。 2020-7-22 对不起，我不想在做孩子了 今天早上终于把量化结果弄好了，基本上可以说明很多问题。结果也不错。。。。至少是契合现实的 今天下午写分析报告，词穷。 今天晚上时间序列 今天外婆到我家里，莫名的悲伤了，想想，我能在和外公和外婆见面的次数和相处时间不多了。 2020-7-21 今天早上改代码 今天下午学习3绘图seaborn,不好用啊，字体和字号的设置，不对头。origin可能以后不能用了，matplotlib searborn 才是王道。绘图色彩搭配很重要 今天晚上学习了数据分析。我发现我太喜欢数据分析了，以后要从事这行了。 2020-7-20 今天早上睡觉 今天下午测试数据（感觉应该没什么问题了） 今天晚上跑了xgboost 2020 30周 最近十天，把八月份要干的事情干完，八月份要准备六级和期末考试了，报告和测试，我的个天，心塞啊，我选了什么课啊！全心全意准备考试。 Input ✔️❌ Output ❌❌✔️ 关键最近老是想睡觉。 论文不再是我的主旋律。小问题没什么意思，大问题做不出来。就是干什么方向不是问题。哪怕别人发了nature ,science 我根本无动于衷。 Input time series 数据分析技能 Output 每两天交接一下 增量和差值学习一下 学习人际交往的本质 2020-7-19 你如今的气质里，藏着你走过的路，读过的书，和爱过的人。 今天早上睡觉 下午跑时间序列 晚上处理数据（不显著啊）关键聚类标准稳健误差，加了之后。数据本身也不行。算了，算了 无争无忧！ 2020-7-18 你如今的气质里，藏着你走过的路，读过的书，和爱过的人。 今天早上睡觉，煮饭。 今天下午时间序列&amp;处理数据 今天晚上看《谁说我结不了婚》 《面朝大海，春暖花开》做个幸福的人、种菜、劈柴，周游世界！ 2020-7-17 今天早上，睡觉 今天下午，调研了很多东西。跑了时间序列，交接了工作，下面都好干了 今天晚上，清洗数据。感觉应该快了 2020-7-16 大头儿子和小头爸爸； 终有一天回头看今天，我才发现我研一2019.11-2020.07这几个月，读论文，入门某个专题的研究，才发现转了好大一个弯道了。想起有个人的科研经历，当你博士毕业的时候回头看，明明几个月，甚至更短就可以做完，结果花了很长很长的时间。这就是积累经验的过程。从不可能到可能。 耶耶全部搞定了，基准初步结果还OK，啦啦啦啦啦!!!!!!!! 问题是样本本身的问题了。 你如今的气质里，藏着你走过的路，读过的书，和爱过的人。 2020-7-15 今天早上处理数据。 今天下午搞时间序列分析。[]倒着索引 今天晚上看论文，对比分析了两篇同类的文章。 明天： ​ 发代码 ​ 把基准结果和学位聚集量化一下，希望出个好结果。慢慢摸索出一条道路。挺好的！ 2020-7-14 今天早上读了几页书籍，感悟颇多。条条大路通罗马，越来越值钱。扯下冠冕堂皇的理由，我之所以拼命，不就是为了富养我自己吗？自始至终都是我的奋斗目标。只要是正当的求生手段，靠自己的本事吃饭，我就干。 今天下午（午睡了很久）学了空间分析工具，工具还挺好用的哇。继续sklearn 今天晚上继续处理数据，跑的时候读了别人的文献，突然觉悟。科研的过程就是浪费时间的过程。 问题：我现在是沦为工具的应用者吗？我原先一直致力于成为这方面的创造者，没有成功。咳咳，现成为工具的应用者，解释一些现象也是十分好的。 2020-7-13 生存还是毁灭？这是个问题。究竟哪样更高贵！要更加厉害！ 今天早上刷视频，学英语！今天下午睡觉&amp;学软件&amp;跑程序 今天晚上读论文。虽然是北大的论文，但是总觉意义和价值感不高。我现在提的科研问题也是这个毛病。没有乐趣往下进行了。垃圾中的战斗机哇！ 明天：1. 时间序列调参，交接工作。2. 调研一下数据归一化的方法。3. 读论文 2020 29周 Input 尝试调研小领域（感觉下载的文献质量非常高） 学习写作 时间序列（机器学习算法） Output PPT 多记点专业词汇和句式（我就是照着别人写的，但是读起来不对劲，不通畅） 2020-7-12 当你知道自己的需求，也知晓别人的需求，所有的事情都十分好干了。如果出现了问题，一定是我做的不好。 今天找了几篇非常好的文献，可以做这方面的细调研，大概就10篇左右。 今天处理了数据，实在是麻烦。 突然发现文章最难的是文献综述。 Input ✔️❌ Output ❌❌✔️ 2020-7-11 看剧，跑程序 今天早上睡觉，看视频 今天下午和晚上，处理数据，啊麻烦 精英精神： 1. 《谁说我结不了婚》 利益：要不要做；风险：该不该博； 能力：该不该干；结果：划不划算；而不是别人告诉我：对不对。 2. 明确各自需求 2020-7-10 跑程序 今天早上跑程序（封装接口） 今天下午跑程序，学习写作的思维 今天晚上跑程序，看纪晓岚和和珅。 2020-7-9 科研写作好痛苦 今天早上听了听力，刷B站 今天下午学习了如何进行科研写作。太不容易了。《Some Tips on Writing》。原因如下：词汇缺乏；直译；毫无讲故事的思路；交代不清晰，含糊。更别提逻辑了，我觉得中文论文的逻辑关系都把握不好。 2020-7-8 你要自己摸爬打滚的度过艰难时期，你可以找人，但绝不能依靠任何一个人。我们是孤独的个体，更是无数关系连边中的一条。要学会和这个世界产生共鸣。 今天早上啃了一个🍎。 刷刷小红书，B站。 下午写了代码注释。封装成接口。规划了下一步方案。 晚上读了一篇Data Science的文章，数据好才能讲好故事啊。引力模型、线性回归模型。变量的构造是亮点。每天坚持写作（英文写作好难） 2020-7-7 当开启**，就像开启了闯关游戏，要一关一关的打怪升级，换先进装备。不成功便成仁。不成功便成仁。不成功便成仁。 今天早上又被刺激到了。 下午实现了xgboost, 数据-&gt;模型-&gt;调超参数 晚上收拾了论文，感觉就像俄罗斯方块，底层没有做好，越往上堆，就要回归底层。写了1000多点的小论文，也不知道每一个优秀的博士要经历什么才能创造出这么多的文字。值得学习。 释放光芒！ 2020-7-6 继续努力的一天 今天早听了英语访谈 今天下午复习了密码学第1-2章（1h); 把高铁的描述性统计做了，感觉结果还是不错！(用时较长) 2020 28周 2020-7-5 要做的事情还很多，我努力做我喜欢的一切，并不是为了赢别人，而是要自己满足。 今天又是睡得晚起的早的一天。a. 我把trfersh完全搞懂了。b. 谈整合资料的重要性，为什么我总是一个专题的资料，每次都要重复弄呢。不好不好不好。 这一周还是不错的，在精神上，每天睡足了10h，啊哈哈哈哈哈。时间序列基本上搞通了。读了几篇不错的论文，还把stata软件学会了，其实计量经济学无非就是要解决因果关系，遗漏变量，序列相关性，异方差性等等，几个问题，要解决是不容易的。sklearn用的不熟练 Input ✔️ Output ❌✔️ 这周主要把统计学里面的基础复习了，总感觉没找到我想要的那种深度。我也不知道我到底需要怎么样的深度。感觉自己又胖了！ Input 密码学复习两章（PPT+习题+百度） 网络安全复习两章（视频+笔记+概念） 写1000字左右的结课论文（关于大数据下孕育而生的计算社会经济学，抄袭抄袭，借鉴咯） 封装特征提取 论文数据分析(现在就差分析结果)（不知道我怎么会给造成一种，我很会写论文的样子，想太多了吧) 现在还是数据驱动研究，并非研究问题驱动数据的阶段 准备六级 reading record Output 基本的数据分析结果 论文阅读：一定要以PPT的形式给出（每周给自己做个1-3页的PPT) 看一道建模题 学习机器学习集成算法 2020-7-4 懒得的我 今天早上又睡了很久！可以是昨天晚上看别人的vlog太久了，羡慕那种独居生活。我什么时候可以过上独居的日子啊！ 今天下午学了stata这个工具，基本上学会了，我发现机器学习开源工具，有些不良心啊！ 今天晚上看了会电视剧。跟踪公众号。学习统计学。 2020-7-3 睡饱喝足，读会论文，&amp; 敲代码 今天下午玩了xgboost,集成学习有点难，但也要手动推导，这可以加深对算法内涵的理解。 晚上统计学习，stata和计量经济学（学的很基础，但实际应用不是那么回事了）。做数据科学，肯定要学习R语言 沉思：😔😔😔。 2020-7-2 陷入深渊的我 今天又是上课，不知道老师扯了些什么，听着听着就关成静音了。(阴险.jpg) 实现了时间序列的特征工程，读了一篇好文章（关于女性政治地位的提高），还找到了一篇EPJ data science上的好文章（追踪文献的重要性） 明天：1）实现xgboost 2) 模型设置（中文）综述和变量 2020-7-1 无聊的我 今天玩了tsfresh库，怎么那么难理解啊！实在是不知道别人怎么编程的！关键是返回的数据结构，和底层实现有关系！再次学习了参数估计（极大似然估计，最大后验估计）。我可能要手把手教那种。 2020-6-30 心累的一天 今天早上，下午上课，是张老师的课，课程难度很大。我也是半听半玩耍。可能是数学学多了，对这个社会问题感悟能力跟不上。就一句话存在即合理！顺道看了tsfresh的doc 晚上，读了一篇science，开创性工作就是厉害啊！从0到1是飞跃，1到2，3…是发展。还有跨界的精英。温习了机器学习里面的基本概念！ 2020-6-29 今天早上，下午上课去了。还去看了回归分析sklearn的实现。 关键点相关性分析-&gt;因果分析， correlation － causality - prediction - control。解释型研究（描述，统计方法)——&gt;充分解释（因果关系）科学研究： 解释，预测，控制闭环（干预），找不到因果关系。 因果关系分析方法：Causality: Models, Reasoning, and Inference 晚上复习统计学day Ox 01（本科我怎么没有作电子稿笔记啊，哭死了，还去反翻看了过去的笔记和博客，菜死了）。加个学习小组，感觉别人做的资料，和我想要的深度差太远了。数据挖掘上次看那个Dr.yuan的课程，笔记不是特别好！还要把时间序列特征工程做了！ 我发现申请实践学分，做助教，怎么那么多人申请啊！ 统计学习方法那个，我发现我做的笔记，好差劲啊！ 2020-6-28 元气少女的独白— 个人之旅 2020 27周：躬行实践 INPUT a. Linear Models &amp; Python 学会 https://scikit-learn.org/stable/supervised_learning.html#supervised-learning b. Time series &amp; Feature select tfresh c. English: youtube &amp; shanbei e. reading record f. 概率论（极大似然估计，假设检验，显著性检测，参数估计） OUTPUT a. kaggle project b. 掌握一个中高难度的数学模型 c. Reading Record 今天早上睡觉啦！ 下午和电脑人一起玩麻将，收拾了本周的资料汇总 晚上看了视频学习关于xgboost,和关于一些公众号的文章 F: ​ 输出（✔️）； 论文阅读（✔️）；读书（❌，主要是没有那种心境，不上瘾）；英语（✔️） S: 论资料的收集的重要性。知识在脑海里会忘记，但硬盘不会。一定要把自己遇到的顶级资料收集好，厘成doc。一定是高质量和高密度的资料才记录。原来一直注重资料的学习，忽略了资料整合的重要性。并且最好每段时间，写一篇大汇总。 切记从头开始。学习回路：资料收集（分等级：全局观—&gt;入门-进阶-高级，科普)——&gt; 把自己的输出提升到别人也可以直接用的水平。不错，是这么回事！这也是对自己的一种要求。 快乐夹杂着悲伤，至少结尾不能让自己太难堪！这条路只会与自己相关。 2020-6-27 外公八十岁生日 白天在外婆家，给教书先生外公过生日。 晚上，听了一场讲座，关于“当交通遇上机器学习”,是北交副教授万怀宇博士讲授的专题。主页：http://faculty.bjtu.edu.cn/8793/ 。全当是科普了，不过，内容挺深奥的。 你永远无法叫醒一个装睡的人 当你真心渴望某样东西时，整个宇宙都会联合起来帮助你完成！ 学到了两句话，我不知道选哪个！ 关键词: 硬核 2020-6-26 今天早上，睡到了11：00 下午，查重，心累！时间序列！ 晚上，读了一篇时间序列在预测能源消费里面的大综述。和本科的经历契合。重点看了最小二乘支持向量机的应用，研究结果拟合精度和预测精度还行。这些模型，我还挺熟悉的。 2020-6-25 端午快乐,biubiubiu 早上，睡到了中午，感觉太热了，不想早起。 中午，我和弟弟去街上吃饭了：豆腐、水饺、水煮肉片。感觉我要是愿意做一次饭，那简直是太阳从西方出来了。妈妈果然是世界上最辛苦的人了。 下午，交接一下时间序列项目，话说这个东西不是很火吗，为什么资料这么少啊！！！跑了一天一夜ARIMA了。帮我表弟写一篇中小企业发展现状的论文，找了几篇文章抄袭，看了别人的硕士学位论文，我才意识到我还是本科论文的水平。 晚上，研读论文。 2020-6-24 7h睡眠不足的我.jpg 今天早上迷迷糊糊起床，和同学唠嗑，昨天睡的好玩，1：30，听说墨西哥地震了，然后我发了消息，居然心安理得的睡着了！ 下午，杉杉说可以偷偷摸摸回去，我心里暗自一想，在家里也很好啊，睡-吃-电脑-吃-睡-醒的无线死循环。。。。。看了本科的学弟学妹毕业了，感叹时光啊，我这一年学了什么，时间都去哪了了？时间都去那儿了？我觉得自己一年什么成功都没有，论文被拒稿，被困家中半年了吧，感觉本科四年都没有在家呆那么长的时间。这不是本科的所有时间，全部回去了吗？ 晚上，了解时间序列的特征提取，什么鬼，怎么那么难啊！晚上看了大数据机器学习时代，科普。 突然想到还有好多复习的课程要考试啊，突然想到了。 看了很久很久的说说了，我发现大学我错过了很多的美好， 怀念大学的时光，无拘无束，可以起很早去实验室，可以认认真真的上课，可以和同学一起建模和熬夜通宵做作业，尤其是和灯夜一起大晚上待在实验室的日子。 Probit vs logit https://www.econometrics-with-r.org/11-2-palr.html 2020-6-23 我要放长假.mp4 今天睡了10h多。晚上十一点睡到早上11.00，中途还迷迷糊糊的在研究生系统打卡！！！！已经是自然状态了啊！ 下午：继续时间序列，理论完全不懂。这是怎么回事。 晚上：开开心心复习四川大学的概率论。好久没有接触机器学习了，差不多还给书本了。 机器学习算法+概率论统计学 2020-6-22 如果可以一直一直睡下去多好啊.eps 今天睡了11多个小时，太幸福了啊！ 下午玩了会时间序列。和灯夜聊了很久的天，就是关于兴趣和人生追求。 2020-6-21 今天上街去了，买了一个大大的冰淇淋。 输出 完成一篇博客 推进时间序列项目（读相关论文） 论文阅读 今天早上和下午睡了好久啊！ 下周规划： 时间序列项目（主线1）：具体把相关的方法整理成笔记 统计学（主线2）：统计学课程每天学一学，做练习。 mooc: https://www.icourse163.org/learn/NJUE-1001752031?tid=1206103246&amp;from=study#/learn/content 编程：SQL、Excel、Python的精髓用法，本着提高效率去的。 看剧 《白箱》 看两篇机器学习回归分析的论文 统计学习补上 书籍阅读： ​ 《大秦帝国》 ​ 《人性的弱点》 每天2h的英语学习，过六级啊 反思： ​ 日出而作，日落而息。 2020-6-20 心塞 今天肚子不舒服，什么都不想干了。好痛，好痛，好痛！！！！ 下午学了python技巧绘制各类bar,barh 2020-06-19 我与春风皆过客，你携秋水揽星河；殊途同归是偶然，背道而驰是常态。 下周规划： 时间序列项目（主线1）：具体把相关的方法整理成笔记 统计学（主线2）：统计学课程每天学一学，做练习。 编程：SQL、Excel、Python的精髓用法，本着提高效率去的。 书籍阅读： ​ 《大秦帝国》 ​ 《人性的弱点》 早上起的晚，还跟导师扯了皮，作为一个从小就逃避老师的人，居然还去打扰大忙人的时间，罪过啊。 下午：搞了会科研，数据不给力啊。哎，内心奔溃了。 晚上：恶补pythonic和统计学 https://mp.weixin.qq.com/s?__biz=MzI1MzAwODMyMQ==&amp;mid=2650338461&amp;idx=1&amp;sn=be67a2565cf5f0922e84e5076fe1c0d2&amp;chksm=f1d75433c6a0dd25fa6e25fae624cb21738b0fc4b28f833db0844f8bbb6e02c1dc9dfddac03a&amp;scene=0&amp;xtrack=1#rd 2020-06-18 😴 心不在焉 。全栈。 今天发现自己离全栈在程度在降低。 ​ 1. 作图、写作、看文献、想科研想法、编程几乎都可以半独立。但是我突破不了，就是往高质量在期刊发，感觉已经达到了自己能力的顶峰了。在继续，可能就是数学层面。要提升啊 自己学了很多东西，但是并没有独特的优势在，问题就是不专业，不透彻。要把自己喜欢在的涉及的知识、编程都要学到位，这很关键。 今天和萌萌组个学习队，加油，考研加油！！！！！！ 就是参考文献的插入。 2020-06-17 玩的fun 明天： 如果不出意外，尽量睡到九点！看会电视剧！ 继续跑程序 学习统计学 今天早上起的很晚，睡了很久！昨天看了好久的电视剧，刷b站。看了关于交通治理与疫情防控的文章。特别学到了新东西，就是矩阵的奇异值分解，所产生的现实意义，可通过降维，获取关键性信息，可得结构信息。https://mp.weixin.qq.com/s?__biz=MjM5MTM5NDAzNA==&amp;mid=2651320319&amp;idx=1&amp;sn=6a0e27ff1e5b9d0f3cc1d4ac5fff2ef0&amp;scene=19#wechat_redirect 下午搞了会数据 晚上： 规划一会统计学学习 看了下自己的塑身计划，纠正体型，感觉还差好多啊 看下自己的技能树 https://github.com/xiemaycherry/picture/blob/master/%E6%88%91%E7%9A%84%E6%8A%80%E8%83%BD%E6%A0%91.png 2020-06-16 今天早上沉思了自己最近的行为。重建了自己的各种行为，我可能脑子里面少了一根筋。哈哈哈哈哈哈哈哈哈 下午继续搬砖。早点办完吧。希望每一段沉潜的时光都可以闪闪发光。虽然在别人看来没有什么意思，但是我依然要努力!发现自己特别没有脑子啊！发觉自己太笨了吧！！！！！！ 下午和爷爷聊天了， 晚上学习了新知识 继续学习统计学 野外一游 古诗词 ​ 愿我如星君如月，夜夜流光相皎洁。 ​ 云想衣裳花想容，春风拂槛露华浓。 ​ 昔我往矣，杨柳依依。今我来思，雨雪霏霏。 ​ 我断不思量，你莫思量我。 ​ 落红不是无情物，化作春泥更护花。 ​ 曾经沧海难为水,除却巫山不是云。 ​ 秋风生渭水，落叶满长安。 2020-6-15 二次元少女的微笑.jpg 哎，我这是怎么了。老是词不达意啊！ 希望大家理解我。我是一个很不合群的人，还有点偏激的人。但不分场合的学习，没关系的啦，不要管我，所有以后我注意下自己的方式方法减少误会哈！！！！！对不起，对不起，对不起，对不起！！！！！！！！！！ 2020-6-14 开心睡觉ing 今天家里没有网，断网啊！ 复盘这周工作，感觉要把某个知识、理论学到脑子里，不忘记，随机应变，太难了！ ​ 第一：跑程序。我觉得要跑大程序，才是把程序精髓学到家，时间效率。 ​ 第二：阅读。显而易见的现象问题啊。 ​ 第三：课程结课。 几门课程，感觉不是自己当初想选的，学起来怎么都不顺心。我就学不来了，配不上！ 最主要的收获 ​ pandas不断新增行的方法。 ​ pandas if 的用法 2020-6-15—2020-6-21计划 感觉自己是听蠢笨的啊。 2020-6-13 不悲不喜.png 今天a .出门拜访亲戚，身心疲惫 b. 好想回学校啊，想念食堂，想念杉杉宝贝 c. 什么能力都在下降。。。。不好的预感啊！过去学的东西，长时间搁置，已经发霉了，啊，啊，啊，啊，啊， 啊——————————— d. 还是要多元化发展，感觉自己已经是单一维度了，不知道怎么回事。 e. 我发现我记忆力衰退的太厉害了，脑子不够灵活，为什么呢。要及时记录啊。 好有效率问题啊，效率啊，效率啊，效率啊！ f. 看清自己，终能看破红尘。 今天：遇到的一些问题： ​ 1. 对应DataFrame数据类型的行索引。在插入新行的问题，从其他DataFrame或者自定义插入。可以用append()在末尾新增行，但传参数切记是列表，最好用DataFrame 2. 还有列表和字符串的相互转换。如果要把[]传入DataFrame 2020-6-12 不纠结了.svg 明天周末了。今晚要追剧；车水马龙；在月光下奔跑，我什么都不想要，你爱我就好 今天是没有课的一天。还有杉杉在学校了。 2020-6-11 半睡半醒 : 明天 ​ 今天的任务没有做完，明天继续 今天 跑程序的过程中，把文件夹清理了 那么有趣，那么有价值-——评判公平性标准 品读了《喜欢你是寂静的》 听了陈奕迅《我要稳稳的幸福》 2020-6-10 开心.eps 明天： 调用时间序列指标 2. 线性回归模型 3. 跑程序，计算人才流动指标 今天 思考了一个问题：职业规划。我应该怎么给自己一个定位？我喜欢什么？我受什么驱动？ 交接了任务，被问惨了。 2020-6-9 愉悦.jpg 今天： 精读了一篇论文，收获满满，因为都是自己学过，看过的方法，看别人论文里面的应用，有所启发！以后要看顶级的资料 喝着安慕希，看别人的数据分析报告。 最近知识输入太过了，虽然都是本科了解过的（数学方面） 重要要清洗出数据，不知道为什么，我发现我的数据，居然只读了十几万条，然后重新跑。 等忙过了这段时间，一定要好好厘清最近的思路 可视化 数据清洗：python 线性预测模型 计量经济学 2020-6-8 元气 开心 喜出望外 明天1. 基本数据统计结果 2. 项目交接！3. good night 今天早上，发现居然错过新型冠状病毒的课程，已经网上结课，不得以询问辅导员、研管科，打了好多电话。研究生的老师居然给我们重新结课的时间，太爱了！！！！！！！！！！！！！太感谢，感动了！ 今天做了的数据序列项目上周的报告！不得不说自己排版能力越来越厉害了！ 但是跑处理的模型ARIMA效果不行！还有感谢张老师提供的服务器！找到跑代码的感觉了 今天终于预处理完论文数据了，但是能不能用好，挖掘更大隐藏信息，还得加把劲！ 还是要写函数，而不是复制粘贴的 今天阅读了本科做的项目，找了两张图片，虽然已经被拒稿了两次，但是我做的过程还是愉快，更重要的拿到英语课上去吹牛！希望好好搞接下来的研究！！！！ 一些读文献的经验值；怎么用语言表达不同场景的文献内容，这是需要加强的。 这周基本上熟悉了pandas,numpy的相关操作和注意事项，接下来要搞好这个项目(主线2) 继续阅读文献，见哥的文章 2020-6-6 今天又是元气满满的一天，和外婆、姨妈等亲人到我家里面做客，吃了🐟；还有邻居老人赠送的🍑 温习了numpy的用法，numpy,pandas,list直接的相互转换关系，以及sklearn metric里面的各类指标，什么叫指标的鲁棒性，怎么按需求选或者构造适合自己的评估指标。 读了高见师兄的综述。 做了指标体系图和高铁站点（Edraw中文坑，怎么不是矢量图啊，Visio破解不安全，AI还不知道怎么操作） 明天录视频， 2020-6-5 今天熟悉了时间序列流程；1. 平稳性。序列平稳性是做分析的基础。平稳性检测—单位根；非平稳性处理：差分~log~分解~平滑处理；2. ARIMA模型 基本思路；相关性和偏相关性定阶数 3. 网格+信息准则 调参：惩罚性 nbeats工具封装好了 4. predict样本内，动态和静态预测 forecase外推，timedelt 绘图，指标 假设检验：统计量和显著性水平，总感觉理解起来太难了，等忙了一定要好好复习统计学，参数估计，显著性检测，好好补。 记不住的函数 pd.date_range(start = sub.index[-1],end = sub.index[-1]+timedelta(days = 2),freq = ‘1h’) stat_rawdata = rawdata[rawdata[‘站点名称’]==stat] 布尔类型的切片，不上很明白原理 plt.xlim(sub.index[0],sub.index[-1]+timedelta(days = 2)) 绘制热力图 seaborn里面的heamap()123456import numpy as npimport seaborn as snsx = np.random.rand(10, 10)sns.heatmap(x, vmin=0, vmax=1, center=0)plt.show() 2020-6-4今天最大的头疼之处和领悟就是，跑大型程序，一定要准备一份下程序，调好结果才放在服务器上面跑，不然结果难以想象。。。。。。。。。。。。。 感觉调包侠也不是想象中那么好当的，一是现实中的数据一般般不规则，我发誓以后一定要做个合格的客户，二是参数默认是最讨厌的 今天又改程序，数据预处理果然头疼啊！希望周末可以出个大概结果，再重新跑数据 绝知此事要躬行，原来觉得自己学各种库还不错，实践的时候还是用不上的时候，😔 还有就是命名的重要性， 准备出一篇实践的心得，月末，主要是现在写程序太安逸了， 知识 pandas新增列 更改索引 range() 行政单位，变更情况 函数，写函数，不知道为什么，像python这种解释型的语言，自己老是重复，不好，不好， 2020-6-3今天还是学了不少东西，效率不高，主要是记不住函数传参数！ 掌握了一些pythonic的用法，想in for if ; if else 的一行代码；lambda单行函数功能的简化等等 一些常用的数据分析函数，sort_values();concat; append; time_range();绘图功能；就是就不知道，肯定是别人已经封装好了！ 2020-6-2今天总体还是满不错的！ 跑了大程序，自己的电脑和服务器的效果还真是不相同。不过，能够做到200行内不调试就能好了，现在找到了写代码的感觉，虽然还是bug~bug~bug, 原因在于开始交接工作没有做好，导致反反复复的修改codes. 在师兄的介绍下，接了一个时间序列的项目。在毕业设计那会，跑天然气数据，出来了的结果太差了，也不知道why,这次希望能够跟进这个项目。 感觉自己还留有余地，还没有恢复到最佳状态呐~ 2020-6-1今天刚好是2020年的一半，忙完第一学期（半梦半醒）,最终如梦初醒！今天接触许多新鲜的玩意： ArcMap绘制地图、流向图（关键是起点和终点坐标，如何根据名称获取坐标，要做连接。Arc ToolBox中提供了许多工具，方便了用户完成一些简单的操作，如Join、Excel to Tabel常用的工具箱。今天学习绘制地图和流向图涉及的操作包括：文件夹链接到工作目录；ArcMap导入Excel坐标数据并显示；XY to Line工具流向图；提取面要素的质心点；多表链接操作；属性设置（bar)。渲染结果的确漂亮 linux系统后端运行 nohup &amp;命令的使用，如何记录日志文件，定向输出；学了ps命令 ps 查看进程；还有|通道，grep查找； ps -ef| grep pyton matplotlib绘图的原理。 123456789101112131415创建figure后，还需要轴fig = plt.figure()ax1 = fig.add_subplot(221)ax2 = fig.add_subplot(222)ax3 = fig.add_subplot(224)fig, axes = plt.subplots(nrows=2, ncols=2)axes[0,0].set(title=&apos;Upper Left&apos;)axes[0,1].set(title=&apos;Upper Right&apos;)axes[1,0].set(title=&apos;Lower Left&apos;)axes[1,1].set(title=&apos;Lower Right&apos;)axes[0,0].plot()axes[0,0].set_xlim([-1,6])axes[0,0].legend() matplotlib.plot的基础绘图流程： 创建画布（选择是否绘制子图，指定画布大小，像素）添加标题—添加x轴的名称，刻度与范围—添加y轴的名称，刻度与范围绘制图形，设置图形的样式，颜色，背景，并添加图例保存图形，显示图形 感谢爷爷提供的CSDN账号 下载不少资料，一次性解决完了！ 2020-5-31 这是上完研究生第一学期上，第二学期下中旬！也是隔了很久才更新个人记录。研一上，换了一个新环境，终于有个特别舒适的环境了！大屏幕，柔软的凳子,太舒服了！怎可辜负呢。电科的课程实在是太多了，都不能安心玩耍了！这段时间算个积淀吧！ 2019.6.2 今天才来写，罪过啊！老师不在，室友不在，无聊至极，只能以视频唯友，解闷也！睡了一个星期，但是我瘦了2斤了。开始不在懒惰了，在这样下去我就完蛋了。。。。。。。 不知道怎么回事，VPN老是 time out ,明明还有$啊！ 造成了只能一段一段翻译，无语凝噎ing 还有百度上说是两篇，可是要求是一篇，于是我多花了2h,天杀的！！！ 下午跑了个步， 越来越发现，事先计划一下，再去做，效率更好了！ 2019.5.12 今天才来更新这个日常，真是罪过，上段时间一直忙其他的事情，毕业设计，回家，处理家务啊。直到今天学完了Course 3,如何改善神经网络的性能，还有英语学习，一定要加油啊！ 2019.4.18 今天学了一部分course two week two的课程，终于学到精彩的部分了，由于今天想练彩铅和帮忙收拾教室，所以就没有学完了，学了几种变种的梯度下降法，提高速度，太棒了，太棒了，太棒了！！！！！！ 2019.4.17 今天学完国course two的第一周课程，大概记时4h，还是颇多收获，学到了以前完全没有接触的东西，正则化方法，梯度消失的和爆炸的问题，感觉很棒， 2019.4.16 今天学了吴恩达老师的第一周关于神经网络的基础的课程，好厉害的 2019.4.15 今天学习了deep-layer neural network的正向传播和反向传播的过程，矩阵化计算的方式。 2019.4.14 今天学习了第三周课程，shallow (2层)神经网络的正向传播和反向传播，以及矩阵化的计算方式，以及和logistics regression的表示上的不同地方 2019.4.13 今天继续学习第二周课程，logistics regression 的模型，策略，算法的相关，如何把学习的到数学知识应用上去。 2019.4.12 今天没有去跑步，处理一些情感问题去了，学了Androw Ng的 第二周的部分课程，主要是链式求导法则，通过计算图，一步一步的复合求导，复合函数用计算图表示，并链式求导 2019.4.11 今天是很愉快的一天，起的很早，精神很足，学了学习，写完了吴恩达老师的第一周深度学习。 2019.4.10 今天刷完了第一遍，完了西瓜书第一章到第十一章，不过呢，还是要继续在刷基础还看论文 这段时间，跑步锻炼第一，睡觉，贪睡第二，第三，开始散漫了，自律的我，在哪里去了啊！ 争取在上研究生这段时间，把机器学习、Python编程能力提升上去 今天晚上弄了一个，吴恩达的deep Learning Ai课程笔记，开始学习这个系列的课程了 笔记模板 英文：http://dl-notes.imshuai.com/#/](http://dl-notes.imshuai.com/#/c1w1) 什么都有的 https://redstonewill.com/category/deeplearning/ 中文笔记： 详细，相当于翻译： http://www.ai-start.com/dl2017/html/lesson1-week1.html 思考和总结： https://link.zhihu.com/?target=http%3A//kyonhuang.top/Andrew-Ng-Deep-Learning-notes/ https://zhuanlan.zhihu.com/p/35333489 http://imshuai.com/tag/deeplearning-ai-notes/ https://zhuanlan.zhihu.com/koalatree 上班族的学习时长： 整理笔记要明显消耗更多的时间，基本上10分钟的视频，做笔记至少要50分钟才能看完。每周的视频大概在2-3个小时，这就意味着看学习视频就要10-15个小时。编程作业3个小时左右，每周平均在15个小时，16周的课，净时间就花了240个小时！ video https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/playlists 西瓜书的公式推导： https://datawhalechina.github.io/pumpkin-book/#/chapter1/chapter1 统计机器学习： https://github.com/SmirkCao/Lihang 2019.3.23-2019.3.24 今天（今天）是周末，两个早上都睡觉去了，处理一下，我弟弟的问题，和孩子交流的重要性，晚上什么都没有干，周日下午跑了步，学习了python sci-learn里面的交叉验证、超参数选择，集成学习库的用法，终于不再迷茫了，慢慢来 及时的记录学习过程 多去看别人的进展 去做别人没有做过的事情 只有你努力，努力的方向是正确的，就可以做出伟大的成就 https://zhuanlan.zhihu.com/p/29704017 2019.3.21 今天了解一下操盘，就是那个股票，感觉做数据科学家好吃香啊，加油，接下来有得忙了 2019.3.20 今天终于看了文献，想出了一个创新点子，可以发文章了，加油喔。 编写完了测试程序，加油 2019.3.19 今天早上看了关于许多灰色模型的现阶段文献，很受启发，感觉读文献真的对能力、思维的提高很大，也发现自己的潜力很大，加油，加油，加油，继续读，产生一些小点子，虽然不够好，但是也是突破 2019.3.18 今天看病去了，睡了好久，药物作用果然出乎意料， 2019.3.17 今天主要学习支持向量机的前世今生，怎么由来的 2019.3.16 今天周六，放松了一下自己，重新回归一下灰色预测模型，尤其是离散方程，其实去掉背景，抓住数学模型，才知道其实就是这样的，希望早日突破自己的界限 2019.3.14 今天改了论文，感觉快完了；学习了BP矩阵推导，人真的是越来越聪明和灵活 2019.3.13 今天晚上学完了回归树，好棒，虽然原理给人的感觉很直接，但是也是一种体现 2019.3.12 今天跑了步，早上改了毕业设计，下午配置了新手机，晚上写了日记 2019.3.11 今天早上听了听力，修改了毕业设计，核函数还是不是很懂；下午看了决策回归树，感觉很棒，过拟合才是该解决的关键问题！跑了步，感觉自己身体状况很不好啊！ 晚上听力电台，写了程序！ 慢慢的学习，慢慢的努力，慢慢的加油，慢慢的遇见自己的憧憬天空 2019.3.10 今天写了日记，说不上自己哪里郁闷，哪里开心！希望早点跑完程序，早点完事 2019.3.9 今天才发现，最近状态非常的不好，可能是无所事事吧，可是我有很多事情要做，加油，少女，加油，少女，以后一定要记得写了!总觉得自己逃不出自己的羁绊，被束缚 2019.2.28今天调整自己的心态，回顾自己的生活，自己太急于求成了，心急吃不了热豆腐啊！！！！！中国有句老话说得对，积少成多，不积跬步无以至千里，不积小流无以成江河，慢慢来，弄透弄清楚 最后，英语+数学+编程+解决问题的能力 2019.2.27今天早上，我读了同校同学写的简书文章，实在是感到好想笑，搜索了别人的解决方法，还是自己的知识量不足啊！！！！！！！！下午看了下假设检验的视频，再去跑了两个小时的步 晚上逛了一下午各位网站 2019.2.26今天看了下某个计算机大佬的历程，深深地感到佩服。《梦想小镇》又多了一块地盘了， 又重新学习了matplotlib,才发现,然后了玩了一下sklearn里面的带cross-validation的lasso的regression 再kaggle housr-prices 里面，rmse=$0.15386$,rank = 2903,不过做得也相当 2019.02.25 今天好像不在状态，可能是焦虑+迷茫，有动力，动力的方向在哪里啊！，还是好好的做好当下吧！ 阿西吧 2019.02.24今天特别不想起床，有点小感冒，整理周志华的第一章笔记下午总结日前学习的python库顺便去kaggle做了小练习，数据的预处理工作。]]></content>
      <categories>
        <category>学习の历程(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>Daily</tag>
      </tags>
  </entry>
</search>
