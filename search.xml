<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ÁßëÁ†îÂ∑•ÂÖ∑]]></title>
    <url>%2F2020%2F07%2F13%2F%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[ÁªòÂõæÁªòÂõæËΩØ‰ª∂‚Äî GephiVideohttps://www.udemy.com/course/gephi/learn/lecture/67426#overview ‰∫§ÈÄöhttps://blog.csdn.net/yirry/article/details/79233176?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare ÂÆòÁΩëhttps://gephi.org/users/download/ ÂÖ∂‰ªñpython https://networkx.github.io/documentation/stable/auto_examples/subclass/plot_antigraph.html#sphx-glr-auto-examples-subclass-plot-antigraph-py]]></content>
      <tags>
        <tag>ÁßëÁ†î</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Research-Career]]></title>
    <url>%2F2020%2F07%2F08%2FResearch-Career%2F</url>
    <content type="text"><![CDATA[ÊòØ‰∏çÊòØÂèØ‰ª•ËÄÉËôëËØªÂçöÂ£´ÂêéËÆ∑ÔºÅÔºÅÔºÅÔºÅ 2020-7-12 ‰ªÄ‰πàÊòØÂá∫Ëâ≤ÁöÑÁßëÁ†î&amp;Êï∞ÊçÆÂ§ÑÁêÜ ÈòÖËØª‰∫Ü‰∏Ä‰∫õÂÖ¨‰ºóÂè∑ÔºåÂÖ≥‰∫éÁßëÁ†îÊñáÁåÆË∞ÉÁ†îÁöÑ ÈòÖËØªÊñáÁåÆÂπ∂‰∏çÈúÄË¶ÅËØªÂ§ßÈáèÁöÑÊñáÁåÆÔºåËÄåÊòØË¶ÅÁ≤æËØªÈ´òË¥®ÈáèËÄå‰∏îÁõ∏ÂÖ≥ÁöÑËÆ∫Êñá„ÄÇÂè™Ë¶ÅÊâæÂà∞Ëøô‰∫õËÆ∫ÊñáÔºåÂä†‰ª•ÁêÜËß£ÂíåÂàÜÊûêÔºåÁî®ÊâπÂà§ÊÄßÊÄùÁª¥„ÄÅÂàõÊñ∞ÊÄùÁª¥ÂéªÊÄªÁªìÂíåÂàÜÊûêÈóÆÈ¢òÔºåÂ∞±ËÉΩÊèêÁÇºÂá∫‰ºòÁßÄÁöÑÁßëÁ†îÊÉ≥Ê≥ï„ÄÇÊàëÁöÑ‰π†ÊÉØÊòØËä±Â§ßÈáèÁöÑÊó∂Èó¥ÂéªÂàùÊ≠•Á≠õÈÄâËÆ∫ÊñáÔºåÁúãÈ¢òÁõÆ„ÄÅÊëòË¶Å„ÄÅ‰ΩúËÄÖÂíå‰ΩúËÄÖÂçï‰ΩçÁ≠âÔºåÂΩìÊàëÁÇπÂáª‰∏ãËΩΩÊó∂ÔºåÊàëÂ§ßÊ¶ÇÂ∞±Áü•ÈÅì‰∫ÜËøôÁØáËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂÜÖÂÆπÔºåÂú®ÂêéÈù¢ÈòÖËØªÂÖ®ÊñáÊó∂ÔºåÂ∞±ÂæàÊúâÈíàÂØπÊÄßÔºåÊî∂Ëé∑Â∞±‰ºöÂæàÂ§ß„ÄÇËøôÊ†∑Á≠õÈÄâËÆ∫ÊñáÔºåËÆ©ÊàëÂè™Ë¶ÅÁî®ÁÆÄÂçïÂá†‰∏™Êñá‰ª∂Â§πÁÆ°ÁêÜÂç≥ÂèØÔºåËÄå‰∏îËÆ©ÊàëÈùûÂ∏∏ÁÜüÊÇâËøô‰∫õËÆ∫ÊñáÁöÑ‰ΩúËÄÖÔºå‰πüÂ∞±Áü•ÈÅì‰∫ÜÂì™‰∫õÊòØÊàëÁöÑÂõΩÈôÖÂêåË°åÔºå‰æø‰∫éÈÇÆ‰ª∂Âíå‰ºöËÆÆËßÅÈù¢‰∫§ÊµÅ„ÄÇ ÂæÄÂæÄÂá∫Ëâ≤ÁöÑÂºïË®ÄÂ∞±‰∫§‰ª£‰∫ÜÂèëÂ±ïËøáÁ®ã„ÄÇ ÊâæÂá∫ÊúÄÂéâÂÆ≥ÁöÑÊñáÁåÆÂíåÂ≠¶ËÄÖÔºåËØª‰ªñ‰ª¨ÁöÑÊñáÁåÆ„ÄÇ Â§ÑÁêÜÂüéÂ∏ÇÂ±ÇÈù¢Êï∞ÊçÆÔºàÂøÉÁ¥ØÔºåÂ¶ÇÊûúÁõ¥Êé•‰π∞Â§öÂ•ΩÂïäÔºåËøòË¶ÅÁõ¥Êé•ÂºÑÊ†ºÂºèÔºåÁºñÁ®ãÁªü‰∏ÄÂåñÔºâ 2020-7-11 Â§ÑÁêÜÊï∞ÊçÆ Êï∞ÊçÆË¢´Âä†ÂØÜ‰∫ÜÔºåÊâãÂä®Ëß£ÂØÜ„ÄÇÂëúÂëúÂëú„ÄÇÂü∫Êú¨‰∏äÊêûÂÆå‰∫Ü 2020-7-9&amp;10 ËÆ∫ÊñáÂÜô‰Ωú(Method) ÊñπÊ≥ïÈÉ®ÂàÜÁÆóÂæàÊú∫Ê¢∞„ÄÇÂè•Â≠ê(Âæà‰∏≠Êñá)-&gt;ÊÆµËêΩ(ËøûË¥ØÊÄßÂ∑Æ)-ÂæàÈöæ ËØ≠Ë®ÄÈ£éÊ†º„ÄÇ ÊñáÁåÆÁÆ°ÁêÜ 2020-7-9 1ÔºéË¶ÅÂÜôÂ•ΩÁßëÁ†îËÆ∫ÊñáÔºåÂøÖÈ°ªÂÖàÂÖªÊàêËØªËã±ÊñáÊñáÁ´†ÁöÑ‰π†ÊÉØÔºå‰∫âÂèñÊØèÂ§©30-60ÂàÜÈíü„ÄÇÂàöÂºÄÂßãÂèØ‰ª•ÈÄâÊã©‰ª•ËØªËã±ÊñáÊä•Á∫∏„ÄÅËã±ÊñáÊñ∞Èóª‰∏∫‰∏ªÔºåÈÄêÊ∏êËΩ¨‰∏∫ËØª‰∏ì‰∏öÊùÇÂøó„ÄÇÊàë‰ºöÂú®ËøëÊúü‰∏ìÈó®ÂÜô‰∏ÄÁØáÂçöÂÆ¢ÊñáÁ´†‰ªãÁªç‰∏ÄÂ•óË°å‰πãÊúâÊïàÁöÑÂ¢ûÂº∫ËØª‰∏ì‰∏öÊùÇÂøóËÉΩÂäõÁöÑÂäûÊ≥ï„ÄÇ 2ÔºéÂÜôÁßëÁ†îËÆ∫ÊñáÔºåÊúÄÈáçË¶ÅÁöÑÊòØÈÄªËæë„ÄÇÈÄªËæëÁöÑÂΩ¢ÊàêÊù•Ëá™ÂØπÂÆûÈ™åÊï∞ÊçÆÁöÑÊÄª‰ΩìÂàÜÊûê„ÄÇÂøÖÈ°ªÂÖàËÆ®ËÆ∫Âá∫‰∏ÄÂ•óÊ∏ÖÊô∞ÁöÑÊÄùË∑ØÔºåÁÑ∂ÂêéÊåâÁÖßÊÄùË∑ØÊù•ÂÅöÂõæ(Figures)ÔºåÊúÄÂêéÊâçËÉΩÊâßÁ¨î„ÄÇ 3ÔºéÂÖ∑‰ΩìÂÜô‰ΩúÊó∂ÔºåÂÖàÊåâÁÖßÊÄùË∑ØÔºàÂç≥FiguresÔºâÂÜô‰∏Ä‰∏™‰ª•subheading‰∏∫‰∏ªÁöÑÊ°ÜÊû∂ÔºåÁÑ∂ÂêéÂºÄÂßãÂÖ∑‰ΩìÂÜô‰Ωú„ÄÇÁ¨¨‰∏ÄÁ®øÔºåÂàáÂøåËøΩÊ±ÇÊØè‰∏ÄÂè•ËØùÁöÑÂÆåÁæéÔºåÊõ¥‰∏çË¶ÅËøΩÊ±ÇËØçËØ≠ÁöÑÂçé‰∏ΩÔºåËÄå‰∏ªË¶ÅÁïôÂøÉÈÄªËæëÔºàlogic flowÔºâÔºåÊ≥®ÊÑèÂâçÂêéÂè•ÁöÑÈÄªËæëÂÖ≥Á≥ª„ÄÅÁõ∏ÈÇª‰∏§ÊÆµÁöÑÈÄªËæëÂÖ≥Á≥ª„ÄÇÂÜô‰ΩúÊó∂ÔºåÂÖ®Âäõ‰ª•Ëµ¥ÔºåÂ∞ΩÂèØËÉΩ‰∏çÂèóÂ§ñÁïå‰∫ãÊÉÖÂπ≤Êâ∞ÔºàÂÖ≥Èó≠ÊâãÊú∫„ÄÅÂ∫ßÊú∫ÔºâÔºå‰∫âÂèñÂú®ÊúÄÁü≠Êó∂Èó¥ÂÜÖÊãøÂá∫Á¨¨‰∏ÄÁ®ø„ÄÇËøòË¶ÅÊ≥®ÊÑèÔºö‰∏ÄÂè•ËØù‰∏çÂèØÂ§™Èïø„ÄÇ 4ÔºéÂ≠¶‰ºöÁÖßËë´Ëä¶ÁîªÁì¢„ÄÇÊ≤°Êúâ‰∫∫Â§©Áîü‰ºöÂÜô‰ºòÁßÄÁöÑÁßëÁ†îËÆ∫ÊñáÔºåÈÉΩÊòØ‰ªéÂà´‰∫∫ÈÇ£ÈáåÂ≠¶Êù•ÁöÑ„ÄÇÂ≠¶‰π†Âà´‰∫∫ÁöÑÊñáÁ´†Ë¶ÅÊ≥®ÊÑè‰∏ì‰∏öÈ¢ÜÂüüÁöÑ‰∏çÂêåÔºåÊúâ‰∫õÈ¢ÜÂüüÔºàÂåÖÊã¨ÊàëÊâÄÂú®ÁöÑÁªìÊûÑÁîüÁâ©Â≠¶ÔºâÊúâÂÆÉÂÜÖÂú®ÁöÑÂÜô‰ΩúËßÑÂæã„ÄÇÁßëÁ†îÊñáÁ´†ÈáåÁöÑ‰∏Ä‰∫õËØùÊòØÂÆöÂºèÔºåÊØîÂ¶Ç ‚ÄúTo investigate the mechanism of ‚Ä¶, we performed ‚Ä¶‚Äù, ‚ÄúThese results support the former, but not the latter, hypothesis ‚Ä¶‚Äù, ‚ÄúDespite recent progress, how ‚Ä¶ remains to be elucidated ‚Ä¶‚Äù Á≠âÁ≠â„ÄÇÁî®‰∏§Ê¨°‰ª•ÂêéÔºåÂ∞±ÈÄêÊ∏êÂ≠¶‰ºöÁÅµÊ¥ªËøêÁî®‰∫Ü„ÄÇÂú®ÂêëÂà´‰∫∫Â≠¶‰π†Êó∂ÔºåÂàáÂøåÊäÑË¢≠„ÄÇÂú®ÁæéÂõΩ‰∏Ä‰∫õÊú∫ÊûÑÔºåËøûÁª≠7‰∏™Ëã±ÊñáÂçïËØçÂú®‰∏ÄËµ∑ÂíåÂà´‰∫∫ÁöÑÂÆåÂÖ®‰∏ÄÊ†∑ÔºåÂéüÂàô‰∏äÂ∞±Ë¢´ËÆ§‰∏∫ÊäÑË¢≠ÔºàplagiarismÔºâ„ÄÇ 5ÔºéÁ¨¨‰∏ÄÁ®øÂÜôÂÆåÂêéÔºåÁªôËá™Â∑±‰∏çË¶ÅË∂ÖËøá‰∏ÄÂ§©ÁöÑ‰ºëÊÅØÊó∂Èó¥ÔºåÂºÄÂßã‰øÆÊîπÁ¨¨‰∫åÁ®ø„ÄÇ‰øÆÊîπÊó∂ÔºåËøòÊòØ‰ª•ÈÄªËæë‰∏∫‰∏ªÔºå‰ΩÜÂØπÊØè‰∏ÄÂè•ËØùÈÉΩË¶ÅÊé®Êï≤‰∏Ä‰∏ãÔºåÂØπabstractÂíåÊ≠£Êñá‰∏≠ÁöÑÂÖ≥ÈîÆËØ≠Âè•Ë¶ÅÂ≠óÊñüÂè•ÈÖå„ÄÇÂ≠¶‰ºöÁî®‚ÄúThesaurus‚ÄùÔºàÂêå‰πâËØçÊõøÊç¢Ôºâ‰ª•ÈÅøÂÖçËøáÂ§öÈáçÂ§ç„ÄÇÁ¨¨‰∫åÁ®øÁöÑ‰øÆÊîπÊûÅ‰∏∫ÂÖ≥ÈîÆÔºåÂÜçÂæÄÂêéÂ∞±‰∏ç‰ºöÂ§ßÊîπ‰∫Ü„ÄÇ 6ÔºéÁ¨¨‰∫åÁ®ø‰ª•ÂêéÁöÑ‰øÆÊîπÔºå‰∏ªË¶ÅÊ≥®ÈáçÂÖ∑‰ΩìÁöÑÂ≠óÂè•Ôºå‰∏ç‰ºöÊîπÂèòÊï¥‰ΩìÈÄªËæë‰∫Ü„ÄÇÊäïÁ®øÂâçÔºå‰∏ÄÂÆöË¶ÅÊï¥‰ΩìËØª‰∏ÄÈÅçÔºåÂØπ‰∏™Âà´ËØçÂè•Áï•‰ΩúÊîπÂä®„ÄÇËÆ∞‰ΩèÔºöÂ≠¶ÊúØÊúüÂàä‰∏ÄËà¨‰∏ç‰ºöÂõ†‰∏∫ÂÖ∑‰ΩìÁöÑËØ≠Ê≥ïÈîôËØØÊãíÁªù‰∏ÄÁØáÊñáÁ´†Ôºå‰ΩÜ‰∏ÄÂÆö‰ºöÂõ†‰∏∫ÈÄªËæëÊ∑∑‰π±ËÄåÊãíÁªù‰∏ÄÁØáÊñáÁ´†„ÄÇ 2020-7-8 Êï∞ÊçÆÂáÜÂ§á ÂºÄÂßã‰∏ÄÊÆµÊñ∞ÊóÖÁ®ãÔºÅÁæ°ÊÖïÂçöÂ≠¶Ê∏äÂçöÁöÑÁßëÁ†îÂ∑•‰ΩúËÄÖ„ÄÇÊó†ËÆ∫ÊòØ‰∏ÄÂπ¥ÔºåËøòÊòØnÂπ¥ÔºåÁ´ã‰∏™flagÔºöÂ∏åÊúõËá™Â∑±ÁªàÊúâ‰∏ÄÂ§©‰πüËÉΩËææÂà∞ÈÇ£Ê†∑ÁöÑÈ´òÂ∫¶„ÄÇ ËÆ∞‰ΩèÔºöËøôÊ≥®ÂÆöÊòØÂ≠§Áã¨‰∏éÊøÄÂãáÁöÑÊóÖÁ®ã„ÄÇ]]></content>
      <tags>
        <tag>ÁßëÁ†îÊó•Âøó</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ËÆ°ÈáèÁªèÊµéÂ≠¶]]></title>
    <url>%2F2020%2F07%2F03%2F%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[ËµÑÊñô ËÆ°ÈáèÁªèÊµéÂ≠¶ÔºöÈôàÂº∫ËÄÅÂ∏àÁöÑ‰π¶Á±çÈùûÂ∏∏Â•ΩÔºÅ https://www.econometrics-with-r.org/11-2-palr.html http://hadley.nz/index.html https://sites.google.com/site/econometricsacademy/econometrics-software/stata Day Ox 01‰∏ªË¶ÅÊòØÊÉ≥ÊãâÈÄö‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÊµÅÁ®ãÔºåÊãâÈÄöÂü∫Êú¨ÁöÑËÆ°ÈáèÂàÜÊûêÁªìÊûú„ÄÇÂú®Ëøô‰∏™ËøáÁ®ã‰∏≠Ôºå‰∏ªË¶ÅÂèÇËÄÉ‰∫Ü„Ää‰∏≠ÂõΩÂ∑•‰∏öÁªèÊµé„ÄãÊèê‰æõÁöÑÊ∫ê‰ª£Á†ÅÈáåÈù¢ÁöÑÂëΩ‰ª§ÔºåÁΩë‰∏äÁôæÂ∫¶ÁöÑËµÑÊñô„ÄÇËøôÊñπÈù¢ÁöÑËµÑÊñôÂ§™Á®ÄÂ∞ë‰∏î‰∏çÁ≥ªÁªü„ÄÇhelpÂ§ßÊ≥ïÂ•ΩÂïäÔºÅÊãâÈÄöË∑ëÂÆå‰∏Ä‰∏™Âà´‰∫∫ÁöÑÁ®ãÂ∫èÔºåÂü∫Êú¨‰∏äÂ∞±Âü∫ÂáÜÂõûÂΩíÂèØ‰ª•Áî®‰∫Ü„ÄÇ Day Ox 02Â∞±ÊòØÊääDID,PSM-DID,IVÊñπÊ≥ïÁúãÂà´‰∫∫ÁöÑÔºåÂ≠¶‰ºö‰∫Ü„ÄÇËøòÊúâÁªòÂõæÂäüËÉΩ‰∏ÄÂπ∂Â≠¶‰ºö‰∫Ü„ÄÇ Introduction to StataPrograms ‚Äì Stata do files (.do) Data files ‚Äì stata data files (.dta) and csv files Data editor ‚Äì where data are imported to Log files - where output is saved Load DataExcelÂØºÂÖ•edit-&gt;Data Editor -&gt;Excel SummarizeÂèØÂàÜÁªÑÊèèËø∞„ÄÇ tabstat anad, by(year) s(sum count) tabstatÊòØStataËá™Â∏¶ÁöÑÁ®ãÂ∫èÂëΩ‰ª§ÔºåStataÁöÑÁ®ãÂ∫èÊ†ºÂºèÈÄöÂ∏∏ÈÉΩÊòØËøôÊ†∑ÂÆâÊéíÁöÑ anadÊòØÊ†áËÆ∞ÂÖ¨Âè∏ÊúâÊ≤°ÊúâÂàÜÊûêÂ∏àË∑üË∏™ÁöÑÂèòÈáè by(year)ÊòØÂàÜÂπ¥ÁªüËÆ°ÁöÑÊÑèÊÄù„ÄÇ s(sum count)ÊÑèÂú®ËæìÂá∫ÂèòÈáèanadÁöÑ‰∏§‰∏™ÁªüËÆ°ÈáèÔºåÊÄªÂíåÔºàsumÔºâÔºåÊÄªËßÇÊµãÊï∞ÔºàcountÔºâ tabstat delta_cash overinv underinv ananum cashflow fcf_p fcf_n absda size lev roa tobinq delta_std , s(count mean median sd min max) tabstatÊòØËæìÂá∫ÊèèËø∞ÊÄßÁªüËÆ°ÈùûÂ∏∏Â•ΩÁî®ÁöÑÂëΩ‰ª§ delta_cash overinv underinv ananum cashflow fcf_p fcf_n absda size lev roa tobinq delta_stdÊòØÊàë‰ª¨Ë¶ÅËøõË°åÁªüËÆ°ÁöÑ‰∏ÄÁªÑÂèòÈáè„ÄÇ s(count mean median sd min max)ÊòØËØ¥Êàë‰ª¨Ë¶ÅÁîüÊàêÔºöÊÄªËßÇÊµãÊï∞„ÄÅÂùáÂÄº„ÄÅ‰∏≠‰ΩçÊï∞„ÄÅÊ†áÂáÜÂ∑Æ„ÄÅÊúÄÂ∞èÂÄºÂíåÊúÄÂ§ßÂÄº„ÄÇÂÖ±5È°πÁªüËÆ°Èáè„ÄÇÂΩìÁÑ∂ÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥ÁîüÊàêÂÖ∂‰ªñÁªüËÆ°ÈáèÔºåÂèØ‰ª•Âú®Êã¨Âè∑ÈáåÊ∑ªÂä†ÔºåÊØîÂ¶ÇÂàÜ‰ΩçÊï∞„ÄÇ eg. tabstat weight, s(mean, count) ,if foreign ==1 tabstat read write math, by(prgtype) stat(n mean sd) gen rename replace drooorder id genderlabel variable schtyp ‚Äútype of school‚Äùrename gender femalegen score=read+write+mathgen score2=score^2gen pass=1 if score&gt;=150replace pass=0 if pass==.drop if read&lt;40drop schtyp stata ÊéíÂ∫èÂíåÂàÜÁªÑ sort ÂçáÂ∫è sort v1 v2 list gsort gsort [+|-] varname [[+|-] varname ‚Ä¶] [, generate(newvar) mfirst] gsort +v1 -v2 3, bysort ÁÆÄÂÜô by reghdfe ÂõûÂΩíÂø´ÈÄüssc install reghdfe help reghdfe reghdfe depvar [indepvars] [if] [in] [weight] , absorb(absvars)[options] Âõ∫ÂÆöÊïàÂ∫îi.industry, i.province Âä†ÂÖ•ËôöÊãüÂèòÈáè https://www.jianshu.com/p/56285c5ff1e3 ÂºÇÊñπÂ∑ÆÁ®≥ÂÅ•Ê†áÂáÜËØØ vce(robust) ÈöèÊú∫Êâ∞Âä®È°πÂíåËá™ÂèòÈáèÊúâÁõ∏ÂÖ≥ÊÄß xtreg cluster(id) ‰∫§‰∫íÊÄßÂíåÂàÜÁªÑÂõûÂΩíÁöÑÂå∫Âà´ÂàÜÁªÑÂõûÂΩíÔºöÊ≤°ÊúâÂÅáËÆæÔºåÁ≥ªÊï∞ÂèØ‰∏ç‰∏ÄËá¥ gen egen‰ΩøÁî®Ê≠•È™§Ox 00 ÊèèËø∞ÊÄßÁªüËÆ° ÂàÜÁªÑÊèèËø∞ tabstat weight, s(mean, count) ,if foreign ==1 tabstat read write math, by(prgtype) stat(n mean sd) Ox 01 Âü∫ÂáÜÂõûÂΩí 12345reg csr X1 lnsize bcash roa lev les tobinq age i.Ind i.year est store m1reg csr X2 lnsize bcash roa lev les tobinq age i.Ind i.yearest store m2esttab m1 m2 m3 m4 using esttab1.rtf, replace 123456789reg csr X1 lnsize bcash roa lev tobinq age i.Ind i.year if zy==1est store m1reg csr X1 lnsize bcash roa lev tobinq age i.Ind i.year if zy==0est store m2reg csr X2 lnsize bcash roa lev tobinq age i.Ind i.year if zy==1est store m3reg csr X2 lnsize bcash roa lev tobinq age i.Ind i.year if zy==0est store m4esttab m1 m2 m3,se scalars(N r2 F p) mtitles title(‚ÄúÂõæ1‚Äù),using esttab1.rtf,replace https://mp.weixin.qq.com/s?__biz=MzIwMTQ3MTY0MA==&amp;mid=2247483904&amp;idx=1&amp;sn=5822658f03a1ee6b14cf37510287ac3e&amp;chksm=96ec21f7a19ba8e1997e9600e876976c92d6cdbfeaa068aa42cc3864dc4b43d4440bade1c2b7&amp;token=204946112&amp;lang=zh_CN#rd Ëá™Áõ∏ÂÖ≥ÂíåÂ∫èÂàóÁõ∏ÂÖ≥ÊÄß1234xtset id timextreg y x i.time, fe rxtreg y x i.time, fe r cluster(id)xtreg y x i.time, fe vce(cluster id) vce(cluster panelvar) https://cloud.tencent.com/developer/news/359411 Ê†áÂáÜËØØÁöÑÈÄâÂèñ1.Â≠òÂú®ÂºÇÊñπÂ∑Æ‰∏îËßÇÊµãÂÄº‰πãÈó¥Áã¨Á´ãÔºövce(robust)ÊàñrobustÊàñr2.Â≠òÂú®ÂºÇÊñπÂ∑Æ‰∏îÂÖÅËÆ∏ËßÇÊµãÂÄºÁªÑÂÜÖÁõ∏ÂÖ≥„ÄÅÁªÑÈó¥Êó†ÂÖ≥Ôºöcluster(id) ‰πüÂ∞±ÊòØÂ∏∏ËØ¥ÁöÑËÅöÁ±ªÁ®≥ÂÅ•Ê†áÂáÜËØØÔºåidÊòØ‰∏™‰ΩìÂèòÈáèÂú®xtreg,feÔºàÂõ∫ÂÆöÊïàÂ∫îÊ®°ÂûãÔºâÊàñxtreg,reÔºàÈöèÊú∫ÊïàÂ∫îÊ®°ÂûãÔºâÂëΩ‰ª§‰∏ãÔºårobustÂíåcluster(id)ÁöÑÂëΩ‰ª§Á≠â‰ª∑„ÄÇÂç≥Ôºöxtreg y x1 x2, fe robust= xtreg y x1 x2, fe cluster(id) Âõ†‰∏∫Èù¢ÊùøÊï∞ÊçÆÔºåÂºÇÊñπÂ∑ÆÊúâ‰∏§ÁßçÂèØËÉΩÔºåÂÖ∂‰∏ÄÊòØÊó∂Èó¥Â∫èÂàó‰∏äÁöÑÔºåÂè¶‰∏ÄÁßçÊòØÊà™Èù¢‰∏äÁöÑÔºå‰Ω†ÁöÑÊòØÂêé‰∏ÄÁßç„ÄÇ cluster (var)ËØ¥ÊòévarËøô‰∏™ÁªÑÂÜÖÊúâÁõ∏ÂÖ≥ÊÄß reghdfe NO false_connect ,absorb(i.stkcd#i.subcityid i.stkcd#i.year i.subcityid#i.year) vce (cl tie) DIDsavesave ‚Äúconnect_simulations.dta‚Äù, replace dropdrop if sgnyea==2010 gengen market_access_g=ln(1+market_g) http://wlm.userweb.mwn.de/Stata/wstatgen.htm DIDareg Dis HSR Cash Invt Lev Size ROA Rec Age PGDP SOE i.year,absorb(company_id) robustest store m_1areg Disw HSR Cash Invt Lev Size ROA Rec Age PGDP SOE i.year,absorb(company_id) robustest store m_2esttab m_1 m_2 using baseline.rtf, replace /// b(%9.4f) star( 0.1 0.05 0.01) t(%9.4f) /// scalar(F N r2_a) nogaps /// mtitle() regress y x ÊâßË°åyÂØπÂçï‰∏™È¢ÑÊµãÂèòÈáèxÁöÑÂ∏∏ËßÑÊúÄÂ∞è‰∫å‰πòÊ≥ïÂõûÂΩí regress y x if ethnic==3&amp;income&gt;50 ÊâßË°åyÂØπxÁöÑÂõûÂΩíÔºå‰ΩÜÂè™‰ΩøÁî®ethnicÁ≠â‰∫é3‰∏îincomeÂ§ß‰∫é50ÁöÑÊï∞ÊçÆÂ≠êÈõÜ predict yhat ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÂèòÈáèyhatÔºå‰ª§ÂÖ∂Á≠â‰∫éÊúÄËøëÂõûÂΩíÊâÄÂæóÂà∞ÁöÑÈ¢ÑÊµãÂÄº predict e, resid ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÂèòÈáèeÔºå‰ª§ÂÖ∂Á≠â‰∫éÊúÄËøëÂõûÂΩíÂæóÂà∞ÁöÑÊÆãÂ∑Æ https://blog.csdn.net/arlionn/article/details/85244336 **Âä®ÊÄÅÊ£ÄÈ™å**gen Dyear=year-company_work_hsrgen Before2=(Dyear==-2)lab var Before2 ‚Äú2 Year Prior‚Äùgen Before1=(Dyear==-1)lab var Before1 ‚Äú1 Year Prior‚Äùgen Current=(Dyear==0)lab var Current ‚ÄúYear of Adoption‚Äùgen After1=(Dyear==1)lab var After1 ‚Äú1 Year After‚Äùgen After2=(Dyear==2)lab var After2 ‚Äú2 Year After‚Äùgen After3_=(Dyear&gt;=3)lab var After3_ ‚Äú3 or More Year After‚Äùareg Disw Before2 Before1 Current After1 After2 After3_ Cash Invt Lev Size ROA Rec Age PGDP SOE i.year,absorb(company_id) robustest store Dynamic1areg Dis Before2 Before1 Current After1 After2 After3_ Cash Invt Lev Size ROA Rec Age PGDP SOE i.year,absorb(company_id) robustest store Dynamic2 esttab Dynamic1 Dynamic2 using Dynamic.rtf, replace /// b(%6.4f) star( 0.1 0.05 0.01) t(%6.4f) /// scalar(N r2_a) nogaps /// mtitle() **Âπ≥Ë°åË∂ãÂäøÂõæ**coefplot Dynamic1, keep(Before2 Before1 Current After1 After2 After3_) vertical recast(connect) lcolor(red0.45) lpattern(-) ciopts(lcolor(edkblue0.8)) mlcolor(gs6) mfcolor(white) msize(1.2) msymbol(h) yline(0,lcolor(edkblue0.6) lwidth(*1.0)) coefplot Dynamic2, keep(Before2 Before1 Current After1 After2 After3_) vertical recast(connect) lcolor(red0.45) lpattern(-) ciopts(lcolor(edkblue0.8)) mlcolor(gs6) mfcolor(white) msize(1.2) msymbol(h) yline(0,lcolor(edkblue0.6) lwidth(*1.0)) PSM-DIDÂÄæÂêëÊÄßÂåπÈÖçÂæóÂàÜ https://mp.weixin.qq.com/s?__biz=MjM5NTM4NjU2OA==&amp;mid=2650714980&amp;idx=1&amp;sn=5bafc8c230fb062d90f04709f683b5b2&amp;chksm=bef356c38984dfd54a0c1ffbf99933b7e8bac528324b6bf1ce5fce888718374bb4cc4fce70d5&amp;scene=0&amp;xtrack=1#rd IVclearuse IV_re2logit hsr near GDPA i.year,rest store IV1esttab IV1 using IV1.rtf, replace /// b(%6.4f) star( 0.1 0.05 0.01) t(%6.4f) /// scalar(N r2_p) nogaps /// mtitle()predict r # ËøôÈáåÂ∞±ÊòØÊúÄËøë‰∏ÄÊ¨°ÂõûÂΩíduplicates drop city_name year,forcemerge 1:m city_name year using distancekeep if _merge==3drop _merge areg Disw r Cash Invt Lev Size ROA Rec Age PGDP SOE Soe2 i.year,absorb(company_id)est store m_1areg Dis r Cash Invt Lev Size ROA Rec Age PGDP SOE i.year,absorb(company_id)est store m_2 esttab m_1 m_2 using iv.rtf, replace /// b(%6.4f) star( 0.1 0.05 0.01) t(%6.4f) /// scalar(N r2_a) nogaps /// mtitle() plothttps://zhuanlan.zhihu.com/p/32432932]]></content>
      <tags>
        <tag>ËÆ°ÈáèÁªèÊµé</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RËØ≠Ë®Ä]]></title>
    <url>%2F2020%2F07%2F03%2FR%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[https://bookdown.org/qiyuandong/intro_r/-r-basics-2.html#section-3.3 ÂÖ•Èó®Ôºö https://rc2e.com/ http://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/intro.html ÂÖ®Èù¢Ôºö https://github.com/harryprince/R-Tutor ËßÜÈ¢ëÔºö ‰∏≠ÊñáÔºö https://www.youtube.com/watch?v=rPj5FsTRboE Ëã±ÊñáÔºöhttps://www.youtube.com/watch?v=32o0DnuRjfg Ëøô‰∏™ÊïôÁ®ãÂ•ΩÔºö https://sites.google.com/site/econometricsacademy/econometrics-models/linear-regression https://www.youtube.com/watch?v=YMt5K68ZvjQ&amp;list=PLRW9kMvtNZOh7Xt1m5Mlhhz2wtr0tCUEE]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xgboost]]></title>
    <url>%2F2020%2F07%2F03%2FXgboost%2F</url>
    <content type="text"><![CDATA[ÁêÜËÆ∫ÈÉ®ÂàÜËØ•ÁÆóÊ≥ïÊÄùÊÉ≥Â∞±ÊòØ‰∏çÊñ≠Âú∞Ê∑ªÂä†Ê†ëÔºå‰∏çÊñ≠Âú∞ËøõË°åÁâπÂæÅÂàÜË£ÇÊù•ÁîüÈïø‰∏ÄÊ£µÊ†ëÔºåÊØèÊ¨°Ê∑ªÂä†‰∏Ä‰∏™Ê†ëÔºåÂÖ∂ÂÆûÊòØÂ≠¶‰π†‰∏Ä‰∏™Êñ∞ÂáΩÊï∞ÔºåÂéªÊãüÂêà‰∏äÊ¨°È¢ÑÊµãÁöÑÊÆãÂ∑Æ„ÄÇÂΩìÊàë‰ª¨ËÆ≠ÁªÉÂÆåÊàêÂæóÂà∞kÊ£µÊ†ëÔºåÊàë‰ª¨Ë¶ÅÈ¢ÑÊµã‰∏Ä‰∏™Ê†∑Êú¨ÁöÑÂàÜÊï∞ÔºåÂÖ∂ÂÆûÂ∞±ÊòØÊ†πÊçÆËøô‰∏™Ê†∑Êú¨ÁöÑÁâπÂæÅÔºåÂú®ÊØèÊ£µÊ†ë‰∏≠‰ºöËêΩÂà∞ÂØπÂ∫îÁöÑ‰∏Ä‰∏™Âè∂Â≠êËäÇÁÇπÔºåÊØè‰∏™Âè∂Â≠êËäÇÁÇπÂ∞±ÂØπÂ∫î‰∏Ä‰∏™ÂàÜÊï∞ÔºåÊúÄÂêéÂè™ÈúÄË¶ÅÂ∞ÜÊØèÊ£µÊ†ëÂØπÂ∫îÁöÑÂàÜÊï∞Âä†Ëµ∑Êù•Â∞±ÊòØËØ•Ê†∑Êú¨ÁöÑÈ¢ÑÊµãÂÄº„ÄÇ boosting: https://zhuanlan.zhihu.com/p/38329631 Xgboost Â∞±ÊòØÂõûÂΩíÊ†ëÁöÑÈõÜÊàê https://www.csuldw.com/2019/07/20/2019-07-20-xgboost-theory/ https://blog.csdn.net/github_38414650/article/details/76061893?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.compare https://blog.csdn.net/qq_24519677/article/details/81809157 ÊúâÁ©∫ÂÜçÊé®ÂØº‰∫Ü Ë∞ÉÁî®Â∫ìPython Êèê‰æõ‰∫Ü‰∏§ÁßçÂ∫ì xgboost xgboost sklearnÊé•Âè£ Êê≠Âª∫Ê®°Âûã ÂèÇÊï∞ËÆæÁΩÆ GridSearchCV Ë∞ÉÂèÇ(ÁΩëÊ†ºÊ≥ï) Ë∞ÉÂèÇÊ≠•È™§ÔºåÂèÇÊï∞ËåÉÂõ¥ https://blog.csdn.net/han_xiaoyang/article/details/52665396 12345678import xgboost as xgbfrom xgboost import XGBRegressorfrom sklearn.metrics import mean_absolute_error,make_scorerfrom sklearn.grid_search import GridSearchCVfrom sklearn.cross_validation import KFold, train_test_splitfrom sklearn.datasets import load_boston https://blog.csdn.net/s09094031/article/details/94871596?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.compare 1sklearn.model_selection.``train_test_split test_size train_sizeÔºö ‚Äã ‰∏âÁßçÁ±ªÂûã„ÄÇfloatÔºåintÔºåNone„ÄÇ floatÔºö0.0-1.0‰πãÈó¥Ôºå‰ª£Ë°®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂç†ÊÄªÊï∞ÊçÆÈõÜÁöÑÊØî‰æã„ÄÇ intÔºö‰ª£Ë°®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂÖ∑‰ΩìÁöÑÊ†∑Êú¨Êï∞Èáè„ÄÇ NoneÔºöËÆæÁΩÆ‰∏∫test_sizeÁöÑË°•„ÄÇ defaultÔºöÈªòËÆ§‰∏∫None„ÄÇ random_stateÔºö‰∏âÁßçÁ±ªÂûã„ÄÇintÔºårandomstate instanceÔºåNone„ÄÇ intÔºöÊòØÈöèÊú∫Êï∞ÁîüÊàêÂô®ÁöÑÁßçÂ≠ê„ÄÇÊØèÊ¨°ÂàÜÈÖçÁöÑÊï∞ÊçÆÁõ∏Âêå„ÄÇ randomstateÔºörandom_stateÊòØÈöèÊú∫Êï∞ÁîüÊàêÂô®ÁöÑÁßçÂ≠ê„ÄÇÔºàËøôÈáåÊ≤°Â§™ÁêÜËß£Ôºâ NoneÔºöÈöèÊú∫Êï∞ÁîüÊàêÂô®ÊòØ‰ΩøÁî®‰∫Ünp.randomÁöÑrandomstate„ÄÇ ÁßçÂ≠êÁõ∏ÂêåÔºå‰∫ßÁîüÁöÑÈöèÊú∫Êï∞Â∞±Áõ∏Âêå„ÄÇÁßçÂ≠ê‰∏çÂêåÔºåÂç≥‰ΩøÊòØ‰∏çÂêåÁöÑÂÆû‰æãÔºå‰∫ßÁîüÁöÑÁßçÂ≠ê‰πü‰∏çÁõ∏Âêå„ÄÇ shuffleÔºöÂ∏ÉÂ∞îÂÄºÔºåÂèØÈÄâÂèÇÊï∞„ÄÇÈªòËÆ§ÊòØNone„ÄÇÂú®ÂàíÂàÜÊï∞ÊçÆ‰πãÂâçÂÖàÊâì‰π±Êï∞ÊçÆ„ÄÇÂ¶ÇÊûúshuffle=FALSEÔºåÂàôstratifyÂøÖÈ°ªÊòØNone„ÄÇ stratifyÔºöarray-likeÊàñËÄÖNoneÔºåÈªòËÆ§ÊòØNone„ÄÇÂ¶ÇÊûú‰∏çÊòØNoneÔºåÂ∞Ü‰ºöÂà©Áî®Êï∞ÊçÆÁöÑÊ†áÁ≠æÂ∞ÜÊï∞ÊçÆÂàÜÂ±ÇÂàíÂàÜ„ÄÇ Ëã•‰∏∫NoneÊó∂ÔºåÂàíÂàÜÂá∫Êù•ÁöÑÊµãËØïÈõÜÊàñËÆ≠ÁªÉÈõÜ‰∏≠ÔºåÂÖ∂Á±ªÊ†áÁ≠æÁöÑÊØî‰æã‰πüÊòØÈöèÊú∫ÁöÑ„ÄÇ Ëã•‰∏ç‰∏∫NoneÊó∂ÔºåÂàíÂàÜÂá∫Êù•ÁöÑÊµãËØïÈõÜÊàñËÆ≠ÁªÉÈõÜ‰∏≠ÔºåÂÖ∂Á±ªÊ†áÁ≠æÁöÑÊØî‰æãÂêåËæìÂÖ•ÁöÑÊï∞ÁªÑ‰∏≠Á±ªÊ†áÁ≠æÁöÑÊØî‰æãÁõ∏ÂêåÔºåÂèØ‰ª•Áî®‰∫éÂ§ÑÁêÜ‰∏çÂùáË°°ÁöÑÊï∞ÊçÆÈõÜ„ÄÇ x_train, y_train, x_test, y_test = train_test_split(x, y, test_size=0.23, random_state=2) https://blog.csdn.net/qq_43288098/article/details/105407204?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4.compare ÂèÇÊï∞ÔºöÂàÜÂºÄË∞É https://blog.csdn.net/zc02051126/article/details/46711047 https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn Ê®°Âûã‰øùÂ≠òhttps://www.fatrabbids.com/2018/10/19/xgboost%e7%9a%84%e4%bf%9d%e5%ad%98%e6%a8%a1%e5%9e%8b%e3%80%81%e5%8a%a0%e8%bd%bd%e6%a8%a1%e5%9e%8b%e3%80%81%e7%bb%a7%e7%bb%ad%e8%ae%ad%e7%bb%83/#more-235]]></content>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F06%2F28%2F%E5%AF%86%E7%A0%81%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[Day1Áé∞‰ª£‰ø°ÊÅØÂÆâÂÖ®ÁöÑÂü∫Êú¨Ë¶ÅÊ±ÇÔºö ‰ø°ÊÅØÁöÑ‰øùÂØÜÊÄß ConfidentialityÔºöÈò≤Ê≠¢‰ø°ÊÅØÊ≥ÑÊºèÁªôÊú™ÁªèÊéàÊùÉÁöÑ‰∫∫ÔºàÂä†ÂØÜËß£ÂØÜÊäÄÊúØÔºâÊú∫ÂØÜÊÄß ‰ø°ÊÅØÁöÑÂÆåÊï¥ÊÄß IntegrityÔºöÈò≤Ê≠¢‰ø°ÊÅØË¢´Êú™ÁªèÊéàÊùÉÁöÑÁØ°ÊîπÔºàÊ∂àÊÅØËÆ§ËØÅÁ†ÅÔºåÊï∞Â≠óÁ≠æÂêçÔºâ ËÆ§ËØÅÊÄß AuthenticationÔºö‰øùËØÅ‰ø°ÊÅØÊù•Ëá™Ê≠£Á°ÆÁöÑÂèëÈÄÅËÄÖÔºàÊ∂àÊÅØËÆ§ËØÅÁ†ÅÔºåÊï∞Â≠óÁ≠æÂêçÔºâËÆ§‰∏∫ ÂÖ∂‰ªñ ‰∏çÂèØÂê¶ËÆ§ÊÄß Non-repudiationÔºö‰øùËØÅÂèëÈÄÅËÄÖ‰∏çËÉΩÂê¶ËÆ§‰ªñ‰ª¨Â∑≤ÂèëÈÄÅÁöÑÊ∂àÊÅØÔºàÊï∞Â≠óÁ≠æÂêçÔºâ Á¨¨‰∏ÄÁ´† ÂºïË®ÄÊ∂âÂèäÁöÑÁü•ËØÜÁÇπÂåÖÊã¨‰ø°ÊÅØÂÆâÂÖ®ÁöÑË¶ÅÊ±ÇÔºà‰∏ªË¶ÅÂõõ‰∏™ÊñπÈù¢ÔºâÔºåÂØÜÁ†ÅÂ≠¶Âü∫Êú¨Ê¶ÇÂøµÔºåÂÆâÂÖ®ÁöÑÂÆö‰πâÔºåÂØÜÁ†ÅÁÆóÊ≥ïÁöÑËÆæËÆ°Ë¶ÅÊ±ÇÔºåÂè§ÂÖ∏ÂØÜÁ†ÅÔºàÊõøÊç¢Ôºå‰ª£ÊõøÔºâ oÂØÜÁ†ÅÂ≠¶Âü∫Êú¨Ê¶ÇÂøµ**,**Â¶ÇÂØÜÁ†ÅÁºñÁ†ÅÂ≠¶„ÄÅÂØÜÁ†ÅÂàÜÊûêÂ≠¶„ÄÅÊòéÊñá„ÄÅÂØÜÊñá„ÄÅÂä†ÂØÜ„ÄÅËß£ÂØÜ oÂØπÁß∞ÂØÜÁ†Å‰ΩìÂà∂ÂíåÈùûÂØπÁß∞ÂØÜÁ†Å‰ΩìÂà∂ oÂè§ÂÖ∏ÂØÜÁ†Å‰ΩìÂà∂ÔºåÂ¶ÇÁΩÆÊç¢ÂØÜÁ†Å„ÄÅÂçïË°®‰ª£Êç¢ÂØÜÁ†Å„ÄÅÂ§öË°®‰ª£Êç¢ÂØÜÁ†ÅÔºàË¶Å‰ºöËÆ°ÁÆóÔºâ Áé∞‰ª£‰ø°ÊÅØÂÆâÂÖ®ÁöÑÂü∫Êú¨Ë¶ÅÊ±ÇÔºö ‰ø°ÊÅØÁöÑ‰øùÂØÜÊÄß ConfidentialityÔºöÈò≤Ê≠¢‰ø°ÊÅØÊ≥ÑÊºèÁªôÊú™ÁªèÊéàÊùÉÁöÑ‰∫∫ÔºàÂä†ÂØÜËß£ÂØÜÊäÄÊúØÔºâ ‰ø°ÊÅØÁöÑÂÆåÊï¥ÊÄß IntegrityÔºöÈò≤Ê≠¢‰ø°ÊÅØË¢´Êú™ÁªèÊéàÊùÉÁöÑÁØ°ÊîπÔºàÊ∂àÊÅØËÆ§ËØÅÁ†ÅÔºåÊï∞Â≠óÁ≠æÂêçÔºâ ËÆ§ËØÅÊÄß AuthenticationÔºö‰øùËØÅ‰ø°ÊÅØÊù•Ëá™Ê≠£Á°ÆÁöÑÂèëÈÄÅËÄÖÔºàÊ∂àÊÅØËÆ§ËØÅÁ†ÅÔºåÊï∞Â≠óÁ≠æÂêçÔºâ ‰∏çÂèØÂê¶ËÆ§ÊÄß Non-repudiationÔºö‰øùËØÅÂèëÈÄÅËÄÖ‰∏çËÉΩÂê¶ËÆ§‰ªñ‰ª¨Â∑≤ÂèëÈÄÅÁöÑÊ∂àÊÅØÔºàÊï∞Â≠óÁ≠æÂêçÔºâ http://yuqiangcoder.com/2019/10/07/%E5%AF%86%E7%A0%81%E5%AD%A6%E6%A6%82%E8%BF%B0.html ÂØÜÁ†ÅÂ≠¶Â∞±ÊòØË¶ÅÈÄöËøáÁÆóÊ≥ïÂíåÂçèËÆÆÂÆûÁé∞Áõ∏Â∫îÁöÑÂäüËÉΩ ÂáØÊííÂØÜÁ†ÅÔºöÁßªÂä®2‰ΩçÔºåH K ÊÅ∫ÊííÂØÜÁ†Å ÂÆâÂÖ®pÊó†Êù°‰ª∂ÂÆâÂÖ®ÁöÑ(‰∏çÂèØÁ†¥ËØëÁöÑ)Ôºö pÊó†ËÆ∫Êà™Ëé∑Â§öÂ∞ëÂØÜÊñáÔºåÈÉΩÊ≤°ÊúâË∂≥Â§ü‰ø°ÊÅØÊù•ÂîØ‰∏ÄÁ°ÆÂÆöÊòéÊñáÔºåÂàôËØ•ÂØÜÁ†ÅÊòØÊó†Êù°‰ª∂ÂÆâÂÖ®ÁöÑÔºåÂç≥ÂØπÁÆóÊ≥ïÁöÑÁ†¥ËØë‰∏çÊØîÁåúÊµãÊúâ‰ºòÂäø pËÆ°ÁÆó‰∏äÂÆâÂÖ®ÁöÑÔºö p‰ΩøÁî®ÊúâÊïàËµÑÊ∫êÂØπ‰∏Ä‰∏™ÂØÜÁ†ÅÁ≥ªÁªüËøõË°åÂàÜÊûêËÄåÊú™ËÉΩÁ†¥ËØëÔºåÂàôËØ•ÂØÜÁ†ÅÊòØÂº∫ÁöÑÊàñËÆ°ÁÆó‰∏äÂÆâÂÖ®ÁöÑ ÂØÜÁ†ÅÁÆóÊ≥ïË¶ÅÊ±ÇÂØÜÁ†ÅÁÆóÊ≥ïÂè™Ë¶ÅÊª°Ë∂≥‰ª•‰∏ã‰∏§Êù°ÂáÜÂàô‰πã‰∏ÄÂ∞±Ë°åÔºö Ôºà1Ôºâ Á†¥ËØëÂØÜÊñáÁöÑ‰ª£‰ª∑Ë∂ÖËøáË¢´Âä†ÂØÜ‰ø°ÊÅØÁöÑ‰ª∑ÂÄº„ÄÇ Ôºà2 ) Á†¥ËØëÂØÜÊñáÊâÄËä±ÁöÑÊó∂Èó¥Ë∂ÖËøá‰ø°ÊÅØÁöÑÊúâÁî®Êúü„ÄÇ Êª°Ë∂≥‰ª•‰∏ä‰∏§‰∏™ÂáÜÂàôÁöÑÂØÜÁ†ÅÁÆóÊ≥ïÂú®ÂÆûÈôÖ‰∏≠ÊòØÂèØÁî®ÁöÑ„ÄÇ ÂçïË°®‰ª£ÊõøÂØÜÁ†ÅÂçïË°®‰ª£ÊõøÂØÜÁ†ÅÂèØÂàÜ‰∏∫ ‚Ä¢ Âä†Ê≥ïÂØÜÁ†Å ‚Ä¢ ‰πòÊ≥ïÂØÜÁ†Å ‚Ä¢ ‰ªøÂ∞ÑÂØÜÁ†Å Âè§ÂÖ∏ÂØÜÁ†ÅÁΩÆÊç¢ÂØÜÁ†Å ÂçïË°®‰ª£ÊõøÂØÜÁ†ÅÁÆóÊ≥ï Â§öË°®‰ª£ÊõøÂØÜÁ†ÅÁÆóÊ≥ï Á¨¨‰∫åÁ´† ÊµÅÂØÜÁ†Å‰∏ÄÊ¨°‰∏ÄÂØÜÔºåÊµÅÂØÜÁ†ÅÔºåÂØÜÈí•ÊµÅ‰∏â‰∏™Ê¶ÇÂøµ„ÄÇ oÊµÅÂØÜÁ†ÅÂü∫Êú¨Ê¶ÇÂøµ„ÄÅÁâπÁÇπ oÁ∫øÊÄßÂèçÈ¶àÁßª‰ΩçÂØÑÂ≠òÂô® oRC4 ‰∏ÄÊ¨°‰∏ÄÂØÜÔºàÁêÜÊÉ≥Ôºâ ‚Ä¢‰ºòÁÇπÔºö ‚Ä¢ÂØÜÈí•ÈöèÊú∫‰∫ßÁîüÔºå‰ªÖ‰ΩøÁî®‰∏ÄÊ¨° ‚Ä¢Êó†Êù°‰ª∂ÂÆâÂÖ® ‚Ä¢Âä†ÂØÜÂíåËß£ÂØÜ‰∏∫Âä†Ê≥ïËøêÁÆóÔºåÊïàÁéáËæÉÈ´ò ‚Ä¢Áº∫ÁÇπÔºö ‚Ä¢ÂØÜÈí•ÈïøÂ∫¶Ëá≥Â∞ë‰∏éÊòéÊñáÈïøÂ∫¶‰∏ÄÊ†∑ÈïøÔºåÂØÜÈí•ÂÖ±‰∫´Âõ∞ÈöæÔºå‰∏çÂ§™ÂÆûÁî® ÊµÅÂØÜÁ†ÅÂØÜÁ†Å‰ΩìÂà∂ Â∫èÂàóÂØÜÁ†Å ‚Ä¢ÊµÅÂØÜÁ†ÅÁöÑÂü∫Êú¨ÊÄùÊÉ≥ ‚Ä¢Âà©Áî®ÂØÜÈí•k‰∫ßÁîü‰∏Ä‰∏™ÂØÜÈí•ÊµÅ ‚Ä¢ÂØÜÈí•ÊµÅ ‚Ä¢Áî±ÂØÜÈí•ÊµÅÂèëÁîüÂô® f ‰∫ßÁîüÔºö √òÂÜÖÈÉ®ËÆ∞ÂøÜÂÖÉ‰ª∂ÁöÑÁä∂ÊÄÅœÉiÁã¨Á´ã‰∫éÊòéÊñáÂ≠óÁ¨¶ÁöÑÂè´ÂÅöÂêåÊ≠•ÊµÅÂØÜÁ†ÅÔºåÂê¶ÂàôÂè´ÂÅöËá™ÂêåÊ≠•ÊµÅÂØÜÁ†Å„ÄÇ ÂØÜÁ†ÅÂàÜÊûêÂ≠¶ÁöÑÁõÆÊ†áÂú®‰∫éÁ†¥ËØëÔºà BC Ôºâ A. ÊòéÊñá B. ÂØÜÊñá C. ÂØÜÈí• D. ÁÆóÊ≥ïÁªìÊûÑ ‰øùÂØÜÈÄö‰ø°Á≥ªÁªüÁöÑÂÆâÂÖ®Â®ÅËÉÅ ‰øùÂØÜÈÄö‰ø°ÁöÑÂÆâÂÖ®Â®ÅËÉÅÔºö Ë¢´Âä®ÊîªÂáªÔºöÁ™ÉÂê¨ÔºåÂóÖÊé¢ÊµÅÈáèÂàÜÊûêÁ≠âÔºå‰∏ªË¶ÅÊòØÁ†¥ÂùèÊ∂àÊÅØÁöÑÊú∫ÂØÜÊÄßÔºõ ‰∏ªÂä®ÊîªÂáªÔºö‰∏≠Êñ≠ÔºåÁØ°ÊîπÔºåÂÅáÂÜíÁ≠â„ÄÇ ‰∏≠Êñ≠Á†¥Âùè‰∫Ü‰ø°ÊÅØÁöÑÂèØÁî®ÊÄß ÁØ°ÊîπÁ†¥Âùè‰∫Ü‰ø°ÊÅØÁöÑÂÆåÊï¥ÊÄß ÂÅáÂÜíÁ†¥Âùè‰∫ÜÁúüÂÆûÊÄßÔºàËÆ§ËØÅÔºâ ÊâÄ‰ª•‰øùÂØÜÈÄö‰ø°Á≥ªÁªüÁöÑÂÆâÂÖ®ÈúÄÊ±ÇÊúâÔºö Êú∫ÂØÜÊÄß‚Äî‚ÄîÈááÁî®Âä†ÂØÜÊú∫Âà∂ ÂÆåÊï¥ÊÄß‚Äî‚ÄîÈááÁî®ÂÆåÊï¥ÊÄßÈ™åËØÅÊú∫Âà∂ÔºåÂ¶ÇHashÂáΩÊï∞ÔºåÊ∂àÊÅØËÆ§ËØÅÁ†Å ÁúüÂÆûÊÄß‚Äî‚ÄîÈááÁî®ËÆ§ËØÅÊú∫Âà∂ÔºåÂ¶ÇÊï∞Â≠óÁ≠æÂêçÔºåËÆ§ËØÅÂçèËÆÆ ‰∏≠Êñ≠‚Äî‚ÄîÁî®ÂØÜÁ†ÅÂ≠¶ÁöÑÊäÄÊúØÊ≤°ÊúâÂ§™Â•ΩÁöÑÂäûÊ≥ïÔºàËøôÊòØÊàë‰∏™‰∫∫ÁöÑÁêÜËß£Ôºâ Âè§ÂÖ∏ÂØÜÁ†ÅÂ≠¶ ÁΩÆÊç¢ÂØÜÁ†ÅÔºöÂèàÁß∞Êç¢‰ΩçÂØÜÁ†ÅÔºåÂä†ÂØÜËøáÁ®ã‰∏≠ÊòéÊñáÁöÑÂ≠óÊØç‰øùÊåÅÁõ∏ÂêåÔºå‰ΩÜÊòØÈ°∫Â∫èË¢´Êâì‰π±„ÄÇÂè™Ë¶ÅÊää‰ΩçÁΩÆÊÅ¢Â§çÔºåÂ∞±ËÉΩÂæóÂà∞ÊòéÊñá„ÄÇ ‰ª£Êç¢ÂØÜÁ†ÅÔºöÊòéÊñá‰∏≠ÁöÑÊØè‰∏Ä‰∏™Â≠óÁ¨¶Ë¢´ÊõøÊç¢ÊàêÂØÜÊñá‰∏≠ÁöÑÂè¶‰∏Ä‰∏™Â≠óÁ¨¶„ÄÇÊé•Êî∂ËÄÖÂØπÂØÜÊñáÂÅöÂèçÂêëÊõøÊç¢Â∞±ÂèØ‰ª•ÊÅ¢Â§çÊòéÊñá„ÄÇ Â§öÂêçÊàñÂêåÈü≥‰ª£ÊõøÂØÜÁ†Å Â§öÂ≠óÊØç‰ª£ÊõøÂØÜÁ†Å Â§öË°®‰ª£ÊõøÂØÜÁ†Å ÊÄªÁªìÂè§ÂÖ∏ÂØÜÁ†ÅÂ≠¶ÁöÑÁâπÁÇπÔºöÂä†ÂØÜÂØπË±°ÔºõÊñπÊ≥ïÔºõ‰øùÂØÜÂÜÖÂÆπÔºõÁ†¥Ëß£Ôºõ ËÆ°ÁÆóÂº∫Â∫¶Â∞è Âá∫Áé∞Âú® DES ‰πãÂâç Êï∞ÊçÆÂÆâÂÖ®Âü∫‰∫éÁÆóÊ≥ïÁöÑ‰øùÂØÜ„ÄÇËøôÂíåÁé∞‰ª£ÂØÜÁ†ÅÊúâÂæàÂ§ßÁöÑÂ∑ÆË∑ùÔºåÂè™Ë¶ÅÁü•ÈÅìÂä†ÂØÜÊñπÊ≥ïÔºåÂ∞±ËÉΩËΩªÊòìÁöÑËé∑ÂèñÊòéÊñá„ÄÇÁé∞‰ª£ÁöÑÂØÜÁ†ÅÂü∫‰∫éÁßòÈí•ÁöÑÂä†ÂØÜÔºåÁÆóÊ≥ïÈÉΩÊòØÂÖ¨ÂºÄÁöÑÔºåËÄå‰∏îÂÖ¨ÂºÄÁöÑÂØÜÁ†ÅÁÆóÊ≥ïÂÆâÂÖ®ÊÄßÊõ¥È´òÔºåËÉΩË¢´Êõ¥Â§ö‰∫∫ËØÑËÆ∫Âíå‰ΩøÁî®ÔºåÂä†Âº∫ÊºèÊ¥ûÁöÑ‰øÆË°•„ÄÇ ‰ª•Â≠óÊØçË°®‰∏∫‰∏ªË¶ÅÂä†ÂØÜÂØπË±°„ÄÇÂè§ÂÖ∏ÂØÜÁ†ÅÂ§ßÂ§öÊï∞ÊòØÂØπÊúâÊÑè‰πâÁöÑÊñáÂ≠óËøõË°åÂä†ÂØÜÔºåËÄåÁé∞‰ª£ÂØÜÁ†ÅÊòØÂØπÊØîÁâπÂ∫èÂàóËøõË°åÂä†ÂØÜ„ÄÇËøô‰πüÊòØÁé∞‰ª£ÂØÜÁ†ÅÂíåÂè§ÂÖ∏ÂØÜÁ†ÅÁöÑÂå∫Âà´ÔºåËÄå‰∏îÂè§ÂÖ∏ÂØÜÁ†ÅÁöÑÂàÜÊûêÊñπÊ≥ï‰πüÊòØÁî®Â≠óÊØçÈ¢ëÁéáÂàÜÊûêË°®Êù•Á†¥Ëß£ÁöÑ„ÄÇ ÊõøÊç¢ÂíåÁΩÆÊç¢ÊäÄÊúØ ÂØÜÁ†ÅÂàÜÊûêÊñπÊ≥ïÂü∫‰∫éÂ≠óÊØç‰∏éÂ≠óÊØçÁªÑÂêàÁöÑÈ¢ëÁéáÁâπÊÄß‰ª•ÂèäÊòéÊñáÁöÑÂèØËØªÊÄß Áé∞‰ª£ÂØÜÁ†ÅÂ≠¶ 1976ÔºöÁî± Diffie Âíå Hellman Âú®„Ää ÂØÜÁ†ÅÂ≠¶ÁöÑÊñ∞ÊñπÂêë„ÄãÔºà„ÄäNew Directions in Cryptography„ÄãÔºâÊèêÂá∫‰∫ÜÂÖ¨Èí•ÂØÜÁ†ÅÂ≠¶‰ΩìÂà∂ÁöÑÊÄùÊÉ≥ 1977Âπ¥ÔºöÁæéÂõΩÂõΩÂÆ∂Ê†áÂáÜÂ±ÄÈ¢ÅÂ∏ÉÊï∞ÊçÆÂä†ÂØÜÊ†áÂáÜ DESÔºàData Encryption StandardÔºâ 1978Âπ¥ÔºöÁ¨¨‰∏Ä‰∏™ÂÖ¨Èí•ÁÆóÊ≥ï RSA ÁÆóÊ≥ïÔºàÁî± Ron Rivest„ÄÅAdi Shamir Âíå Leonard Adleman ÁöÑÂßìÊ∞èÈ¶ñÂ≠óÊØçÁªÑÊàêÔºâ Áé∞‰ª£ÂØÜÁ†ÅÂ≠¶‰∏ªË¶ÅÊúâ‰∏â‰∏™ÊñπÂêëÔºöÁßÅÈí•ÂØÜÁ†ÅÔºàÂØπÁß∞ÂØÜÁ†ÅÔºâ„ÄÅÂÖ¨Èí•ÂØÜÁ†ÅÔºàÈùûÂØπÁß∞ÂØÜÁ†ÅÔºâ„ÄÅÂÆâÂÖ®ÂçèËÆÆ„ÄÇ ÁßÅÈí•ÂØÜÁ†Å‰πüÁß∞ÂØπÁß∞ÂØÜÁ†ÅÔºåÊòØÂØπÊñáÂ≠óÁöÑÂä†ÂØÜËΩ¨Êç¢ÊàêÂØπÊØîÁâπÂ∫èÂàóÁöÑÂä†ÂØÜÔºàÁõ∏ÂØπ‰∫éÂè§ÂÖ∏ÂØÜÁ†ÅÔºâÔºåÁî®Âêå‰∏Ä‰∏™ÂØÜÈí•ËøõË°åÂä†ÂØÜÂíåËß£ÂØÜÊìç‰ΩúÔºåËøô‰∏™ÂØÜÈí•ÂèëÈÄÅÊñπÂíåÊé•Êî∂ÊñπÈÉΩÊòØË¶Å‰øùÂØÜÁöÑÔºåÊâÄ‰ª•Áß∞‰∏∫ÁßÅÈí•ÂØÜÁ†Å„ÄÇÂÆÉÁöÑ‰∏§‰∏™Âü∫Êú¨Êìç‰ΩúÂ∞±ÊòØ‰ª£Êç¢ÂíåÁΩÆÊç¢Â∞±ÊòØÊù•Ê∫ê‰∫éÂè§ÂÖ∏ÂØÜÁ†ÅÂ≠¶ÁöÑ„ÄÇ ÂØπÁß∞ÂØÜÁ†ÅÊúâ‰∏§‰∏™ËÆæËÆ°ÂéüÂàôÔºå‰∏Ä‰∏™ÊòØÊâ©Êï£ÔºàDiffusionÔºâÔºöÊòéÊñáÁöÑÁªüËÆ°ÁªìÊûÑË¢´Êâ©Êï£Ê∂àÂ§±Âà∞ÂØÜÊñáÁöÑÈïøÁ®ãÁªüËÆ°ÁâπÊÄßÔºå‰ΩøÂæóÊòéÊñáÂíåÂØÜÊñá‰πãÈó¥ÁöÑÁªüËÆ°ÂÖ≥Á≥ªÂ∞ΩÈáèÂ§çÊùÇ„ÄÇ Âè¶‰∏Ä‰∏™ÊòØÊ∑∑‰π±ÔºàconfusionÔºâÔºö‰ΩøÂæóÂØÜÊñáÁöÑÁªüËÆ°ÁâπÊÄß‰∏éÂØÜÈí•ÁöÑÂèñÂÄº‰πãÈó¥ÁöÑÂÖ≥Á≥ªÂ∞ΩÈáèÂ§çÊùÇ„ÄÇ ÂØπÁß∞ÂØÜÁ†ÅÁöÑ‰ª£Ë°®Êúâ DES ÁÆóÊ≥ïÂíå AES ÁÆóÊ≥ïÔºå ÂÖ¨Èí•ÂØÜÁ†Å DH ÂØÜÈí•‰∫§Êç¢ÂçèËÆÆ RSA ÁÆóÊ≥ïÊòØÁ¨¨‰∏Ä‰∏™ÂÖ¨Èí•ÂØÜÁ†ÅÁÆóÊ≥ïÔºå‰πüÊòØÁ¨¨‰∏Ä‰∏™Êï∞Â≠óÁ≠æÂêçÁÆóÊ≥ï„ÄÇ p q pi(n) =(p-1)(q-1);‰∏én‰∫íË¥®ÁöÑ‰π¶&lt;=n ÈÄâe ‰∏épi(n)ÊúÄÂ§ßÂÖ¨Á∫¶Êï∞1Ôºå‰∫íË¥®Ôºå ÊâædÔºåe*d/pi(n)=1 (n,e)ÂÖ±Ôºàn,d)ÁßÅÈí• a^emod n =b b^d mod n=c Ê†πÊçÆ‰ª•‰∏äÂØÜÈí•ÂØπÁöÑÁîüÊàêËøáÁ®ãÔºö Â¶ÇÊûúÊÉ≥Áü•ÈÅì d ÈúÄË¶ÅÁü•ÈÅìÊ¨ßÊãâÂáΩÊï∞ œÜ(n) Â¶ÇÊûúÊÉ≥Áü•ÈÅìÊ¨ßÊãâÂáΩÊï∞ œÜ(n) ÈúÄË¶ÅÁü•ÈÅì P Âíå Q Ë¶ÅÁü•ÈÅì P Âíå Q ÈúÄË¶ÅÂØπ n ËøõË°åÂõ†Êï∞ÂàÜËß£„ÄÇ ÂØπ‰∫éÊú¨‰æã‰∏≠ÁöÑ 4757 ‰Ω†ÂèØ‰ª•ËΩªÊùæËøõË°åÂõ†Êï∞ÂàÜËß£Ôºå‰ΩÜÂØπ‰∫éÂ§ßÊï¥Êï∞ÁöÑÂõ†Êï∞ÂàÜËß£ÔºåÊòØ‰∏Ä‰ª∂ÂæàÂõ∞ÈöæÁöÑ‰∫ãÊÉÖÔºåÁõÆÂâçÈô§‰∫ÜÊö¥ÂäõÁ†¥Ëß£ÔºåËøòÊ≤°ÊúâÊõ¥Â•ΩÁöÑÂäûÊ≥ïÔºåÂ¶ÇÊûú‰ª•ÁõÆÂâçÁöÑËÆ°ÁÆóÈÄüÂ∫¶ÔºåÁ†¥Ëß£ÈúÄË¶Å50Âπ¥‰ª•‰∏äÔºåÂàôËøô‰∏™ÁÆóÊ≥ïÂ∞±ÊòØÂÆâÂÖ®ÁöÑ Ê§≠ÂúÜÊõ≤Á∫øÂä†ÂØÜÁÆóÊ≥ïÔºåÁÆÄÁß∞ECCÔºåÊòØÂü∫‰∫éÊ§≠ÂúÜÊõ≤Á∫øÊï∞Â≠¶ÁêÜËÆ∫ÂÆûÁé∞ÁöÑ‰∏ÄÁßçÈùûÂØπÁß∞Âä†ÂØÜÁÆóÊ≥ï„ÄÇÁõ∏ÊØîRSAÔºåECC‰ºòÂäøÊòØÂèØ‰ª•‰ΩøÁî®Êõ¥Áü≠ÁöÑÂØÜÈí•ÔºåÊù•ÂÆûÁé∞‰∏éRSAÁõ∏ÂΩìÊàñÊõ¥È´òÁöÑÂÆâÂÖ®ÔºåRSAÂä†ÂØÜÁÆóÊ≥ï‰πüÊòØ‰∏ÄÁßçÈùûÂØπÁß∞Âä†ÂØÜÁÆóÊ≥ï ÈáçÂêà Âõõ„ÄÅÂêå‰ΩôËøêÁÆóÂêå‰ΩôÂ∞±ÊòØÊúâÁõ∏ÂêåÁöÑ‰ΩôÊï∞Ôºå‰∏§‰∏™Êï¥Êï∞ a„ÄÅ bÔºåËã•ÂÆÉ‰ª¨Èô§‰ª•Ê≠£Êï¥Êï∞ mÊâÄÂæóÁöÑ‰ΩôÊï∞Áõ∏Á≠âÔºåÂàôÁß∞ aÔºå bÂØπ‰∫éÊ®°mÂêå‰Ωô„ÄÇ ‰πòÊ≥ïÈÄÜÂÖÉÔºõ ÂÖ≠„ÄÅ‰πòÊ≥ïÈÄÜÂÖÉÂú®Ê®°7‰πòÊ≥ï‰∏≠Ôºö 1ÁöÑÈÄÜÂÖÉ‰∏∫1 (1*1)%7=1 2ÁöÑÈÄÜÂÖÉ‰∏∫4 (2*4)%7=1 3ÁöÑÈÄÜÂÖÉ‰∏∫5 (3*5)%7=1 4ÁöÑÈÄÜÂÖÉ‰∏∫2 (4*2)%7=1 5ÁöÑÈÄÜÂÖÉ‰∏∫3 (5*3)%7=1 6ÁöÑÈÄÜÂÖÉ‰∏∫6 (6*6)%7=1 https://zhuanlan.zhihu.com/p/101907402 Day 2 Áé∞‰ª£ÂØÜÁ†Å‰∏ÄÊ¨°ÊÄßÂØÜÁ†ÅFrank Miller Âú®1882 Âπ¥ÊèêÂá∫‰∫Ü‰∏ÄÊ¨°ÊÄßÂØÜÁ†ÅÔºàOne-time padÔºâÁöÑÊ¶ÇÂøµ‚Äî‚ÄîÂä†ÂØÜÔºöÂ∞ÜÊ∂àÊÅØÂíåÁßÅÈí•ËøõË°åÂºÇÊàñËøêÁÆóÂæóÂà∞ÂØÜÊñáÔºõËß£ÂØÜÔºöÂ∞ÜÂØÜÈí•ÂíåÂØÜÊñáËøõË°åÂºÇÊàñËøêÁÆóÂæóÂà∞ÂéüÊ∂àÊÅØÔºåËøô‰∏™ËøáÁ®ãÁ±ª‰ºº‰∫éÂâçÈù¢ÊèêÂà∞ÁöÑ a ‚äï b ‚äï a = b „ÄÇ‰∏ÄÊ¨°ÊÄßÂØÜÁ†ÅÁöÑÂÆö‰πâÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö Êó†Êù°‰ª∂ÂÆâÂÖ® ÂØÜÈí•ÈöèÊú∫‰∫ßÁîüÁöÑÔºåÂè™ËÉΩÁî®‰∏ÄÊ¨° ÂºÇÊàñ 1+1 =0 Ôºå0+1 = 1 ÂÖ±‰∫´ÂØÜÈí•Èöæ ÊµÅÂØÜÁ†Å‰ΩìÂà∂ÂØÜÈí•k,‰∫ßÁîüÂØÜÈí•ÊµÅÔºàÂèë ÂêåÊ≠•ÊµÅÂØÜÁ†ÅÔºàÁä∂ÊÄÅÊó†ÂÖâÔºâ ‰∏Ä.Âä†ÂØÜÊñπÊ≥ïÁöÑÂàÜÁ±ªÔºöÊåâÁÖß‰∏çÂêåÁöÑÊ†áÂáÜÊúâ‰∏çÂêåÁöÑÂàÜÁ±ªÊ†áÂáÜÔºö1.ÊåâÁÖßÂØÜÈí•ÁöÑÁâπÂæÅ‰∏çÂêåÔºåÂèØ‰ª•ÂàÜ‰∏∫ÂØπÁß∞ÂØÜÁ†Å‰∏éÈùûÂØπÁß∞ÂØÜÁ†Å„ÄÇ2.ÊåâÁÖßÂä†ÂØÜÊñπÂºèÁöÑ‰∏çÂêåÔºåÂèØ‰ª•ÂàÜ‰∏∫ÊµÅÂØÜÁ†ÅÂíåÂàÜÁªÑÂØÜÁ†Å„ÄÇ3.ÈùûÂØπÁß∞ÂØÜÁ†ÅÂùáÂ±û‰∫éÂàÜÁªÑÂØÜÁ†Å„ÄÇ 1.ÊµÅÂØÜÁ†Å„ÄÇÂèàÂêçÂ∫èÂàóÂØÜÁ†Å„ÄÇÊòéÊñáÁß∞‰∏∫ÊòéÊñáÊµÅÔºå‰ª•Â∫èÂàóÁöÑÊñπÂºèË°®Á§∫„ÄÇÂä†ÂØÜÊó∂ÂÄôÔºåÂÖàÁî±ÁßçÂ≠êÂØÜÈí•ÁîüÊàê‰∏Ä‰∏™ÂØÜÈí•ÊµÅ„ÄÇÁÑ∂ÂêéÂà©Áî®Âä†ÂØÜÁÆóÊ≥ïÊääÊòéÊñáÊµÅÂíåÂØÜÈí•ÊµÅËøõË°åÂä†ÂØÜÔºå‰∫ßÁîüÂØÜÊñáÊµÅ„ÄÇÊµÅÂØÜÁ†ÅÊØèÊ¨°Âè™ÈíàÂØπÊòéÊñáÊµÅ‰∏≠ÁöÑÂçï‰∏™ÊØîÁâπ‰ΩçËøõË°åÂä†ÂØÜÂèòÊç¢ÔºåÂä†ÂØÜËøáÁ®ãÊâÄÈúÄË¶ÅÁöÑÂØÜÈí•ÊµÅÁî±ÁßçÂ≠êÂØÜÈí•ÈÄöËøáÂØÜÈí•ÊµÅÁîüÊàêÂô®‰∫ßÁîü„ÄÇÊµÅÂØÜÁ†ÅÁöÑ‰∏ªË¶ÅÂéüÁêÜÊòØÈÄöËøáÈöèÊú∫Êï∞ÂèëÁîüÂô®‰∫ßÁîüÊÄßËÉΩ‰ºòËâØÁöÑ‰º™ÈöèÊú∫Â∫èÂàóÔºå‰ΩøÁî®ËØ•Â∫èÂàóÂä†ÂØÜÊòéÊñáÊµÅÔºàÊåâÊØîÁâπ‰ΩçÂä†ÂØÜÔºâÔºåÂæóÂà∞ÂØÜÊñáÊµÅ„ÄÇÁî±‰∫éÊØè‰∏Ä‰∏™ÊòéÊñáÈÉΩÂØπÂ∫î‰∏Ä‰∏™ÈöèÊú∫ÁöÑÂä†ÂØÜÂØÜÈí•ÔºåÊâÄ‰ª•ÊµÅÂØÜÁ†ÅÂú®ÁªùÂØπÁêÜÊÉ≥ÁöÑÊù°‰ª∂‰∏ãÂ∫îËØ•ÊòØÁÆó‰∏ÄÁßçÊó†Êù°‰ª∂ÂÆâÂÖ®ÁöÑ‰∏ÄÊ¨°‰∏ÄÂØÜÂØÜÁ†Å„ÄÇÊú∫ÂØÜÊµÅÁ®ãÔºöÁßçÂ≠êÂØÜÁ†Å-&gt;ÈöèÊú∫Êï∞ÂèëÁîüÂô®-&gt;ÂØÜÈí•ÊµÅÊòéÊñáÊµÅ-&gt;(ÈÄöËøáÂØÜÈí•ÊµÅ)-&gt;Âä†ÂØÜÂèòÊç¢-&gt;ÂØÜÊñáÊµÅËÆæÊòéÊñáÊµÅ‰∏∫Ôºöm=m1m2¬∑¬∑¬∑¬∑¬∑mi¬∑¬∑¬∑¬∑¬∑ÔºåÂØÜÈí•ÊµÅÁî±ÂØÜÈí•ÊµÅÂèëÁîüÂô®f‰∫ßÁîüÔºözi=fÔºàkÔºåaiÔºâÔºåaiÊåáÂä†ÂØÜÂô®Â≠òÂÇ®Âô®Âú®iÊó∂ÂàªÁöÑÁä∂ÊÄÅÔºåfÊòØÁî±ÁßçÂ≠êÂØÜÈí•kÂíåai‰∫ßÁîüÁöÑÂáΩÊï∞ÔºåËÆæÊúÄÁªàÁöÑÂØÜÈí•ÊµÅ‰∏∫k=k1k2¬∑¬∑¬∑ki¬∑¬∑¬∑¬∑¬∑ÔºåÂä†ÂØÜÁªìÊûú‰∏∫c=c1c2¬∑¬∑¬∑¬∑ci¬∑¬∑¬∑¬∑¬∑=Ek1Ôºàm1Ôºâ.„ÄÇ„ÄÇ„ÄÇEkiÔºàmiÔºâÔºåËß£ÂØÜÁªìÊûú‰∏∫m=Dk1Ôºàc1ÔºâDk2Ôºàc2Ôºâ¬∑¬∑¬∑DkiÔºàciÔºâ=m1m2¬∑¬∑¬∑miÔºåÊó†ËÆ∫Âä†ÂØÜËß£ÂØÜÔºåÂÖ∂ÂÖ≥ÈîÆÈÉΩÊòØÂØÜÈí•ÊµÅ„ÄÇ 2.ÊµÅÂØÜÁ†ÅÁöÑÂàÜÁ±ªÂàÜ‰∏∫ÂêåÊ≠•ÊµÅÂØÜÁ†ÅÂíåËá™ÂêåÊ≠•ÊµÅÂØÜÁ†Å3.ÊµÅÂØÜÁ†ÅÁöÑÁâπÊÄßÔºöÊûÅÂ§ßÁöÑÂë®ÊúüÔºåËâØÂ•ΩÁöÑÁªüËÆ°ÁâπÊÄßÔºåÊäóÁ∫øÊÄßÂàÜÊûê„ÄÇ4.ÊµÅÂØÜÁ†ÅÁöÑÂÆâÂÖ®ÊÄßÂèñÂÜ≥‰∫éÂØÜÈí•ÊµÅÁöÑÂÆâÂÖ®ÊÄßÔºåË¶ÅÊ±ÇÂØÜÈí•ÊµÅÂ∫èÂàóÊúâËæÉÂ•ΩÁöÑÈöèÊú∫ÊÄß„ÄÇ5.‰∏çÊòéÂØÜÈí•ÁöÑ‰∫∫Â¶Ç‰ΩïÂØπÊµÅÂØÜÁ†ÅËøõË°åÂàÜÊûê„ÄÇËøôÁßçÂØÜÈí•ÊµÅ‰∏ÄËà¨ÈÉΩÊòØÂë®ÊúüÁöÑÔºåÂÅöÂà∞ÂÆåÂÖ®ÈöèÊú∫ÊòØÂõ∞ÈöæÁöÑÔºåËøôÊ†∑‰º™ÈöèÊú∫Â∫èÂàóÔºåÁêÜËÆ∫‰∏äÊòØÂèØ‰ª•ÂàÜÊûêÂá∫Êù•ÁöÑ„ÄÇ‰∏æ‰∏™‰æãÂ≠ê„ÄÇÊïåÊñπÊà™Ëé∑‰∫ÜÂØÜÊñá‰∏≤Ôºö101101011110010ÊòéÊñá‰∏≤Ôºö011001111111001ÂØÜÈí•ÊµÅÔºö110100100001011ÂèØ‰ª•Ê†πÊçÆÂâç10‰∏™ÊØîÁâπÂª∫Á´ãÂ¶Ç‰∏ãÊñπÁ®ã ÂØÜÈí•ÊµÅÁîüÊàêÂô®Ôºö È´òË¶ÅÊ±ÇÂÖ≥ÈîÆ Ë¶ÅÊ±ÇÔºö Ê∏∏Á®ãÔºöÂë®Êúü 0.1 ÂèëËÅ©ÂáΩÊï∞ ‰∫ßÁîüÂØÜÈí•ÊµÅÁöÑË¶ÅÊ±ÇÔºåÊñπÊ≥ï„ÄÅËÆæËÆ° ÂèçÈ¶àÁßª‰ΩçÂØÑÂ≠òÂô® ‚Äã ÔºöÂØÑÂ≠òÂô® ‚Äã Ôºö ËøîÂõûÂáΩÊï∞ ÂàùÂßãÁä∂ÊÄÅ Á∫øÊÄßÂèçÈ¶àÁßª‰ΩçÂØÑÂ≠òÂô® Âø´ Âë®Êúü‚Äú ËæìÂá∫ÂΩ¢Áä∂ÔºöÂèëËÅ©ÂáΩÊï∞ ÁÆóÊ≥ï RC4 ÊµÅÂØÜÁ†ÅÊòØ‰∏ÄÊ¨°‰∏ÄÂØÜÂêóÔºü‰∏çÊòØ RC4Ê≤°ÊúâÂÆûÁé∞ÁöÑm-Â∫èÂàó‰∏çÂèØÁ∫¶„Ää2^n-1 ÂÖÖË¶Å Êú¨ÂéüÂ§öÈ°πÂºè ÂèçÈ¶àÂáΩÊï∞ÂΩ¢Âºè ‰º™ÈöèÊú∫ÊÄß Ê±Ç‰Ω†12 Day 3 3_13 ÂàÜÁªÑÂØÜÁ†ÅÂ∫îÁî®ËÆæËÆ°ÁªìÊûÑÂéüÁêÜÂÆâÂÖ®ÊÄßÂéüÂàôÊ∑∑Ê∑ÜÂéüÂàô Êâ©Êï£ÂéüÂàô ‚Äã ÊòéÊñáÔºàÁªüËÆ°„ÄÅÁªìÊûÑËßÑÂæãÊï£Â∞ÑÂà∞Áõ∏ÂΩìÈïøÁöÑ‰∏ÄÊÆµÁªüËÆ°‰∏≠ÂéªÔºâ ÁÆóÊ≥ïË¶ÅÊ±ÇÂàÜÁªÑÈïøÂ∫¶Ë∂≥Â§üÂ§ß ÂØÜÈí•ÈáèË∂≥Â§üÂ§ß DESÁÆóÊ≥ï56-64 IBMÁ¨¨‰∏Ä‰∏™ÂïÜ‰∏ö ÂêéÈù¢Âá∫Áé∞‰∫ÜAES ÁÆóÊ≥ïÊ°ÜÂõæ IP ÂàùÂßãÁΩÆÊç¢ ËÆ∫ÂáΩÊï∞ 16ËÆ∫ ÂàÜÂ∑¶Âè≥32bit ÂÖ¨ÂºèÔºöÂáΩÊï∞ÔºàR,ËΩÆÂØÜÈí•Ôºâ SÁõí ËæìÂÖ•ÂÖ≠‰ΩçÔºå8‰∏™ÁõíÂ≠ê ËæìÂá∫32bit step1; 32bit-48bit() ÈÄâÊã©Êâ©Â±ïËøêÁÆó E 8*4-„Äã‰∏§Á´Ø ÁΩÆÊç¢ SÁõí 4*16 ÈÄâÊã©ÂéãÁº©ËøêÁÆó ‚Äã ËæìÂÖ•ËæìÂá∫ ‚Äã ËæìÂÖ•Ôºö6bit ‰∫åËøõÂà∂-„ÄãÂçÅËøõÂà∂ Á°ÆÂÆö‰ΩçÁΩÆ PÁõíÁΩÆÊç¢ 32 -32 ÂØÜÈí•ÁºñÊéí ÁΩÆÊç¢-„Äã‰∏§ÁªÑ-„ÄãÂæ™ÁéØÂ∑¶Áßª„Äã16ËΩÆÂØÜÈí• ÊÄßË¥®Ôºö‰∫íË°•ÊÄßÂíåÂº±ÂØÜÈí•ÊÄß 2DES 56+1 = 57 ‰∏≠Èó¥‰∫∫Áõ∏ÈÅáÂ∑•ÂÖ∑ 3DES ÂàÜÁªÑÂØÜÁ†ÅÁöÑÂ∑•‰ΩúÊ®°Âºè ‰∏∫‰ªÄ‰πàÔºüÂàÜÁªÑÈïøÂ∫¶ÊòØÂõ∫ÂÆöÔºåËÄåÊï∞ÊçÆÈïøÂ∫¶ÂíåÊ†ºÂºèÊòØ‰∏çÂêåÁöÑÔºå ÁîµÁ†ÅÊú¨Ê®°Âºè ÂØÜÁ†ÅÂàÜÁªÑÈìæÊé•Ê®°Âºè ‚Äã CBCÂä†ÂØÜ ÂÆåÊï¥ÊÄßÔºàËÆ§ËØÅÁ†ÅÁîüÊàêÔºâÂä†ÂØÜÔºåÂØπÊØî ÊòéÊñáÊ†°È™åÁ†Å-„ÄãCBC(M.r)&gt;ÂØπÊØî Ëß£ÂÜ≥ÔºöÊòéÊñáÁªüËÆ°ËßÑÂæãÈöêËóè Â∑•‰ΩúÊ®°Âºè 2 Êï∞ÊçÆÊ†ºÂºèÔºö ‚Äã Â≠óËäÇ„ÄÅÊØîÁâπ„ÄÅÁ≠âÁ≠âÊïÖ‰∫ã ÂàÜÁªÑÂØÜÁ†ÅÊ¶ÇËø∞ÂÖ±‰∫´ÂØÜÈí• IV ÊúâÈôêÂüüÁöÑÂü∫Êú¨Ê¶ÇÂøµÂçï‰ΩçÂÖÉÔºöÂä†Ê≥ï ÈÄÜÂÖÉÔºö‰πòÊ≥ï AESÂ≠óËäÇ‰∏∫Â§ÑÁêÜÂçïÂÖÉ 8bits Âä†Ê≥ïÔºömod 2 Â§öÈ°πÂºèÈô§Ê≥ï 8 4 3 1 0 ÁöÑÊú´Â§öÈ°πÂºèÂèñÊ®° Â§öÈ°πÂºèËøêÁÆó 128 128Ôºå192Ôºå256 S-ÊåâÂàó Âõõ‰∏™Âü∫Êú¨ÂÅöÂá∫ 10 12 14 s:16 Â≠óËäÇ‰ª£Êç¢ She 16*16 SÈáåÈù¢Êü•Ë°®‰ª£Êç¢ ‰∫åËøõÂà∂ ÂçÅÂÖ≠ËøõÂ±ï Ê±ÇÈÄÜ Ê∑∑Ê∑ÜÊïàÂ∫î ‰π±‰∫Ü Ë°åÁßª‰Ωç ‚Äã Âæ™ÁéØÂ∑¶Áßª ÂàóÊ∑∑Ê∑Ü ‚Äã ÊØè‰∏ÄÂàóÁü©ÈòµÁé∞Âú∫ ‚Äã ÁúãÊàêÂ§öÈ°πÂºè ‚Äã Áü©ÈòµÈÄâÊã© ËΩÆÂØÜÈí•Âä† ÂºÇÊàñÔºöÂ≠êÂØÜÈí• Â±Ö‰Ωè ‚Äã ÂàùÂßãÂØÜÈí•Ôºå AES ‚Äã Âõõ‰∏™‰Ωç-„Äã‰∏Ä‰∏™Â≠óËäÇ-„Äã16ËøõÂà∂ ‰∏Ä‰∏™Â≠óËäÇ=„Äã‰∏§‰∏™ÂçÅÂÖ≠ËøõÊï∞ ÂØÜÈí•Êâ©Â±ïÁÆóÊ≥ï ÈÄÜSÁõí Day 4 Áé∞‰ª£ÂØÜÁ†ÅÂ≠¶ 3-27ÂÖ¨Èí•ÔºöÂØÜÈí•ÁÆ°ÁêÜÔºå ÈùûÂØπÁß∞ÂØÜÁ†Å‰ΩìÂà∂ ÂØÜÈí•ÂØπ pk sk Âä†ÂØÜÔºöÂÖ¨Èí• ‰ºòÂäøÔºö ‚Äã ÂØÜÈí•ÂàÜÂèë ‚Äã ÂØÜÈí•ÁÆ°ÁêÜ Ôºö1 N-1 ‚Äã ÂºÄÊîæÁ≥ªÁªü RSAÂä†ÂØÜÁÆóÊ≥ïÊï∞Â≠¶Áü•ËØÜ ‚Äã ÁÆóÊ≥ï Â§ßÊï∞ÊçÆÂàÜËß£ ÂØÜÈí•ÁîüÊàê ÊúÄÂ§ßÂÖ¨Âõ†Â≠êÂíå‰πòÊ≥ïÈÄÜÂÖÉÁöÑËÆ°ÁÆóÊñπÊ≥ï„ÄÇ https://blog.csdn.net/boksic/article/details/7014386 https://blog.csdn.net/a745233700/article/details/102341542?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-5&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-5 https://blog.csdn.net/weixin_34138377/article/details/92199465?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-4&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-4 https://blog.csdn.net/weixin_41482303/article/details/85417302?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-3 Á≠æÂêç ËØÅ‰π¶ https://blog.csdn.net/weixin_34007879/article/details/85528967?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2&amp;utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2]]></content>
  </entry>
  <entry>
    <title><![CDATA[English-Daily]]></title>
    <url>%2F2020%2F06%2F23%2FEnglish-Daily%2F</url>
    <content type="text"><![CDATA[2020-7-6coincide with v. ‰∏é‚Ä¶Áõ∏Á¨¶ stalk v. ÊΩúËøëÔºàÁåéÁâ©Êàñ‰∫∫ÔºâÔºõÔºàÈùûÊ≥ïÔºâË∑üË∏™ÔºõÊÄíÂÜ≤ÂÜ≤Âú∞Ëµ∞ÔºõË∂æÈ´òÊ∞îÊâ¨Âú∞Ëµ∞ n. ÁßÜÔºõÊüÑÔºõÔºàÂè∂ÔºâÊüÑÔºõÔºàËä±ÔºâÊ¢ó verge Bella was on the verge of tears when she heard the news. Âê¨Âà∞Ëøô‰∏™Ê∂àÊÅØÊó∂ÔºåË¥ùÊãâÂ∑ÆÁÇπÂ∞±Ë¶ÅÂì≠‰∫Ü„ÄÇ resistant adj. ÊäµÂà∂ÁöÑÔºåÂèçÊäóÁöÑÔºåÊäóÊãíÁöÑÔºõÊúâÊäµÊäóÂäõÁöÑÔºõÊäµÊäó‚Ä¶ÁöÑÔºõ‰∏çÂèó‚Ä¶‚Ä¶ÊçüÂÆ≥ÁöÑ People are usually resistant to change. ‰∫∫‰ª¨ÈÄöÂ∏∏ÊäóÊãíÊîπÂèò„ÄÇ liar The tall guy was a notorious liar. ÈÇ£‰∏™È´ò‰∏™Â≠êÊòØ‰∏™Ëá≠ÂêçÊò≠ËëóÁöÑÈ™óÂ≠ê„ÄÇ politics n. ÊîøÊ≤ªÔºõÊîøÊ≤ª‰∫ãÁâ©ÔºàÊ¥ªÂä®ÔºâÔºõÊîøËßÅÔºõÊùÉÊúØ oblige (‰ª•Ê≥ïÂæã„ÄÅ‰πâÂä°Á≠â)Âº∫Ëø´, Ëø´‰Ωø; Â∏ÆÂøô, ÊïàÂä≥; [Â∏∏Áî®Ë¢´Âä®]‰ΩøÊÑüÊøÄ; ‰Ωø(Ë°å‰∏∫Á≠â)Êàê‰∏∫ÂøÖË¶Å phrase. (feel obliged to do sth.)ËßâÂæóÊúâ‰πâÂä°ÂÅöÔºõ‰∏çÂæó‰∏çÂÅö I felt obliged to leave after such an unpleasant quarrel. ÂèëÁîü‰∫ÜËøôÊ†∑‰∏çÊÑâÂø´ÁöÑ‰∫âÂêµ‰πãÂêéÔºåÊàëËßâÂæóÊúâÂøÖË¶ÅÁ¶ªÂºÄ„ÄÇ 2020-7-1jelly n. ÊûúÂÜªÔºõËÇâÂÜªÔºõÊûúÈÖ±ÔºõËÉ∂Áä∂Áâ©ÔºåËÉ∂ÂáùÁâ©ÔºõËΩª‰æøÂ°ëÊñôÈûã oval adj. Ê§≠ÂúÜÂΩ¢ÁöÑÔºõÂçµÂΩ¢ÁöÑ n. Ê§≠ÂúÜÂΩ¢ÔºõÂçµÂΩ¢ rigorous /‚Äòr…™…°…ôr…ôs/ adj. Ë∞®ÊÖéÁöÑÔºåÁªÜËá¥ÁöÑÔºõ‰∏•Ê†ºÁöÑÔºå‰∏•ÂéâÁöÑ He makes a rigorous study of the plants in the area. ‰ªñÂØπËØ•Âú∞ÁöÑÊ§çÁâ©ËøõË°å‰∫ÜÁºúÂØÜÁöÑÁ†îÁ©∂„ÄÇ ultimately UK/‚Äò ålt…™m…ôtli/ adv. ÊúÄÁªà, ÊúÄÂêé, ÂΩíÊ†πÁªìÂ∫ï, ÁªàÁ©∂ Everything will ultimately depend on what is said at the meeting. ‰∏ÄÂàáÂ∞ÜÊúÄÁªàÂèñÂÜ≥‰∫é‰ºöËÆÆÁöÑÂÜÖÂÆπ„ÄÇ sturdy UK/‚Äòst…úÀêdi/ adj. ÁªìÂÆûÁöÑÔºåÂùöÂõ∫ÁöÑÔºõÂº∫Â£ÆÁöÑÔºõÂÅ•Â£ÆÁöÑÔºõÂùöÂÜ≥ÁöÑÔºåÈ°ΩÂº∫ÁöÑ broaden UK/‚Äòbr…îÀêdn/ You should broaden your experience by travelling more. ‰Ω†Â∫îËØ•Â§öÂà∞ÂêÑÂú∞Ëµ∞Ëµ∞‰ª•Â¢ûÂπøËßÅËØÜ. broaden the horizon ÂºÄÊãìËßÜÈáé propel UK/pr…ô‚Äôpel/ v. Êé®ËøõÔºåÊé®Âä®ÔºõÈ©±‰ΩøÔºõËø´‰Ωø voyage UK/‚Äòv…î…™…™d í/ n. Ëà™Ë°å, ÔºàÂ∞§ÊåáÔºâËà™Êµ∑ v. Ëà™Ë°å, ËøúË°å, ÔºàÂ∞§ÊåáÔºâËøúËà™ ‰æãÂè• The voyage from England to India used to take 3 weeks. ‰ªéËã±Ê†ºÂÖ∞Âà∞Âç∞Â∫¶ÁöÑËà™Ë°åÊõæÁªèÈúÄË¶Å‰∏âÂë®„ÄÇ 2020-6-28moist UK/m…î…™st/ adj. ÂæÆÊπøÁöÑ, ÊπøÊ∂¶ÁöÑ insult UK/…™n‚Äôs ålt/v. ‰æÆËæ±ÔºåËæ±È™Ç n. ‰æÆËæ±ÔºåËæ±È™Ç spontaneous UK/sp…ín‚Äôte…™ni…ôs/ They greeted him with spontaneous applause. ‰ªñ‰ª¨Ëá™ÂèëÂú∞ÈºìËµ∑ÊéåÊù•Ê¨¢Ëøé‰ªñ„ÄÇ slender UK/‚Äòslend…ô(r)/ perimeter UK/p…ô‚Äôr…™m…™t…ô(r)/ n. Âë®ÈïøÔºõÂ§ñÁºòÔºåËæπÁºò blouse UK/bla äz/ He pointed out a woman passing by who was wearing a skirt and blouse. ‰ªñÊåáÂá∫‰∫Ü‰∏Ä‰∏™Á©øÁùÄË£ôÂ≠êÂíåË°¨Ë°´ÁöÑËøáË∑ØÂ•≥Â≠ê„ÄÇ perfume UK/‚Äòp…úÀêfjuÀêm/ n. È¶ôÊ∞¥, È¶ôÊñô, Ëä≥È¶ô v. ‰Ωø‚Ä¶ÂèëÂá∫È¶ôÊ∞î, Ê¥íÈ¶ôÊ∞¥ 2020-6-272020-6-26Functional foods are food products that have a potentially positive effect on health beyond basic nutritional benefits. Functional foods aim to solve not only all the needs that regular foods provide, but also to address functional needs, which can range from maintaining and improving physical or mental health to adjusting energy levels and moods. Food has been historically used as preventive medicine in many cultures around the world, but the recent rise of functional foods can be directly linked to the rise of the wellness economy, which, in turn, is largely driven by influencer marketing and social media use. 2020-6-25IT IS A truth universally acknowledged that inequalityÔºà‰∏çÂπ≥Á≠âÔºâin the rich worldÔºàÂèëËææÂõΩÂÆ∂Ôºâis high and rising. Or, at least, it used to be. A growing band of economists are challenging the receivedÔºàË¢´ÂÖ¨ËÆ§ÁöÑÔºâwisdom, pointing out that trends in the distributionÔºàÂàÜÂ∏ÉÔºåÂàÜÈÖçÔºâof income and wealth may not be as bad as is often thought. ‰ºóÊâÄÂë®Áü•ÔºåÂØåË£ïÂõΩÂÆ∂ÁöÑ‰∏çÂπ≥Á≠âÁé∞Ë±°ÈùûÂ∏∏‰∏•ÈáçÔºåËÄå‰∏îËøòÂú®Âä†Ââß„ÄÇÊàñËÄÖËØ¥ÔºåËá≥Â∞ëÊõæÁªèÊòØËøôÊ†∑ÁöÑ„ÄÇË∂äÊù•Ë∂äÂ§öÁöÑÁªèÊµéÂ≠¶ÂÆ∂ÂºÄÂßãË¥®ÁñëÊó¢ÊúâÁöÑËßÇÁÇπÔºå‰ªñ‰ª¨ÊåáÂá∫Êî∂ÂÖ•ÂíåË¥¢ÂØåÁöÑÂàÜÂ∏ÉË∂ãÂäøÂèØËÉΩ‰∏çÊòØÂÉèÈÄöÂ∏∏Ë¢´ËÆ§‰∏∫ÁöÑÈÇ£‰πàÁ≥üÁ≥ï„ÄÇ 2020-6-24imaginary adj. ÊÉ≥Ë±°‰∏≠ÁöÑ, ÂπªÊÉ≥ÁöÑ, ËôöÊûÑÁöÑ carriage n. ËøêËæìÔºõËøêË¥πÔºåÔºàÊóßÊó∂ÔºâÈ©¨ËΩ¶ÔºõÁÅ´ËΩ¶ËΩ¶Âé¢Ôºõ‰ª™ÊÄÅÔºåÂßøÊÄÅÔºå‰∏æÊ≠¢ message messenger n. ‰ø°‰Ωø, ÈÄÅ‰ø°‰∫∫, ÈÄö‰ø°Âëò, ÈÇÆÈÄíÂëò pavement n. ‰∫∫Ë°åÈÅì postpone v. Âª∂Êúü, Âª∂Ëøü, ÊöÇÁºì We‚Äôll have to postpone the meeting until next week. Êàë‰ª¨Â∞Ü‰∏çÂæó‰∏çÊää‰ºöËÆÆÊé®ËøüÂà∞‰∏ãÂë®‰∏æË°å„ÄÇ velocity n. ÈÄüÂ∫¶ÔºåÈÄüÁéáÔºõÈ´òÈÄü reconcile v. ‰ΩøÂíåË∞ê‰∏ÄËá¥ÔºåË∞ÉÂíåÔºõ‰ΩøÂíåËß£ÔºõÂ∞ÜÂ∞±ÔºåÂ¶•Âçè It‚Äôs difficult to reconcile these two different points of view. ÂæàÈöæÂÖºÈ°æËøô‰∏§Áßç‰∏çÂêåÁöÑËßÇÁÇπ„ÄÇ 2020-6-23ÔøºThe success of the brand wasn‚Äôt built through big marketing campaigns, but through a savvy digital marketing strategy that increased brand awareness and generated high engagement, traffic, and conversions. ËØ•ÂìÅÁâåÁöÑÊàêÂäüÂπ∂‰∏çÂª∫Á´ã‰∫éÂ§ßÂûãËê•ÈîÄÊ¥ªÂä®ÔºåËÄåÊòØÂª∫Á´ã‰∫éÁ≤æÂáÜÁöÑÊï∞Â≠óËê•ÈîÄÁ≠ñÁï•ÔºåËØ•Á≠ñÁï•ÊèêÈ´ò‰∫ÜÂìÅÁâåÁöÑÁü•ÂêçÂ∫¶ÔºåËé∑Âæó‰∫ÜÂæàÈ´òÁöÑÂèÇ‰∏éÂ∫¶„ÄÅÊµÅÈáèÂíåËΩ¨ÂåñÁéá„ÄÇ traffic: ‰ø°ÊÅØÊµÅÈáèÔºåÈÄö‰ø°Èáè With only 40 physical stores, which are mostly used to drive consumers to e-commerce portals, Perfect Diary maintains momentum primarily through its digital footprint. Currently, it has a powerful presence on Little Red Book, Bilibili, Weibo, WeChat, Tmall, and Douyin. Thereafter she wrote articles for papers and magazines for a living. Ê≠§ÂêéÂ•πÁªôÊä•Á∫∏ÂíåÊùÇÂøóÊí∞Á®øË∞ãÁîü„ÄÇ adv. Ê≠§Âêé, ‰πãÂêé, ‰ª•Âêé spur n. Âà∫ÊøÄ, ÊøÄÂä±, Èû≠Á≠ñ; Ë∏¢È©¨Âà∫, Èù¥Âà∫; È™®Âà∫; Â±±Âò¥, Â∞ñÂù° v. Âà∫ÊøÄ, ÊøÄÂä±, ‰øÉËøõ, Èû≠Á≠ñ stick adj. ÈªèÔºàÊÄßÔºâÁöÑ, ‰∏ÄÈù¢Â∏¶ÈªèËÉ∂ÁöÑ, Èó∑ÁÉ≠ÁöÑ, ÊÑüÂà∞ÁÉ≠ÂæóÈöæÂèóÁöÑ n. Âëä‰∫ãË¥¥ I have to take a shower before going out because the sweat had made my skin sticky. Âá∫Èó®ÂâçÊàëÂæóÂÜ≤‰∏™Êæ°ÔºåÂõ†‰∏∫Ê±óÊ∞¥ËÆ©ÊàëÁöÑÁöÆËÇ§Èªè‰πé‰πéÁöÑ devotion n. ÂÖ≥Áà±ÔºåÂÖ≥ÁÖßÔºõÂ•âÁåÆÔºõÂø†ËØöÔºõÂÆóÊïôÁ§ºÊãú The career needs our devotion for all our lives. ËøôÈ°π‰∫ã‰∏öÈúÄË¶ÅÊàë‰ª¨ÊØïÁîüÁöÑÂ•âÁåÆ„ÄÇ reckless adj. È≤ÅËéΩÁöÑÔºõ‰∏çËÆ°ÂêéÊûúÁöÑÔºõÊó†ÊâÄÈ°æÂøåÁöÑ wag v. ÊëáÂä®ÔºõÊëÜÔºàÂ∞æÂ∑¥ÔºâÔºåÔºàÂ∞æÂ∑¥ÔºâÊëáÔºåÊëÜÂä® n. ÊëáÊëÜÔºåÊëÜÂä®ÔºõËÄÅÂºÄÁé©Á¨ëÁöÑ‰∫∫ÔºåÁà±ÈóπÁùÄÁé©ÁöÑ‰∫∫ keen adj. ÁÉ≠Ë°∑ÁöÑ, ÁÉ≠ÊÉÖÁöÑ; Ê∏¥ÊúõÁöÑ; ÊïèÊç∑ÁöÑ; ÁÅµÊïèÁöÑ; ÈîãÂà©ÁöÑ; Âº∫ÁÉàÁöÑ n. ÊÅ∏Âì≠; ÊåΩÊ≠å v. (‰∏∫Ê≠ªËÄÖ)ÊÅ∏Âì≠ be keen on sthÂØπ ÊÑüÂÖ¥Ë∂£ be keen to do Ê∏¥ÊúõÂÅöÊüê‰∫ã offspring n. Â≠êÂ•≥ÔºåÂêé‰ª£ÔºõÂπºÂ¥ΩÔºõÂπºËãó receipt n. Êî∂ÊçÆÔºåÊî∂ÂÖ•]]></content>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[day]]></title>
    <url>%2F2020%2F06%2F22%2Ftime-series-01%2F</url>
    <content type="text"><![CDATA[Êó∂Èó¥Â∫èÂàóÂèäÂÖ∂ÂàÜËß£ Êó∂Èó¥Â∫èÂàóÂàÜÁ±ªÂπ≥Á®≥Â∫èÂàóÔºàstationary series)Â∫èÂàó‰∏≠ÁöÑÂêÑËßÇÂØüÂÄºÂü∫Êú¨‰∏äÂú®Êüê‰∏™Âõ∫ÂÆöÁöÑÊ∞¥Âπ≥‰∏äÊ≥¢Âä®ÔºåÂú®‰∏çÂêåÊó∂FÈó¥ÊÆµÊ≥¢Âä®Á®ãÂ∫¶‰∏çÂêåÔºå‰ΩÜ‰∏çÂ≠òÂú®ÊüêÁßçËßÑÂæã„ÄÇÂπ≥Á®≥ÊÄßÊó∂Èó¥Â∫èÂàóÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÈÉΩÊòØÂ∏∏Êï∞„ÄÇ ÊñπÊ≥ïÔºöa) ÁúãÂéüÂõæ„ÄÇÊòØÂê¶Âú®Êüê‰∏™Â∏∏Êï∞ÈôÑËøëÊ≥¢Âä®Ôºå‰∏îÊ≥¢Âä®ËåÉÂõ¥ÊúâÁïå„ÄÇÂ¶ÇÊûúÊúâÊòéÊòæÁöÑË∂ãÂäøÊÄßÊàñËÄÖÂë®ÊúüÊÄßÔºåÂàô‰∏çÊòØ„ÄÇb) ADFÂçï‰ΩçÊ†πÊ£ÄÊµã„ÄÇpÂÄº„ÄÇ ÈùûÂπ≥Á®≥Â∫èÂàóÔºànon-stationary series)Ê∂âÂèäË∂ãÂäø„ÄÅÂ≠£ËäÇÊÄßÂíåÂë®Êúü‰∏âÁßçÁâπÊÄßÔºåÂåÖÂê´ÂÖ∂‰∏≠‰∏ÄÁßçÊàñËÄÖÂ§öÁßçÊàêÂàÜ„ÄÇ Ë∂ãÂäø(trend)Êó∂Èó¥Â∫èÂàóÂú®ÈïøÊó∂ÊúüÂÜÖÂëàÁé∞Âá∫Êù•ÁöÑÊüêÁßç‰∏äÂçáÊàñËÄÖ‰∏ãÈôçÁöÑË∂ãÂäø„ÄÇÂàÜ‰∏∫Á∫øÊÄßÂíåÈùûÁ∫øÊÄß„ÄÇ Â≠£ËäÇÊÄßÔºàseasonality)ÊòØÊåáÊó∂Èó¥Â∫èÂàóÂú®‰∏ÄÂπ¥ÂÜÖÈáçÂ§çÂá∫Áé∞ÁöÑÂë®ÊúüÊ≥¢Âä®„ÄÇÂõ†Â≠£ËäÇ‰∏çÂêåËÄåÂèëÁîüÂèòÂåñÔºåÂ¶ÇÊóÖÊ∏∏Êó∫Â≠£ÔºåÊóÖÊ∏∏Ê∑°Â≠£„ÄÇ Âë®ÊúüÊÄßÔºàcyclicityÔºâÊòØÊåáÊó∂Èó¥Â∫èÂàóÂëàÁé∞Âá∫ÁöÑÈïøÊúüË∂ãÂäø„ÄÇÂë®ÊúüÊÄß‰∏çÂêå‰∫éË∂ãÂäøÂèòÂä®ÔºåÂÆÉÊòØÊ∂®ËêΩÁõ∏Èó¥ÁöÑ‰∫§ÊõøÊ≥¢Âä®„ÄÇ‰∏çÂêåÊÑèÂ≠£ËäÇÂèòÂä®ÔºåÂÆÉÊó†Âõ∫ÂÆöËßÑÂæãÔºåÂèòÂä®Âë®ÊúüÂ§öÂú®‰∏ÄÂπ¥‰ª•‰∏äÔºå‰∏îÂë®ÊúüÈïøÁü≠‰∏ç‰∏Ä„ÄÇÂë®ÊúüÊÄßÈÄöÂ∏∏ÊòØÁî±ÁªèÊµéÁéØÂ¢ÉÁöÑÂèòÂåñÂºïËµ∑ÁöÑ„ÄÇ ÂÅ∂ÁÑ∂ÊÄßÂõ†Á¥†ÂÖ∂ÂØºËá¥Êó∂Èó¥Â∫èÂàóÂëàÁé∞Âá∫ÊüêÁßçÈöèÊú∫Ê≥¢Âä®„ÄÇ Êó∂Èó¥Â∫èÂàóÁöÑÊàêÂàÜÂèØÂàÜ‰∏∫ÔºöË∂ãÂäøÔºàT),Â≠£ËäÇÊÄßÔºàS),Âë®ÊúüÊÄßÔºàC),ÈöèÊú∫ÊÄßÔºàI)„ÄÇ Âπ≥Á®≥Êó∂Èó¥Â∫èÂàóÂàÜÊûêARÊ®°Âûã Ëá™ÂõûÂΩíÊ®°ÂûãAR Ëá™ÂõûÂΩíÊ®°ÂûãÊèèËø∞ÂΩìÂâçÂÄº‰∏éÂéÜÂè≤ÂÄº‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÁî®ÂèòÈáèËá™Ë∫´ÁöÑÂéÜÂè≤Êó∂Èó¥Êï∞ÊçÆÂØπËá™Ë∫´ËøõË°åÈ¢ÑÊµã„ÄÇËá™ÂõûÂΩíÊ®°ÂûãÂøÖÈ°ªÊª°Ë∂≥Âπ≥Á®≥ÊÄßÁöÑË¶ÅÊ±Ç„ÄÇ ÁßªÂä®Âπ≥ÂùáÊ®°ÂûãMA ÁßªÂä®Âπ≥ÂùáÊ®°ÂûãÂÖ≥Ê≥®ÁöÑÊòØËá™ÂõûÂΩíÊ®°Âûã‰∏≠ÁöÑËØØÂ∑ÆÈ°πÁöÑÁ¥ØÂä† Ëá™ÂõûÂΩíÁßªÂä®Âπ≥ÂùáÊ®°ÂûãARMA Ëá™ÂõûÂΩíÊ®°ÂûãARÂíåÁßªÂä®Âπ≥ÂùáÊ®°ÂûãMAÊ®°ÂûãÁõ∏ÁªìÂêàÔºåÊàë‰ª¨Â∞±ÂæóÂà∞‰∫ÜËá™ÂõûÂΩíÁßªÂä®Âπ≥ÂùáÊ®°ÂûãARMA(p,q) Â∑ÆÂàÜËá™ÂõûÂΩíÁßªÂä®Âπ≥ÂùáÊ®°ÂûãARIMA Â∞ÜËá™ÂõûÂΩíÊ®°Âûã„ÄÅÁßªÂä®Âπ≥ÂùáÊ®°ÂûãÂíåÂ∑ÆÂàÜÊ≥ïÁªìÂêàÔºåÊàë‰ª¨Â∞±ÂæóÂà∞‰∫ÜÂ∑ÆÂàÜËá™ÂõûÂΩíÁßªÂä®Âπ≥ÂùáÊ®°ÂûãARIMA(p,d,q) ÂèÇÊï∞Á°ÆÂÆöÊãñÂ∞æÂíåÊà™Â∞æÊãñÂ∞æÊåáÂ∫èÂàó‰ª•ÊåáÊï∞ÁéáÂçïË∞ÉÈÄíÂáèÊàñÈúáËç°Ë°∞ÂáèÔºåËÄåÊà™Â∞æÊåáÂ∫èÂàó‰ªéÊüê‰∏™Êó∂ÁÇπÂèòÂæóÈùûÂ∏∏Â∞è„ÄÇ ARIMAÂª∫Ê®°ËøáÁ®ã Â∞ÜÂ∫èÂàóÂπ≥Á®≥ÔºàÂ∑ÆÂàÜÊ≥ïÁ°ÆÂÆödÔºâ pÂíåqÈò∂Êï∞Á°ÆÂÆöÔºöACF‰∏éPACF ARIMAÔºàp,d,qÔºâ Ê®°Âûã ACF PACF ARÔºàpÔºâ Ë°∞ÂáèË∂ã‰∫éÈõ∂ÔºàÂá†‰ΩïÂûãÊàñÊåØËç°ÂûãÔºâ pÈò∂ÂêéÊà™Â∞æ MAÔºàqÔºâ qÈò∂ÂêéÊà™Â∞æ Ë°∞ÂáèË∂ã‰∫éÈõ∂ÔºàÂá†‰ΩïÂûãÊàñÊåØËç°ÂûãÔºâ ARMAÔºàp,qÔºâ qÈò∂ÂêéË°∞ÂáèË∂ã‰∫éÈõ∂ÔºàÂá†‰ΩïÂûãÊàñÊåØËç°ÂûãÔºâ pÈò∂ÂêéË°∞ÂáèË∂ã‰∫éÈõ∂ÔºàÂá†‰ΩïÂûãÊàñÊåØËç°ÂûãÔºâ ÂèÇÊï∞ p,q ÁöÑËá™Âä®Á°ÆÂÆöÊñπÂºè‰ø°ÊÅØÂáÜÂàôÂú®ÂèÇÊï∞‰º∞ËÆ°ÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨ÂèØ‰ª•ÈááÁî®‰ººÁÑ∂ÂáΩÊï∞‰Ωú‰∏∫ÁõÆÊ†áÂáΩÊï∞„ÄÇÂèØ‰ª•ÈÄöËøáÂä†ÂÖ•Ê®°ÂûãÂ§çÊùÇÂ∫¶ÁöÑÊÉ©ÁΩöÈ°πÈÅøÂÖçËøáÊãüÂêàÈóÆÈ¢ò„ÄÇÊØîÂ¶ÇËµ§Ê±†‰ø°ÊÅØÂáÜÂàôÔºàAIC)ÂíåË¥ùÂè∂ÊñØ‰ø°ÊÅØÂáÜÂàô(BIC) AIC=2k‚àí2ln(L)‰∏ÄÊñπÈù¢ÂºïÂÖ•ÊÉ©ÁΩöÈ°πÔºå‰ΩøÂæóÊ®°ÂûãÂèÇÊï∞Â∞ΩÂø´Â∞ëÔºåÂáèÂ∞ëËøáÊãüÂêà„ÄÇÂè¶‰∏ÄÊñπÈù¢Ôºå‰πüÂ∏åÊúõÊèêÈ´òÊ®°ÂûãÁöÑÊãüÂêàÂ∫¶ÔºàÊûÅÂ§ß‰ººÁÑ∂Ôºâ BIC=kLn(n)‚àí2ln(L)k‰∏∫Ê®°ÂûãÂèÇÊï∞‰∏™Êï∞Ôºån‰∏∫Ê†∑Êú¨Êï∞ÈáèÔºåL‰∏∫‰ººÁÑ∂ÂáΩÊï∞„ÄÇÂºïÂÖ•$Kln(n)$ÊÉ©ÁΩöÈ°πÂú®Áª¥Â∫¶ËøáÂ§ß‰∏îÊ†∑Êú¨Êï∞ÊçÆÁõ∏ÂØπËæÉÂ∞ëÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØ‰ª•ÊúâÊïàÈÅøÂÖçÂá∫Áé∞Áª¥Â∫¶ÁÅæÈöæ„ÄÇ Êó∂Èó¥Â∫èÂàóÁöÑÂàÜËß£Âä†Ê≥ïÊ®°Âûã X_t = T_t + C_t+S_t + I_t ,t = 1,2,..,nÊØè‰∏™Êó∂Èó¥Â∫èÂàóÁúãÊàêÊòØ‰∏â‰∏™ÈÉ®ÂàÜÁöÑÂè†Âä†ÔºåÂàÜÂà´ÊòØË∂ãÂäøÈ°π„ÄÅÂæ™ÁéØÈ°πÔºåÂ≠£ËäÇÈ°πÔºåÈöèÊú∫È°π ‰πòÊ≥ïÊ®°Âûã X_t = T_t*C_t*S_t*I_tË∂ãÂäøÂàÜÊûêË∂ãÂäøÊãüÂêàÊ≥ïÂ∞±ÊòØÊääÊó∂Èó¥‰Ωú‰∏∫Ëá™ÂèòÈáèÔºåÁõ∏Â∫îÁöÑÂ∫èÂàóËßÇÂØüÂÄº‰Ωú‰∏∫Âõ†ÂèòÈáèÔºåÂª∫Á´ãÂ∫èÂàóÂÄºÈöèÊó∂Èó¥ÂèòÂåñÁöÑÂõûÂΩíÊ®°Âûã„ÄÇÂèØÂàÜ‰∏∫Á∫øÊÄßÊãüÂêàÂíåÊõ≤Á∫øÊãüÂêà„ÄÇ Á∫øÊÄßÊãüÂêàÂ¶ÇÊûúÈïøÊúüË∂ãÂäøÂëàÁé∞Âá∫Á∫øÊÄßÁâπÂæÅÔºåÂèØÁî®Á∫øÊÄßÊ®°ÂûãÊãüÂêàÔºå \left\{\begin{array}{c} x_t = a+bt+I_t\\ E(I_t) = 0,Var(I_t) = \sigma^2 \end{array} \right.ÂÖ∂‰∏≠Ôºå$T_t = a+bt$Â∞±ÊòØÊ∂àÈô§ÈöèÊú∫Ê≥¢Âä®ÂΩ±ÂìçÂêéÁöÑËØ•Â∫èÂàóÁöÑÈïøÊúüË∂ãÂäø„ÄÇ Êõ≤Á∫øÊãüÂêàÂ¶ÇÊûúÈïøÊúüË∂ãÂäøÂëàÁé∞Âá∫Á∫øÊÄßÁâπÂæÅÔºåÂèØÁî®Êõ≤Á∫øÊ®°ÂûãÊù•ÊãüÂêà \left\{ \begin{array}{c|c|c} ‰∫åÊ¨°Âûã& T_t = a+bt+ct^2& ÂèòÊç¢ÂêéÔºåÁ∫øÊÄßÊúÄÂ∞è‰∫å‰πòÊ≥ï\\ ÊåáÊï∞Âûã&T_t = ab^t& ÂØπÊï∞ÂèòÂåñ & ÊúÄÂ∞è‰∫å‰πòÊ≥ï\\ ‰øÆÊ≠£ÊåáÊï∞Âûã&T_t = a+bc^t& &Ëø≠‰ª£Ê≥ï\\ GompertzÂûã& T_t = e^{a+bc^t}& & Ëø≠‰ª£Ê≥ï\\ Logistic & T_t = \frac{1}{a+bc^t}& Ëø≠‰ª£Ê≥ï \end{array} \right.Âπ≥ÊªëÊ≥ïÁßªÂä®Âπ≥ÂùáÊ≥ïÂÅáËÆæÂú®ÊØîËæÉÁü≠ÁöÑÊó∂Èó¥Èó¥ÈöîÈáåÔºåÂ∫èÂàóÁöÑÂèñÂÄºÊòØËæÉÁ®≥ÂÆöÁöÑÔºåËøôÁßçÂ∑ÆÂºÇÊòØÁî±ÈöèÊú∫Ê≥¢Âä®ÈÄ†ÊàêÁöÑ„ÄÇÁî±Ê≠§ÔºåÂèØÁî®‰∏ÄÂÆöÊó∂Èó¥Èó¥ÈöîÂÜÖÁöÑÂπ≥ÂùáÂÄº‰Ωú‰∏∫Êüê‰∏ÄÊúüÁöÑ‰º∞ËÆ°ÂÄº„ÄÇ nÊúü‰∏≠ÂøÉÁßªÂä®Âπ≥Âùá \widetilde{x_t} = \frac{1}{n}(\frac{1}{2}x_{t-\frac{n}{2}}+x_{t-\frac{n}{2}+1}+\dots+x_{t+\frac{n}{2}-1}+\frac{1}{2}x_{t+\frac{n}{2}})nÊúüÁßªÂä®Âπ≥Âùá \widetilde{x_t} = \frac{1}{n}(x_t+x_{t-1}+\dots+x_{t-n+1})ÊåáÊï∞Âπ≥ÊªëÊ≥ïÁÆÄÂçïÊåáÊï∞Âπ≥Êªë \widetilde{x_t} = \alpha x_t+\alpha (1-\alpha )x_{t-1}+\dots)Â≠£ËäÇÊïàÂ∫îÂ≠£ËäÇÊÄßÊïàÂ∫îÁöÑÂ≠òÂú®Ôºå‰ΩøÂæóÊ∞îÊ∏©‰ºöÂú®‰∏çÂêåÂπ¥‰ªΩÁöÑÁõ∏ÂêåÊúà‰ªΩÂëàÁé∞Âá∫Áõ∏‰ººÁöÑÊÄßË¥®„ÄÇ Â¶ÇÊûúÂè™ÊòØÂ≠òÂú®Â≠£ËäÇÊÄßÂíåÈöèÊú∫Ê≥¢Âä®ÊÄß x_{ij} = \hat{x}S_j+I_{ij}ÂÖ∂‰∏≠$S_j$Ë°®Á§∫Á¨¨j‰∏™ÊúàÁöÑÂ≠£ËäÇÊåáÊï∞Ôºå$\hat{x}$‰∏∫ÂêÑÊúàÂπ≥ÂùáÊ∞îÊ∏©„ÄÇ Â≠£ËäÇÊåáÊï∞ÁöÑËÆ°ÁÆó: Step1: ËÆ°ÁÆóÂë®ÊúüÂÜÖÂêÑÊúüÁöÑÂπ≥ÂùáÊï∞ \hat{x}_k = \frac{\sum_{i= 1}^{n}x_{ik}}{n}Ôºàk = 1,2,...,m)ÂÖ∂‰∏≠ÔºåmË°®Á§∫Âë®ÊúüÔºånË°®Á§∫Âë®ÊúüÁöÑÊï∞Èáè Step2: ËÆ°ÁÆóÊÄªÂπ≥ÂùáÊï∞ \hat{x} = \frac{\sum_{i = 1}^{n}\sum_{k = 1}^{m}x_{ik}}{nm}Step3: ËÆ°ÁÆóÂ≠£ËäÇÊåáÊï∞ S_k = \frac{\hat{x}_k}{\hat{x}}Ê∑∑ÂêàÊïàÂ∫îÂä†Ê≥ïÊ®°Âûã x_t = T_t + S_t + I_t‰πòÊ≥ïÊ®°Âûã x_t = T_t*S_t*I_tÊ∑∑ÂêàÊ®°Âûã x_t = S_t*T_t+I_t\\ x_t = S_t*(T_t+I_t)Â¶ÇÊûúÂ≠£ËäÇÊ≥¢Âä®ÁöÑÊåØÂπÖ‰∏çÂèóË∂ãÂäøÂèòÂä®ÁöÑÂΩ±ÂìçÔºåÂàôËØ¥ÊòéÂ≠£ËäÇÊÄß‰∏éË∂ãÂäø‰πãÈó¥Ê≤°ÊúâÁõ∏‰∫í‰ΩúÁî®ÂÖ≥Á≥ªÔºåÂèØÂä†„ÄÇÂ¶ÇÊûúÂ≠£ËäÇÊ≥¢Âä®ÁöÑÊåØÂπÖÈöèË∂ãÂäøÁöÑÂèòÂåñËÄåÂèòÂåñÔºåÊòØÁõ∏‰∫í‰ΩúÁî®ÁöÑÂÖ≥Á≥ªÔºåÂèØÂ∞ùËØïÊ∑∑ÂêàÊ®°ÂûãÂíå‰πòÊ≥ïÊ®°Âûã„ÄÇ Tool in Python: xfreshÁâπÂæÅÊèêÂèñÂÆòÁΩëÔºö https://tsfresh.readthedocs.io/en/latest/text/quick_start.html ‰∏≠ÊñáÔºö https://github.com/SimaShanhe/tsfresh-feature-translation Data Formatscolumn_id: Features will be extracted individually for each entity(id); one row per id. column_sort: sorting the time series. ÁâπÂæÅÊèêÂèñ: ÂèØ‰ª•‰∏ÄÊ¨°ÊÄßÊèêÂèñÂÆåÔºõ‰πüÂèØ‰ª•ÂçïÁã¨ÊèêÂèñkind_to_parameters ËÆæÁΩÆÂèÇÊï∞ÔºõËøòÂèØ‰ª•ÊèêÂèñ ÂèØÂàÜÂ∏ÉÂºèËÆ°ÁÆó the rolling mechanism È¶ñÂÖàÁ°ÆÂÆöÊªëÂä®Á™óÂè£ Step1 : ÂÆûÁé∞ÂçïÂèòÈáèÁâπÂæÅÁöÑÊèêÂèñ Step2 : ÂÆûÁé∞Â§öÂèòÈáèÁâπÂæÅÁöÑÊèêÂèñ Day Ox 01Áü•ËØÜÊ∏ÖÂçï: ÁâπÂæÅÊèêÂèñÔºöÂ§ßÊ¶Ç‰∏äÂçÉÁßçÁâπÂæÅÔºàÂá†ÂçÅÁßçÊñπÊ≥ïÔºâ tsfresh.feature_extraction.extraction.extract_features(timeseries_container,default_fc_parameters=None, kind_to_fc_parameters=None**, column_id=None, column_sort=None, column_kind=None, column_value=None, chunksize=None, n_jobs=1, show_warnings=False, disable_progressbar=False, impute_function=None, profile=False, profiling_filename=‚Äôprofile.txt‚Äô, profiling_sorting=‚Äôcumulative‚Äô, distributor=None)** pandas.DataFrame containing the different time series column_id (str) ‚Äì The name of the id column to group by. column_sort (str) ‚Äì The name of the sort column. n_jobs (int) ‚Äì The number of processes to use for parallelization. Êó∂Èó¥Â∫èÂàóÁöÑÊªëÂä®Á™óÂè£ÔºàÂçïÂ∫èÂàóÂàíÂàÜÊàêÂ§öÂ∫èÂàóÔºâ tsfresh.utilities.dataframe_functions.``roll_time_series(*df_or_dict*, column_id**, column_sort=None, column_kind=None, rolling_direction=1, max_timeshift=None, min_timeshift=0, chunksize=None, n_jobs=1, show_warnings=False, disable_progressbar=False, distributor=None)** max_timeshift (int) ‚Äì If not None, the cut-out window is at maximum max_timeshift large. If none, it grows infinitely. min_timeshift (int) ‚Äì Throw away all extracted forecast windows smaller or equal than this. Must be larger than or equal 0. n_jobs (int) ‚Äì The number of processes to use for parallelization. If zero, no parallelization is used. show_warnings=False ÔºàÊåáÂÆöÔºâÁâπÂæÅÊèêÂèñ ÊòæËëóÊÄßÊ£ÄÊµã https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_selection.html?highlight=select_features#tsfresh.feature_selection.selection.select_features Áõ∏ÂÖ≥ÊÄßÊ£ÄÊµã https://tsfresh.readthedocs.io/en/latest/text/parallelization.html#parallelization-of-feature-selection 123456789101112131415161718192021222324252627282930313233from tsfresh import extract_features, select_features,extract_relevant_featuresfrom tsfresh.utilities.dataframe_functions import imputefrom tsfresh.utilities.dataframe_functions import roll_time_series, make_forecasting_frameimport pandas as pdimport tsfresh as tsf fc_parameters_value1 = &#123;"length": None, "sum_values": None&#125;fc_parameters_value2 = &#123;"maximum": None, "minimum": None&#125;kind_to_fc_parameters = &#123; "value1": fc_parameters_value1, "value2": fc_parameters_value2&#125;if __name__ == '__main__': # ceate data rawdata = &#123;'id1': [0,0,0,0,0,1,1,1,1,1],'time': [1,2,3,4,5,10,11,12,13,14],\ 'value1': [1,2,3,4,5,6,7,8,9,10], 'value2': [1,2,3,4,5,6,7,8,9,10] &#125; df = pd.DataFrame(rawdata)# ËÆæÁΩÆÈïøÂ∫¶+1 = ÁúüÂÆûÈïøÂ∫¶,ÊòØÂΩìÂâçÁºñÂè∑ÂæÄ‰∏äÊï∞. df_rolled = roll_time_series(df, column_id="id1", column_sort="time", max_timeshift=1, min_timeshift=0)# roll_time_seriesÁöÑËøîÂõûÂÄº print(df_rolled) df_rolled = df_rolled.drop('id1',axis = 1)# column_id: ËÅöÂêàÂàó column_sort:ÊéíÂ∫èÔºå‰∏Ä‰∏™column_idÂ∞±ÂØπÂ∫î‰∏Ä‰∏™ÁâπÂæÅ extracted_features = extract_features(df_rolled, column_id='id', column_sort='time', kind_to_fc_parameters = kind_to_fc_parameters, show_warnings=False) print(extracted_features) Day Ox 02 Êü•ÁúãÊèêÂèñÁâπÂæÅÂèØÊ†πÊçÆÊ≠§ÊèêÂèñËá™Âä®ÊèêÂèñÁöÑÁâπÂæÅÔºåÁî®‰∫éÈ¢ÑÊµãÊó∂ÂÄôÁöÑÊèêÂèñÁâπÂæÅ 1kind_to_fc_parameters = tsf.feature_extraction.settings.from_columns(extracted_features) 1234# 5. ÁâπÂæÅÊäΩÂèñ‰∏éËøáÊª§ÂêåÊó∂ËøõË°åÔºà‰∏ÄÊ≠•Âà∞‰ΩçÔºåÁúÅÂéªÂ§ö‰ΩôËÆ°ÁÆóÔºâ# column_id: group by #features_filtered_direct = extract_relevant_features(timeseries, y, column_id='id', column_sort='time')#print(features_filtered_direct.head()) Â≠¶‰π†Ë∑ØÂæÑÔºö 1. Êï∞ÊçÆÊ†ºÂºè 2. ÊªëÂä®Á™óÂè£ËÆæÁΩÆ 3. ÁâπÂæÅÊèêÂèñ 4. ÁâπÂæÅÈÄâÊã© ‰∏ìÈ¢ò Êó∂Èó¥Â∫èÂàóÁöÑÁ´ûËµõÊñπÊ°àhttps://mp.weixin.qq.com/s?__biz=MzU1Nzc1NjI0Nw==&amp;mid=2247485604&amp;idx=1&amp;sn=6283ec080344665bfad90570bf1504a4&amp;chksm=fc31b29ccb463b8acac7acf4d89494aaad0c76620becb2b07c370ccbfaff850edc3c1ad4e0fd&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1593390448780&amp;sharer_shareid=fb5716a8ad12ea6329433df53d4cbf64#rd https://www.zhihu.com/question/21229371/answer/533770345 Prophet Â∑•ÂÖ∑]]></content>
  </entry>
  <entry>
    <title><![CDATA[ÂõûÂΩíÂàÜÊûê]]></title>
    <url>%2F2020%2F06%2F20%2F%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[[TOC] ÂõûÂΩíÂàÜÊûêÊúÄÁÆÄÂçïÁöÑÁ∫øÊÄßÂõûÂΩíÔºåÈÅøÂÖçÂ§öÈáçÂÖ±Á∫øÊÄßÔºåËøáÊãüÂêàÔºåÂºïÂÖ•Ê≠£ÂàôÈ°πÁöÑÁ∫øÊÄßÂõûÂΩíÊ®°Âûã„ÄÇÊ∂âÂèäÂà∞ÁöÑÊï∞Â≠¶Áü•ËØÜÔºö‰∏ÄËåÉÊï∞Ôºå‰∫åËåÉÊï∞ÔºåÂ§öÂÖÉÂáΩÊï∞Ê±ÇÊûÅÂÄº„ÄÇÊ®°ÂûãÁöÑÂê´‰πâÔºåÂèÇÊï∞Ê±ÇËß£ÁÆóÊ≥ïÔºåÁõÆÊ†áÂáΩÊï∞Ôºå‰ª•ÂèäÂêÑÁßçÊ®°ÂûãÁöÑ‰ºòÁº∫ÁÇπ„ÄÇ ÂÆö‰πâÂõûÂΩíÂàÜÊûêÊòØÂØªÊâæËá™ÂèòÈáèÂíåÂõ†ÂèòÈáè‰πãÈó¥ÁöÑÊï∞ÈáèÂÖ≥Á≥ªÔºåÁî®‰∫éÈ¢ÑÊµãÂª∫Ê®°ÁöÑÊñπÊ≥ï„ÄÇÂÖ∂‰∏ÄÔºåÂÆÉÂèØ‰ª•Êè≠Á§∫Ëá™ÂèòÈáèÂíåÂõ†ÂèòÈáè‰πãÈó¥ÁöÑÊòæËëóÊÄßÊ£ÄÊµã„ÄÇÂÖ∂‰∫åÔºåÊè≠Á§∫Â§ö‰∏™Ëá™ÂèòÈáèÂØπ‰∏Ä‰∏™Âõ†ÂèòÈáèÁöÑÂΩ±ÂìçÁ®ãÂ∫¶Â§ßÂ∞è„ÄÇ ÂõûÂΩíÁ±ªÂûã1ÔºâÁã¨Á´ãÂèòÈáèÁöÑÊï∞Èáè 2ÔºâÂ∫¶ÈáèÂèòÈáèÁöÑÁ±ªÂûã 3ÔºâÂõûÂΩíÁ∫øÁöÑÂΩ¢Áä∂ 1. Á∫øÊÄßÂõûÂΩíÔºàLinear Regression)Âõ†ÂèòÈáèÔºöËøûÁª≠Ôºõ Ëá™ÂèòÈáèÔºöËøûÁª≠ÊàñËÄÖÁ¶ªÊï£ Ê®°ÂûãÁöÑÂΩ¢Âºè Y = a+bX+ùúÄ\\ \left(\begin{array}{c} y_{1} \\ y_{2} \\ \vdots \\ y_{n} \end{array}\right)=\left(\begin{array}{cccc} 1 & x_{11} & \cdots & x_{1(p-1)} \\ 1 & x_{21} & \cdots & x_{2(p-1)} \\ \vdots & \vdots & \vdots & \vdots \\ 1 & x_{n 1} & \cdots & x_{n(p-1)} \end{array}\right) \beta+\left(\begin{array}{c} e_{1} \\ e_{2} \\ \vdots \\ e_{n} \end{array}\right)\\ Y_{n*1} = X_{n*p}\beta+ùúÄwhere $a$ and $b$ are the regression coefficients, and ùúÄ is the random error. ÁõÆÊ†áÂáΩÊï∞ min SSR = \sum_{i}(y_i-f(x_i))^2\\ min_{w}||Xw-y||_2^2ÂèÇÊï∞‰º∞ËÆ°ÊúÄÂ∞è‰∫å‰πòÊ≥ïÔºàLease Square Method)ÔºàOLS) This approach is called the method of ordinary least squares. Ê®°ÂûãËØÑ‰º∞ÊãüÂêà‰ºòÂ∫¶ R-square , coefficient of determinationLarger $R^2$ indicates a better fit and means that the model can better explain the variation of the output with different inputs. https://realpython.com/linear-regression-in-python/ Ë¶ÅÊ±Ç Ëá™ÂèòÈáèÂíåÂõ†ÂèòÈáè‰πãÈó¥ÂøÖÈ°ªÊª°Ë∂≥Á∫øÊÄßÂÖ≥Á≥ª„ÄÇ Â§öÂÖÉÂõûÂΩíÂ≠òÂú®Â§öÈáçÂÖ±Á∫øÊÄßÔºåËá™Áõ∏ÂÖ≥ÊÄßÂíåÂºÇÊñπÂ∑ÆÊÄß„ÄÇ Á∫øÊÄßÂõûÂΩíÂØπÂºÇÂ∏∏ÂÄºÈùûÂ∏∏ÊïèÊÑü„ÄÇÂºÇÂ∏∏ÂÄº‰ºö‰∏•ÈáçÂΩ±ÂìçÂõûÂΩíÁ∫øÂíåÊúÄÁªàÁöÑÈ¢ÑÊµãÂÄº„ÄÇ Â§öÈáçÂÖ±Á∫øÊÄß‰ºöÂ¢ûÂä†Á≥ªÊï∞‰º∞ËÆ°ÁöÑÊñπÂ∑ÆÔºåÂπ∂‰∏î‰ΩøÂæó‰º∞ËÆ°ÂØπÊ®°Âûã‰∏≠ÁöÑÂæÆÂ∞èÂèòÂåñÈùûÂ∏∏ÊïèÊÑü„ÄÇÁªìÊûúÊòØÁ≥ªÊï∞‰º∞ËÆ°‰∏çÁ®≥ÂÆö„ÄÇ Âú®Â§ö‰∏™Ëá™ÂèòÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨ÂèØ‰ª•ÈááÁî®Ê≠£ÂêëÈÄâÊã©„ÄÅÂêëÂêéÊ∂àÈô§ÂíåÈÄêÊ≠•ÈÄâÊã©ÁöÑÊñπÊ≥ïÊù•ÈÄâÊã©ÊúÄÈáçË¶ÅÁöÑËá™ÂèòÈáè„ÄÇ ÈÄªËæëÂõûÂΩíÔºàLogistic Regression)Logistic ÂõûÂΩíÁöÑÊú¨Ë¥®ÊòØÔºöÂÅáËÆæÊï∞ÊçÆÊúç‰ªéËøô‰∏™ÂàÜÂ∏ÉÔºåÁÑ∂Âêé‰ΩøÁî®ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÂÅöÂèÇÊï∞ÁöÑ‰º∞ËÆ°„ÄÇ Logistic ÂàÜÂ∏É F(x) = P(X]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>ÂõûÂΩíÂàÜÊûê</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êó•Âøó]]></title>
    <url>%2F2020%2F06%2F20%2F%E6%88%91%E4%BB%BB%E6%80%A7%2F</url>
    <content type="text"><![CDATA[‰ªäÂ§©ÂíåÊàëÊú¨ÁßëÂêåÂ≠¶ÔºåÁ°ïÂ£´‰πüÂú®Âêå‰∏ÄÊâÄÂ§ßÂ≠¶„ÄÇÂØπÊñπÂ∑≤ÁªèÂñúÊèê‰∏ÄÁØáÈ´òÂàÜSCI„ÄÇÂíåÊàëËÅä‰∫ÜËÅäËØªÂçöËøô‰ª∂‰∫ãÊÉÖ„ÄÇÂèåÊñπÂè™ÊòØÂæàÊÄÄÁñëÂçöÂ£´ÁîüÊ∂ØÁöÑÊàêÂäüÂê¶Ôºü ÁîüÊ¥ªÂÖ∂ÂÆû‰πü‰∏çÈöæ ÁîüÂ≠òËøòÊòØÊØÅÁÅ≠ÔºüËøôÊòØ‰∏™ÈóÆÈ¢ò„ÄÇÁ©∂Á´üÂì™Ê†∑Êõ¥È´òË¥µÔºåÂéªÂøçÂèóÈÇ£ÁãÇÊö¥ÁöÑÂëΩËøêÊó†ÊÉÖÁöÑÊëßÊÆã,ËøòÊòØÊå∫Ë∫´ÂéªÂèçÊäóÈÇ£Êó†ËæπÁöÑÁÉ¶ÊÅºÔºåÊääÂÆÉÊâ´‰∏Ä‰∏™Âπ≤ÂáÄ„ÄÇÂéªÊ≠ªÔºåÂéªÁù°Â∞±ÁªìÊùü‰∫ÜÔºåÂ¶ÇÊûúÁù°Áú†ËÉΩÁªìÊùüÊàë‰ª¨ÂøÉÁÅµÁöÑÂàõ‰º§ÂíåËÇâ‰ΩìÊâÄÊâøÂèóÁöÑÂçÉÁôæÁßçÁóõËã¶ÔºåÈÇ£ÁúüÊòØÁîüÂ≠òÊ±Ç‰πã‰∏çÂæóÁöÑÂ§©Â§ßÁöÑÂ•Ω‰∫ã„ÄÇÂéªÊ≠ªÔºåÂéªÁù°ÔºåÂéªÁù°Ôºå‰πüËÆ∏‰ºöÂÅöÊ¢¶ÔºÅÂîâÔºåËøôÂ∞±È∫ªÁÉ¶‰∫ÜÔºåÂç≥‰ΩøÊëÜËÑ±‰∫ÜËøôÂ∞ò‰∏ñÂèØÂú®ËøôÊ≠ªÁöÑÁù°Áú†ÈáåÂèà‰ºöÂÅö‰∫õ‰ªÄ‰πàÊ¢¶Âë¢ÔºüÁúüÂæóÊÉ≥‰∏ÄÊÉ≥ÔºåÂ∞±ËøôÁÇπÈ°æËôë‰Ωø‰∫∫ÂèóÁùÄÁªàË∫´ÁöÑÊäòÁ£®,Ë∞ÅÁîòÂøÉÂøçÂèóÈÇ£Èû≠ÊâìÂíåÂò≤ÂºÑÔºåÂèó‰∫∫ÂéãËø´ÔºåÂèóÂ∞Ω‰æÆËîëÂíåËΩªËßÜÔºåÂøçÂèóÈÇ£Â§±ÊÅãÁöÑÁóõËã¶ÔºåÊ≥ïÂ∫≠ÁöÑÊãñÂª∂ÔºåË°ôÈó®ÁöÑÊ®™ÂæÅÊö¥ÊïõÔºåÈªòÈªòÊó†ÈóªÁöÑÂä≥Á¢åÂç¥Âè™Êç¢Êù•Â§öÂ∞ëÂáåËæ±„ÄÇ‰ΩÜ‰ªñËá™Â∑±Âè™Ë¶ÅÁî®ÊääÂ∞ñÂàÄÂ∞±ËÉΩËß£ËÑ±‰∫Ü„ÄÇ‚ÄîËééÂ£´ÊØî‰∫ö ÊàëÈÄÇÂêàËØªÂçöÂêóÔºü‚ÄîÊù°‰ª∂ÂèÇËÄÉÔºö https://mp.weixin.qq.com/s?__biz=MjM5Nzc3ODkyMA==&amp;mid=210024495&amp;idx=3&amp;sn=8088481127fadb9e9cb1e7b862555025&amp;chksm=2fa4dca818d355be6d02c5f1fe597a5a034880afc1fddb02bce9829259663bb0434f8b84b23e&amp;mpshare=1&amp;scene=24&amp;srcid=0707XZ5UVRbuOujh3mA4rsRH&amp;sharer_sharetime=1594093816416&amp;sharer_shareid=0e2d0ffe45c3a6dfb66aa422c3a1381d#rd ËØÑ‰ª∑‰∏Ä‰∏ãËá™Â∑±ÊòØÂê¶Êúâ‰∏ãËø∞ËÉΩÂäõÔºö Êô∫ÂäõÔºöÊòæÁÑ∂ ËØÑ‰ª∑:Êú¨ÁßëÊï∞Â≠¶ÔºåÁúãÁé∞Áä∂‰∏ì‰∏öÈ¢ÜÂüüÁöÑÊñáÁåÆÂ§ßÈÉ®ÂàÜÊòØOK Êó∂Èó¥ÔºöÂæÄÂæÄÊØî‰Ω†ÊÉ≥ÊÉ≥ÁöÑË¶ÅÈïøÔºå‰Ω†ËÉΩÊâøÂèóÂêó? ËØÑ‰ª∑ÔºöÊú¨ÁßëÂíåÁ†îÁ©∂ÁîüÂèØ‰ª•ÂÖ®Ë∫´ÂøÉÊäïÂÖ•„ÄÇ ÂàõÈÄ†ÂäõÔºöËØªÂçöÈúÄË¶Å‰Ω†Áî®Êñ∞ÁöÑÊÄùË∑ØÁúãÂæÖÈóÆÈ¢ò„ÄÇÈóÆÈóÆËá™Â∑±ÂñúÊ¨¢‚ÄúËÑëÁ≠ãÊÄ•ËΩ¨ÂºØ‚ÄùÂêóÔºü‰Ω†Â≠¶È´òÊï∞Êó∂ÊÑüÂà∞ÊúâÊÑèÊÄùÂêóÔºü ËØÑ‰ª∑ÔºöÁé∞Áä∂ËøòÊ≤°Êúâ‰ªÄ‰πàÂàõÊñ∞ËÉΩÂäõ„ÄÇ Â•ΩÂ•áÂøÉÔºö‰Ω†ÊòØ‰∏çÊòØÂº∫ÁÉàÁöÑÊÉ≥Áü•ÈÅìÂë®Âõ¥‰∫ãÁâ©ËÉåÂêéÁöÑËßÑÂæãÔºü ËØÑ‰ª∑Ôºö‰∏çÊÄé‰πàÊÄùËÄÉ ÈÄÇÂ∫îËÉΩÂäõÔºöËØªÂçöÂ∏∏‰ºöÂá∫‰πéÊÑèÊñôÁöÑÂõ∞ÈöæÔºå‰Ω†ÂèØËÉΩ‰ºöÂà∞‰∏Ä‰∏™Ê≤°Êúâ‰∫∫Áü•ÈÅìÁ≠îÊ°àÁöÑÈ¢ÜÂüü„ÄÇ‰Ω†ËÉΩÂøçÂèóÈÉÅÈó∑ÂêóÔºüËÉΩÂøçÂèó‰ΩèÊâæÊ≤°‰∫∫Áü•ÈÅìÁöÑÁ≠îÊ°àÊó∂ÁöÑÊûØÁá•ÂêóÔºü ËØÑ‰ª∑ÔºöÊàëÂñúÊ¨¢Áã¨Â§ÑÂïä„ÄÇ Ëá™ÊàëÈ©±Âä®ÔºöÊïôÊéà‰∏ç‰ºöÂëäËØâ‰Ω†ÊÄé‰πàÂÅöÔºå‰Ω†ËÉΩËá™Â∑±ÁªôËá™Â∑±ÈïøÊúüÁßëÁ†îÁöÑÂä®ÂäõÂêó? ËØÑ‰ª∑ÔºöÊ≤°‰ªÄ‰πàÈóÆÈ¢ò„ÄÇ Á´û‰∫âËÉΩÂäõÔºö‰Ω†Â∞Ü‰∏éÊúÄËÅ™ÊòéÁöÑ‰∫∫ÂÖ±‰∫ãÔºåÂà´‰∫∫‰ºöÂ∞Ü‰Ω†‰∏éËøô‰∫õ‰∫∫ÊØîËæÉÔºå‰Ω†ÊâõÂæó‰ΩèÂêó? ËØÑ‰ª∑ÔºöË¶ÅÂÅöÂà∞ËØ•È¢ÜÂüü5%‰ª•Á±ªÁöÑÁ†îÁ©∂„ÄÇË¶ÅË∑üÂæàÂ§ö‰∫∫Êä¢È•≠Á¢ó„ÄÇ ÊàêÁÜüÔºöËØªÂçöÊó∂Èó¥Â§ßÈÉ®ÂàÜÁî±‰Ω†Ëá™Â∑±ÊîØÈÖçÔºå‰Ω†Ë¶ÅËá™Â∑±ÂÆâÊéíËá™Â∑±ÁöÑÊó•Á®ã ËØÑ‰ª∑ÔºöÁªÜËäÇËá™Â∑±Â§ÑÁêÜ Ê∏ÖË¥´Ôºö ËØÑ‰ª∑ÔºöÂçöÂ£´Ê∑∑ÁöÑÂ•ΩÂπ∂‰∏ç‰∫ö‰∫éÊâæÂ∑•‰ΩúÁöÑÁîüÊ¥ªÊù°‰ª∂„ÄÇ Áªº‰∏äÊâÄËø∞ÔºöÊúÄÂ§ßÁöÑÈóÆÈ¢ò‰∏çÂú®‰∫éËÇØ‰∏çËÇØÂêÉËã¶ÔºåËÄåÂú®‰∫éËÉΩ‰∏çËÉΩÈ°∫Âà©ÂÆåÊàêÂçöÂ£´ÁîüÊ∂Ø„ÄÇÊÑüËßâËá™Â∑±ÂèóÈôê‰∫éËá™Â∑±ÁöÑÊô∫ÂïÜÂêß! ‰πüÂèØËÉΩÊàëÂØπÂçöÂ£´Ë¶ÅÂ§™È´ò‰∫ÜÂêßÔºÅ Ê∏¥ÊúõËØªÂçöÁöÑÂéüÂõ†‰∏ªË¶ÅÊòØ‰∏§ÊñπÈù¢Ôºö‰∏ÄÊòØËá™Ë∫´ÁöÑÁªèÂéÜ„ÄÇÊú¨ÁßëÊúüÈó¥ÁöÑÁ°ÆÊéåÊè°‰∫ÜÂü∫Á°ÄÁöÑÁßëÁ†îÊÄùË∑ØÔºåÁßëÁ†îÂ∑•ÂÖ∑„ÄÇ‰∫åÊòØËá™Ë∫´ÁöÑÂñúÂ•Ω„ÄÇÁßëÁ†îÁéØÂ¢ÉÁõ∏ÂØπËΩªÊùæÔºå‰∫∫ÈôÖÂÖ≥Á≥ª‰∏çÈÇ£‰πàÂ§çÊùÇ„ÄÇËØÑÂà§Ê†áÂáÜ‰πüÁõ∏ÂØπÁÆÄÂçï„ÄÇ Èöæ&lt;&lt;‚Äî‚Äî&gt;&gt;ÊÉ≥ ÂçöÂºàÁöÑËøáÁ®ã„ÄÇ]]></content>
      <categories>
        <category>Êó•Âøó</category>
      </categories>
      <tags>
        <tag>ÁîüÊ¥ªÊó•Âøó</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Áü•ËØÜÊ∏ÖÂçï]]></title>
    <url>%2F2020%2F06%2F19%2F%E7%9F%A5%E8%AF%86%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[‰∏ªË¶ÅÊòØÂàóÂá∫ÂÖ≥‰∫éÊó•Â∏∏‰∏≠ÈÅáÂà∞ÁöÑÂæàÂ•ΩÁöÑËµÑÊñôÔºåËá™Â∑±‰∏çÊ∏ÖÊ•öÁöÑÊñáÁ´†ÂíåËµÑÊñô„ÄÇ 2020-6-29 ZÊ£ÄÊµãÂíåTÊ£ÄÊµãhttps://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&amp;mid=2247485455&amp;idx=1&amp;sn=857066158bf8c2de38939f3037416035&amp;chksm=eb9321b9dce4a8afd68d764c295f8bcc69c62f2b1d000f3e1c5e61a7d9b6e2ec3de8df068174&amp;mpshare=1&amp;scene=24&amp;srcid=&amp;sharer_sharetime=1593403964973&amp;sharer_shareid=0e2d0ffe45c3a6dfb66aa422c3a1381d#rd 2020-6-28ËßÜÈ¢ëÔºö http://www.julyedu.com/video/play/58/405 2020-6-19SQL ‰∏≠Êñá: https://www.liaoxuefeng.com/wiki/1177760294764384 Ëã±ÊñáÔºö https://www.codecademy.com/courses/learn-sql/lessons/manipulation/exercises/sql ËßÜÈ¢ëÔºö https://www.jikexueyuan.com/course/sql/ Âü∫Á°Ä https://study.163.com/course/courseMain.htm?courseId=215012&amp;_trace_c_p_k2_=f68f3d2867a343789ac2d3cfa92dd308 https://www.nowcoder.com/discuss/95812?type=2 https://www.cnblogs.com/zsh-blogs/category/1413021.html]]></content>
      <categories>
        <category>ËßÑÂàí</category>
      </categories>
      <tags>
        <tag>ÊäÄËÉΩ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÁªüËÆ°Â≠¶]]></title>
    <url>%2F2020%2F06%2F19%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[ÁªüËÆ°Â≠¶ day Ox00day Ox00 È¶ñÂÖà‰ªãÁªçÈöèÊú∫ÂÆûÈ™åËÆæËÆ°Âà∞ÁöÑÂü∫Êú¨Ê¶ÇÂøµÔºåÂåÖÊã¨ÈöèÊú∫ÂÆûÈ™åÔºåÊ†∑Êú¨ÁÇπÔºåÊ†∑Êú¨Á©∫Èó¥ÔºåÂü∫Êú¨‰∫ã‰ª∂ÔºåÈöèÊú∫‰∫ã‰ª∂ÔºõÂÖ∂Ê¨°‰ªãÁªçÊ¶ÇÁéáËÆ∫ÁöÑÂü∫Êú¨Ê¶ÇÂøµÔºåÂåÖÊã¨Ê¶ÇÁéáÁöÑÂÖ¨ÁêÜÂåñÂÆö‰πâÔºåÂè§ÂÖ∏Ê¶ÇÁéáÔºåÊù°‰ª∂Ê¶ÇÁéáÔºåÂÖ®Ê¶ÇÁéáÔºåË¥ùÂè∂ÊñØÂÖ¨ÂºèÁ≠âÁ≠â„ÄÇÁâπÂà´Ê≥®ÊÑè‰∏§‰∏™ÂÆπÊòìÊ∑∑Ê∑ÜÁöÑÊ¶ÇÂøµÔºö‰∫ã‰ª∂ÁöÑÁã¨Á´ãÊÄßÂíå‰∫íÊñ•„ÄÇ day Ox01 È¶ñÂÖàÂºïÂá∫ÈöèÊú∫ÂèòÈáèÁöÑÂÆö‰πâÔºå‰ªéÁ¶ªÊï£ÈöèÊú∫ÂèòÈáèÂíåËøûÁª≠ÈöèÊú∫ÂèòÈáè‰∏§‰∏™Áª¥Â∫¶Ôºå‰ªãÁªç‰∫ÜÂÖ∏ÂûãÁöÑÂàÜÂ∏ÉÂáΩÊï∞„ÄÇÂÖ∂‰∏≠Ê¶ÇÁéáÂáΩÊï∞ÂíåÂàÜÂ∏ÉÂáΩÊï∞ÊòØÈùûÂ∏∏ÈáçË¶ÅÁöÑÊ¶ÇÂøµ„ÄÇ Âü∫Êú¨Ê¶ÇÂøµÈöèÊú∫ËØïÈ™åÔºöËÆ∞‰Ωú$E$ Ê†∑Êú¨ÁÇπÔºö ÈöèÊú∫ËØïÈ™å‰∏≠Âá∫Áé∞ÁöÑÂèØËÉΩÁªìÊûúÁß∞‰∏∫Ê†∑Êú¨ÁÇπÔºåËÆ∞‰Ωú $\omega$ Ê†∑Êú¨Á©∫Èó¥Ôºö ÊâÄÊúâÊ†∑Êú¨ÁÇπÁªÑÊàêÁöÑÈõÜÂêàÁß∞‰∏∫Ê†∑Êú¨Á©∫Èó¥ÔºåÈöèÊú∫ÂÆûÈ™åÊâÄÊúâÁöÑÁªìÊûúÁöÑÈõÜÂêàÔºåËÆ∞‰Ωú$\Omega$ ‰∫ã‰ª∂Ôºö Ê†∑Êú¨Á©∫Èó¥ÁöÑÂ≠êÈõÜÔºåÂè´ÂÅöÈöèÊú∫‰∫ã‰ª∂ÔºåËÆ∞‰ΩúA,B,C„ÄÇ ‚Äã ÂàÜÁ±ªÔºöÂü∫Êú¨‰∫ã‰ª∂ÔºàÁî±‰∏Ä‰∏™Ê†∑Êú¨ÁÇπÊûÑÊàêÔºâÔºå‰∏çÂèØËÉΩ‰∫ã‰ª∂Ôºà‰∏çÂåÖÂê´‰ªª‰ΩïÊ†∑Êú¨ÁÇπÔºâÔºåÂøÖÁÑ∂‰∫ã‰ª∂ÔºàÊ†∑Êú¨Á©∫Èó¥ÁöÑÊâÄÊúâÊ†∑Êú¨ÁÇπÁªÑÊàêÔºâ ‰∫ã‰ª∂ÁöÑÂÖ≥Á≥ªÂíåËøêÁÆó ‚Äã A‰∏éB‰∫íÊñ•Ôºà‰∫í‰∏çÁõ∏ÂÆπÔºâÔºåÂπ∂‰∏∫Á©∫ÈõÜ„ÄÇ‰∏çÂèØËÉΩÂêåÊó∂ÂèëÁîü„ÄÇ ‚Äã ÂØπÁ´ãÔºà‰∫íÈÄÜÔºâÔºöA,BÂú®‰∏ÄÊ¨°ÂÆûÈ™å‰∏≠Êúâ‰∏î‰ªÖÊúâ‰∏Ä‰∏™ÂèëÁîü„ÄÇ ‰∫ã‰ª∂Èó¥ÁöÑÂÖ≥Á≥ªÂåÖÂê´ Áõ∏Á≠â ‰∫í‰∏çÁõ∏ÂÆπÊÄßÔºö‰∏çÂèØËÉΩÂêåÊó∂ÂèëÁîüÔºåÊ≤°Êúâ‰∫§ÈõÜ ‰∫ã‰ª∂ÁöÑÊ¶ÇÁéáÊ¶ÇÁéáÁöÑÂÖ¨ÁêÜÂåñÂÆö‰πâËÆ°ÁÆóÊñπÊ≥ïÂè§ÂÖ∏ÊñπÊ≥ïÈöèÊú∫‰∫ã‰ª∂ÁöÑË¶ÅÊ±ÇÔºö(1). Ê∂âÂèäÁöÑÈöèÊú∫Áé∞Ë±°Âè™ÊúâÊúâÈôê‰∏™Âü∫Êú¨ÁªìÊûúÔºà2). ÊØè‰∏™Âü∫Êú¨ÁªìÊûúÂá∫Áé∞ÁöÑÂèØËÉΩÊÄßÊòØÁõ∏ÂêåÁöÑÔºàÁ≠âÂèØËÉΩÊÄßÔºâ ‰∫ã‰ª∂ÁöÑÂü∫Êú¨ÁªìÊûúÔºö P(A) = \frac{k}{n} = \frac{‰∫ã‰ª∂ÂåÖÂê´ÁöÑÂü∫Êú¨‰∫ã‰ª∂ÁöÑ‰∏™Êï∞}{ÂÖ®Á©∫Èó¥ÂåÖÂê´ÁöÑÂü∫Êú¨ÁªìÊûúÊÄªÊï∞}‰∫ã‰ª∂ÁöÑÁã¨Á´ãÊÄß‰∏§‰∏™‰∫ã‰ª∂ÁöÑÁã¨Á´ãÊÄßÊòØÊåá‰∏Ä‰∏™‰∫ã‰ª∂ÁöÑÂèëÁîü‰∏çÂΩ±ÂìçÂè¶‰∏Ä‰∏™‰∫ã‰ª∂ÁöÑÂèëÁîüÔºå P(AB) = P(A)P(B)Â§ö‰∏™‰∫ã‰ª∂ÁöÑÁã¨Á´ãÊÄß P(A_iA_j) = P(A_i)P(A_j)\\ P(A_iA_jA_k) = P(A_i)P(A_j)P(A_k)\\ \vdots P(A_1A_2\cdots A_n) = P(A_1)P(A_2)\cdots P(A_n)ÂÆûÈ™åÁöÑÁã¨Á´ãÊÄßÂÆûÈ™å$E_1$ÁöÑ‰ªªÊÑè‰∏Ä‰∏™ÁªìÊûúÔºà‰∫ã‰ª∂Ôºâ‰∏éÂÆûÈ™å$E_2$ÁöÑ‰ªª‰∏Ä‰∏™ÁªìÊûúÈÉΩÊòØÁõ∏‰∫íÁã¨Á´ãÁöÑ‰∫ã‰ª∂ÔºåÂàôÁß∞ÂÆûÈ™åÁõ∏‰∫íÁã¨Á´ã Êù°‰ª∂Ê¶ÇÁéá P(A|B) = \frac{P(AB)}{P(B)}‰πòÊ≥ïÂÖ¨Âºè P(A_1A_2A_3) = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)ÂÖ®Ê¶ÇÁéáÂÖ¨Âºè P(A) = P(A|B)P(B)+P(A|\hat{B})P(\hat{B}) P(A) = \sum_{i = 1}^nP(A|B_i)P(B_i)Ë¥ùÂè∂ÊñØÂÖ¨Âºè P(B_k|A) = \frac{P(A|B_k)P(B_k)}{\sum_{i = 1}^nP(A|B_k)P(B_k)}ÈöèÊú∫ÂèòÈáè day Ox01ÈöèÊú∫ÂèòÈáèË°®Á§∫ÈöèÊú∫Áé∞Ë±°ÁªìÊûúÔºå‰∏ÄËà¨Â§ßÂÜôÂ≠óÊØçX,Y,Z, ÈöèÊú∫ÂèòÈáèÂèñÂÄºÁî®Â∞èÂÜôÂ≠óÊØçx,y,zÁ≠âË°®Á§∫„ÄÇ Áî®Á≠âÂè∑ÊàñËÄÖ‰∏çÁ≠âÂè∑ÊääX‰∏éxËÅîÁ≥ªËµ∑Êù•Â∞±ÂæàÂ§öÊúâË∂£ÁöÑ‰∫ã‰ª∂ÔºåX=x,Y&lt;y,Á≠âÁ≠âÊûÑÊàê‰∫Ü‰∫ã‰ª∂„ÄÇ ÈöèÊú∫ÂèòÈáèÂÆö‰πâÂú®Âü∫Êú¨Á©∫Èó¥$\Omega$‰∏äÁöÑÂÆûÂÄºÂáΩÊï∞$X = X(w)$Êàê‰∏∫ÈöèÊú∫Á©∫Èó¥ X: w->ÂÆûÊï∞ÂüüÔºàÊò†Â∞Ñ)ÈöèÊú∫ÂèòÈáèÁöÑÂàÜÂ∏ÉÂáΩÊï∞ÂàÜÂ∏ÉÂáΩÊï∞ÁöÑÂÆö‰πâ F(x) = P(X]]></content>
      <categories>
        <category>Êï∞Â≠¶</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Data-Science]]></title>
    <url>%2F2020%2F06%2F11%2FData-Science%2F</url>
    <content type="text"><![CDATA[CourseTsinghua Dr. Yuan Data Mining: Theories and Algorithms for Tackling Big Data ToolsStata: https://www.stata.com/why-use-stata/ https://www.youtube.com/watch?v=AyXeh7iojuA BOOOOOOKhttps://www-users.cs.umn.edu/~kumar001/dmbook/index.php]]></content>
      <tags>
        <tag>Data Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÊØèÂ§©‰∫ÜËß£Â§ö‰∏ÄÁÇπ]]></title>
    <url>%2F2020%2F06%2F11%2F%E6%AF%8F%E5%A4%A9%E4%BA%86%E8%A7%A3%E5%A4%9A%E4%B8%80%E7%82%B9%2F</url>
    <content type="text"><![CDATA[2020-6-29 ‰∏âÁßçÁõ∏ÂÖ≥ÂàÜÊûêÊñπÊ≥ïÁöÑÂºÇÂêå ÂêçÁß∞ ‰∏≠Êñá ÂÖ¨Âºè Êï∞ÊçÆË¶ÅÊ±Ç Âê´‰πâ person correlation coefficient ÁöÆÂ∞îÊ£ÆÁõ∏ÂÖ≥Á≥ªÊï∞ $\rho_{X, Y}=\frac{\operatorname{Cov}(X, Y)}{\sqrt{D(X) D(Y)}}$ a. Ê≠£Â§™ÂàÜÂ∏É„ÄÇÂõ†‰∏∫ËÆ°ÁÆóÁ≥ªÊï∞ÂêéÔºåÈÄöÂ∏∏ÈúÄË¶ÅtÊ£ÄÊµãÔºåtÊ£ÄÊµãÊòØÂü∫‰∫éÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑ„ÄÇb. ÂÆûÈ™åÊï∞ÊçÆ‰πãÈó¥ÁöÑÂ∑ÆË∑ù‰∏çÂêåÂ§™Â§ö [-1,1Á∫øÊÄßÁõ∏ÂÖ≥ spearman correlation coefficient ÊñØÁöÆÂ∞îÊõºÁõ∏ÂÖ≥ÊÄßÁ≥ªÊï∞ $\rho=1-\frac{6 \sum_{i=1}^{N} d_{i}^{2}}{n\left(n^{2}-1\right)}$ Ê≤°Êúâ„ÄÇ‰ΩøÁî®ËåÉÂõ¥ËæÉÂπø kendall correlation coefficient ËÇØÂæ∑Â∞îÁõ∏ÂÖ≥ÊÄßÁ≥ªÊï∞ $\tau=\frac{C-D}{\frac{1}{2} N(N-1)}$ÂÖ∂‰∏≠CË°®Á§∫XY‰∏≠Êã•Êúâ‰∏ÄËá¥ÊÄßÁöÑÂÖÉÁ¥†ÂØπÊï∞Ôºà‰∏§‰∏™ÂÖÉÁ¥†‰∏∫‰∏ÄÂØπÔºâÔºõDË°®Á§∫XY‰∏≠Êã•Êúâ‰∏ç‰∏ÄËá¥ÊÄßÁöÑÂÖÉÁ¥†ÂØπÊï∞„ÄÇ ÂØπË±°ÊòØÂàÜÁ±ªÂèòÈáè„ÄÇÂàÜÊúâÂ∫èÂíåÊó†Â∫è„ÄÇ 2020-6-20ÂΩì‰Ω†ÈÄöËøáÊï∞ÊçÆÂèØËßÜÂåñÊù•Ë°®ËææËßÇÁÇπÔºåÈ¶ñÂÖàËá™Â∑±Ë¶ÅÊúâ‰∏Ä‰∏™Ê∏ÖÊô∞ÁöÑÁªìËÆ∫ÔºåÁÑ∂ÂêéÊúâÈíàÂØπÊÄßÂú∞Á™ÅÂá∫Ëá™Â∑±ÊÉ≥Ë¶ÅË°®ËææÁöÑË¶ÅÁÇπÔºåÈÄöËøáËßÜËßâÂåñÁöÑÂÖÉÁ¥†ÔºåÂºïÂØºËßÇ‰ºóÊ≠£Á°ÆÂú∞ÁêÜËß£Ëá™Â∑±ÁöÑËßÇÁÇπÔºåËÄå‰∏çË¶ÅËÆ©ËßÇ‰ºóËá™Ë°åÂæóÂá∫ÁªìËÆ∫„ÄÇ ÊúÄÂêéÔºåÂàÜ‰∫´‰∏Ä‰∏ãÊàëÂú®Áü•ËØÜÊòüÁêÉ‰∏äÈù¢ÂèëÁöÑÂÖ≥‰∫éÂàÜÊûêÊï∞ÊçÆÁöÑ 5 ÁÇπÊÄùËÄÉ„ÄÇ 1. ÂàÜÊûêÊï∞ÊçÆÁöÑÂπ≥ÂùáÂÄºÂíå‰∏≠‰ΩçÊï∞ ÊØîÂ¶ÇËØ¥ÔºåÂÆ¢Êà∑ÁöÑÂπ¥ÈæÑ„ÄÅË¥≠‰π∞ÈáëÈ¢ùÁöÑÂπ≥ÂùáÂÄºÂíå‰∏≠‰ΩçÊï∞ÂàÜÂà´ÊòØÂ§öÂ∞ëÔºü‰∏≠‰ΩçÊï∞ÂæÄÂæÄÊØîÂπ≥ÂùáÂÄºÊõ¥ÂÖ∑ÊúâÂàÜÊûê‰ª∑ÂÄº„ÄÇ 2. ÂàÜÊûêÊï∞ÊçÆÁöÑÊûÅÂÄºÂíåÂàÜÂ∏É ÊØîÂ¶ÇËØ¥ÔºåÂú®ÊâÄÊúâÁöÑË¥≠‰π∞ËÄÖ‰∏≠ÔºåÂπ¥ÈæÑÊúÄÂ§ßÁöÑÊòØË∞ÅÔºüÂπ¥ÈæÑÊúÄÂ∞èÁöÑÊòØË∞ÅÔºüÊòØÂê¶Êúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏ÉÔºü 3. ÂàÜÊûêÊï∞ÊçÆÁöÑÁõ∏ÂÖ≥ÊÄß ÊØîÂ¶ÇËØ¥ÔºåÂπ¥ÈæÑ‰∏éË¥≠‰π∞ÈáëÈ¢ùÊòØÂê¶ÊúâÁõ∏ÂÖ≥ÊÄßÔºüÁõ∏ÂÖ≥Á≥ªÊï∞ÊòØÂ§öÂ∞ëÔºü 4. ÂàÜÊûêÊï∞ÊçÆËÉåÂêéÁöÑÂéüÂõ† ÊØîÂ¶ÇËØ¥ÔºåÂÆ¢Êà∑‰∏∫‰ªÄ‰πà‰ºöË¥≠‰π∞ÔºüÈîÄÂîÆ‰∏ãÈôçÁöÑÂéüÂõ†ÊòØ‰ªÄ‰πàÔºü 5. ÊèêÂá∫Êï∞ÊçÆÂàÜÊûêÁöÑÂª∫ËÆÆ ÊØîÂ¶ÇËØ¥ÔºåÁªèËøáÂâçÊúüÁöÑÂàÜÊûêÔºåÁü•ÈÅìÈîÄÂîÆ‰∏ãÈôçÁöÑÂéüÂõ†Ôºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ËÄÅÂπ¥ÂÆ¢Êà∑Áæ§‰ΩìÁöÑËÆ§Áü•Â∫¶ÂÅè‰ΩéÔºåÊïÖÊèêÂá∫Âª∫ËÆÆÔºöÈíàÂØπËÄÅÂπ¥ÂÆ¢Êà∑Áæ§‰ΩìÂºÄÂ±ïÂÆ£‰º†Ê¥ªÂä®„ÄÇ ÊÄª‰πãÔºå‰∏çË¶ÅÂΩì‰∏Ä‰∏™Á∫ØÁ≤πÁöÑ„ÄåÁªüËÆ°ËÄÖ„ÄçÔºåË¶ÅÂÅöÊï∞ÊçÆÁöÑÂàÜÊûêËÄÖÔºåÊúÄÂ•ΩÊòØÈóÆÈ¢òÁöÑËß£ÂÜ≥ËÄÖ„ÄÇ 2020-6-19 ËÆ∫Êï∞ÊçÆÁöÑÈáçË¶ÅÊÄßÔºÅÔºÅÔºÅÔºÅÔºÅ ËÆ∫Êï∞ÊçÆÁöÑÈáçË¶ÅÊÄßÔºÅÔºÅÔºÅÔºÅÔºÅ 2020-6-18 et-alÊ†ºÂºèÁöÑÂèÇËÄÉÊñáÁåÆÁöÑËÆæÁΩÆÊñπÊ≥ï ÂèÇËÄÉÊñáÁåÆÁ±ªÂûã 2020-6-17 ÁªüËÆ°Â≠¶ËµÑÊñôÊî∂ÈõÜÔºö ËßÜÈ¢ë Âìà‰ΩõÂ§ßÂ≠¶ https://www.bilibili.com/video/av455440626/?p=2 2020-6-16 ‰∏ÄÈù¢ 1h 1.Ëá™Êàë‰ªãÁªç 2.ÁªüËÆ° 1ÔºâÂÅáËÆæÊ£ÄÈ™åÔºåAÂüéÂ∏ÇÂπ≥ÂùáÂ∑•ËµÑX1ÔºåÊñπÂ∑Æsigma1ÔºåBÂüéÂ∏ÇÂπ≥ÂùáÂ∑•ËµÑX2ÔºåÊñπÂ∑Æsigma2. ËÆæËÆ°ÂÅáËÆæÊ£ÄÈ™åÔºåÈ™åËØÅX1ÊòæËëóÂ§ß‰∫éX2ÔºåË¶ÅÊ±ÇH0ÔºåH1ÔºåÂÅáËÆæÊ£ÄÈ™åÁªüËÆ°ÈáèÂèäÂÖ∂ÂàÜÂ∏É 2ÔºâÊ¶ÇÁéáËÆ∫ÔºåÁî≤‰πô‰∏§‰∫∫ÂêåÊó∂Áé©‰∏Ä‰∏™Ê∏∏ÊàèÔºåÊäïËá≥ÂùáÂåÄÈ™∞Â≠êÔºå‰∏§‰∏™‰∫∫ËΩÆÊµÅÊäïÊé∑ÔºåË∞ÅÂÖàÊäïÂà∞6Ë∞ÅËµ¢Ôºå ÈóÆÁî≤ÂÖàÊäïÔºåËé∑ËÉúÁöÑÊ¶ÇÁéá 3.Êú∫Âô®Â≠¶‰π† 1Ôºâ100‰∏á‰∏™Ê†∑Êú¨ÈáèÔºå70‰∏™featureÔºåÂ¶ÇÊûú‰Ω†Áî®random forestÔºå‰Ω†‰ºöÈÄâÊã©Â§öÊ∑±ÁöÑÊ†ë 2ÔºâÁÆÄËø∞baggingÔºårfÔºåboostingÁöÑÂå∫Âà´ 4.‰∫ßÂìÅ 1ÔºâÂø´ÊâãÂÜÖÈÉ®ÁîüÊÄÅÂàÜ‰∏∫Áîü‰∫ßËÄÖÂíåÊ∂àË¥πËÄÖÔºå‰Ω†Â¶Ç‰ΩïÈÄâÊã©‰∏Ä‰∏™ÊåáÊ†áÔºåÁî®‰∫éËØÑÂà§Â∞ΩÂèØËÉΩÂ§öÁöÑ‰∫∫Âèë‰ΩúÂìÅ 2ÔºâÁõ¥Êí≠Èó¥‰ª∑ÂÄºÁî®ËßÇÁúãÊó∂ÈïøÊù•ËØÑÂà§ÔºåÈÄâÊã©‰∏Ä‰∏™ÊåáÊ†áÂéªËØÑÂà§Áõ¥Êí≠Èó¥ÁöÑ‰ª∑ÂÄº 5.python ‰∏Ä‰∏™ÁÆÄÂçïÁºñÁ®ãÔºålist a = [10,20,30,40,50,60,70,80,90] Âú®list ‰∏≠ÂÖàÂèñÂá∫20ÁöÑÂÄçÊï∞Ôºåi.e., 20,40,60,80ÔºåÂÜçÂèñÂá∫30ÁöÑÂÄçÊï∞Ôºå‰æùÊ≠§Á±ªÊé®‰∏ÄÁõ¥Âà∞90ÔºåÊúÄÂêéÂèñÂá∫10ÁöÑÂÄçÊï∞ 6.Sql leetcode, top three department salary ÂèòÁßç ‰∫åÈù¢ 30mins ÈöîÂ§© 1.ÂæóÂà∞‰∏Ä‰∏™AB testÁªìÊûúÔºå‰Ω†ÊÄé‰πàÂàÜÊûê 2.ÁªèÂÖ∏productÈ¢òÁõÆÔºå‰∏Ä‰∏™ÊåáÊ†á‰∏ãÈôç‰∫ÜÔºåÂ¶Ç‰ΩïÂàÜÊûê 3.ÂÆèËßÇÈóÆÈ¢òÔºåÊÄé‰πàÂàÜÊûêÂçóÂåóÊñπÁî®Êà∑Â∑ÆÂºÇ 2020-6-13 Â§ßÊï∞ÊçÆÊåñÊéò‰∏éÂàÜÊûêÂÆû‰π†Áîü‰∏ÄÈù¢Èù¢Áªè Ëá™Êàë‰ªãÁªç sqlÊÄé‰πàÊ†∑ pythonÊÄé‰πàËá™Â≠¶ÁöÑ ËÆ≤‰∏Ä‰∏™Êï∞ÊçÆÊåñÊéò‰∏éÂàÜÊûêÂÆû‰π†ÊàñÈ°πÁõÆ„ÄÇ 5.ËÆ≤ÂÆû‰π†ÁªèÂéÜ https://www.nowcoder.com/contestRoom Â∞ΩÁÆ°ÊàëÁúã‰∫ÜÂæàÂ§ßËøôÊñπÈù¢ÁöÑ‰∏úË•øÔºå 2020-6-12 ‰∏Ä‰∏™Êï∞ÊçÆÂàÜÊûêÊä•ÂëäÁöÑÂÜÖÂÆπ ÊàëÂØπËøô‰∏™ÂæàÊÑüÂÖ¥Ë∂£ÂïäÔºÅÔºÅÔºÅÔºÅ ÈÅµÂæ™ÁöÑÊ†áÂáÜÂ±ïÁ§∫ÂàÜÊûêÁªìÊûú-„ÄãÈ™åËØÅÂàÜÊûêË¥®Èáè-„ÄãÊèê‰æõÂÜ≥Á≠ñÂèÇËÄÉ ÈúÄÊ±ÇÂ±ÇÔºöÁõÆÁöÑÔºåÁõÆÊ†á Êï∞ÊçÆÂ±ÇÔºöÊï∞ÊçÆÊ∏ÖÊ¥ó ÂàÜÊûêÂ±ÇÔºöÊèèËø∞ÂàÜÊûêÂíåÂª∫Ê®°ÂàÜÊûêÔºåÂâçËÄÖÊòØÊ¥ûÂØüÁªìËÆ∫ÔºåÂêéËÄÖÊòØÊ®°ÂûãÊµãËØïÔºå ËæìÂá∫Â±ÇÔºöÊ®°Âûã+Êä•ÂëäÊí∞ÂÜô„ÄÇ ÁÆÄÂçïÁâàÊú¨ È°πÁõÆËÉåÊôØÔºöÁÆÄËø∞È°πÁõÆÁõ∏ÂÖ≥ËÉåÊôØÔºå‰∏∫‰ªÄ‰πàÂÅöÔºåÁõÆÁöÑÊòØ‰ªÄ‰πà È°πÁõÆËøõÂ∫¶ÔºöÁªºËø∞È°πÁõÆÁöÑÊï¥‰ΩìËøõÁ®ãÔºå‰ª•ÂèäÁõÆÂâçÁöÑÊÉÖÂÜµ ÂêçËØçËß£ÈáäÔºöÂÖ≥ÈîÆÊÄßÊåáÊ†áÂÆö‰πâÊòØ‰ªÄ‰πàÔºå‰∏∫‰ªÄ‰πàËøô‰πàÂÆö‰πâ ÂêçËØçËß£ÈáäÔºöÂÖ≥ÈîÆÊÄßÊåáÊ†áÁöÑÂÆö‰πâÊòØËØ¥ÊòéÔºå‰∏∫‰ªÄ‰πàËøô‰πàÂÆö‰πâ„ÄÇÊúâ‰Ωï‰∏çÂêå„ÄÇ Êï∞ÊçÆËé∑ÂèñÊñπÊ≥ïÔºöÂ¶Ç‰ΩïÂèñÊ†∑ÔºåÊÄé‰πàËé∑ÂèñÂà∞ÁöÑÊï∞ÊçÆÔºå‰ºöÊúâÂì™‰∫õÈóÆÈ¢ò Êï∞ÊçÆËé∑ÂèñÊñπÊ≥ïÔºöÂ¶Ç‰ΩïÂèñÊ†∑ÔºåÁõ∏ÂÖ≥ÈóÆÈ¢ò„ÄÇÂåÖÊã¨ÂºÇÂ∏∏ÂÄºË°•ÂÖÖÔºåÂ¶Ç‰ΩïÂ°´ÂÖÖ„ÄÇÊï∞ÊçÆÊ∏ÖÊ¥óÂíåÊï∞ÊçÆË°•ÂÖ®„ÄÇ Êï∞ÊçÆÊ¶ÇËßàÔºöÈáçË¶ÅÊåáÊ†áÁöÑË∂ãÂäøÔºåÂèòÂåñÊÉÖÂÜµÔºåÈáçË¶ÅÊãêÁÇπÊàêÂõ†Ëß£Èáä ÂèØËßÜÂåñÊàñËÄÖË°®Ê†ºÂ±ïÁ§∫ Êï∞ÊçÆÊãÜÂàÜÔºöÊ†πÊçÆÈúÄË¶ÅÊãÜÂàÜ‰∏çÂêåÁöÑÁª¥Â∫¶Ôºå‰Ωú‰∏∫ÁªÜËäÇË°•ÂÖÖ ÁªìËÆ∫Ê±áÊÄªÔºöÊ±áÊÄª‰πãÂâçÊï∞ÊçÆÂàÜÊûêÁöÑ‰∏ªË¶ÅÁªìËÆ∫Ôºå‰Ωú‰∏∫Ê¶ÇËßà ÂêéÁª≠ÊîπËøõÔºöÂàÜÊûêÁõÆÂâçÂ≠òÂú®ÁöÑÈóÆÈ¢òÔºåÂπ∂ÁªôÂá∫Ëß£ÂÜ≥ÊîπËøõÈò≤ËåÉ Ëá¥Ë∞¢ ÈôÑ‰ª∂ÔºöËØ¶ÁªÜÊï∞ÊçÆ ‰∏ì‰∏öÁâàÊú¨ Ê†áÈ¢ò‰∏ö ÁõÆÊ†á ÂâçË®Ä a. ÂÖ∂ÂØπËÉΩÂê¶Ëß£ÂÜ≥‰∏öÂä°ÈóÆÈ¢òÔºåËµ∑Âà∞ÂÜ≥Á≠ñÊÄß‰ΩúÁî®„ÄÇÂåÖÊã¨ÂàÜÊûêËÉåÊôØ„ÄÅÁõÆÁöÑ‰ª•ÂèäÊÄùË∑Ø„ÄÇ Ê≠£Êñá a. Êä•ÂëäÁöÑÊ†∏ÂøÉËßÇÁÇπÊòØ‰ªÄ‰πàÔºüÁî±Âì™‰∫õÂ≠êËßÇÁÇπÁªÑÊàêÔºåÊîØÊåÅÊØè‰∏™Â≠êËßÇÁÇπÁöÑÊï∞ÊçÆÔºåÂ±ïÁ§∫ÊñπÂºèÔºåÈáèÂåñÁªìÊûúÔºàÈÅµÂæ™ÈáëÂ≠óÂ°îÂéüÁêÜ) Áé∞Áä∂ÊèèËø∞Ôºõ2Ôºå ‰ªÄ‰πàÈáèÂåñÊåáÊ†áÔºõ3ÔºåÂêåÊØî„ÄÅÁéØÊØîÁ≠âÁ≠â 4. ÈÄâÊã©ÂéüÂõ† ÁªìËÆ∫ a. Ê†πÊçÆÂàÜÊûêÁªìÊûúÔºåÈÄöÂ∏∏‰ª•ÁªºËø∞ÊÄßÊñáÂ≠óÊù•ËØ¥ÊòéÔºåË¶ÅÁ¥ßÂØÜÁªìÂêà‰∏öÂä°„ÄÇ ÈôÑÂΩï a. ËµÑÊñôÔºåÈáçË¶ÅÊï∞ÊçÆÔºåÂú∞Âõæ ÂÖ≥ÈîÆËØçÔºö ÊåáÊ†áÔºõÊï∞ÊçÆÔºõÂèØËßÜÂåñÔºõÈÄªËæëÔºõÂÆû‰∫ãÊ±ÇÊòØ ÈöæÁÇπÔºö Âª∫Ê®°ÂàÜÊûêÔºõËÆ°Èáè ÈáçÁÇπÔºö Áª¥Â∫¶ÔºõÊåáÊ†á ÊúÄÂøåËÆ≥ÁöÑÂ∞±ÊòØÂàÜÊûêÂá∫‰∏ÄÂ§ßÊé®ÊòæËÄåÊòìËßÅÁöÑÁªìËÆ∫ÔºåÂ∞±Ë°åÊï∞ÊçÆÊåñÊéò‰∏ÄÊ†∑„ÄÇ 2020-6-11 ÁªèÊµéÂ≠¶ÁöÑÈ°∂Á∫ßÊúüÂàä„ÄÇ ÁªèÊµéÂ≠¶‰∏≠ÊñáÊúüÂàäÁöÑ ‚ÄúÂõõÂ§ßÈáëÂàö‚ÄùÔºåÂç≥„ÄäÁªèÊµéÁ†îÁ©∂„Äã„ÄÅ„ÄäÁªèÊµéÂ≠¶(Â≠£Âàä)„Äã„ÄÅ„Ää‰∏ñÁïåÁªèÊµé„Äã„ÄÅ„ÄäÁÆ°ÁêÜ‰∏ñÁïå„Äã ÁªèÊµéÂ≠¶Ëã±ÊñáÊúüÂàäÁöÑ ‚ÄúTop 5‚ÄùÔºåÂç≥ American Economic ReviewÔºåEconometricaÔºåJournal of Political EconomyÔºåQuarterly Journal of EconomicsÔºåReview of Economic Studies„ÄÇ https://www.aeaweb.org/journals/mac]]></content>
      <tags>
        <tag>Êó•Â∏∏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reading-Record]]></title>
    <url>%2F2020%2F06%2F09%2FReading-Record%2F</url>
    <content type="text"><![CDATA[2020-7-9 News and the city: understanding online press consumption patternsthrough mobile data 2020-7-6 diversity of city(neighborhood)MIT: Economic outcomes predicted by diversityin cities Shannon entropy Abstract: Gap: Much recent work has illuminated the growth, innovation, and prosperity of entire cities, but there is relatively less evidence concerning the growth and prosperity of individual neighborhoods. Aiming;: In this paper we show that diversity of amenities within a city neighborhood, computed from openly available points of interest on digital maps, accurately predicts human mobility (‚Äúflows‚Äù) between city neighborhoods and that these flows accurately predict neighborhood economic productivity Conclusion: Our results suggest that the diversity of goods and services within a city neighborhood is the largest single factor driving both human mobility and economic growth 2020-7-5 Supporting Information S1: New Metrics for the Economic Complexity of Countries and ProducesKey: Revealed Comparative Advantage 2020-7-5 What Do We Mean by Economic Complexity?2020-7-2 Women‚Äôs Political Participation and Gender Gaps of Education in China: 1950‚Äì1990yang yao and wuyue you Á§æ‰ºöÁßëÂ≠¶ÈóÆÈ¢òÔºö Â•≥ÊÄßÊîøÊ≤ªÂú∞‰ΩçÁöÑÊèêÈ´òÊòØÂê¶‰øÉËøõÊÄßÂà´Âπ≥Á≠â„ÄÇ Â•≥ÊÄßÊîøÊ≤ªÂú∞‰ΩçÔºö Â•≥ÊÄßÂÖöÂëòÊØî‰æã ÊÄßÂà´Âπ≥Á≠âÔºöÁî∑Â•≥Âá∫ÁîüÊØî‰æã ËÆ°ÈáèÊ®°ÂûãÁöÑËÆæÁΩÆÔºåÊéßÂà∂ÂèòÈáèÁöÑÈÄâÊã©ÔºÅËøòÂÅö‰∫ÜÂΩ±ÂìçÊú∫Âà∂ÁöÑÊ£ÄÊµãÔºÅ 2020-6-26 A review on time series forecasting techniques for building energy consumptionThis study presents a comprehensive review of existing nine most popular forecasting techniques. Beautiful!!!!1 2.1 Artificial neural network(ANN) 2.1.1 Overview of ANN 2.1.2 Review of application studies on ANN Model: Hamzacebi [42] developed forecasting models using ANN for net electricity consumption of Turkey. Input time series: The ANN model forecasted the sectoral electricity consumption of the four sectors which were the transportation, agriculture, residential and the industry sectors respectively. Dataset: The data used to develop, validate and test the model was for a period between 1970 and 2004. Results: The MAPE computed for the four sector‚Äôs electricity consumption were 23.59%, 3.56%, 3.26% and 2.25% respectively . Very clear! 2.2 Autoregressive Integrated Moving Average(ARIMA) 2.3 Support Vector Machine(SVM) 2.4. Case-Based Reasoning (CBR) 2.4.1. Overview of CBR 2.4.2. Review of application studies using CBR 2.5. Fuzzy time series 2.6. Grey prediction model 2.7. Moving average and exponential smoothing (MA &amp; ES) 2.8. K ‚Äì Nearest Neighbor prediction method (kNN) 2.9. Hybrid models Summary of qualitative comparison for the 9 major time series forecasting techniques. the distance between theory and application. Application is the first step, then the theory is next creation. 2020-6-25 Time Series PredictionTitle: Financial Time Series Prediction Using Least SquaresSupport Vector Machines Within the Evidence Framework The least squares support support vector machine(LS-SVM) regression applied to predict financial time series. 2020-6-18 Data MiningPreprocessing Methods and Pipelines of Data Mining: An Overview. IEEE conference, 2019. arXiv: 1906.008510 ÊÄªÁªì‰∫ÜÂ∏∏Áî®Âú®Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊµÅÁ®ã: data preprocessing technoques which are categorized as the data cleaning, data transformation and data preprocessing is given. data type data distribution noise of data Exploring ‚Äã Before modeling the data, people may want to get to know the underlying distribution of the data, the correlation between variables, and the their correlation with the lables. Modeling with underling patterns existed in the data source, modeling makes it possible to represent the pattern explicitly with the data mining models. data mining models, loss functions are defined. mean squared error cross entropy Interpreting ÊÑüËßâËøô‰∏™ÁªºËø∞ÂÜôÁöÑËøòÊòØÁÆÄÂçïÔºåÂ§ßËá¥ÊµÅÁ®ãÁªôÂá∫Êù•‰∫Ü„ÄÇ 2020-6-17Áñ´ÊÉÖ‰∏ãÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÈÄöÂã§ÂÆ¢ÊµÅÊµãÁÆóÂèä‰ºòÂåñÂª∫ËÆÆ‚Äî‚Äî‰ª•Âåó‰∫¨Â∏Ç‰∏∫‰æã ÁàÜÂèë‰∫é2019Âπ¥12ÊúàÁöÑÊñ∞ÂûãÂÜ†Áä∂ÁóÖÊØíËÇ∫ÁÇé(‰ª•‰∏ãÁÆÄÁß∞‚ÄúÊñ∞ÂÜ†ËÇ∫ÁÇé‚Äù)ËôΩÁÑ∂Â∑≤ÂæóÂà∞ÂàùÊ≠•ÊéßÂà∂Ôºå‰ΩÜ‰ªçÂØπ‰∏≠ÂõΩÁªèÊµéÁ§æ‰ºöÂèëÂ±ïÂíåÂüéÂ∏ÇÂ±ÖÊ∞ëÁîüÊ¥ªÈÄ†ÊàêÂæàÂ§ßÂΩ±Âìç„ÄÇÂèçÊò†Âú®ÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÊñπÈù¢ÔºåÁñ´ÊÉÖÂØºËá¥Â§ßÈáèÂüéÂ∏ÇÁöÑÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÂ§Ñ‰∫éÈùûÂ∏∏ÊÄÅÁöÑËøêËê•Áä∂ÂÜµÔºåÂÆ¢ËøêÈáèËøú‰Ωé‰∫éÊ≠£Â∏∏Ê∞¥Âπ≥„ÄÇÁ©∂ÂÖ∂ÂéüÂõ†Ôºö‰∏ÄÊòØÁñ´ÊÉÖ‰∏≠‰∫§ÈÄöËøêËæìÈÉ®Èó®ÂØπÊò•ËøêËøÅÂæôËøõË°å‰∫ÜÈ´òÂº∫Â∫¶ÁöÑÁÆ°Âà∂Ôºå‰ΩøÂæóÂ§ßÈáèÂ∑•‰Ωú‰∫∫Âè£ÊªûÁïôÂºÇÂú∞ÔºåÊó†Ê≥ïÊ≠£Â∏∏ËøîÂ≤óÂíåÈÄöÂã§Ôºõ‰∫åÊòØ‰∏∫ÂáèÂ∞ë‰∫∫ÂëòÊé•Ëß¶ÈÄ†ÊàêÁñ´ÊÉÖ‰º†Êí≠ÔºåÂ§ßÈÉ®ÂàÜÂüéÂ∏ÇÂùáÂá∫Âè∞‰∫ÜÂ±ÖÂÆ∂ÈöîÁ¶ª„ÄÅËøúÁ®ãÂäûÂÖ¨Á≠âÂ∫îÊÄ•ÊîøÁ≠ñÔºåËøõ‰∏ÄÊ≠•ÂéãÁº©‰∫ÜÈÄöÂã§Âá∫Ë°åÈúÄÊ±Ç„ÄÇ Âú®ÈùûÂ∏∏ÊÄÅËÉåÊôØ‰∏ãÔºåÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüËê•Ëøê‰∫üÈ°ªËøõË°åÈáçÁªÑÁªáÂíåÂÜç‰ºòÂåñÔºå‰ª•ÂÆûÁé∞ÂÆâÂÖ®„ÄÅÊïàÁéá‰∏éÁªèÊµé‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇ‰ªéÂÆâÂÖ®ËßíÂ∫¶Êù•ÁúãÔºåÁõÆÂâçÊñ∞ÂÜ†ËÇ∫ÁÇéÁñ´ÊÉÖÈò≤Ê≤ªÂ∑•‰Ωú‰ªçÂ§ÑÂú®ÊîªÂùöÊúüÔºå‰∏ÄÊó¶ÊéßÂà∂‰∏çÊÖéÔºå‰ªçÂèØËÉΩÈÄ†Êàê‰∫åÊ¨°‰º†Êí≠Ôºå‰∏∫ÂüéÂ∏ÇÈò≤Áñ´Â∑•‰ΩúÂíåÂ±ÖÊ∞ëÁîüÂëΩË¥¢‰∫ßÂÆâÂÖ®Â∏¶Êù•Â∑®Â§ßÊçüÂ§±ÔºåËÄåÊéßÂà∂‰πòÂÆ¢ÂØÜÂ∫¶ÂØπ‰∫éÂáèÂ∞ë‰∫∫Áæ§Êé•Ëß¶„ÄÅÈÅèÂà∂ÁóÖÊØíÊâ©Êï£ÂçÅÂàÜÂøÖË¶ÅÔºõ‰ªéÊïàÁéáËßíÂ∫¶Êù•ÁúãÔºåÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÂøÖÈ°ªÊª°Ë∂≥ÂüéÂ∏ÇÂøÖË¶ÅÁöÑÈÄöÂã§Âá∫Ë°åÈúÄÊ±ÇÔºåÁª¥ÊåÅÂüéÂ∏ÇÂêÑÈ°πÂäüËÉΩÂü∫Êú¨Ê≠£Â∏∏ËøêËΩ¨ÔºåÂ∞ΩÂèØËÉΩÂú∞ÂáèÂ∞ëÁñ´ÊÉÖÂØπÂüéÂ∏ÇÁªèÊµéÁ§æ‰ºöÂèëÂ±ïÈÄ†ÊàêÁöÑË¥üÈù¢ÂΩ±ÂìçÔºõ‰ªéÁªèÊµéËßíÂ∫¶Êù•ÁúãÔºåÂ∞ÜÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöËê•ËøêÁöÑ‰æõÈúÄÊØîÊéßÂà∂Âú®‰∏Ä‰∏™ÂêàÁêÜÊ∞¥Âπ≥ÔºåÊúâÂä©‰∫éÁº©ÂáèÂÖ¨ÂÖ±‰∫§ÈÄöÁöÑË¥¢ÊîøÂºÄÊîØÔºåËøõ‰∏ÄÊ≠•ÂáèÂ∞ëÁñ´ÊÉÖÂ∏¶Êù•ÁöÑÊçüÂ§±„ÄÇÁî±Ê≠§ÂèØËßÅÔºåÁßëÂ≠¶ÂêàÁêÜÂú∞ÊµãÁÆóÈùûÈ•±ÂíåËøêËê•ÊÉÖÊôØ‰∏ãÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÁöÑÂÆ¢ÊµÅÂº∫Â∫¶ÔºåÂØπ‰∫éÊòéÁ°ÆÁ≥ªÁªüÁöÑËøêÂäõÈúÄÊ±ÇÔºåÂà∂ÂÆöÂêàÁêÜÁöÑË∞ÉÂ∫¶ÂíåËê•ËøêÊñπÊ°àÔºåÂÆûÁé∞È´òÊïà„ÄÅ‰ΩéÈ£éÈô©ËøêË°åÂÖ∑ÊúâÈáçË¶ÅÁöÑÂü∫Á°ÄÊîØÊíë‰ΩúÁî®„ÄÇ Â∑≤ÊúâÊµãÁÆóÊâãÊÆµÂÖ∑ÊúâÂáÜÁ°ÆÂ∫¶È´ò„ÄÅÊï∞ÊçÆËøûÁª≠ÁöÑ‰ºòÂäøÔºå‰ΩÜÊòØÂ≠òÂú®‰∏§Â§ß‰∏çË∂≥Ôºö1)Êï∞ÊçÆËé∑ÂèñÈöæÂ∫¶Áõ∏ÂØπËæÉÂ§ßÔºåËé∑ÂèñÊï∞ÊçÆÁöÑÊàêÊú¨ËæÉÈ´òÔºåÊï∞ÊçÆÊõ¥Êñ∞Âë®ÊúüËæÉÈïøÔºåÂä®ÊÄÅÊÄß‰∏çË∂≥Ôºõ2)‰∏ÄËà¨Êï∞ÊçÆË¶ÜÁõñËåÉÂõ¥ÂèóÈôêÔºåÈöæ‰ª•ÂÆûÁé∞Âú®Êï¥‰∏™ÂüéÂ∏ÇÂ∞∫Â∫¶‰∏ãËøõË°åÊÄª‰ΩìÁöÑÂàÜÊûêÂíå‰ºòÂåñ„ÄÇÂõ†Ê≠§ÔºåÁé∞ÊúâÊäÄÊúØÊñπÊ≥ïÈöæ‰ª•ÂÆåÂÖ®Êª°Ë∂≥ÂΩìÂâçÈùûÂ∏∏ÊÄÅÊÉÖÊôØ‰∏ãÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÂø´ÈÄüË∞ÉÊï¥„ÄÅÁÅµÊ¥ªÂ∫îÂØπÁöÑÁâπÊÆäÈúÄÊ±Ç„ÄÇ ÈíàÂØπ‰∏äËø∞‰∏çË∂≥ÔºåÊú¨ÊñáÁ´ãË∂≥Êñ∞ÂÜ†ËÇ∫ÁÇéÁñ´ÊÉÖ‰∏ãÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÁöÑÈùûÈ•±ÂíåËøêËê•ÊÉÖÊôØÔºå‰ª•Âåó‰∫¨Â∏Ç‰∏∫‰æãÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÈù¢ÂêëÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÂÆ¢ÊµÅÂº∫Â∫¶ÊµãÁÆóÂíåÈáçË¶ÅÊÄßÁ†îÂà§ÁöÑÊäÄÊúØÊ°ÜÊû∂„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊäÄÊúØÊ°ÜÊû∂ÂÖÖÂàÜÂà©Áî®Êµ∑Èáè‰∫íËÅîÁΩëÊó∂Á©∫Â§ßÊï∞ÊçÆÊ†∑Êú¨ÈáèÂ§ß„ÄÅÊï∞ÊçÆÂèØËé∑ÂèñÂ∫¶È´ò„ÄÅÂä®ÊÄÅÊÄßÂº∫ÁöÑÁâπÁÇπÔºåÂ∞ÜÂ§ßÊï∞ÊçÆÊåñÊéòÊäÄÊúØ‰∏éÂú∞ÁêÜÁªüËÆ°ÊñπÊ≥ïÁõ∏ÁªìÂêàÔºåÂú®ÂáÜÁ°ÆËØÜÂà´ËøîÂ≤óÂ∑•‰Ωú‰∫∫Âè£ÁöÑÈÄöÂã§ÊµÅ‰πãÂêéÔºåÂØπÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÂÆ¢ÊµÅÂº∫Â∫¶ÂèäÂÖ∂Á©∫Èó¥ÂàÜÂ∏ÉËøõË°åÊµãÁÆó„ÄÇÂêåÊó∂ÔºåÂü∫‰∫éËØ•Ê°ÜÊû∂ÂèØ‰ª•ÂÆûÁé∞ÂØπÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÂÆ¢ÊµÅÂº∫Â∫¶ÁöÑÂÆûÊó∂ÁõëÊµãÂíåÂä®ÊÄÅÂèçÈ¶àÔºåÂú®Áñ´ÊÉÖÊúüÈó¥ÂüéÂ∏ÇÈÄöÂã§Ê†ºÂ±ÄÂø´ÈÄüÂèòÂåñÁöÑÊÉÖÂΩ¢‰∏ãÔºåÊúâÂäõÂú∞ÊîØÊåÅÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁªüÈ´òÊïàË∞ÉÂ∫¶„ÄÅÂø´ÈÄüÂìçÂ∫î„ÄÇ Êú¨Êñá‰∏ªË¶Å‰ΩøÁî®ËøîÂ≤óÈÄöÂã§Êï∞ÊçÆ„ÄÅÂÖ¨ÂÖ±‰∫§ÈÄöÈÄöÂã§Êï∞ÊçÆ„ÄÅÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ∫øË∑Ø‰∏âÁ±ªÊó∂Á©∫Â§ßÊï∞ÊçÆÔºåÊØèÁ±ªÊï∞ÊçÆÂ§ÑÁêÜÂêÑÁéØËäÇÂùáÂåøÂêçÂåñÔºåÂêÑÁéØËäÇÂèäËæìÂá∫Âùá‰∏çÊ∂âÂèä‰∏™‰ΩìÈöêÁßÅ„ÄÇ 1ÔºâËøîÂ≤óÁî®Êà∑ÈÄöÂã§Êï∞ÊçÆ„ÄÇ Âü∫‰∫é2020Âπ¥1Êúà31Êó•‚Äî2Êúà23Êó•ÁôæÂ∫¶Âú∞ÂõæÊÖßÁúºÊó∂Á©∫Â§ßÊï∞ÊçÆÔºåÊ†πÊçÆÁî®Êà∑Ë°å‰∏∫ÁâπÂæÅËØÜÂà´ËøîÂ≤óÂ∑•‰Ωú‰∫∫Âè£ÔºåÂπ∂ÈÄöËøáÊï¥ÂêàÂéªÈöêÁßÅÂåñÁöÑ‰ΩçÁΩÆ„ÄÅPOIÁ≠âÂ§öÊ∫êÊï∞ÊçÆÔºåÂæóÂà∞ËØ•Áæ§‰ΩìÁöÑËÅå‰ΩèÁ©∫Èó¥ÂàÜÂ∏ÉÁâπÂæÅ„ÄÇÂ∞ÜÊµ∑ÈáèËøîÂ≤óÁî®Êà∑ÁöÑÂ±Ö‰ΩèÂú∞‰∏éÂ∑•‰ΩúÂú∞Áõ∏ËøûÊé•ÔºåÂæóÂà∞ÂÖ∂ÈÄöÂã§ODÊï∞ÊçÆ„ÄÇÊú¨ÊñáÂ∞ÜÂ±Ö‰ΩèÂú∞ÂíåÂ∑•‰ΩúÂú∞‰ª•ÁΩëÊ†ºÂΩ¢ÂºèËøõË°åËÅöÂêàÔºåÁΩëÊ†ºÂ∞∫ÂØ∏‰∏∫100m√ó100mÔºåÂ±Ö‰ΩèÂú∞ÁΩëÊ†ºiÂíåÂ∑•‰ΩúÂú∞ÁΩëÊ†ºj‰πãÈó¥ÁöÑÈÄöÂã§ÊµÅÂº∫Â∫¶Fij‰∏∫‰∏§ÁΩëÊ†º‰πãÈó¥ÁöÑÂ∑≤ËøîÂ≤óÈÄöÂã§‰∫∫Êï∞ÊÄªÂíå„ÄÇ 2ÔºâÂÖ¨ÂÖ±‰∫§ÈÄöÈÄöÂã§Êï∞ÊçÆ„ÄÇ Âü∫‰∫éÁôæÂ∫¶Âú∞ÂõæÊÖßÁúºÁªºÂêàÈÄöÂã§Êó∂Èó¥„ÄÅ‰∫∫Âè£ÁîªÂÉèÂ±ûÊÄß„ÄÅÂ±Ö‰Ωè/Â∑•‰ΩúÂú∞ÂàÜÂ∏ÉÁ≠âÂéÜÂè≤‰ø°ÊÅØÂíåÁâπÂæÅÔºåËøêÁî®Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÊåñÊéòÊµ∑ÈáèÁî®Êà∑ÁöÑÈÄöÂã§ÊñπÂºèÔºåÂπ∂ÈÄöËøáÈÄöÂã§Ë∞ÉÊü•Êï∞ÊçÆÔºåÂØπËÆ°ÁÆóÁªìÊûúÁöÑÂèØÈù†ÊÄßËøõË°åÊ†°Ê†∏„ÄÇÁî±Ê≠§ÂæóÂà∞‰ªéÂ±Ö‰ΩèÂú∞ÁΩëÊ†ºiËá≥Â∑•‰ΩúÂú∞ÁΩëÊ†ºjÁöÑÈÄöÂã§ÊµÅ‰∏≠ÂÖ¨ÂÖ±‰∫§ÈÄöÂá∫Ë°åÊØî‰æãPij „ÄÇÂõ†Ê≠§ÔºåÂÖ¨ÂÖ±‰∫§ÈÄöÊó•ÂùáÈÄöÂã§ÂÆ¢ÊµÅÂº∫Â∫¶Êï∞ÊçÆ(‰ª•‰∏ãÂØπ‰∫éÂÆ¢ÊµÅÂº∫Â∫¶ÁöÑÊèèËø∞ÂùáÊåáÂÖ¨ÂÖ±‰∫§ÈÄöÊó•ÂùáÈÄöÂã§ÂÆ¢ÊµÅÂº∫Â∫¶)Tij=FijPij „ÄÇÈúÄË¶ÅËØ¥ÊòéÁöÑÊòØÔºåÊú¨Êñá‰ΩøÁî®ÁöÑÊï∞ÊçÆÂπ∂ÈùûË¶ÜÁõñÊâÄÊúâ‰∫∫Âè£ÁöÑÂÖ®Ê†∑Êú¨Êï∞ÊçÆÔºåÂõ†ËÄåÂàÜÊûêÂæóÂà∞ÁöÑÂÆ¢ÊµÅÂº∫Â∫¶‰ªÖË°®ËææÁõ∏ÂØπÂº∫Â∫¶ÔºåËÄåÈùûÂáÜÁ°ÆÁöÑÁªùÂØπÊï∞ÂÄº„ÄÇ 3ÔºâÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ∫øË∑ØÊï∞ÊçÆ„ÄÇ Êú¨ÊñáÂü∫‰∫éÁôæÂ∫¶Âú∞ÂõæÁöÑÂåó‰∫¨Â∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ∫øË∑ØÂíåËΩ¶Á´ôÊï∞ÊçÆÔºåÂåÖÂê´Á∫¶1200Êù°ÂÖ¨ÂÖ±Ê±ΩËΩ¶Á∫øË∑ØÂíå22Êù°ËΩ®ÈÅì‰∫§ÈÄöÁ∫øË∑Ø„ÄÇ Á†îÁ©∂Êï∞ÊçÆÂíåÊñπÊ≥ïÁ†îÁ©∂Êï∞ÊçÆÊú¨Êñá‰∏ªË¶Å‰ΩøÁî®ËøîÂ≤óÈÄöÂã§Êï∞ÊçÆ„ÄÅÂÖ¨ÂÖ±‰∫§ÈÄöÈÄöÂã§Êï∞ÊçÆ„ÄÅÂüéÂ∏ÇÂÖ¨ÂÖ±‰∫§ÈÄöÁ∫øË∑Ø‰∏âÁ±ªÊó∂Á©∫Â§ßÊï∞ÊçÆÔºåÊØèÁ±ªÊï∞ÊçÆÂ§ÑÁêÜÂêÑÁéØËäÇÂùáÂåøÂêçÂåñÔºåÂêÑÁéØËäÇÂèäËæìÂá∫Âùá‰∏çÊ∂âÂèä‰∏™‰ΩìÈöêÁßÅ„ÄÇ 2 Á†îÁ©∂Ê°ÜÊû∂ ÂàÜÊûêÁªìÊûúËøîÂ≤óÂ∑•‰Ωú‰∫∫Âè£ÁöÑÂ±Ö‰ΩèÂú∞ÂàÜÂ∏É ËøîÂ≤óÂ∑•‰ΩúÁöÑÂ∑•‰ΩúÂú∞ÂàÜÂ∏É ËøîÂ≤óÂ∑•‰Ωú‰∫∫Âè£ÂÖ¨ÂÖ±‰∫§ÈÄöÈÄöÂã§ÊµÅÁöÑÁ©∫Èó¥ÂàÜÂ∏É ÂÖ¨ÂÖ±‰∫§ÈÄöÁ∫øË∑ØÂÆ¢ÊµÅÂº∫Â∫¶ÂàÜÊûê ÂüéÈôÖ‰∫§ÈÄöËøêËæìÈúÄÊ±ÇÁöÑÁ©∫Èó¥ÁâπÂæÅ ODÁü©Èòµ 2020-6-13I have summarized some metric which are used to measure social and economic development. Symbolic Explanation c : represents unit region c n : the number of region a : represents a economic index of region m : the number of region The complexity location quotient(LQ)[1] LQ_{c,a} = \frac{\frac{P_{c,a}}{\sum_{a=1}^{n}p_{s,a}}}{\frac{\sum_{s=1}^{n}p_{s,a}}{\sum_{a=1}^{m}\sum_{s = 1}^{n}p_{s,a}}}where $p_{s,a}$ is the number of index a in region s, $\sum_{a =1}^{m}p_{s,a}$ is the total number of index belongs to a in regions, $\sum_{s = 1}^{n}p_{s,a}$; $\sum_{s = 1}^{n}\sum_{a = 1}^{m}p_{s,a}$ is the number of index a in countrywide. The $m_{s, a}$ matrix is definded by \left\{ \begin{array}{cc} 1 & if \ LQ_{s,a}>LQ^{*}\\ 0 & otherwise \end{array} \right.The diversification Diversification: k_{s,0}=\sum_{a = 1}^{n}m_{s,a} Ubiquity: k_{a,o} = \sum_{s=1}^{n}m_{s,a}The average value of the prior period‚Äôs level for the measures of diversification and ubiquity. The iterative process is defined as follows: K_{s,N} = \frac{1}{K_{s,0}}\sum_{a = 1}^{m}m_{s,a}\cdot K_{a,N-1} K_{a,N} = \frac{1}{K_{a,o}}\sum_{s = 1}^{n}m_{s,a}\cdot K_{s,N-1} [1] Economic Complexity and Regional Growth Performance: Evidence from the Mexican Economy 2020-6-11Reference: Gao Jian, Jun Bogang, et al. Collective Learning in China‚Äôs Regional Economic Development. arXiv: 1703.01369, 2017. Using China‚Äôs stock market extracted from the RESSET Financial Research Database coving 1990-2015, Gao et-al investigated inter-regional and inter-industry learning which make contribution to economic development. I mainly learn how the quantify industry similarities and dominance of region by mathematical tool. Beautiful results tell good story. 2020-6-9 Transportation large-scale dataset reveals economic development statusTitle: Estimation of Regional Economic Development Indicator from Transportation Network Analytics. Reference: Li BIn, Gao Song, et al. Estimation of regional economic development indicator from transportation network analytics. Scientific reports, 2020: 10(1), 1-15. I appreciate this works, not only the dataset but all the content. I can try all my knowledge to understand it. At the same time, I can learn how to analyze complex questions to get deep information. I will detail to review as following structure. The Introduction P1.S1: The meaning of economy: With the booming economy in China, many researches have pointed out that the improvement of regional trans-portation infrastructure, the mobility of labor and capital, and industry reform along with other socioeconomicfactors play an important role on economic growth. The meaning of policy maker; Timely estimation of social and economic status of cities and regions has important implications for enterprise investment and government policy making. P1.S2: The disadvantage. Traditional inference approaches to economic status mainly rely on official reports and census surveys, which usually take a long period and are labor intensive. P1.S3: The applicaiton of big data. With the rapid development of information, communication and technology (ICT), new data sources of human activities 1 and vehicle movement flow , air transport flow , financial flow, information flow , communication flow , and others have become available for better understanding and monitoring the status of our socioeconomic environments . For example: Liu et al. 1 found that online social activity could reflect the macro economic status of 282 prefecture-level cities in China. Recently, Gao et al. 16 conducted a comprehensive review on data resources, computational tools, analytical methods, theoretical models, and applications in computational socioeconomics. P2 Review on old methodology. In the past decades, a wealth of works have been dedicated to studying the pattern of human mobility involving passenger transportation . Foucuse on their topic : A comparatively smaller literature has been dedicated to the pattern of transportation activity embedded in goods movement . The shortest : The scarcity of reliable data sources on freight transportation appears to be one of the challenges. Early studies rely on traditional freight traffic surveys, which are typically enterprise questionnaire surveys to obtain information such as the traffic volume and speed in specific road sections. MethodsData : A excellent research must be constructed by a high-quality dataset.Information: annual transportation data between years 2014 and 2017 for the Shaanxi, Jiangsu, Liaoning at city level. The vehicle is divided into two categories: cars&amp;buses, and freight trucks. The aiming is to investigate the relationship between regional economy and transportation networks. Indictors: the total number of vehicles, the sum of passengers(for cars and buses) and the weights(for trucks) as well as the distance (km) between paired cities were calculated respectively. Models: ÊúÄÂ∞è‰∫å‰πòÊ≥ï+ Ê≠£ÂàôÂåñÊ≥õÂåñËÉΩÂäõÔºàËøáÊãüÂêàÔºüÔºüÔºüÔºüÔºâÊàëËßâÂæóÊòØÂ§öÈáçÂÖ±Á∫øÊÄßÁöÑÂéüÂõ†ÔºåËß£ÈáäÂèòÈáè‰πãÈó¥Â§™Á∫øÊÄßÁõ∏ÂÖ≥‰∫ÜÔºÅÔºÅÔºÅÔºÅÔºÅ MLR(multiple linear regression) with ordinary least squares (OLS) paramete estimation is used to discover the realtionship between the transport flows of people and goods. The independent variables includs. incoming flow, outgoing flow, intracity flow. Linear model with regularization combine machine learning with MLR to improve generalization. Êú¨ÁßëÈò∂ÊÆµÂ∑≤ÁªèÂ≠¶Ëøá Ridge regression applies and Lasso regression with different regularization term. Â¶ÇÊûúÊîπÊàêÊîØÊåÅÂêëÈáèÂõûÂΩíÂ∞±Êõ¥Â•Ω‰∫ÜÔºåÊàëÂáÜÂ§áËØïËØï http://kernelsvm.tripod.com/ ÊÑüËßâÂè™ÊòØÂú®Â∫îÁî®‰∏§‰∏™Ê®°ÂûãÔºåÂπ∂Ê≤°ÊúâÂàõÊñ∞‰πãÂ§Ñ Ë¶ÅÊääËøôÁßçÁ∫øÊÄßÊ®°ÂûãÊ≠£ÂàôÂåñÊäÄÊúØÊï¥ÁêÜ‰∏ÄÁØá Model 2: The gravity model fitting with linear regression and linear G_{ij} = k\frac{p_ip_j}{d_{ij}^{\beta}}(i!=j)ÂèÇÊï∞$\beta$ ÁöÑÁ°ÆÂÆöÔºå‰ª£Ë°®‰∫ÜË∑ùÁ¶ªÁöÑÂΩ±ÂìçÔºådË∑ùÁ¶ªË°∞ÂáèÂΩ±ÂìçÔºåÊãüÂêàÔºåÂèÇÊï∞ÊòæËëóÊÄßÊ£ÄÈ™åÔºå Model 3: The null model Model 4: Network structure analyses ‚Äã 1. ËÆ°ÁÆóÊàñÂÆö‰πâ‰∫ÜÊúâ‰∫õÁΩëÁªúËøûÈÄöÊÄßÁöÑÂàªÁîªÊåáÊ†á Betw„ÄÅClose„ÄÅ PageRanke 1. Áõ∏ÂÖ≥ÊÄßÁöÑÂàªÁîª 2. ‰∏ªÊàêÂàÜÂàÜÊûêÔºàËøò‰∏çÁü•ÈÅìÔºâ È¶ñÂÖàÔºåÊàë‰ªé‰∏≠Â≠¶Âà∞‰∫ÜÊää‰∏Ä‰∏™ÈóÆÈ¢òÊãÜËß£ÔºåÊÄé‰πà‰∏ÄÊ≠•‰∏ÄÊ≠•ÂàÜÊûêÁöÑÔºåÊúâÊó∂ÂÄôÊï∞ÊçÆÈáèÂ§ß‰∫ÜÔºåËÉΩÂæóÂà∞ÁöÑ‰ø°ÊÅØÂ∞±Ë∂äÂ§ßÔºåÊÄé‰πàÊåñÊéòËá™Â∑±ËßâÂæóÊúâ‰ª∑ÂÄºÁöÑ‰∏úË•øÔºåËÄå‰∏çÊòØ‰∫∫‰∫∫ÈÉΩËÉΩÁúãÁöÑÁªìÊûú„ÄÇÊú¨ÊñáÂ∞±‰ªé‰∏â‰∏™Â§ßÁöÑÊñπÈù¢ÔºåÂàÜÂ±ÇÊ¨°Á†îÁ©∂‰∫Ü‰∫§ÈÄöÊµÅ‰∏éÁªèÊµéÂèëÂ±ïÁöÑÂÖ≥Á≥ª„ÄÇ‰ΩÜÊòØËøôÁØáÊñáÁ´†ÂæàÂ§ßÁ®ãÂ∫¶ÊòØÂª∫Á´ãÂú®Â§ßÊï∞ÊçÆ‰πã‰∏äÁöÑÔºå‰ΩÜÊúâÂàõÊñ∞ÁöÑÊñπÊ≥ïÈÉΩÊòØÂÄüÈâ¥ËøáÊù•ÁöÑÔºåÂÅèÈáç‰∫éÊñπÊ≥ïÁöÑÂ∫îÁî®ÔºåËß£ÈáäÁªèÊµéÂèëÂ±ï‰∏é‰∫§ÈÄöÊµÅ„ÄÇËØªÂÆåËøôÁØáÊñáÁ´†ÔºåÊúâÁßçËá™Â∑±ÊàêÂ∞±ÊÑüÔºåËøô‰∏™Â•ΩÁöÑÊñáÁ´†ÊàëÈÉΩÁúãÊòéÁôΩ‰∫Ü„ÄÇ‰∏çÂæó‰∏çËØ¥ÔºåÁü•ËØÜÈù¢„ÄÅÂÜô‰ΩúÊñπÊ≥ï„ÄÅÂàÜÊûêÊÄùË∑Ø„ÄÅÁªòÂõæÔºåÈÉΩÊòØËØ•Â∏∏Â∏∏ÊãøÂá∫Êù•Â≠¶‰π†ÁöÑÊñáÁ´†„ÄÇ ËÆ°ÈáèÁªèÊµéÂ≠¶Ê®°ÂûãÂíåÊú∫Âô®Â≠¶‰π†È¢ÑÊµãÊ®°ÂûãÁöÑÂå∫Âà´Âú®Âì™ÈáåÔºüOLS‰º∞ËÆ° https://www.nber.org/chapters/c14009.pdf Êú∫Âô®Â≠¶‰π†Èïø‰∫éÈ¢ÑÊµãÔºåËÆ°ÈáèÁªèÊµéÂ≠¶Èïø‰∫éËß£Èáä https://www.aeaweb.org/conference/cont-ed/2018-webcasts ËÆ°ÈáèÊ®°ÂûãÁöÑËÆæÁΩÆÔºöÂº∫Ë∞ÉÂèÇÊï∞‰º∞ËÆ°ÁöÑÁªèÊµéÂ≠¶Âê´‰πâ log-normal log -log Á≠âÁ≠âÔºåÊúâÁªèÊµéÂ≠¶Âê´‰πâÔºåÂπ∂‰∏îË¶ÅÂèÇÊï∞ÊòæËëóÊÄßÊ£ÄÈ™å Êú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºöÊ®°ÂûãÁöÑÁªìÊûúÔºåÈ¢ÑÊµãÊïàÊûú Á∫øÊÄßÊ®°ÂûãÔºåÊ†∏ÊäÄÂ∑ß https://otexts.com/fppcn/causality.html]]></content>
      <tags>
        <tag>Paper Recording</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F18%2F%E5%B8%8C%E8%85%8A%E5%AD%97%E6%AF%8D%2F</url>
    <content type="text"><![CDATA[$\alpha$ $\beta$ $\gamma$ $\Gamma$ $\delta$ $\Delta$ $\epsilon$ $\varepsilon$ $\zeta$ $\eta$ $\theta$ $\Theta$ $\vartheta$ $\iota$ $\kappa$ $\lambda$ $\Lambda$ $\mu$ $\nu$ $\xi$ $\Xi$ $\pi$ $\Pi$ $\varpi$ $\rho$ $\varrho$ $\sigma$ $\Sigma$ $\varsigma$ $\tau$ $\upsilon$ $\Upsilon$ $\phi$ $\Phi$ $\varphi$ $\chi$ $\psi$ $\Psi$ $\Omega$ $\omega$ alpha beta gamma delta epsilon theta]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python Basics]]></title>
    <url>%2F2019%2F05%2F28%2FPython-basic%2F</url>
    <content type="text"><![CDATA[ÈáçÊñ∞Â≠¶‰π†ÂºÄÂßãÂæà‰π±ÁöÑÂ≠¶‰π†PythonÔºåÁé∞Âú®ÊÉ≥Á≥ªÁªüÂ≠¶‰π†Âü∫Á°ÄÔºåÁúüÊ≠£‰∫ÜËß£pythonic, zip() functiona. function: zip() ÂáΩÊï∞Áî®‰∫éÂ∞ÜÂèØËø≠‰ª£ÁöÑÂØπË±°‰Ωú‰∏∫ÂèÇÊï∞ÔºåÂ∞ÜÂØπË±°‰∏≠ÂØπÂ∫îÁöÑÂÖÉÁ¥†ÊâìÂåÖÊàê‰∏Ä‰∏™‰∏™ÂÖÉÁªÑÔºåÁÑ∂ÂêéËøîÂõûÁî±Ëøô‰∫õÂÖÉÁªÑÁªÑÊàêÁöÑÂàóË°®„ÄÇ b. zip([iterable, ‚Ä¶]) c. return : ËøîÂõûÂÖÉÁªÑÂàóË°®„ÄÇÂú® Python 3.x ‰∏≠‰∏∫‰∫ÜÂáèÂ∞ëÂÜÖÂ≠òÔºåzip() ËøîÂõûÁöÑÊòØ‰∏Ä‰∏™ÂØπË±°„ÄÇÂ¶ÇÈúÄÂ±ïÁ§∫ÂàóË°®ÔºåÈúÄÊâãÂä® list() ËΩ¨Êç¢„ÄÇ 123456a = [1,2,3]b = [4,5,6]zipped = zip(a,b) # ÊâìÂåÖ‰∏∫ÂÖÉÁªÑÁöÑÂàóË°®[(1, 4), (2, 5), (3, 6)]zip(*zipped) # ‰∏é zip Áõ∏ÂèçÔºåÂèØÁêÜËß£‰∏∫Ëß£ÂéãÔºåËøîÂõû‰∫åÁª¥Áü©ÈòµÂºè[(1, 2, 3), (4, 5, 6)] enumerate() functiona. Â∞ÜÂèØÈÅçÂéÜÁöÑÊï∞ÊçÆÂØπË±°(ÂàóË°®„ÄÅÂÖÉÁªÑÊàñËÄÖÂ≠óÁ¨¶‰∏≤) ÁªÑÂêà‰∏∫‰∏Ä‰∏™Á¥¢ÂºïÂ∫èÂàóÔºåÂêåÊó∂ÁªôÂá∫Êï∞ÊçÆÂíåÊï∞ÊçÆ‰∏ãÊ†á„ÄÇ‰∏ªË¶ÅÁî®‰∫éforÂæ™ÁéØ b. enumerate(sequence, [start=0]) c. return : enumerate(Êûö‰∏æ) ÂØπË±°„ÄÇ d eg: 123seq = ['one', 'two', 'three']for i, element in enumerate(seq): print(i, element) Matplotlib bar() Âíåbarh()a. ÁªòÂà∂Áõ¥ÊñπÂõæÂíåÊù°ÂΩ¢ÂõæÔºå‰∏ªË¶ÅÁî®‰∫éÊü•ÁúãÂêÑ‰∏™ÂàÜÁªÑÁöÑÊï∞ÈáèÂàÜÂ∏É b. atplotlib.pyplot.bar(left, height, width=0.8, bottom=None, hold=None, data=None, **kwargs) c.ÂèÇÊï∞ ÂèÇÊï∞ Êé•Êî∂ÂÄº ËØ¥Êòé ÈªòËÆ§ÂÄº left array xËΩ¥ Êó† height arrat Êü±Áä∂ÂõæÁöÑÈ´òÂ∫¶ Êó† alpha Êï∞ÂÄº È¢úËâ≤ÈÄèÊòéÂ∫¶ 1 width Êï∞ÂÄº ÂÆΩÂ∫¶ 0.8 color string Â°´ÂÖÖÈ¢úËâ≤ ÈöèÊú∫Ëâ≤ label string ÊØè‰∏™ÂõæÂÉèÁöÑ‰ª£Ë°®ÁöÑÂê´‰πâ Êó† linewidth Êï∞ÂÄº Á∫øÁöÑÂàªÂ∫¶ 1 d ‰æãÂ≠ê Demo 1: Âü∫Êú¨È™®Êû∂ 123456789101112131415161718192021import pandas as pdimport matplotlib.pyplot as plt #ËØªÂèñÊï∞ÊçÆdatafile = u'D:\\pythondata\\learn\\matplotlib.xlsx'data = pd.read_excel(datafile)# ÁîªÂ∏Éplt.figure(figsize = (10, 5))plt.title('Example of Histogram', fontsize = 20)plt.xlabel(u'x-year', fontsize = 14)plt.ylabel(u'y-income',fontsize = 14)# Â§ö‰∏™Êü±Áä∂ÂõæÁöÑÂàÜÁ¶ªË∑ùÁ¶ªwidth_val = 0.4plt.bar(data['time'],data['manincome'],width = width_val)plt.bar(data['time']+width_val, data['femaleincome'],width = width_vale)plt.legend(loc = 2)plt.show() Demo 2Ôºö ÊòæÁ§∫Áõ¥ÊñπÂõæÁöÑÊï∞ÂÄº 1234567891011rect1 = plt.bar(data['time'],data['manincome'],width = width_val)rect2 = plt.bar(data['time']+width_val, data['femaleincome'],width = width_vale)python# Ê∑ªÂä†Êï∞ÊçÆÊ†áÁ≠ædef add_labels(rects): for rect in rects: height = rect.get_height() plt.text(rect.get_x() + rect.get_width()/2, height, height, ha='center', va='bottom') rect.set_edgecolor('white') Demo 3: Áõ¥ÊñπÂõæÂ†ÜÂè†ÊòæÁ§∫Ôºàbottom ÂèÇÊï∞Ë∞ÉËäÇ) 123plt.bar(data['time'],data['manincome'],width = width_val)plt.bar(data['time'], data['femaleincome'], bottom = data['manincome'], width = width_vale)python Case 2 ÂÆûÁé∞ÁªÜËäÇÔºöbarhÁöÑÂèÇÊï∞ leftËÆæÁΩÆÔºöThe x coordinates of the left sides of the bars (default: 0). matplotlib.pyplot.barh(y, width, height=0.8, left=None, ,align=‚Äôcenter‚Äô, kwargs) DataFrame.interpolateÊèíÂÄºÊ≥ïÔºåÂ°´ÂÖÖNaN 1DataFrame.interpolate(self, method=&apos;linear&apos;, axis=0, limit=None, inplace=False, limit_direction=&apos;forward&apos;, limit_area=None, downcast=None, **kwargs)[source] ÂèÇÊï∞ 12345method: str,default&quot;linear&apos;&apos;linear&apos;&apos;time&apos;&apos;index&apos;&apos;nearest&apos;,&apos;zero&apos;,&apos;slinear&apos;,‚Äòquadratic‚Äô, ‚Äòcubic‚Äô, ‚Äòspline‚Äô, ‚Äòbarycentric‚Äô, ‚Äòpolynomial‚Äô ‰æãÂ≠ê 12 subplots_adjust(wspace, hspace)functionÔºöË∞ÉÊï¥Â≠êÂõæÈó¥Ë∑ù DateTimetimedeltaÊó∂Èó¥ÂíåÊó•ÊúüÁöÑËÆ°ÁÆó Ë°®Á§∫Êó•ÊúüÂ∑Æ ËÆ°ÁÆóËßÑÂàô 1234from datetime import timedeltatd = timedelta(days=92) # days hours minutesprint(d1 + td) datetime.strptime()Â≠óÁ¨¶‰∏≤ËΩ¨Êç¢‰∏∫Êó•ÊúüÂíåÊó∂Èó¥Á±ªÂûã 12from datetime import datetimecday = datetime.strptime('2017-8-1 18:20:20', '%Y-%m-%d %H:%M:%S') datetime.strftime()datatimeËΩ¨Âåñ‰∏∫Â≠óÁ¨¶‰∏≤ 123from datetime import datetimenow = datetime.now()print(now.strftime(&apos;%a, %b %d %H:%M&apos;)) DataFrame.rolling Á™óÂè£ÂáΩÊï∞1DataFrame.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None) ÂèÇÊï∞ËØ¥ÊòéÔºö window:Êó∂Èó¥Á™óÁöÑÂ§ßÂ∞è,Êï∞ÂÄºint,Âç≥ÂêëÂâçÂá†‰∏™Êï∞ÊçÆ(ÂèØ‰ª•ÁêÜËß£Â∞ÜÊúÄËøëÁöÑÂá†‰∏™ÂÄºËøõË°ågroup by)min_periods:ÊúÄÂ∞ëÈúÄË¶ÅÊúâÂÄºÁöÑËßÇÊµãÁÇπÁöÑÊï∞Èáè,ÂØπ‰∫éintÁ±ªÂûãÔºåÈªòËÆ§‰∏éwindowÁõ∏Á≠âcenter:ÊääÁ™óÂè£ÁöÑÊ†áÁ≠æËÆæÁΩÆ‰∏∫Â±Ö‰∏≠,Â∏ÉÂ∞îÂûã,ÈªòËÆ§Falsewin_type: Á™óÂè£ÁöÑÁ±ªÂûã,Êà™ÂèñÁ™óÁöÑÂêÑÁßçÂáΩÊï∞„ÄÇÂ≠óÁ¨¶‰∏≤Á±ªÂûãÔºåÈªòËÆ§‰∏∫Noneon: ÂèØÈÄâÂèÇÊï∞,ÂØπ‰∫édataframeËÄåË®ÄÔºåÊåáÂÆöË¶ÅËÆ°ÁÆóÊªöÂä®Á™óÂè£ÁöÑÂàó,ÂÄº‰∏∫ÂàóÂêçclosedÔºöÂÆö‰πâÂå∫Èó¥ÁöÑÂºÄÈó≠ÔºåÊîØÊåÅintÁ±ªÂûãÁöÑwindow,ÂØπ‰∫éoffsetÁ±ªÂûãÈªòËÆ§ÊòØÂ∑¶ÂºÄÂè≥Èó≠ÁöÑÂç≥ÈªòËÆ§‰∏∫right,ÂèØ‰ª•Ê†πÊçÆÊÉÖÂÜµÊåáÂÆö‰∏∫left„ÄÅbothÁ≠âaxisÔºöÊñπÂêëÔºàËΩ¥Ôºâ,‰∏ÄËà¨ÈÉΩÊòØ0 Â∏∏Áî®ËÅöÂêàÂáΩÊï∞Ôºö mean() Ê±ÇÂπ≥Âùácount() ÈùûÁ©∫ËßÇÊµãÂÄºÊï∞Èáèsum() ÂÄºÁöÑÊÄªÂíåmedian() ÂÄºÁöÑÁÆóÊúØ‰∏≠ÂÄºmin() ÊúÄÂ∞èÂÄºmax() ÊúÄÂ§ßstd() Ë¥ùÂ°ûÂ∞î‰øÆÊ≠£Ê†∑Êú¨Ê†áÂáÜÂ∑Ævar() Êó†ÂÅèÊñπÂ∑Æskew() Ê†∑ÂìÅÂÅèÊñúÂ∫¶Ôºà‰∏âÈò∂Áü©Ôºâkurt() Ê†∑ÂìÅÂ≥∞Â∫¶ÔºàÂõõÈò∂Áü©Ôºâquantile() Ê†∑Êú¨ÂàÜ‰ΩçÊï∞ÔºàÁôæÂàÜ‰Ωç‰∏äÁöÑÂÄºÔºâcov() Êó†ÂÅèÂçèÊñπÂ∑ÆÔºà‰∫åÂÖÉÔºâcorr() Áõ∏ÂÖ≥Ôºà‰∫åËøõÂà∂Ôºâ Ê≥®ÊÑèÔºöËÆæÁΩÆÁöÑÁ™óÂè£window=3Ôºå‰πüÂ∞±ÊòØ3‰∏™Êï∞Âèñ‰∏Ä‰∏™ÂùáÂÄº„ÄÇindex 0,1 ‰∏∫NaN]]></content>
      <categories>
        <category>ÁºñÁ®ãËØ≠Ë®Ä</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êï∞ÊçÆÂàÜÊûê]]></title>
    <url>%2F2019%2F05%2F22%2FPandas%20%E5%81%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Pandas ÂÅöÊï∞ÊçÆÂàÜÊûêhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv Step 1: ËØªÂèñÊñá‰ª∂ csv/txt names: columnsÔºåÂΩìnamesÊ≤°Ë¢´ËµãÂÄºÊó∂Ôºåheader‰ºöÂèòÊàê0ÔºåÂç≥ÈÄâÂèñÊï∞ÊçÆÊñá‰ª∂ÁöÑÁ¨¨‰∏ÄË°å‰Ωú‰∏∫ÂàóÂêç„ÄÇÂΩì names Ë¢´ËµãÂÄºÔºåheader Ê≤°Ë¢´ËµãÂÄºÊó∂ÔºåÈÇ£‰πàheader‰ºöÂèòÊàêNone„ÄÇÂ¶ÇÊûúÈÉΩËµãÂÄºÔºåÂ∞±‰ºöÂÆûÁé∞‰∏§‰∏™ÂèÇÊï∞ÁöÑÁªÑÂêàÂäüËÉΩ„ÄÇheader = 0:ÊòØÁ¨¨‰∏ÄË°åÊòØÂêçÂ≠ó sep=‚Äò\t‚Äô header=None:ÊåáÂÆöÂàóÂêçÔºåÊï∞ÊçÆÂºÄÂßãË°åÊï∞„ÄÇÈªòËÆ§0Ë°å;None = Êó†Ê†áÈ¢ò index_col :NoneÔºõ ÊåáÂÆöÂàó‰Ωú‰∏∫Ë°åÁ¥¢Âºï Êï∞ÂÄº„ÄÇ FalseË°®Á§∫Êó†Á¥¢Âºï usecols ; Â¶ÇÊûúÂàóÊúâÂæàÂ§öÔºåËÄåÊàë‰ª¨‰∏çÊÉ≥Ë¶ÅÂÖ®ÈÉ®ÁöÑÂàó„ÄÅËÄåÊòØÂè™Ë¶ÅÊåáÂÆöÁöÑÂàóÂ∞±ÂèØ‰ª•‰ΩøÁî®Ëøô‰∏™ÂèÇÊï∞„ÄÇ prefix .prefix ÂèÇÊï∞ÔºåÂΩìÂØºÂÖ•ÁöÑÊï∞ÊçÆÊ≤°Êúâ header Êó∂ÔºåËÆæÁΩÆÊ≠§ÂèÇÊï∞‰ºöËá™Âä®Âä†‰∏Ä‰∏™ÂâçÁºÄ„ÄÇ https://www.jianshu.com/p/42f1d2909bb6 Step 2: Áº∫ÁúÅÂÄºÂ§ÑÁêÜ1. dropDataFrame.drop(*self*, *labels=None*, *axis=0*, index=None**, columns=None, level=None, inplace=False, errors=‚Äôraise‚Äô)** Parameters labels single label or list-like Index or column labels to drop. axis {0 or ‚Äòindex‚Äô, 1 or ‚Äòcolumns‚Äô}, default 0 Whether to drop labels from the index (0 or ‚Äòindex‚Äô) or columns (1 or ‚Äòcolumns‚Äô). index single label or list-like Alternative to specifying axis (labels, axis=0 is equivalent to index=labels).New in version 0.21.0. columns single label or list-like Alternative to specifying axis (labels, axis=1 is equivalent to columns=labels).New in version 0.21.0. level int or level name, optional For MultiIndex, level from which the labels will be removed. inplace bool, default False If True, do operation inplace and return None. errors{‚Äòignore‚Äô, ‚Äòraise‚Äô}, default ‚Äòraise‚Äô If ‚Äòignore‚Äô, suppress error and only existing labels are dropped. 2. dropnaDataFrame.dropna(*self*, *axis=0*, *how=‚Äôany‚Äô*, thresh=None**, *subset=None*, *inplace=False*)**4 3. isna()DataFrame.isna(*self*) Â°´ÂÖÖ 4. fillna()DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs) Â°´ÂÖÖÁ©∫ÂÄºÔºåvaluesÂèØ‰ª•ÊòØÂ≠óÂÖ∏ values = {‚ÄòA‚Äô: 0, ‚ÄòB‚Äô: 1, ‚ÄòC‚Äô: 2, ‚ÄòD‚Äô: 3} drop‰ΩøÁî® dropnaÂà†Èô§Áº∫ÁúÅÂÄº df = df.drop(some labels) df = df.drop(df[].index) ‚Äã axis: 0(index) ‚Äã how: any all ‚Äã subset: label ‚Äã inplace : bool dropÂà†Èô§ drop(labels(index, column labels), axis=0(Ë°å), level=None, inplace=False, errors=‚Äôraise‚Äô) ‚Äã axis: label ‚Äã index, columns fillna Â°´ÂÖÖ ‚Äã fillna(value, method, limit) Step 3: ÈÄâÂèñÂ≠óÊÆµ Âàó df[labels] Ë°å df.loc[index_label,] df.iloc[Êï¥Êï∞ÂÄº,] Âàó[‚Äú‚Äù] ‰πüÂèØ‰ª•‰º†ÂÖ•Êù°‰ª∂ËØ≠Âè• ÊèèËø∞ÊÄßÁªüËÆ°ÂáΩÊï∞sum().mean().count()‚Äã axis:1;ÊåâÂàóÊ±ÇÂíåÔºåÊ∞¥Âπ≥Á∫ø „ÄÇÂæÄÂè≥ÁúãÔºõÊâÄÊúâÂàóËÆ°ÁÆó ‚Äã axis:0; ÊåâË°åÊ±ÇÂíå, ÂûÇÁõ¥Á∫øÔºõÊääÂ≠óÊÆµÁöÑÊâÄÊúâÁöÑÊâÄÊúâË°åÂíå„ÄÇÂæÄ‰∏ãÊåâ ÔºõÊâÄÊúâË°åËÆ°ÁÆó .describle() .transpose()ÂäüËÉΩÊÄßÂáΩÊï∞groupby[].Â§ö‰∏™DataFameÂΩíÂπ∂‚Äã ### pd.merge(left, right, how=‚Äôinner‰∫§ÈõÜ/OuterÂπ∂ÈõÜÔºàÂ≠òÂú®‰∏çÈáçÂêàÁöÑkeyÊòØ‚Äô, on=‚Äôkey‚ÄôÂèØ‰ª•ÊòØ‰∏Ä‰∏™ÂàóË°®[]) .apply()ÊñπÊ≥ï ‰º†ÂÖ•ÂáΩÊï∞ÂêçÔºöÁÑ∂ÂêéÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ÈÉΩËÉåËÆ°ÁÆó‚Äã lambda x : x*x ÊéíÂ∫è .sort_values()Êï¥‰∏™Ë°®Ê†ºÈÉΩËøô‰πàÊéíÂàó DataFrame.sort_values(by=[lables],axis=[0,1], ascending:, kind:ÊéíÂ∫èÁÆóÊ≥ïÔºå) Êó•ÊúüÁ±ªÂûã: Êó•ÊúüÁ≠âÊï∞ÂÄºÂ§ÑÁêÜstrÂàóËΩ¨Êç¢ÊàêÊó•ÊúüÁ±ªÂûã pd.to_datetimeÊ≥®ÊÑèÈùûÊó•ÊúüÁ±ªÂûãÁöÑÁâπÊÆäÂ§ÑÁêÜ 12345678df[&apos;Êó•ÊúüÊó∂Èó¥&apos;] = pd.to_datetime(df[&apos;Êó•ÊúüÊó∂Èó¥&apos;],format=&apos;%Y/%m/%d %H:%M:%S&apos;) #Ëé∑Âèñ Êó•ÊúüÊï∞ÊçÆ ÁöÑÂπ¥„ÄÅÊúà„ÄÅÊó•„ÄÅÊó∂„ÄÅÂàÜdf[&apos;Âπ¥&apos;] = df[&apos;Êó•ÊúüÊó∂Èó¥&apos;].dt.yeardf[&apos;Êúà&apos;] = df[&apos;Êó•ÊúüÊó∂Èó¥&apos;].dt.monthdf[&apos;Êó•&apos;] = df[&apos;Êó•ÊúüÊó∂Èó¥&apos;].dt.daydf[&apos;Êó∂&apos;] = df[&apos;Êó•ÊúüÊó∂Èó¥&apos;].dt.hourdf[&apos;ÂàÜ&apos;] = df[&apos;Êó•ÊúüÊó∂Èó¥&apos;].dt.minute ÊåáÂÆöÁ±ªÂûã‚Äã method1 df1[‚Äòyear_month‚Äô] = df1[‚Äòdate‚Äô].apply(lambda x : x.strftime(‚Äò%Y-%m‚Äô)) ‚Äã method2 df1[‚Äòperiod‚Äô] = df1[‚Äòdate‚Äô].dt.to_period(‚ÄòM‚Äô) ÂèÇÊï∞ M Ë°®Á§∫Êúà‰ªΩÔºåQ Ë°®Á§∫Â≠£Â∫¶ÔºåA Ë°®Á§∫Âπ¥Â∫¶ÔºåD Ë°®Á§∫ÊåâÂ§© strp/ftimeÂ≠óÁ¨¶‰∏≤ÂíåÊó•ÊúüÁöÑËΩ¨Êç¢ strftime: time-&gt;str strptime: str-&gt;time datetime.timedeltaË°®Á§∫Êó∂Èó¥Èó¥ÈöîÔºå‰∏§‰∏™Êó∂Èó¥ÁÇπ‰πãÈó¥ÁöÑÈïøÂ∫¶Ôºå‰∏ªË¶ÅÁî®‰∫éÊó∂Èó¥ËÆ°ÁÆó,Â¶ÇÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑÊó∂ÂÄôÔºåÈúÄË¶ÅÂ§ñÊé®ÔºåÂèØËÉΩÊ∂âÂèäÂà∞Êó∂Èó¥ÁöÑËÆ°ÁÆó1timedelta(weeks=0, days=0, hours=0, minutes=0, seconds=0, milliseconds=0, microseconds=0, ) #‰æùÊ¨°‰∏∫ "Âë®" "Â§©", "Êó∂","ÂàÜ","Áßí","ÊØ´Áßí","ÂæÆÁßí" datetimeÊ®°Âùó Á±ªÂûã ËØ¥Êòé date ‰ª•ÂÖ¨ÂéÜÂΩ¢ÂºèÂ≠òÂÇ®Êó•ÂéÜÊó•ÊúüÔºàÂπ¥„ÄÅÊúà„ÄÅÊó•Ôºâ time Â∞ÜÊó∂Èó¥Â≠òÂÇ®‰∏∫Êó∂„ÄÅÂàÜ„ÄÅÁßí„ÄÅÊØ´Áßí datetime Â≠òÂÇ®Êó•ÊúüÂíåÊó∂Èó¥ 1ÔºâpythonÊ†áÂáÜÂ∫ìÂáΩÊï∞ Êó•ÊúüËΩ¨Êç¢ÊàêÂ≠óÁ¨¶‰∏≤ÔºöÂà©Áî®str Êàñstrftime Â≠óÁ¨¶‰∏≤ËΩ¨Êç¢ÊàêÊó•ÊúüÔºödatetime.strptime 12345678910stamp = datetime(2017,6,27)str(stamp) '2017-06-27 00:00:00'stamp.strftime('%y-%m-%d')#%YÊòØ4‰ΩçÂπ¥Ôºå%yÊòØ2‰ΩçÂπ¥ '17-06-27'#ÂØπÂ§ö‰∏™Êó∂Èó¥ËøõË°åËß£ÊûêÊàêÂ≠óÁ¨¶‰∏≤date = ['2017-6-26','2017-6-27']datetime2 = [datetime.strptime(x,'%Y-%m-%d') for x in date]datetime2[datetime.datetime(2017, 6, 26, 0, 0), datetime.datetime(2017, 6, 27, 0, 0)] 3ÔºâpandasÂ§ÑÁêÜÊàêÁªÑÊó•Êúü pandasÈÄöÂ∏∏Áî®‰∫éÂ§ÑÁêÜÊàêÁªÑÊó•ÊúüÔºå‰∏çÁÆ°Ëøô‰∫õÊó•ÊúüÊòØDataFrameÁöÑËΩ¥Á¥¢ÂºïËøòÊòØÂàóÔºåto_datetimeÊñπÊ≥ïÂèØ‰ª•Ëß£ÊûêÂ§öÁßç‰∏çÂêåÁöÑÊó•ÊúüË°®Á§∫ÂΩ¢Âºè„ÄÇ datetime Ê†ºÂºèÂÆö‰πâ ‰ª£Á†Å ËØ¥Êòé %Y 4‰ΩçÊï∞ÁöÑÂπ¥ %y 2‰ΩçÊï∞ÁöÑÂπ¥ %m 2‰ΩçÊï∞ÁöÑÊúà[01,12] %d 2‰ΩçÊï∞ÁöÑÊó•[01Ôºå31] %H Êó∂Ôºà24Â∞èÊó∂Âà∂Ôºâ[00,23] %l Êó∂Ôºà12Â∞èÊó∂Âà∂Ôºâ[01,12] %M 2‰ΩçÊï∞ÁöÑÂàÜ[00,59] %S Áßí[00,61]ÊúâÈó∞ÁßíÁöÑÂ≠òÂú® %w Áî®Êï¥Êï∞Ë°®Á§∫ÁöÑÊòüÊúüÂá†[0ÔºàÊòüÊúüÂ§©ÔºâÔºå6] %F %Y-%m-%dÁÆÄÂÜôÂΩ¢Âºè‰æãÂ¶ÇÔºå2017-06-27 %D %m/%d/%yÁÆÄÂÜôÂΩ¢Âºè Â≠óÁ¨¶‰∏≤ËΩ¨Êç¢ÊàêdatetimeÊ†ºÂºè: strptimedatetime.strptime(str, ‚Äò%Y/%m/%d‚Äô).date() datetimeÂèòÂõûstringÊ†ºÂºè: strftime1234567df = pd.DataFrame(&#123;&quot;y&quot;: [1, 2, 3]&#125;,... index=pd.to_datetime([&quot;2000-03-31 00:00:00&quot;,... &quot;2000-05-31 00:00:00&quot;,... &quot;2000-08-31 00:00:00&quot;]))&gt;&gt;&gt; df.index.to_period(&quot;M&quot;)PeriodIndex([&apos;2000-03&apos;, &apos;2000-05&apos;, &apos;2000-08&apos;], dtype=&apos;period[M]&apos;, freq=&apos;M&apos;) 1‚Äîpd.Period()ÂèÇÊï∞Ôºö‰∏Ä‰∏™Êó∂Èó¥Êà≥ÁîüÊàêÂô® Step 4: Êï∞ÊçÆÈÄèÊûêË°® pivot_table.pivot_table Êï∞ÊçÆÈÄèÊûêË°® ÂàÜÁ±ªÊ±áÊÄªÁöÑÁªüËÆ°Êï∞ÊçÆ‚Äã (data,values= column to aggregate optional, index = grouper, columns=grouper, aggfunc:np.sum ) ‚Äã table = pd.pivot_table(df, values=[‚ÄòD‚Äô, ‚ÄòE‚Äô], index=[‚ÄòA‚Äô, ‚ÄòC‚Äô], aggfunc={‚ÄòD‚Äô: np.mean, ‚ÄòE‚Äô: np.mean}) .groupby() Áî±‰∫éÈÄöËøágroupby()ÂáΩÊï∞ÂàÜÁªÑÂæóÂà∞ÁöÑÊòØ‰∏Ä‰∏™DataFrameGroupByÂØπË±°ÔºåËÄåÈÄöËøáÂØπËøô‰∏™ÂØπË±°Ë∞ÉÁî®get_group()ÔºåËøîÂõûÁöÑÂàôÊòØ‰∏Ä‰∏™¬∑DataFrame¬∑ÂØπË±°ÔºåÊâÄ‰ª•ÂèØ‰ª•Â∞ÜDataFrameGroupByÂØπË±°ÁêÜËß£‰∏∫ÊòØÂ§ö‰∏™DataFrameÁªÑÊàêÁöÑ„ÄÇ 12grouped = df.groupby('Gender')grouped_muti = df.groupby(['Gender', 'Age']) 123456789print(grouped.get_group('Female'))print(grouped_muti.get_group(('Female', 17))) Name Gender Age Score2 Cidy Female 18 934 Ellen Female 17 967 Hebe Female 22 98 Name Gender Age Score4 Ellen Female 17 96 123456789101112131415print(grouped.count())print(grouped.max()[['Age', 'Score']])print(grouped.mean()[['Age', 'Score']]) Name Age ScoreGender Female 3 3 3Male 5 5 5 Age ScoreGender Female 22 98Male 21 100 Age ScoreGender Female 19.0 95.666667Male 19.6 89.000000 Â¶ÇÊûúÂÖ∂‰∏≠ÁöÑÂáΩÊï∞Êó†Ê≥ïÊª°Ë∂≥‰Ω†ÁöÑÈúÄÊ±ÇÔºå‰Ω†‰πüÂèØ‰ª•ÈÄâÊã©‰ΩøÁî®ËÅöÂêàÂáΩÊï∞aggregateÔºå‰º†ÈÄínumpyÊàñËÄÖËá™ÂÆö‰πâÁöÑÂáΩÊï∞ÔºåÂâçÊèêÊòØËøîÂõû‰∏Ä‰∏™ËÅöÂêàÂÄº 12345678910def getSum(data): total = 0 for d in data: total+=d return totalprint(grouped.aggregate(np.median))print(grouped.aggregate(&#123;'Age':np.median, 'Score':np.sum&#125;))print(grouped.aggregate(&#123;'Age':getSum&#125;)) Ëø≠‰ª£ 1234567891011121314151617grouped = df.groupby('A')for name, group in grouped: print(name) print(group) bar A B C D1 bar one 0.254161 1.5117633 bar three 0.215897 -0.9905825 bar two -0.077118 1.211526foo A B C D0 foo one -0.575247 1.3460612 foo two -1.143704 1.6270814 foo two 1.193555 -0.4416526 foo one -0.408530 0.2685207 foo three -0.862495 0.024580 ÂèØËßÜÂåñ ÂØπÁªÑÂÜÖÁöÑÊï∞ÊçÆÁªòÂà∂Ê¶ÇÁéáÂØÜÂ∫¶ÂàÜÂ∏ÉÔºö 12grouped['Age'].plot(kind='kde', legend=True)plt.show() ËÆ°ÁÆó‰∏çÂêåÁªÑÁöÑÊüê‰∏ÄÂàóÁöÑÂÄº 12data.groupby('race')['age'].mean()Ë¶ÅÊ±ÇË¢´‰∏çÂêåÁßçÊóèÂÜÖË¢´ÂáªÊØô‰∫∫ÂëòÂπ¥ÈæÑÁöÑÂùáÂÄº: ÂØπ‰∏çÂêåÂèñÂÄºÁöÑËÆ°Êï∞: .value_counts() 12data.groupby('race')['signs_of_mental_illness'].value_counts()Ê±Ç‰∏çÂêåÁßçÊóèÂÜÖ, ÊòØÂê¶ÊúâÁ≤æÁ•ûÂºÇÂ∏∏ËøπË±°ÁöÑÂàÜÂà´ÊúâÂ§öÂ∞ë‰∫∫ 12data.groupby('race')['signs_of_mental_illness'].value_counts().unstack()ÁªÑÂÜÖÊìç‰ΩúÁöÑÁªìÊûú‰∏çÊòØÂçï‰∏™ÂÄº, ÊòØ‰∏Ä‰∏™Â∫èÂàó, Êàë‰ª¨ÂèØ‰ª•Áî®.unstack()Â∞ÜÂÆÉÂ±ïÂºÄÔºåÂæóÂà∞DateFrame 1data.groupby('race')['flee'].value_counts().unstack().plot(kind='bar', figsize=(20, 4)) ËøôÈáåÊúâ‰∏Ä‰∏™‰πãÂâç‰ªãÁªçÁöÑ.unstackÊìç‰Ωú, Ëøô‰ºöËÆ©‰Ω†ÂæóÂà∞‰∏Ä‰∏™DateFrame, ÁÑ∂ÂêéË∞ÉÁî®Êù°ÂΩ¢Âõæ, pandasÂ∞±‰ºöÈÅçÂéÜÊØè‰∏Ä‰∏™ÁªÑ(unstackÂêé‰∏∫ÊØè‰∏ÄË°å), ÁÑ∂Âêé‰ΩúÂêÑÁªÑÁöÑÊù°ÂΩ¢Âõæ Êåâ‰∏çÂêåÈÄÉÈÄ∏Á±ªÂûãÂàÜÁªÑ, ÁªÑÂÜÖÁöÑÂπ¥ÈæÑÂàÜÂ∏ÉÊòØÂ¶Ç‰ΩïÁöÑ?1data.groupby('flee')['age'].plot(kind='kde', legend=True, figsize=(20, 5)) ËøôÈáådata.groupby(&#39;flee&#39;)[&#39;age&#39;]ÊòØ‰∏Ä‰∏™SeriesGroupbyÂØπË±°, È°æÂêçÊÄù‰πâ, Â∞±ÊòØÊØè‰∏Ä‰∏™ÁªÑÈÉΩÊúâ‰∏Ä‰∏™Series. Âõ†‰∏∫ÂàíÂàÜ‰∫Ü‰∏çÂêåÈÄÉÈÄ∏Á±ªÂûãÁöÑÁªÑ, ÊØè‰∏ÄÁªÑÂåÖÂê´‰∫ÜÁªÑÂÜÖÁöÑÂπ¥ÈæÑÊï∞ÊçÆ, ÊâÄ‰ª•Áõ¥Êé•plotÁõ∏ÂΩì‰∫éÈÅçÂéÜ‰∫ÜÊØè‰∏Ä‰∏™ÈÄÉÈÄ∏Á±ªÂûã, ÁÑ∂ÂêéÂàÜÂà´ÁîªÂàÜÂ∏ÉÂõæ. Step 5: ÂÜôÂÖ•Ë°®Ê†ºto_csvÔºàpath_or_bufÔºåsepÔºåheader: bool or list of str : defaultÔºötrueÔºå index: bool, default trueÔºâ Â≠óÁ¨¶‰∏≤Â§ÑÁêÜÂ§ö‰∏™Â≠óÁ¨¶‰∏≤ÂàÜÂâ≤Python‰∏≠ÁöÑspiltÊñπÊ≥ïÂè™ËÉΩÈÄöËøáÊåáÂÆöÁöÑÊüê‰∏™Â≠óÁ¨¶ÂàÜÂâ≤Â≠óÁ¨¶‰∏≤ÔºåÂ¶ÇÊûúÈúÄË¶ÅÊåáÂÆöÂ§ö‰∏™Â≠óÁ¨¶ÔºåÈúÄË¶ÅÁî®Âà∞reÊ®°ÂùóÈáåÁöÑsplitÊñπÊ≥ï„ÄÇ 1234567&gt;&gt;&gt; import re&gt;&gt;&gt; a = &quot;Hello world!How are you?My friend.Tom&quot;&gt;&gt;&gt; re.split(&quot; |!|\?|\.&quot;, a)[&apos;Hello&apos;, &apos;world&apos;, &apos;How&apos;, &apos;are&apos;, &apos;you&apos;, &apos;My&apos;, &apos;friend&apos;, &apos;Tom&apos;] ÂéªÊéâÂ§ö‰ΩôÁ©∫Ê†º filter aStr_splited = aStr.split(‚Äò ‚Äò) print(filter(lambda x : x, aStr_splited)) list(filter(None,s.split(‚Äò,‚Äô))) ÂàóË°® [x for x in s.split(‚Äò,‚Äô) if x] Ê≠£ÂàôË°®ËææÂºèhttps://docs.python.org/zh-cn/3/library/re.html Ê≠£ÂàôË°®ËææÂºèÔºö‰∏ÄÁßçÁâπÊÆäÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÁî®‰∫éÊü•ÊâæÊüêÁßçÂΩ¢ÂºèÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÊª°Ë∂≥ÊüêÁßçÊù°‰ª∂ÁöÑÊ†ºÂºè„ÄÇ Áî®‰∫éÊü•ÊâæÔºåÂåπÈÖç reÊ®°Âùó re. pattern ‚Äã [a-z] [abc] ‚Äã ab 1\d ÂåπÈÖç‰ªª‰ΩïÂçÅËøõÂà∂Êï∞Â≠óÔºõËøôÁ≠â‰ª∑‰∫éÁ±ª [0-9]„ÄÇ 1\D ÂåπÈÖç‰ªª‰ΩïÈùûÊï∞Â≠óÂ≠óÁ¨¶ÔºõËøôÁ≠â‰ª∑‰∫éÁ±ª [^0-9]„ÄÇ 1\s ÂåπÈÖç‰ªª‰ΩïÁ©∫ÁôΩÂ≠óÁ¨¶ÔºõËøôÁ≠â‰ª∑‰∫éÁ±ª [ \t\n\r\f\v]„ÄÇ 1\S ÂåπÈÖç‰ªª‰ΩïÈùûÁ©∫ÁôΩÂ≠óÁ¨¶ÔºõËøôÁõ∏ÂΩì‰∫éÁ±ª [^ \t\n\r\f\v]„ÄÇ 1\w ÂåπÈÖç‰ªª‰ΩïÂ≠óÊØç‰∏éÊï∞Â≠óÂ≠óÁ¨¶ÔºõËøôÁõ∏ÂΩì‰∫éÁ±ª [a-zA-Z0-9_]„ÄÇ 1\W ÂåπÈÖç‰ªª‰ΩïÈùûÂ≠óÊØç‰∏éÊï∞Â≠óÂ≠óÁ¨¶ÔºõËøôÁõ∏ÂΩì‰∫éÁ±ª [^a-zA-Z0-9_]„ÄÇ ca*t Â∞ÜÂåπÈÖç &#39;ct&#39; (0‰∏™ &#39;a&#39; Â≠óÁ¨¶)Ôºå&#39;cat&#39; (1‰∏™ &#39;a&#39; )Ôºå &#39;caaat&#39; (3‰∏™ &#39;a&#39; Â≠óÁ¨¶) Âè¶‰∏Ä‰∏™ÈáçÂ§çÁöÑÂÖÉÂ≠óÁ¨¶ÊòØ +ÔºåÂÆÉÂåπÈÖç‰∏ÄÊ¨°ÊàñÂ§öÊ¨°„ÄÇ Ë¶ÅÁâπÂà´Ê≥®ÊÑè * Âíå + ‰πãÈó¥ÁöÑÂå∫Âà´Ôºõ* ÂåπÈÖç Èõ∂Ê¨° ÊàñÊõ¥Â§öÊ¨°ÔºåÂõ†Ê≠§ÈáçÂ§çÁöÑ‰ªª‰Ωï‰∏úË•øÈÉΩÂèØËÉΩÊ†πÊú¨‰∏çÂ≠òÂú®ÔºåËÄå + Ëá≥Â∞ëÈúÄË¶Å ‰∏ÄÊ¨°„ÄÇ ‰ΩøÁî®Á±ª‰ººÁöÑ‰æãÂ≠êÔºåca+t Â∞ÜÂåπÈÖç &#39;cat&#39; (1 ‰∏™ &#39;a&#39;)Ôºå&#39;caaat&#39; (3 ‰∏™ &#39;a&#39;)Ôºå‰ΩÜ‰∏ç‰ºöÂåπÈÖç &#39;ct&#39;„ÄÇ ÊúÄÂ§çÊùÇÁöÑÈáçÂ§çÈôêÂÆöÁ¨¶ÊòØ {m,n}ÔºåÂÖ∂‰∏≠ m Âíå n ÊòØÂçÅËøõÂà∂Êï¥Êï∞„ÄÇ Ëøô‰∏™ÈôêÂÆöÁ¨¶ÊÑèÂë≥ÁùÄÂøÖÈ°ªËá≥Â∞ëÈáçÂ§ç m Ê¨°ÔºåÊúÄÂ§öÈáçÂ§ç n Ê¨°„ÄÇ ‰æãÂ¶ÇÔºåa/{1,3}b Â∞ÜÂåπÈÖç &#39;a/b&#39; Ôºå&#39;a//b&#39; Âíå &#39;a///b&#39; „ÄÇ ÂÆÉ‰∏çÂåπÈÖçÊ≤°ÊúâÊñúÁ∫øÁöÑ &#39;ab&#39;ÔºåÊàñËÄÖÊúâÂõõ‰∏™ÁöÑ &#39;a////b&#39;„ÄÇ ^Ë°®Á§∫Ë°åÁöÑÂºÄÂ§¥Ôºå^\dË°®Á§∫ÂøÖÈ°ª‰ª•Êï∞Â≠óÂºÄÂ§¥„ÄÇ `Ë°®Á§∫Ë°åÁöÑÁªìÊùüÔºå`\dË°®Á§∫ÂøÖÈ°ª‰ª•Êï∞Â≠óÁªìÊùü„ÄÇ ÊñπÊ≥ï / Â±ûÊÄß ÁõÆÁöÑ match() Á°ÆÂÆöÊ≠£ÂàôÊòØÂê¶‰ªéÂ≠óÁ¨¶‰∏≤ÁöÑÂºÄÂ§¥ÂåπÈÖç„ÄÇ search() Êâ´ÊèèÂ≠óÁ¨¶‰∏≤ÔºåÊü•ÊâæÊ≠§Ê≠£ÂàôÂåπÈÖçÁöÑ‰ªª‰Ωï‰ΩçÁΩÆ„ÄÇ findall() ÊâæÂà∞Ê≠£ÂàôÂåπÈÖçÁöÑÊâÄÊúâÂ≠êÂ≠óÁ¨¶‰∏≤ÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨‰Ωú‰∏∫ÂàóË°®ËøîÂõû„ÄÇ finditer() ÊâæÂà∞Ê≠£ÂàôÂåπÈÖçÁöÑÊâÄÊúâÂ≠êÂ≠óÁ¨¶‰∏≤ÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨ËøîÂõû‰∏∫‰∏Ä‰∏™ iterator„ÄÇ ÂåπÈÖçÂØπË±°ËøîÂõûÂÄºÁöÑÂáΩÊï∞ ÊñπÊ≥ï / Â±ûÊÄß ÁõÆÁöÑ 1group() ËøîÂõûÊ≠£ÂàôÂåπÈÖçÁöÑÂ≠óÁ¨¶‰∏≤ 1start() ËøîÂõûÂåπÈÖçÁöÑÂºÄÂßã‰ΩçÁΩÆ 1end() ËøîÂõûÂåπÈÖçÁöÑÁªìÊùü‰ΩçÁΩÆ 1span() ËøîÂõûÂåÖÂê´ÂåπÈÖç (start, end) ‰ΩçÁΩÆÁöÑÂÖÉÁªÑ 123456&gt;&gt;&gt; m.group()&apos;tempo&apos;&gt;&gt;&gt; m.start(), m.end()(0, 5)&gt;&gt;&gt; m.span()(0, 5) re.``search(pattern, string, flags=0)¬∂ Êâ´ÊèèÊï¥‰∏™ Â≠óÁ¨¶‰∏≤ ÊâæÂà∞ÂåπÈÖçÊ†∑ÂºèÁöÑÁ¨¨‰∏Ä‰∏™‰ΩçÁΩÆÔºåÂπ∂ËøîÂõû‰∏Ä‰∏™Áõ∏Â∫îÁöÑ ÂåπÈÖçÂØπË±°„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÂåπÈÖçÔºåÂ∞±ËøîÂõû‰∏Ä‰∏™ None Ôºõ Ê≥®ÊÑèËøôÂíåÊâæÂà∞‰∏Ä‰∏™Èõ∂ÈïøÂ∫¶ÂåπÈÖçÊòØ‰∏çÂêåÁöÑ„ÄÇ re.``match(pattern, string, flags=0) Â¶ÇÊûú string ÂºÄÂßãÁöÑ0ÊàñËÄÖÂ§ö‰∏™Â≠óÁ¨¶ÂåπÈÖçÂà∞‰∫ÜÊ≠£ÂàôË°®ËææÂºèÊ†∑ÂºèÔºåÂ∞±ËøîÂõû‰∏Ä‰∏™Áõ∏Â∫îÁöÑ ÂåπÈÖçÂØπË±° „ÄÇ Â¶ÇÊûúÊ≤°ÊúâÂåπÈÖçÔºåÂ∞±ËøîÂõû None ÔºõÊ≥®ÊÑèÂÆÉË∑üÈõ∂ÈïøÂ∫¶ÂåπÈÖçÊòØ‰∏çÂêåÁöÑ„ÄÇ Matplotlibimport matplotlib.pyplot as plt plt.figure() ÁîªÂ∏É plt.subplot() ÂàíÂàÜÂ≠êÂõæÂü∫Êú¨Â±ûÊÄßÊ†áÈ¢ò market linstyle plt.text() ËÆæÁΩÆÂõæÂÜÖÊñáÊú¨ ËΩ¥Ê†áÁ≠æ plt.xlabel() ËÆæÁΩÆÂùêÊ†áËΩ¥Ê†áÁ≠æ plt.ylabel() ËåÉÂõ¥ plt.xlim() ËÆæÁΩÆÂùêÊ†áÂèñÂÄºËåÉÂõ¥ ÂÖÉÁªÑ plt.ylim() plt.imshow() plt.axis(‚Äúoff‚Äù) ËÆæÁΩÆËÆ∞Âè∑ÂàªÂ∫¶ ÂàªÂ∫¶Ê†áÁ≠æplt.xticksÔºà[-np.piÔºå-np.pi / 2,0Ôºånp.pi / 2Ôºånp.pi]Ôºâ plt.yticksÔºà[ - 1Ôºå0Ôºå+1]Ôºâ plt.xticks(new_ticks)plt.yticks([-2, -1.8, -1, 1.22, 3], [r‚Äô$really\ bad$‚Äô, r‚Äô$bad$‚Äô, r‚Äô$normal\ \alpha$‚Äô, r‚Äô$good$‚Äô, r‚Äô$really\ good$‚Äô]) ËÆæÁΩÆËÆ∞Âè∑Ê†áÁ≠æplt.xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi], [r‚Äô$-\pi$‚Äô, r‚Äô$-\pi/2$‚Äô, r‚Äô$0$‚Äô, r‚Äô$+\pi/2$‚Äô, r‚Äô$+\pi$‚Äô]) plt.yticks([-1, 0, +1], [r‚Äô$-1$‚Äô, r‚Äô$0$‚Äô, r‚Äô$+1$‚Äô]) ËÆæÁΩÆÂùêÊ†áplt.plot([],[],) linestyle marker marketsize https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html linestyle or ls {‚Äò-‚Äò, ‚Äò‚Äî‚Äò, ‚Äò-.‚Äô, ‚Äò:‚Äô, ‚Äò‚Äô, (offset, on-off-seq), ‚Ä¶} linewidth or lw float marker marker style markeredgecolor or mec color markeredgewidth or mew float markerfacecolor or mfc color markerfacecoloralt or mfcalt color markersize or ms float plt.axis([2011,2014,0.04,0.18]) plt.xticks(np.arange(2011,2015,1)) plt.yticks(np.arange(0.04,0.20,0.02)) plt.ylabel(‚ÄúRMSE‚Äù) plt.xlabel(‚Äú(a) LSTM‚Äù) plt.grid(TrueÔºâ plt.figure(2) #plt.subplot(222) pandas1df.plot(subplots=True, figsize=(6, 6)); plt.legend(loc=&apos;best&apos;) data : DataFrame x : label or position, default None y : label or position, default None Allows plotting of one column versus another kind : str ‚Äòline‚Äô : line plot (default) ‚Äòbar‚Äô : vertical bar plot ‚Äòbarh‚Äô : horizontal bar plot ‚Äòhist‚Äô : histogram ‚Äòbox‚Äô : boxplot ‚Äòkde‚Äô : Kernel Density Estimation plot ‚Äòdensity‚Äô : same as ‚Äòkde‚Äô ‚Äòarea‚Äô : area plot ‚Äòpie‚Äô : pie plot ‚Äòscatter‚Äô : scatter plot ‚Äòhexbin‚Äô : hexbin plot ax : matplotlib axes object, default None Êó∂Èó¥Â∫èÂàó123456789101112131415161718192021222324252627import datetimeimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesimport numpy as npfig, ax = plt.subplots()months = mdates.MonthLocator()dateFmt = mdates.DateFormatter("%m/%d/%y")ax.xaxis.set_major_formatter(dateFmt)ax.xaxis.set_minor_locator(months)ax.tick_params(axis="both", direction="out", labelsize=10)date1 = datetime.date(2005, 8, 8)date2 = datetime.date(2015, 6, 6)delta = datetime.timedelta(days=5)dates = mdates.drange(date1, date2, delta)y = np.random.normal(100, 15, len(dates))ax.plot_date(dates, y, "#FF8800", alpha=0.7)fig.autofmt_xdate()plt.show() plot()12345678910111213141516171819202122232425262728293031323334DataFrame.plot(self, *args, **kwargs)kindstrThe kind of plot to produce:‚Äòline‚Äô : line plot (default)‚Äòbar‚Äô : vertical bar plot‚Äòbarh‚Äô : horizontal bar plot‚Äòhist‚Äô : histogram‚Äòbox‚Äô : boxplot‚Äòkde‚Äô : Kernel Density Estimation plot‚Äòdensity‚Äô : same as ‚Äòkde‚Äô‚Äòarea‚Äô : area plot‚Äòpie‚Äô : pie plot‚Äòscatter‚Äô : scatter plot‚Äòhexbin‚Äô : hexbin plot.figsizea tuple (width, height) in inchesx :labely:labelxlim:xticks:titlepandas.DataFrame.plot.barpandas.DataFrame.plot.barhpandas.DataFrame.plot.boxpandas.DataFrame.plot.densitypandas.DataFrame.plot.hexbinpandas.DataFrame.plot.histpandas.DataFrame.plot.kdepandas.DataFrame.plot.linepandas.DataFrame.plot.piepandas.DataFrame.plot.scatterpandas.DataFrame.boxplotpandas.DataFrame.hist https://blog.csdn.net/fengbingchun/article/details/81035861?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase https://www.cnblogs.com/Summer-skr--blog/p/11705925.html Êó∂Èó¥Â∫èÂàóÁªòÂõæÂùêÊ†áËΩ¥xtickËÆæÁΩÆÊñπÊ≥ïplt.xticks(statiem,[datetime.strftime(x,‚Äô%Y-%m‚Äô) for x in statiem]) Ê≥®ÊÑè‰∫ãÈ°πlist() dict()ÁöÑÊã∑Ë¥ù1„ÄÅb = a: ËµãÂÄºÂºïÁî®Ôºåa Âíå b ÈÉΩÊåáÂêëÂêå‰∏Ä‰∏™ÂØπË±°„ÄÇ 2„ÄÅb = a.copy(): ÊµÖÊã∑Ë¥ù, a Âíå b ÊòØ‰∏Ä‰∏™Áã¨Á´ãÁöÑÂØπË±°Ôºå‰ΩÜ‰ªñ‰ª¨ÁöÑÂ≠êÂØπË±°ËøòÊòØÊåáÂêëÁªü‰∏ÄÂØπË±°ÔºàÊòØÂºïÁî®Ôºâ„ÄÇ b = copy.deepcopy(a): Ê∑±Â∫¶Êã∑Ë¥ù, a Âíå b ÂÆåÂÖ®Êã∑Ë¥ù‰∫ÜÁà∂ÂØπË±°ÂèäÂÖ∂Â≠êÂØπË±°Ôºå‰∏§ËÄÖÊòØÂÆåÂÖ®Áã¨Á´ãÁöÑ„ÄÇ LinuxnohuplinuxÂêéÂè∞ÊâßË°åÂëΩ‰ª§Ôºö&amp;Âíånohup Áî®ÈÄîÔºö‰∏çÊåÇÊñ≠Âú∞ËøêË°åÂëΩ‰ª§„ÄÇ ËØ≠Ê≥ïÔºönohup Command [ Arg ‚Ä¶ ] [ &amp; ] ‰æãÂ≠êÔºö nohup sh example.sh &amp; nohup ÂëΩ‰ª§ÂèØ‰ª•‰ΩøÂëΩ‰ª§Ê∞∏‰πÖÁöÑÊâßË°å‰∏ãÂéªÔºåÂíåÁªàÁ´ØÊ≤°ÊúâÂÖ≥Á≥ªÔºåÈÄÄÂá∫ÁªàÁ´Ø‰πü‰∏ç‰ºöÂΩ±ÂìçÁ®ãÂ∫èÁöÑËøêË°åÔºõ&amp; ÊòØÂêéÂè∞ËøêË°åÁöÑÊÑèÊÄùÔºå‰ΩÜÂΩìÁî®Êà∑ÈÄÄÂá∫ÁöÑÊó∂ÂÄôÔºåÂëΩ‰ª§Ëá™Âä®‰πüË∑üÁùÄÈÄÄÂá∫„ÄÇÈÇ£‰πàÔºåÊää‰∏§‰∏™ÁªìÂêàËµ∑Êù•nohup ÂëΩ‰ª§ &amp;ËøôÊ†∑Â∞±ËÉΩ‰ΩøÂëΩ‰ª§Ê∞∏‰πÖÁöÑÂú®ÂêéÂè∞ÊâßË°å nohup ÂëΩ‰ª§ &gt; output.log 2&gt;&amp;1 &amp;ËÆ©ÂëΩ‰ª§Âú®ÂêéÂè∞ÊâßË°å„ÄÇ ÂÖ∂‰∏≠ 0„ÄÅ1„ÄÅ2ÂàÜÂà´‰ª£Ë°®Â¶Ç‰∏ãÂê´‰πâÔºö0 ‚Äì stdin (standard input)1 ‚Äì stdout (standard output)2 ‚Äì stderr (standard error) nohup+ÊúÄÂêéÈù¢ÁöÑ&amp;ÊòØËÆ©ÂëΩ‰ª§Âú®ÂêéÂè∞ÊâßË°å &gt;output.log ÊòØÂ∞Ü‰ø°ÊÅØËæìÂá∫Âà∞output.logÊó•Âøó‰∏≠ 2&gt;&amp;1ÊòØÂ∞ÜÊ†áÂáÜÈîôËØØ‰ø°ÊÅØËΩ¨ÂèòÊàêÊ†áÂáÜËæìÂá∫ÔºåËøôÊ†∑Â∞±ÂèØ‰ª•Â∞ÜÈîôËØØ‰ø°ÊÅØËæìÂá∫Âà∞output.log Êó•ÂøóÈáåÈù¢Êù•„ÄÇ &amp; ÂêéÂè∞ÊâßË°å> ËæìÂá∫Âà∞‰∏çËøáËÅîÂêà‰ΩøÁî®‰πüÊúâÂÖ∂‰ªñÊÑèÊÄùÔºåÊØîÂ¶ÇnohupËæìÂá∫ÈáçÂÆöÂêë‰∏äÁöÑÂ∫îÁî®‰æãÂ≠êÔºönohup abc.sh &gt; nohup.log 2&gt;&amp;1 &amp;ÂÖ∂‰∏≠2&gt;&amp;1 ÊåáÂ∞ÜSTDERRÈáçÂÆöÂêëÂà∞ÂâçÈù¢Ê†áÂáÜËæìÂá∫ÂÆöÂêëÂà∞ÁöÑÂêåÂêçÊñá‰ª∂‰∏≠ÔºåÂç≥&amp;1Â∞±ÊòØnohup.log ps -ef|grep pythonpsÂëΩ‰ª§Â∞ÜÊüê‰∏™ËøõÁ®ãÊòæÁ§∫Âá∫Êù• grepÂëΩ‰ª§ÊòØÊü•Êâæ ‰∏≠Èó¥ÁöÑ|ÊòØÁÆ°ÈÅìÂëΩ‰ª§ ÊòØÊåápsÂëΩ‰ª§‰∏égrepÂêåÊó∂ÊâßË°å PSÊòØLINUX‰∏ãÊúÄÂ∏∏Áî®ÁöÑ‰πüÊòØÈùûÂ∏∏Âº∫Â§ßÁöÑËøõÁ®ãÊü•ÁúãÂëΩ‰ª§ grepÂëΩ‰ª§ ÊòØÊü•ÊâæÔºå ÊòØ‰∏ÄÁßçÂº∫Â§ßÁöÑÊñáÊú¨ÊêúÁ¥¢Â∑•ÂÖ∑ÔºåÂÆÉËÉΩ ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºè ÊêúÁ¥¢ÊñáÊú¨ÔºåÂπ∂ÊääÂåπ ÈÖçÁöÑË°åÊâìÂç∞Âá∫Êù•„ÄÇ grepÂÖ®Áß∞ÊòØGlobal Regular Expression PrintÔºåË°®Á§∫ÂÖ®Â±ÄÊ≠£ÂàôË°®ËææÂºèÁâàÊú¨ÔºåÂÆÉÁöÑ‰ΩøÁî®ÊùÉÈôêÊòØÊâÄÊúâÁî®Êà∑„ÄÇ ‰ª•‰∏ãËøôÊù°ÂëΩ‰ª§ÊòØÊ£ÄÊü• java ËøõÁ®ãÊòØÂê¶Â≠òÂú®Ôºöps -ef |grep java Â≠óÊÆµÂê´‰πâÂ¶Ç‰∏ãÔºöUID PID PPID C STIME TTY TIME CMD zzw 14124 13991 0 00:38 pts/0 00:00:00 grep ‚Äîcolor=auto dae UID ÔºöÁ®ãÂ∫èË¢´ËØ• UID ÊâÄÊã•Êúâ PID ÔºöÂ∞±ÊòØËøô‰∏™Á®ãÂ∫èÁöÑ ID PPID ÔºöÂàôÊòØÂÖ∂‰∏äÁ∫ßÁà∂Á®ãÂ∫èÁöÑID C ÔºöCPU‰ΩøÁî®ÁöÑËµÑÊ∫êÁôæÂàÜÊØî STIME ÔºöÁ≥ªÁªüÂêØÂä®Êó∂Èó¥ TTY ÔºöÁôªÂÖ•ËÄÖÁöÑÁªàÁ´ØÊú∫‰ΩçÁΩÆ TIME Ôºö‰ΩøÁî®ÊéâÁöÑCPUÊó∂Èó¥„ÄÇ CMD ÔºöÊâÄ‰∏ãËææÁöÑÊòØ‰ªÄ‰πàÊåá‰ª§]]></content>
      <categories>
        <category>Êï∞ÊçÆÂàÜÊûê</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Êï∞ÊçÆÂàÜÊûê</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Networks]]></title>
    <url>%2F2019%2F05%2F12%2FDeel%20Learning%20ai_Convolutional%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[C4 : Convolutional Neural Networks(Âç∑ÁßØÁ•ûÁªèÁΩëÁªú)W1 :Convolutional Neural Networks(Âç∑ÁßØÁ•ûÁªèÁΩëÁªú)L1: Computer Vision Image classification Object detection Neural Style Transfer Problem : input big Á•ûÁªèÁΩëÁªúÁªìÊûÑÂ§çÊùÇÔºåÊï∞ÊçÆÈáèÁõ∏ÂØπËæÉÂ∞ëÔºåÂÆπÊòìÂá∫Áé∞ËøáÊãüÂêàÔºõ ÊâÄÈúÄÂÜÖÂ≠òÂíåËÆ°ÁÆóÈáèÂ∑®Â§ß„ÄÇ L2: Edge detection exampleÊàë‰ª¨‰πãÂâçÊèêÂà∞ËøáÔºåÁ•ûÁªèÁΩëÁªúÁî±ÊµÖÂ±ÇÂà∞Ê∑±Â±ÇÔºåÂàÜÂà´ÂèØ‰ª•Ê£ÄÊµãÂá∫ÂõæÁâáÁöÑËæπÁºòÁâπÂæÅ„ÄÅÂ±ÄÈÉ®ÁâπÂæÅÔºà‰æãÂ¶ÇÁúºÁùõ„ÄÅÈºªÂ≠êÁ≠âÔºâÔºåÂà∞ÊúÄÂêéÈù¢ÁöÑ‰∏ÄÂ±ÇÂ∞±ÂèØ‰ª•Ê†πÊçÆÂâçÈù¢Ê£ÄÊµãÁöÑÁâπÂæÅÊù•ËØÜÂà´Êï¥‰ΩìÈù¢ÈÉ®ËΩÆÂªì„ÄÇËøô‰∫õÂ∑•‰ΩúÈÉΩÊòØ‰æùÊâòÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊù•ÂÆûÁé∞ÁöÑ„ÄÇ Âç∑ÁßØËøêÁÆóÔºàConvolutional OperationÔºâÊòØÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÊúÄÂü∫Êú¨ÁöÑÁªÑÊàêÈÉ®ÂàÜ„ÄÇÊàë‰ª¨‰ª•ËæπÁºòÊ£ÄÊµã‰∏∫‰æãÔºåÊù•Ëß£ÈáäÂç∑ÁßØÊòØÊÄéÊ†∑ËøêÁÆóÁöÑ„ÄÇ Â∏∏ËßÅÁöÑËæπÁºòÊ£ÄÊµã ÂûÇÁõ¥ËæπÁºòÔºàVertical Edges) Âíå Ê∞¥Âπ≥ËæπÁºòÔºàhorizontal Edges) ËøôÂº†ÂõæÁöÑÊ†èÊùÜÂ∞±ÂØπÂ∫îÂûÇÁõ¥Á∫øÔºåÊ†èÊùÜÁöÑÊ∞¥Âπ≥Á∫øÊòØÊ∞¥Âπ≥ËæπÁºò„ÄÇ ÈÇ£‰πàÂõæÁâáÊòØÊÄé‰πàÊ£ÄÊµãËæπÁºòÁöÑÂë¢Ôºü ËøáÊª§Âô®Ôºöfilter Âú®Êï∞Â≠¶‰∏≠‚Äú‚ÄùÂ∞±ÊòØÂç∑ÁßØÁöÑÊ†áÂáÜÊ†áÂøóÔºå‰ΩÜÊòØÂú®Python‰∏≠ÔºåËøô‰∏™Ê†áËØÜÂ∏∏Â∏∏Ë¢´Áî®Êù•Ë°®Á§∫‰πòÊ≥ïÊàñËÄÖÂÖÉÁ¥†‰πòÊ≥ï„ÄÇ Output; 4 by 4 ÂÖ∑‰ΩìËøêÁÆóÔºö 1Ôºâ ‰∏∫‰∫ÜËÆ°ÁÆóÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåÂú®4√ó4Â∑¶‰∏äËßíÁöÑÈÇ£‰∏™ÂÖÉÁ¥†Ôºå‰ΩøÁî®3√ó3ÁöÑËøáÊª§Âô®ÔºåÂ∞ÜÂÖ∂Ë¶ÜÁõñÂú®ËæìÂÖ•ÂõæÂÉèÔºåÂ¶Ç‰∏ãÂõæÊâÄÁ§∫„ÄÇÁÑ∂ÂêéËøõË°åÂÖÉÁ¥†‰πòÊ≥ïÔºàelement-wise productsÔºâËøêÁÆó 2Ôºâ‰∏∫‰∫ÜÂºÑÊòéÁôΩÁ¨¨‰∫å‰∏™ÂÖÉÁ¥†ÊòØ‰ªÄ‰πàÔºå‰Ω†Ë¶ÅÊääËìùËâ≤ÁöÑÊñπÂùóÔºåÂêëÂè≥ÁßªÂä®‰∏ÄÊ≠•ÔºåÂÉèËøôÊ†∑ÔºåÊääËøô‰∫õÁªøËâ≤ÁöÑÊ†áËÆ∞ÂéªÊéâÔºö 6√ó6Áü©ÈòµÂíå3√ó3Áü©ÈòµËøõË°åÂç∑ÁßØËøêÁÆóÂæóÂà∞4√ó4Áü©Èòµ„ÄÇËøô‰∫õÂõæÁâáÂíåËøáÊª§Âô®ÊòØ‰∏çÂêåÁª¥Â∫¶ÁöÑÁü©ÈòµÔºå‰ΩÜÂ∑¶ËæπÁü©ÈòµÂÆπÊòìË¢´ÁêÜËß£‰∏∫‰∏ÄÂº†ÂõæÁâáÔºå‰∏≠Èó¥ÁöÑËøô‰∏™Ë¢´ÁêÜËß£‰∏∫ËøáÊª§Âô®ÔºåÂè≥ËæπÁöÑÂõæÁâáÊàë‰ª¨ÂèØ‰ª•ÁêÜËß£‰∏∫Âè¶‰∏ÄÂº†ÂõæÁâá„ÄÇËøô‰∏™Â∞±ÊòØÂûÇÁõ¥ËæπÁºòÊ£ÄÊµãÂô®„ÄÇ ‰∏æ‰æãËØ¥ÊòéÔºö Vertical edge detection ËøôÈáåÂú®ÁªìÊûúÂèØËÉΩÊúâÁÇπ‰∏çÂØπÂ§¥ÔºåÊ£ÄÊµãÂà∞ÁöÑËæπÁºòÂ§™Á≤ó‰∫ÜÔºå‰∏ªË¶ÅÊòØÂõæÁâáÂ§™Â∞è‰∫ÜÔºå Âç∑ÁßØÊìç‰ΩúAPI Âú® Python ‰∏≠ÔºåÂç∑ÁßØÁî®conv_forward()Ë°®Á§∫Ôºõ Âú® Tensorflow ‰∏≠ÔºåÂç∑ÁßØÁî®tf.nn.conv2d()Ë°®Á§∫Ôºõ Âú® keras ‰∏≠ÔºåÂç∑ÁßØÁî®Conv2D()Ë°®Á§∫„ÄÇ L3: Edge Detection Example È¢úËâ≤Áî±ÊöóÂà∞‰∫ÆÔºåËøòÊòØ‰∫ÆÂà∞Êöó ËøôÁßçÊª§Ê≥¢Âô®ÂèØ‰ª•Âå∫ÂàÜÊòéÊöóÂèòÂåñÔºåÂèñÁªùÂØπÂÄºÊ≤°ÊúâÂå∫Âà´‰∫Ü Ê∞¥Âπ≥ËæπÁºò ‰∏äËæπÁõ∏ÂØπËæÉ‰∫ÆÔºåËÄå‰∏ãÊñπÁõ∏ÂØπËæÉÊöó Â§çÊùÇÊ†óÂ≠ê ËøôÂùóÂå∫ÂüüÂ∑¶Ëæπ‰∏§ÂàóÊòØÊ≠£ËæπÔºåÂè≥Ëæπ‰∏ÄÂàóÊòØË¥üËæπÔºåÊ≠£ËæπÂíåË¥üËæπÁöÑÂÄºÂä†Âú®‰∏ÄËµ∑ÂæóÂà∞‰∫Ü‰∏Ä‰∏™‰∏≠Èó¥ÂÄº„ÄÇ‰ΩÜÂÅáÂ¶ÇËøô‰∏™‰∏Ä‰∏™ÈùûÂ∏∏Â§ßÁöÑ1000√ó1000ÁöÑÁ±ª‰ººËøôÊ†∑Ê£ãÁõòÈ£éÊ†ºÁöÑÂ§ßÂõæÔºåÂ∞±‰∏ç‰ºöÂá∫Áé∞Ëøô‰∫õ‰∫ÆÂ∫¶‰∏∫10ÁöÑËøáÊ∏°Â∏¶‰∫ÜÔºåÂõ†‰∏∫ÂõæÁâáÂ∞∫ÂØ∏ÂæàÂ§ßÔºåËøô‰∫õ‰∏≠Èó¥ÂÄºÂ∞±‰ºöÂèòÂæóÈùûÂ∏∏Â∞è„ÄÇ filter sobelËøáÊª§Âô®Ôºå‰ºòÁÇπÂú®‰∫éÂ¢ûÂä†‰∫Ü‰∏≠Èó¥‰∏ÄË°åÂÖÉÁ¥†ÁöÑÊùÉÈáçÔºåËøô‰ΩøÂæóÁªìÊûúÁöÑÈ≤ÅÊ£íÊÄß‰ºöÊõ¥È´ò‰∏Ä‰∫õ„ÄÇ charrËøáÊª§Âô®ÔºåÂÆÉÊúâÁùÄÂíå‰πãÂâçÂÆåÂÖ®‰∏çÂêåÁöÑÁâπÊÄßÔºåÂÆûÈôÖ‰∏ä‰πüÊòØ‰∏ÄÁßçÂûÇÁõ¥ËæπÁºòÊ£ÄÊµãÔºåÂ¶ÇÊûú‰Ω†Â∞ÜÂÖ∂ÁøªËΩ¨90Â∫¶Ôºå‰Ω†Â∞±ËÉΩÂæóÂà∞ÂØπÂ∫îÊ∞¥Âπ≥ËæπÁºòÊ£ÄÊµã„ÄÇ Â≠¶‰π†ÁöÑÂÖ∂‰∏≠‰∏Ä‰ª∂‰∫ãÂ∞±ÊòØÂΩì‰Ω†ÁúüÊ≠£ÊÉ≥ÂéªÊ£ÄÊµãÂá∫Â§çÊùÇÂõæÂÉèÁöÑËæπÁºòÔºå‰Ω†‰∏ç‰∏ÄÂÆöË¶ÅÂéª‰ΩøÁî®ÈÇ£‰∫õÁ†îÁ©∂ËÄÖ‰ª¨ÊâÄÈÄâÊã©ÁöÑËøô‰πù‰∏™Êï∞Â≠óÔºå‰ΩÜ‰Ω†ÂèØ‰ª•‰ªé‰∏≠Ëé∑ÁõäÂå™ÊµÖ„ÄÇÊääËøôÁü©Èòµ‰∏≠ÁöÑ9‰∏™Êï∞Â≠óÂΩìÊàê9‰∏™ÂèÇÊï∞ÔºåÂπ∂‰∏îÂú®‰πãÂêé‰Ω†ÂèØ‰ª•Â≠¶‰π†‰ΩøÁî®ÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÔºåÂÖ∂ÁõÆÊ†áÂ∞±ÊòØÂéªÁêÜËß£Ëøô9‰∏™ÂèÇÊï∞„ÄÇ ËøôÊ†∑ÂèØËÉΩÂæóÂà∞‰∏Ä‰∏™Âá∫Ëâ≤ÁöÑËæπÁºòÊ£ÄÊµã Áõ∏ÊØîËøôÁßçÂçïÁ∫ØÁöÑÂûÇÁõ¥ËæπÁºòÂíåÊ∞¥Âπ≥ËæπÁºòÔºåÂÆÉÂèØ‰ª•Ê£ÄÊµãÂá∫45¬∞Êàñ70¬∞Êàñ73¬∞ÔºåÁîöËá≥ÊòØ‰ªª‰ΩïËßíÂ∫¶ÁöÑËæπÁºò„ÄÇÊâÄ‰ª•Â∞ÜÁü©ÈòµÁöÑÊâÄÊúâÊï∞Â≠óÈÉΩËÆæÁΩÆ‰∏∫ÂèÇÊï∞ÔºåÈÄöËøáÊï∞ÊçÆÂèçÈ¶àÔºåËÆ©Á•ûÁªèÁΩëÁªúËá™Âä®ÂéªÂ≠¶‰π†ÂÆÉ‰ª¨ÔºåÊàë‰ª¨‰ºöÂèëÁé∞Á•ûÁªèÁΩëÁªúÂèØ‰ª•Â≠¶‰π†‰∏Ä‰∫õ‰ΩéÁ∫ßÁöÑÁâπÂæÅÔºå‰æãÂ¶ÇËøô‰∫õËæπÁºòÁöÑÁâπÂæÅ„ÄÇ ‰∏çÁÆ°ÊòØÂûÇÁõ¥ÁöÑËæπÁºòÔºåÊ∞¥Âπ≥ÁöÑËæπÁºòÔºåËøòÊúâÂÖ∂‰ªñÂ•áÊÄ™ËßíÂ∫¶ÁöÑËæπÁºòÔºåÁîöËá≥ÊòØÂÖ∂ÂÆÉÁöÑËøûÂêçÂ≠óÈÉΩÊ≤°ÊúâÁöÑËøáÊª§Âô®„ÄÇ PaddingÊåâÁÖßÊàë‰ª¨‰∏äÈù¢ËÆ≤ÁöÑÂõæÁâáÂç∑ÁßØÔºåÂ¶ÇÊûúÂéüÂßãÂõæÁâáÂ∞∫ÂØ∏‰∏∫$n x n$ÔºåfilterÂ∞∫ÂØ∏‰∏∫$f x f$ÔºåÂàôÂç∑ÁßØÂêéÁöÑÂõæÁâáÂ∞∫ÂØ∏‰∏∫$(n-f+1) x (n-f+1)$ÔºåÊ≥®ÊÑèf‰∏ÄËà¨‰∏∫Â•áÊï∞„ÄÇËøôÊ†∑‰ºöÂ∏¶Êù•‰∏§‰∏™ÈóÆÈ¢òÔºö Âç∑ÁßØËøêÁÆóÂêéÔºåËæìÂá∫ÂõæÁâáÂ∞∫ÂØ∏Áº©Â∞è ÂéüÂßãÂõæÁâáËæπÁºò‰ø°ÊÅØÂØπËæìÂá∫Ë¥°ÁåÆÂæóÂ∞ëÔºåËæìÂá∫ÂõæÁâá‰∏¢Â§±ËæπÁºò‰ø°ÊÅØ ËæπÁºòÂÉèÁ¥†ÁÇπÂè™Ë¢´‰∏Ä‰∏™ËæìÂá∫ÊâÄËß¶Á¢∞ÊàñËÄÖ‰ΩøÁî®Ôºå ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÂèØ‰ª•Âú®ËøõË°åÂç∑ÁßØÊìç‰ΩúÂâçÔºåÂØπÂéüÂßãÂõæÁâáÂú®ËæπÁïå‰∏äËøõË°åÂ°´ÂÖÖÔºàPaddingÔºâÔºå‰ª•Â¢ûÂä†Áü©ÈòµÁöÑÂ§ßÂ∞è„ÄÇÈÄöÂ∏∏Â∞Ü 0 ‰Ωú‰∏∫Â°´ÂÖÖÂÄº„ÄÇ ÁªèËøápadding‰πãÂêéÔºåÂ°´ÂÖÖp,ÂéüÂßãÂõæÁâáÂ∞∫ÂØ∏‰∏∫$(n+2p) x (n+2p)$ÔºåfilterÂ∞∫ÂØ∏‰∏∫$f x f$ÔºåÂàôÂç∑ÁßØÂêéÁöÑÂõæÁâáÂ∞∫ÂØ∏‰∏∫$(n+2p-f+1) x (n+2p-f+1)$„ÄÇËã•Ë¶Å‰øùËØÅÂç∑ÁßØÂâçÂêéÂõæÁâáÂ∞∫ÂØ∏‰∏çÂèòÔºåÂàôpÂ∫îÊª°Ë∂≥Ôºö$ p=(f-1)/2$,fÈÄöÂ∏∏ÊòØÂ•áÊï∞ÔºåÂ¶ÇÊûúÊòØÂÅ∂Êï∞ÔºåÈÄ†Êàê‰∏çÂØπÁß∞Â°´ÂÖÖÔºåÁ¨¨‰∫å‰∏™ÂéüÂõ†ÊòØÂΩì‰Ω†Êúâ‰∏Ä‰∏™Â•áÊï∞Áª¥ËøáÊª§Âô®ÔºåÊØîÂ¶Ç3√ó3ÊàñËÄÖ5√ó5ÁöÑÔºåÂÆÉÂ∞±Êúâ‰∏Ä‰∏™‰∏≠ÂøÉÁÇπ„ÄÇÊúâÊó∂Âú®ËÆ°ÁÆóÊú∫ËßÜËßâÈáåÔºåÂ¶ÇÊûúÊúâ‰∏Ä‰∏™‰∏≠ÂøÉÂÉèÁ¥†ÁÇπ‰ºöÊõ¥Êñπ‰æøÔºå‰æø‰∫éÊåáÂá∫ËøáÊª§Âô®ÁöÑ‰ΩçÁΩÆ p=0,Valid convolution p=((f-1))/2,Same convolution L05: Strided convolutionÔºàÂç∑ÁßØÊ≠•ÈïøÔºâStrideË°®Á§∫filterÂú®ÂéüÂõæÁâá‰∏≠Ê∞¥Âπ≥ÊñπÂêëÂíåÂûÇÁõ¥ÊñπÂêëÊØèÊ¨°ÁöÑÊ≠•ËøõÈïøÂ∫¶„ÄÇ‰πãÂâçÊàë‰ª¨ÈªòËÆ§stride=1„ÄÇËã•stride=2ÔºåÂàôË°®Á§∫filterÊØèÊ¨°Ê≠•ËøõÈïøÂ∫¶‰∏∫2ÔºåÂç≥Èöî‰∏ÄÁÇπÁßªÂä®‰∏ÄÊ¨°„ÄÇ Êàë‰ª¨Áî®sË°®Á§∫strideÈïøÂ∫¶ÔºåpË°®Á§∫paddingÈïøÂ∫¶ÔºåÂ¶ÇÊûúÂéüÂßãÂõæÁâáÂ∞∫ÂØ∏‰∏∫n x nÔºåfilterÂ∞∫ÂØ∏‰∏∫f x fÔºåÂàôÂç∑ÁßØÂêéÁöÑÂõæÁâáÂ∞∫ÂØ∏‰∏∫Ôºö \left\lfloor\frac{n+2 p-f}{s}+1\right\rfloor X\left\lfloor\frac{n+2 p-f}{s}+1\right\rfloorÂêë‰∏ãÂèñÊï¥ ÁõÆÂâç‰∏∫Ê≠¢Êàë‰ª¨Â≠¶‰π†ÁöÑ‚ÄúÂç∑ÁßØ‚ÄùÂÆûÈôÖ‰∏äË¢´Áß∞‰∏∫‰∫íÁõ∏ÂÖ≥Ôºàcross-correlationÔºâÔºåËÄåÈùûÊï∞Â≠¶ÊÑè‰πâ‰∏äÁöÑÂç∑ÁßØ„ÄÇÁúüÊ≠£ÁöÑÂç∑ÁßØÊìç‰ΩúÂú®ÂÅöÂÖÉÁ¥†‰πòÁßØÊ±ÇÂíå‰πãÂâçÔºåË¶ÅÂ∞ÜÊª§Ê≥¢Âô®Ê≤øÊ∞¥Âπ≥ÂíåÂûÇÁõ¥ËΩ¥ÁøªËΩ¨ÔºàÁõ∏ÂΩì‰∫éÊóãËΩ¨ 180 Â∫¶Ôºâ„ÄÇÂõ†‰∏∫ËøôÁßçÁøªËΩ¨ÂØπ‰∏ÄËà¨‰∏∫Ê∞¥Âπ≥ÊàñÂûÇÁõ¥ÂØπÁß∞ÁöÑÊª§Ê≥¢Âô®ÂΩ±Âìç‰∏çÂ§ßÔºåÊåâÁÖßÊú∫Âô®Â≠¶‰π†ÁöÑÊÉØ‰æãÔºåÊàë‰ª¨ÈÄöÂ∏∏‰∏çËøõË°åÁøªËΩ¨Êìç‰ΩúÔºåÂú®ÁÆÄÂåñ‰ª£Á†ÅÁöÑÂêåÊó∂‰ΩøÁ•ûÁªèÁΩëÁªúËÉΩÂ§üÊ≠£Â∏∏Â∑•‰Ωú„ÄÇ ‰∫íÁõ∏ÂÖ≥ÔºöËøáÊª§Âô®Ê≤øÊ∞¥Âπ≥ÂíåÂûÇÁõ¥ËΩ¥ÁøªËΩ¨ÔºåÂÖÉÁ¥†Áõ∏‰πòÊù•ËÆ°ÁÆóÔºåËøô‰∫õËßÜÈ¢ë‰∏≠ÂÆö‰πâÂç∑ÁßØËøêÁÆóÊó∂ÔºåÊàë‰ª¨Ë∑≥Ëøá‰∫ÜËøô‰∏™ÈïúÂÉèÊìç‰Ωú„ÄÇÔºà‰∏çËøõË°åÁøªËΩ¨Êìç‰ΩúÔºâÂè´ÂÅöÂç∑ÁßØÊìç‰Ωú L06: Convolution over volumes(‰∏âÁª¥Âç∑ÁßØ) Âç∑ÁßØËøêÁÆó ËøáÁ®ãÊòØÂ∞ÜÊØè‰∏™ÂçïÈÄöÈÅìÔºàRÔºåGÔºåBÔºâ‰∏éÂØπÂ∫îÁöÑfilterËøõË°åÂç∑ÁßØËøêÁÆóÊ±ÇÂíåÔºåÁÑ∂ÂêéÂÜçÂ∞Ü3ÈÄöÈÅìÁöÑÂíåÁõ∏Âä†ÔºåÂæóÂà∞ËæìÂá∫ÂõæÁâáÁöÑ‰∏Ä‰∏™ÂÉèÁ¥†ÂÄº„ÄÇ ‰∏çÂêåÈÄöÈÅìÁöÑÊª§Ê≥¢ÁÆóÂ≠êÂèØ‰ª•‰∏çÁõ∏Âêå„ÄÇ‰æãÂ¶ÇRÈÄöÈÅìfilterÂÆûÁé∞ÂûÇÁõ¥ËæπÁºòÊ£ÄÊµãÔºåGÂíåBÈÄöÈÅì‰∏çËøõË°åËæπÁºòÊ£ÄÊµãÔºåÂÖ®ÈÉ®ÁΩÆÈõ∂ÔºåÊàñËÄÖÂ∞ÜRÔºåGÔºåB‰∏âÈÄöÈÅìfilterÂÖ®ÈÉ®ËÆæÁΩÆ‰∏∫Ê∞¥Âπ≥ËæπÁºòÊ£ÄÊµã„ÄÇ ‰∏∫‰∫ÜËøõË°åÂ§ö‰∏™Âç∑ÁßØËøêÁÆóÔºåÂÆûÁé∞Êõ¥Â§öËæπÁºòÊ£ÄÊµãÔºåÂèØ‰ª•Â¢ûÂä†Êõ¥Â§öÁöÑÊª§Ê≥¢Âô®ÁªÑ„ÄÇ‰æãÂ¶ÇËÆæÁΩÆÁ¨¨‰∏Ä‰∏™Êª§Ê≥¢Âô®ÁªÑÂÆûÁé∞ÂûÇÁõ¥ËæπÁºòÊ£ÄÊµãÔºåÁ¨¨‰∫å‰∏™Êª§Ê≥¢Âô®ÁªÑÂÆûÁé∞Ê∞¥Âπ≥ËæπÁºòÊ£ÄÊµã„ÄÇËøôÊ†∑Ôºå‰∏çÂêåÊª§Ê≥¢Âô®ÁªÑÂç∑ÁßØÂæóÂà∞‰∏çÂêåÁöÑËæìÂá∫Ôºå‰∏™Êï∞Áî±Êª§Ê≥¢Âô®ÁªÑÂÜ≥ÂÆö„ÄÇ ‰∏∫‰∫ÜËøõË°åÂ§ö‰∏™Âç∑ÁßØËøêÁÆóÔºåÂÆûÁé∞Êõ¥Â§öËæπÁºòÊ£ÄÊµãÔºåÂèØ‰ª•Â¢ûÂä†Êõ¥Â§öÁöÑÊª§Ê≥¢Âô®ÁªÑ„ÄÇ‰æãÂ¶ÇËÆæÁΩÆÁ¨¨‰∏Ä‰∏™Êª§Ê≥¢Âô®ÁªÑÂÆûÁé∞ÂûÇÁõ¥ËæπÁºòÊ£ÄÊµãÔºåÁ¨¨‰∫å‰∏™Êª§Ê≥¢Âô®ÁªÑÂÆûÁé∞Ê∞¥Âπ≥ËæπÁºòÊ£ÄÊµã„ÄÇËøôÊ†∑Ôºå‰∏çÂêåÊª§Ê≥¢Âô®ÁªÑÂç∑ÁßØÂæóÂà∞‰∏çÂêåÁöÑËæìÂá∫Ôºå‰∏™Êï∞Áî±Êª§Ê≥¢Âô®ÁªÑÂÜ≥ÂÆö„ÄÇ Ëã•ËæìÂÖ•ÂõæÁâáÁöÑÂ∞∫ÂØ∏‰∏∫n x n x ncÔºånc: ÈÄöÈÅìÊï∞ÁõÆÔºåfilterÂ∞∫ÂØ∏‰∏∫f x f x ncÔºåÂàôÂç∑ÁßØÂêéÁöÑÂõæÁâáÂ∞∫ÂØ∏‰∏∫(n-f+1) x (n-f+1) x nc‚Ä≤„ÄÇÂÖ∂‰∏≠Ôºånc‰∏∫ÂõæÁâáÈÄöÈÅìÊï∞ÁõÆÔºånc‚Ä≤‰∏∫Êª§Ê≥¢Âô®ÁªÑ‰∏™Êï∞„ÄÇ L7 : One layer of a convolution network (ÂçïÂ±ÇÁ•ûÁªèÁΩëÁªú) CNNÂçïÂ±ÇÁöÑÊâÄ‰ª•Ê†áËÆ∞Á¨¶Âè∑ÔºåËÆæÂ±ÇÊï∞$l$, \begin{array}{l}{f^{[l]}=\text { filter size }} \\ {p^{[l]}=\text { padding }} \\ {g^{[l]}=\text { stride }} \\ {n_{c}^{[l]}=\text { number of filters }}\end{array} \begin{array}{c}{n_{H}^{[l]}=\left\lfloor\frac{n_{H}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right\rfloor} \\ { n_{W}^{[l]}=\left\lfloor\frac{n_{W}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right\rfloor}\end{array}Â¶ÇÊûú$m$‰∏™Ê†∑Êú¨ÔºåËøõË°åÂêëÈáèÂåñËøêÁÆóÔºåÁõ∏Â∫îÁöÑËæìÂá∫Áª¥Â∫¶Ôºå‰∏∫ \mathrm{m} \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}L8 : A simple convolution network exampleÔºàÁÆÄÂçïÂç∑ÁßØÁΩëÁªúÁ§∫‰æãÔºâ ‰∏ÄËà¨ËÄåË®ÄÔºåÂõæÁâáÁöÑheight $n^{[l]}_{H}$Âíåwidth $n^{[l]}_W$ÈöèÁùÄÂ±ÇÊï∞ÁöÑÂ¢ûÂä†ÈÄêÊ∏êÈôç‰ΩéÔºå‰ΩÜchannel $n^{[l]}_C$ÈÄêÊ∏êÂ¢ûÂä†„ÄÇ CNNÊúâ‰∏âÁßçÁ±ªÂûãÁöÑlayerÔºö ConvolutionÂ±ÇÔºàCONVÔºâ PoolingÂ±ÇÔºàPOOLÔºâ Fully connectedÂ±ÇÔºàFCÔºâ L9: Pooling layers(Ê±†ÂåñÂ±Ç)Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÈô§‰∫ÜÂç∑ÁßØÂ±ÇÔºåËøòÊúâÊ±†ÂåñÂ±ÇÊù•Áº©ÂáèÊ®°ÂûãÁöÑÂ§ßÂ∞èÔºåÊèêÈ´òËøêÁÆóÈÄüÂ∫¶ÂíåÈ≤ÅÊ£íÊÄß Ê±†ÁöÑÁ±ªÂûãÊúâmax pooling(ÊúÄÂ§ßÊ±†Âåñ) ËøôÈáåÊ≠•ÂπÖÊòØs=2Ôºåfilter = 2*2ÊòØÊúÄÂ§ßÊ±†ÂåñÁöÑË∂ÖÂèÇÊï∞,Â¶ÇÊûúÊòØ‰∏âÁª¥ÔºåÂàôÂçïÁã¨Âú®ÊØè‰∏™ÈÄöÈÅìÊâßË°åÊúÄÂ§ßÊ±†ÂåñÊìç‰Ωú ÂÖ≥‰∫émax poolingÁöÑÁõ¥ËßâËß£ÈáäÔºö ÂÖÉÁ¥†ËæÉÂ§ßÁöÑÂÄºÔºåÂèØËÉΩÊòØÂç∑ÁßØËøáÁ®ã‰∏≠ÊèêÂèñÂà∞ÁöÑÊüê‰∫õÁâπÂæÅÔºàÊØîÂ¶ÇËæπÁïåÔºâÔºåËÄåmax poolingÂàôÂú®ÂéãÁº©‰∫ÜÁü©ÈòµÂ§ßÂ∞èÁöÑÊÉÖÂÜµ‰∏ãÔºå‰øùÁïôÊØè‰∏™ÂàÜÂå∫ÂÜÖÊúÄÂ§ßÁöÑËæìÂá∫ÔºåÂç≥‰øùÁïô‰∫ÜÊèêÂèñÁöÑÁâπÂæÅ„ÄÇ‰ΩÜÁêÜËÆ∫‰∏äËøòÊ≤°ÊúâËØÅÊòémax poolingÁöÑÂéüÁêÜÔºåmax poolingÂ∫îÁî®ÁöÑÂéüÂõ†ÊòØÂú®ÂÆûË∑µ‰∏≠ÊïàÊûúÂæàÂ•Ω„ÄÇ Pooling layer: Average pooling ‰ΩÜÊòØÊúÄÂ§ßÊ±†ÂåñÊõ¥Â•ΩÁî® summary : ËæìÂÖ•$n_Hn_Wn_C$,Â¶ÇÊûúÊ≤°Êúâpadding,ËæìÂá∫$(n_h-f)/s+1(n_w-f)/s+1n_c$ L10: Convolutional neural network example (Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÂÆû‰æã)ÂÅö‰∏Ä‰∏™ËØÜÂà´Êï∞Â≠óÁöÑCNNÁΩëÁªú LeNet-5Êû∂ÊûÑÂ¶Ç‰∏ãÔºö ÈÄöÂ∏∏Conv LayerÂíåPooling LayerÂêàÂú®‰∏ÄËµ∑ÁÆó‰∏Ä‰∏™layerÔºåÂõ†‰∏∫pooling layerÂπ∂Ê≤°ÊúâÂèÇÊï∞ËÆ≠ÁªÉ Â∏∏ËßÅÁöÑÁªìÊûÑÔºöConv ==&gt; Pool ==&gt; Conv ==&gt; Pool ==&gt; FC ==&gt; FC ==&gt; softmax ÊúÄÁªàËøò‰ºöÁî®FCÂ±ÇÔºàÂÖ®ËøûÊé•Â±ÇÔºâÔºå‰∏é‰∏ÄËà¨NNÁöÑÂ§ÑÁêÜ‰∏ÄÊ†∑ÔºõÂπ∂Âú®ËæìÂá∫Â±ÇÔºåÂ∫îÁî®softmaxÂæóÂà∞10‰∏™Êï∞Â≠óÁöÑÊ¶ÇÁéá„ÄÇ Âú®Êï¥‰∏™ÁΩëÁªú‰∏≠ÔºåHeightÂíåWidthÊòØÈÄêÊ∏êÈÄíÂáèÁöÑÔºå‰ΩÜchannelÂíåfilterÊòØÈÄíÂ¢ûÁöÑ„ÄÇ ÂÖ≥‰∫éCNNÂ¶Ç‰ΩïÈÄâÊã©Ë∂ÖÂèÇÔºöÂèØ‰ª•ÂèÇËÄÉËÆ∫ÊñáÁöÑÁªèÈ™å„ÄÇ Activation shape Activation Size #parameters Input: (32, 32, 3) 3072 0 CONV1(f=5, s=1) (28, 28, 6) 4704 156 (=556+6) POOL1 (14, 14, 6) 1176 0 CONV2(f=5, s=1) (10, 10, 16) 1600 416 (=5516+16) POOL2 (5, 5, 16) 400 0 FC3 (120, 1) 120 48120 (=120*400+120) FC4 (84, 1) 84 10164 (=84*120+84) Softmax (10, 1) 10 850 (=10*84+10) L11 Why convolution ÂèÇÊï∞ÂÖ±‰∫´Ôºàparameter sharing) Â¶ÇÊûúÁî®FCÁöÑËØùÔºåÂèÇÊï∞ÁàÜÁÇ∏ÂïäÔºÅÂ¶ÇÊûúconv layer Â∞±ÈúÄË¶ÅfilterÊ£ÄÊµãÂô®ÔºåËøô‰∏™ÂèÇÊï∞Â∞±Â∞ë‰∫ÜÔºåËøòÂèÇÊï∞ÂÖ±‰∫´ Á®ÄÁñèËøûÊé•(sparsity of connection) ËæìÂá∫‰∏≠ÁöÑÊØè‰∏™ÂçïÂÖÉ‰ªÖÂíåËæìÂÖ•ÁöÑ‰∏Ä‰∏™Â∞èÂàÜÂå∫Áõ∏ÂÖ≥ÔºåÊØîÂ¶ÇËæìÂá∫ÁöÑÂ∑¶‰∏äËßíÁöÑÂÉèÁ¥†‰ªÖ‰ªÖÁî±ËæìÂÖ•Â∑¶‰∏äËßíÁöÑ9‰∏™ÂÉèÁ¥†ÂÜ≥ÂÆöÔºàÂÅáËÆæfilterÂ§ßÂ∞èÊòØ3*3ÔºâÔºåËÄåÂÖ∂‰ªñËæìÂÖ•ÈÉΩ‰∏ç‰ºöÂΩ±Âìç„ÄÇ summary1. Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÁöÑÂü∫Êú¨ÊûÑÈÄ†ÂíåËÆ°ÁÆóËøáÁ®ã 2. Â¶Ç‰ΩïÊï¥ÂêàËøô‰∫õÊ®°Âûã 3. Âì™‰∫õË∂ÖÂèÇÊï∞ 4. ‰∏∫‰ªÄ‰πà‰ΩøÁî®Âç∑ÁßØ W2 : Deep convolutional models: case studies(Ê∑±Â∫¶Âç∑ÁßØÁΩëÁªúÔºöÂÆû‰æãÊé¢Á©∂)L1 : Why look at case studies?(‰∏∫‰ªÄ‰πàË¶ÅËøõË°åÂÆû‰æãÊé¢Á©∂Ôºü)Êú¨ÊñáÂ∞Ü‰∏ªË¶Å‰ªãÁªçÂá†‰∏™ÂÖ∏ÂûãÁöÑCNNÊ°à‰æã„ÄÇÈÄöËøáÂØπÂÖ∑‰ΩìCNNÊ®°ÂûãÂèäÊ°à‰æãÁöÑÁ†îÁ©∂ÔºåÊù•Â∏ÆÂä©Êàë‰ª¨ÁêÜËß£Áü•ËØÜÂπ∂ËÆ≠ÁªÉÂÆûÈôÖÁöÑÊ®°Âûã„ÄÇ ÂÖ∏ÂûãÁöÑCNNÊ®°ÂûãÂåÖÊã¨Ôºö LeNet-5 AlexNet VGG Ëøò‰ºö‰ªãÁªçResidual NetworkÔºàResNetÔºâ„ÄÇÂÖ∂ÁâπÁÇπÊòØÂèØ‰ª•ÊûÑÂª∫ÂæàÊ∑±ÂæàÊ∑±ÁöÑÁ•ûÁªèÁΩëÁªúÔºàÁõÆÂâçÊúÄÊ∑±ÁöÑÂ•ΩÂÉèÊúâ152Â±ÇÔºâ„ÄÇËøò‰ºö‰ªãÁªçInception Neural Network L2 : Classic networks(ÁªèÂÖ∏ÁΩëÁªú)1. LeNet-5LeNet-5ÊòØÈíàÂØπÁÅ∞Â∫¶ÂõæÁâáËÆ≠ÁªÉÁöÑÔºå‰ΩøÁî®6‰∏™5√ó5ÁöÑËøáÊª§Âô®ÔºåÊ≠•ÂπÖ‰∏∫1„ÄÇÁî±‰∫é‰ΩøÁî®‰∫Ü6‰∏™ËøáÊª§Âô®ÔºåÊ≠•ÂπÖ‰∏∫1Ôºåpadding‰∏∫0ÔºåËæìÂá∫ÁªìÊûú‰∏∫28√ó28√ó6ÔºåÂõæÂÉèÂ∞∫ÂØ∏‰ªé32√ó32Áº©Â∞èÂà∞28√ó28„ÄÇÁÑ∂ÂêéËøõË°åÊ±†ÂåñÊìç‰ΩúÔºåÂú®ËøôÁØáËÆ∫ÊñáÂÜôÊàêÁöÑÈÇ£‰∏™Âπ¥‰ª£Ôºå‰∫∫‰ª¨Êõ¥ÂñúÊ¨¢‰ΩøÁî®Âπ≥ÂùáÊ±†ÂåñÔºåËÄåÁé∞Âú®Êàë‰ª¨ÂèØËÉΩÁî®ÊúÄÂ§ßÊ±†ÂåñÊõ¥Â§ö‰∏Ä‰∫õ„ÄÇÂú®Ëøô‰∏™‰æãÂ≠ê‰∏≠ÔºåÊàë‰ª¨ËøõË°åÂπ≥ÂùáÊ±†ÂåñÔºåËøáÊª§Âô®ÁöÑÂÆΩÂ∫¶‰∏∫2ÔºåÊ≠•ÂπÖ‰∏∫2ÔºåÂõæÂÉèÁöÑÂ∞∫ÂØ∏ÔºåÈ´òÂ∫¶ÂíåÂÆΩÂ∫¶ÈÉΩÁº©Â∞è‰∫Ü2ÂÄçÔºåËæìÂá∫ÁªìÊûúÊòØ‰∏Ä‰∏™14√ó14√ó6ÁöÑÂõæÂÉè„ÄÇÊàëËßâÂæóËøôÂº†ÂõæÁâáÂ∫îËØ•‰∏çÊòØÂÆåÂÖ®ÊåâÁÖßÊØî‰æãÁªòÂà∂ÁöÑÔºåÂ¶ÇÊûú‰∏•Ê†ºÊåâÁÖßÊØî‰æãÁªòÂà∂ÔºåÊñ∞ÂõæÂÉèÁöÑÂ∞∫ÂØ∏Â∫îËØ•ÂàöÂ•ΩÊòØÂéüÂõæÂÉèÁöÑ‰∏ÄÂçä„ÄÇ ËØ•LeNetÊ®°ÂûãÊÄªÂÖ±ÂåÖÂê´‰∫ÜÂ§ßÁ∫¶6‰∏á‰∏™ÂèÇÊï∞„ÄÇÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåÂΩìÊó∂Yann LeCunÊèêÂá∫ÁöÑLeNet-5Ê®°ÂûãÊ±†ÂåñÂ±Ç‰ΩøÁî®ÁöÑÊòØaverage poolÔºåËÄå‰∏îÂêÑÂ±ÇÊøÄÊ¥ªÂáΩÊï∞‰∏ÄËà¨ÊòØSigmoidÂíåtanh„ÄÇÁé∞Âú®ÔºåÊàë‰ª¨ÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅÔºåÂÅöÂá∫ÊîπËøõÔºå‰ΩøÁî®max poolÂíåÊøÄÊ¥ªÂáΩÊï∞ReLU„ÄÇ 1. AlexNetAlexNetÊ®°ÂûãÊòØÁî±Alex Krizhevsky„ÄÅIlya SutskeverÂíåGeoffrey HintonÂÖ±ÂêåÊèêÂá∫ÁöÑÔºåÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö AlexNetÈ¶ñÂÖàÁî®‰∏ÄÂº†227√ó227√ó3ÁöÑÂõæÁâá‰Ωú‰∏∫ËæìÂÖ•ÔºåÂÆûÈôÖ‰∏äÂéüÊñá‰∏≠‰ΩøÁî®ÁöÑÂõæÂÉèÊòØ224√ó224√ó3Ôºå‰ΩÜÊòØÂ¶ÇÊûú‰Ω†Â∞ùËØïÂéªÊé®ÂØº‰∏Ä‰∏ãÔºå‰Ω†‰ºöÂèëÁé∞227√ó227Ëøô‰∏™Â∞∫ÂØ∏Êõ¥Â•Ω‰∏Ä‰∫õ„ÄÇÁ¨¨‰∏ÄÂ±ÇÊàë‰ª¨‰ΩøÁî®96‰∏™11√ó11ÁöÑËøáÊª§Âô®ÔºåÊ≠•ÂπÖ‰∏∫4ÔºåÁî±‰∫éÊ≠•ÂπÖÊòØ4ÔºåÂõ†Ê≠§Â∞∫ÂØ∏Áº©Â∞èÂà∞55√ó55ÔºåÁº©Â∞è‰∫Ü4ÂÄçÂ∑¶Âè≥„ÄÇÁÑ∂ÂêéÁî®‰∏Ä‰∏™3√ó3ÁöÑËøáÊª§Âô®ÊûÑÂª∫ÊúÄÂ§ßÊ±†ÂåñÂ±Ç,f=3ÔºåÊ≠•ÂπÖ‰∏∫2ÔºåÂç∑ÁßØÂ±ÇÂ∞∫ÂØ∏Áº©Â∞è‰∏∫27√ó27√ó96„ÄÇÊé•ÁùÄÂÜçÊâßË°å‰∏Ä‰∏™5√ó5ÁöÑÂç∑ÁßØÔºåpadding‰πãÂêéÔºåËæìÂá∫ÊòØ27√ó27√ó276„ÄÇÁÑ∂ÂêéÂÜçÊ¨°ËøõË°åÊúÄÂ§ßÊ±†ÂåñÔºåÂ∞∫ÂØ∏Áº©Â∞èÂà∞13√ó13„ÄÇÂÜçÊâßË°å‰∏ÄÊ¨°sameÂç∑ÁßØÔºåÁõ∏ÂêåÁöÑpaddingÔºåÂæóÂà∞ÁöÑÁªìÊûúÊòØ13√ó13√ó384Ôºå384‰∏™ËøáÊª§Âô®„ÄÇÂÜçÂÅö‰∏ÄÊ¨°sameÂç∑ÁßØÔºåÂ∞±ÂÉèËøôÊ†∑„ÄÇÂÜçÂÅö‰∏ÄÊ¨°ÂêåÊ†∑ÁöÑÊìç‰ΩúÔºåÊúÄÂêéÂÜçËøõË°å‰∏ÄÊ¨°ÊúÄÂ§ßÊ±†ÂåñÔºåÂ∞∫ÂØ∏Áº©Â∞èÂà∞6√ó6√ó256„ÄÇ6√ó6√ó256Á≠â‰∫é9216ÔºåÂ∞ÜÂÖ∂Â±ïÂºÄ‰∏∫9216‰∏™ÂçïÂÖÉÔºåÁÑ∂ÂêéÊòØ‰∏Ä‰∫õÂÖ®ËøûÊé•Â±Ç„ÄÇÊúÄÂêé‰ΩøÁî®softmaxÂáΩÊï∞ËæìÂá∫ËØÜÂà´ÁöÑÁªìÊûúÔºåÁúãÂÆÉÁ©∂Á´üÊòØ1000‰∏™ÂèØËÉΩÁöÑÂØπË±°‰∏≠ÁöÑÂì™‰∏Ä‰∏™„ÄÇ ÂÆûÈôÖ‰∏äÔºåËøôÁßçÁ•ûÁªèÁΩëÁªú‰∏éLeNetÊúâÂæàÂ§öÁõ∏‰ºº‰πãÂ§ÑÔºå‰∏çËøáAlexNetË¶ÅÂ§ßÂæóÂ§ö„ÄÇÊ≠£Â¶ÇÂâçÈù¢ËÆ≤Âà∞ÁöÑLeNetÊàñLeNet-5Â§ßÁ∫¶Êúâ6‰∏á‰∏™ÂèÇÊï∞ÔºåËÄåAlexNetÂåÖÂê´Á∫¶6000‰∏á‰∏™ÂèÇÊï∞„ÄÇÂΩìÁî®‰∫éËÆ≠ÁªÉÂõæÂÉèÂíåÊï∞ÊçÆÈõÜÊó∂ÔºåAlexNetËÉΩÂ§üÂ§ÑÁêÜÈùûÂ∏∏Áõ∏‰ººÁöÑÂü∫Êú¨ÊûÑÈÄ†Ê®°ÂùóÔºåËøô‰∫õÊ®°ÂùóÂæÄÂæÄÂåÖÂê´ÁùÄÂ§ßÈáèÁöÑÈöêËóèÂçïÂÖÉÊàñÊï∞ÊçÆÔºåËøô‰∏ÄÁÇπAlexNetË°®Áé∞Âá∫Ëâ≤„ÄÇAlexNetÊØîLeNetË°®Áé∞Êõ¥‰∏∫Âá∫Ëâ≤ÁöÑÂè¶‰∏Ä‰∏™ÂéüÂõ†ÊòØÂÆÉ‰ΩøÁî®‰∫ÜReLuÊøÄÊ¥ªÂáΩÊï∞„ÄÇÂéü‰ΩúËÄÖËøòÊèêÂà∞‰∫Ü‰∏ÄÁßç‰ºòÂåñÊäÄÂ∑ßÔºåÂè´ÂÅöLocal Response Normalization(LRN)„ÄÇ ËÄåÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåLRNÁöÑÊïàÊûúÂπ∂‰∏çÁ™ÅÂá∫„ÄÇ 3. VGG-16 È¶ñÂÖàÁî®3√ó3ÔºåÊ≠•ÂπÖ‰∏∫1ÁöÑËøáÊª§Âô®ÊûÑÂª∫Âç∑ÁßØÂ±ÇÔºåpaddingÂèÇÊï∞‰∏∫sameÂç∑ÁßØ‰∏≠ÁöÑÂèÇÊï∞„ÄÇÁÑ∂ÂêéÁî®‰∏Ä‰∏™2√ó2ÔºåÊ≠•ÂπÖ‰∏∫2ÁöÑËøáÊª§Âô®ÊûÑÂª∫ÊúÄÂ§ßÊ±†ÂåñÂ±Ç„ÄÇÂõ†Ê≠§VGGÁΩëÁªúÁöÑ‰∏ÄÂ§ß‰ºòÁÇπÊòØÂÆÉÁ°ÆÂÆûÁÆÄÂåñ‰∫ÜÁ•ûÁªèÁΩëÁªúÁªìÊûÑÔºå‰∏ãÈù¢Êàë‰ª¨ÂÖ∑‰ΩìËÆ≤ËÆ≤ËøôÁßçÁΩëÁªúÁªìÊûÑ„ÄÇ Êï∞Â≠ó16ÔºåÂ∞±ÊòØÊåáÂú®Ëøô‰∏™ÁΩëÁªú‰∏≠ÂåÖÂê´16‰∏™Âç∑ÁßØÂ±ÇÂíåÂÖ®ËøûÊé•Â±Ç„ÄÇÊÄªÂÖ±ÂåÖÂê´Á∫¶1.38‰∫ø‰∏™ÂèÇÊï∞ L3 : Residual Networks (ResNets)(ÊÆãÂ∑ÆÁΩëÁªú(ResNets))Êàë‰ª¨Áü•ÈÅìÔºåÂ¶ÇÊûúÁ•ûÁªèÁΩëÁªúÂ±ÇÊï∞Ë∂äÂ§öÔºåÁΩëÁªúË∂äÊ∑±ÔºåÊ∫ê‰∫éÊ¢ØÂ∫¶Ê∂àÂ§±ÂíåÊ¢ØÂ∫¶ÁàÜÁÇ∏ÁöÑÂΩ±ÂìçÔºåÊï¥‰∏™Ê®°ÂûãÈöæ‰ª•ËÆ≠ÁªÉÊàêÂäü„ÄÇËß£ÂÜ≥ÁöÑÊñπÊ≥ï‰πã‰∏ÄÊòØ‰∫∫‰∏∫Âú∞ËÆ©Á•ûÁªèÁΩëÁªúÊüê‰∫õÂ±ÇË∑≥Ëøá‰∏ã‰∏ÄÂ±ÇÁ•ûÁªèÂÖÉÁöÑËøûÊé•ÔºåÈöîÂ±ÇÁõ∏ËøûÔºåÂº±ÂåñÊØèÂ±Ç‰πãÈó¥ÁöÑÂº∫ËÅîÁ≥ª„ÄÇËøôÁßçÁ•ûÁªèÁΩëÁªúË¢´Áß∞‰∏∫Residual Networks(ResNets)„ÄÇ L4: Why ResNets work?(ÊÆãÂ∑ÆÁΩëÁªú‰∏∫‰ªÄ‰πàÊúâÁî®Ôºü) Âõ†Ê≠§ÔºåËøô‰∏§Â±ÇÈ¢ùÂ§ñÁöÑÊÆãÂ∑ÆÂùó‰∏ç‰ºöÈôç‰ΩéÁΩëÁªúÊÄßËÉΩ„ÄÇËÄåÂ¶ÇÊûúÊ≤°ÊúâÂèëÁîüÊ¢ØÂ∫¶Ê∂àÂ§±Êó∂ÔºåËÆ≠ÁªÉÂæóÂà∞ÁöÑÈùûÁ∫øÊÄßÂÖ≥Á≥ª‰ºö‰ΩøÂæóË°®Áé∞ÊïàÊûúËøõ‰∏ÄÊ≠•ÊèêÈ´ò„ÄÇ Ê≥®ÊÑèÔºåÂ¶ÇÊûú$ a[l]$‰∏é $a[l+2]$ÁöÑÁª¥Â∫¶‰∏çÂêåÔºåÈúÄË¶ÅÂºïÂÖ•Áü©Èòµ $W_s$‰∏é $a_{[l]}$Áõ∏‰πòÔºå‰ΩøÂæó‰∫åËÄÖÁöÑÁª¥Â∫¶Áõ∏ÂåπÈÖç„ÄÇÂèÇÊï∞Áü©Èòµ $W_s$Êó¢ÂèØ‰ª•ÈÄöËøáÊ®°ÂûãËÆ≠ÁªÉÂæóÂà∞Ôºå‰πüÂèØ‰ª•‰Ωú‰∏∫Âõ∫ÂÆöÂÄºÔºå‰ªÖ‰Ωø $a[l]$Êà™Êñ≠ÊàñËÄÖË°•Èõ∂„ÄÇ L5 : Network in Network and 1√ó1 convolutions(ÁΩëÁªú‰∏≠ÁöÑÁΩëÁªú‰ª•Âèä 1√ó1 Âç∑ÁßØ) ‰ΩúÁî® ÂÅáËÆæËøôÊòØ‰∏Ä‰∏™28√ó28√ó192ÁöÑËæìÂÖ•Â±ÇÔºå‰Ω†ÂèØ‰ª•‰ΩøÁî®Ê±†ÂåñÂ±ÇÂéãÁº©ÂÆÉÁöÑÈ´òÂ∫¶ÂíåÂÆΩÂ∫¶ÔºåËøô‰∏™ËøáÁ®ãÊàë‰ª¨ÂæàÊ∏ÖÊ•ö„ÄÇ‰ΩÜÂ¶ÇÊûúÈÄöÈÅìÊï∞ÈáèÂæàÂ§ßÔºåËØ•Â¶Ç‰ΩïÊääÂÆÉÂéãÁº©‰∏∫28√ó28√ó32Áª¥Â∫¶ÁöÑÂ±ÇÂë¢Ôºü‰Ω†ÂèØ‰ª•Áî®32‰∏™Â§ßÂ∞è‰∏∫1√ó1ÁöÑËøáÊª§Âô®Ôºå‰∏•Ê†ºÊù•ËÆ≤ÊØè‰∏™ËøáÊª§Âô®Â§ßÂ∞èÈÉΩÊòØ1√ó1√ó192Áª¥ÔºåÂõ†‰∏∫ËøáÊª§Âô®‰∏≠ÈÄöÈÅìÊï∞ÈáèÂøÖÈ°ª‰∏éËæìÂÖ•Â±Ç‰∏≠ÈÄöÈÅìÁöÑÊï∞Èáè‰øùÊåÅ‰∏ÄËá¥„ÄÇ‰ΩÜÊòØ‰Ω†‰ΩøÁî®‰∫Ü32‰∏™ËøáÊª§Âô®ÔºåËæìÂá∫Â±Ç‰∏∫28√ó28√ó32ÔºåËøôÂ∞±ÊòØÂéãÁº©ÈÄöÈÅìÊï∞Ôºà$n_c$ÔºâÁöÑÊñπÊ≥ïÔºåÂØπ‰∫éÊ±†ÂåñÂ±ÇÊàëÂè™ÊòØÂéãÁº©‰∫ÜËøô‰∫õÂ±ÇÁöÑÈ´òÂ∫¶ÂíåÂÆΩÂ∫¶ doing something pretty non-trivial ÂÆÉÁªôÁ•ûÁªèÁΩëÁªúÊ∑ªÂä†‰∫Ü‰∏Ä‰∏™ÈùûÁ∫øÊÄßÂáΩÊï∞Ôºå‰ªéËÄåÂáèÂ∞ëÊàñ‰øùÊåÅËæìÂÖ•Â±Ç‰∏≠ÁöÑÈÄöÈÅìÊï∞Èáè‰∏çÂèòÔºåÂΩìÁÑ∂Â¶ÇÊûú‰Ω†ÊÑøÊÑèÔºå‰πüÂèØ‰ª•Â¢ûÂä†ÈÄöÈÅìÊï∞Èáè„ÄÇ L6 : Inception network motivation(Ë∞∑Ê≠å Inception ÁΩëÁªúÁÆÄ‰ªã) Êúâ‰∫ÜËøôÊ†∑ÁöÑInceptionÊ®°ÂùóÔºå‰Ω†Â∞±ÂèØ‰ª•ËæìÂÖ•Êüê‰∏™ÈáèÔºåÂõ†‰∏∫ÂÆÉÁ¥ØÂä†‰∫ÜÊâÄÊúâÊï∞Â≠óÔºåËøôÈáåÁöÑÊúÄÁªàËæìÂá∫‰∏∫32+32+128+64=256„ÄÇÊúâ‰∫ÜËøôÊ†∑ÁöÑInceptionÊ®°ÂùóÔºå‰Ω†Â∞±ÂèØ‰ª•ËæìÂÖ•Êüê‰∏™ÈáèÔºåÂõ†‰∏∫ÂÆÉÁ¥ØÂä†‰∫ÜÊâÄÊúâÊï∞Â≠óÔºåËøôÈáåÁöÑÊúÄÁªàËæìÂá∫‰∏∫32+32+128+64=256„ÄÇInception ÁΩëÁªúÈÄâÁî®‰∏çÂêåÂ∞∫ÂØ∏ÁöÑÊª§Ê≥¢Âô®ËøõË°å Same Âç∑ÁßØÔºåÂπ∂Â∞ÜÂç∑ÁßØÂíåÊ±†ÂåñÂæóÂà∞ÁöÑËæìÂá∫ÁªÑÂêàÊãºÊé•Ëµ∑Êù•ÔºåÊúÄÁªàËÆ©ÁΩëÁªúËá™Â∑±ÂéªÂ≠¶‰π†ÈúÄË¶ÅÁöÑÂèÇÊï∞ÂíåÈááÁî®ÁöÑÊª§Ê≥¢Âô®ÁªÑÂêà„ÄÇ 1x1 ÁöÑÂç∑ÁßØÂ±ÇÈÄöÂ∏∏Ë¢´Áß∞‰ΩúÁì∂È¢àÂ±ÇÔºàBottleneck layerÔºâ ËÆ°ÁÆóÈáè‰∏∫ 28x28x32x5x5x192 = 1.2‰∫ø 28x28x192x16 + 28x28x32x5x5x15 = 1.24 ÂçÉ‰∏áÔºåÂáèÂ∞ë‰∫ÜÁ∫¶ 90%„ÄÇ L7 : Inception network(Inception ÁΩëÁªú) L8 : Using open-source implementations( ‰ΩøÁî®ÂºÄÊ∫êÁöÑÂÆûÁé∞ÊñπÊ°à)ÂºÄÊ∫êÈ°πÁõÆ L9 Ôºö Transfer LearningÔºàËøÅÁßªÂ≠¶‰π†ÔºâÂ¶ÇÊûú‰Ω†‰∏ãËΩΩÂà´‰∫∫Â∑≤ÁªèËÆ≠ÁªÉÂ•ΩÁΩëÁªúÁªìÊûÑÁöÑÊùÉÈáçÔºå‰Ω†ÈÄöÂ∏∏ËÉΩÂ§üËøõÂ±ïÁöÑÁõ∏ÂΩìÂø´ÔºåÁî®Ëøô‰∏™‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÔºåÁÑ∂ÂêéËΩ¨Êç¢Âà∞‰Ω†ÊÑüÂÖ¥Ë∂£ÁöÑ‰ªªÂä°‰∏ä„ÄÇ Âè™ÊúâÂæàÂ∞èÊï∞ÊçÆÈõÜÔºö ÂèØ‰ª•‰Ω†Âè™ÈúÄË¶ÅËÆ≠ÁªÉsoftmaxÂ±ÇÁöÑÊùÉÈáçÔºåÊääÂâçÈù¢Ëøô‰∫õÂ±ÇÁöÑÊùÉÈáçÈÉΩÂÜªÁªì„ÄÇ Á®çÂæÆÊõ¥Â§ßÁöÑÊï∞ÊçÆÈõÜÔºö ‰Ω†Â∫îËØ•ÂÜªÁªìÊõ¥Â∞ëÁöÑÂ±ÇÔºåÊØîÂ¶ÇÂè™ÊääËøô‰∫õÂ±ÇÂÜªÁªìÔºåÁÑ∂ÂêéËÆ≠ÁªÉÂêéÈù¢ÁöÑÂ±Ç„ÄÇÂ¶ÇÊûú‰Ω†ÁöÑËæìÂá∫Â±ÇÁöÑÁ±ªÂà´‰∏çÂêåÔºåÈÇ£‰πà‰Ω†ÈúÄË¶ÅÊûÑÂª∫Ëá™Â∑±ÁöÑËæìÂá∫ÂçïÂÖÉÔºõÊàñËÄÖ‰Ω†ÂèØ‰ª•Áõ¥Êé•ÂéªÊéâËøôÂá†Â±ÇÔºåÊç¢Êàê‰Ω†Ëá™Â∑±ÁöÑÈöêËóèÂçïÂÖÉÂíå‰Ω†Ëá™Â∑±ÁöÑsoftmaxËæìÂá∫Â±ÇÔºåËøô‰∫õÊñπÊ≥ïÂÄºÂæó‰∏ÄËØï„ÄÇ Â§ßÈáèÊï∞ÊçÆÔºö ‰Ω†ÂèØ‰ª•Áî®‰∏ãËΩΩÁöÑÊùÉÈáçÂè™‰Ωú‰∏∫ÂàùÂßãÂåñÔºåÁî®ÂÆÉ‰ª¨Êù•‰ª£ÊõøÈöèÊú∫ÂàùÂßãÂåñÔºåÊé•ÁùÄ‰Ω†ÂèØ‰ª•Áî®Ê¢ØÂ∫¶‰∏ãÈôçËÆ≠ÁªÉÔºåÊõ¥Êñ∞ÁΩëÁªúÊâÄÊúâÂ±ÇÁöÑÊâÄÊúâÊùÉÈáç„ÄÇ L10 Ôºö Data augmentationÔºàÊï∞ÊçÆÂ¢ûÂº∫ÔºâÊï∞ÊçÆÈáèËøúËøú‰∏çÂ§ü Mirroring Random Cropping ÂΩ©Ëâ≤ËΩ¨Êç¢color shifting r,g,bÊï∞ÊçÆÊîπÂèò Èô§‰∫ÜÈöèÊÑèÊîπÂèòRGBÈÄöÈÅìÊï∞ÂÄºÂ§ñÔºåËøòÂèØ‰ª•Êõ¥ÊúâÈíàÂØπÊÄßÂú∞ÂØπÂõæÁâáÁöÑRGBÈÄöÈÅìËøõË°åPCA color augmentationÔºå‰πüÂ∞±ÊòØÂØπÂõæÁâáÈ¢úËâ≤ËøõË°å‰∏ªÊàêÂàÜÂàÜÊûêÔºåÂØπ‰∏ªË¶ÅÁöÑÈÄöÈÅìÈ¢úËâ≤ËøõË°åÂ¢ûÂä†ÊàñÂáèÂ∞ëÔºåÂèØ‰ª•ÈááÁî®È´òÊñØÊâ∞Âä®ÂÅöÊ≥ï„ÄÇËøôÊ†∑‰πüËÉΩÂ¢ûÂä†ÊúâÊïàÁöÑÊ†∑Êú¨Êï∞Èáè„ÄÇÂÖ∑‰ΩìÁöÑPCA color augmentationÂÅöÊ≥ïÂèØ‰ª•Êü•ÈòÖAlexNetÁöÑÁõ∏ÂÖ≥ËÆ∫Êñá„ÄÇ Â∏∏Áî®ÁöÑÂÆûÁé∞Êï∞ÊçÆÊâ©ÂÖÖÁöÑÊñπÊ≥ïÊòØ‰ΩøÁî®‰∏Ä‰∏™Á∫øÁ®ãÊàñËÄÖÊòØÂ§öÁ∫øÁ®ãÔºåËøô‰∫õÂèØ‰ª•Áî®Êù•Âä†ËΩΩÊï∞ÊçÆÔºåÂÆûÁé∞ÂèòÂΩ¢Â§±ÁúüÔºåÁÑ∂Âêé‰º†ÁªôÂÖ∂‰ªñÁöÑÁ∫øÁ®ãÊàñËÄÖÂÖ∂‰ªñËøõÁ®ãÔºåÊù•ËÆ≠ÁªÉËøô‰∏™ÔºàÁºñÂè∑2ÔºâÂíåËøô‰∏™ÔºàÁºñÂè∑1ÔºâÔºåÂèØ‰ª•Âπ∂Ë°åÂÆûÁé∞„ÄÇ L11ÔºöThe state of computer vision(ËÆ°ÁÆóÊú∫ËßÜËßâÁé∞Áä∂) Á•ûÁªèÁΩëÁªúÈúÄË¶ÅÊï∞ÊçÆÔºå‰∏çÂêåÁöÑÁΩëÁªúÊ®°ÂûãÊâÄÈúÄÁöÑÊï∞ÊçÆÈáèÊòØ‰∏çÂêåÁöÑ„ÄÇObject dectionÔºåImage recognitionÔºåSpeech recognitionÊâÄÈúÄÁöÑÊï∞ÊçÆÈáè‰æùÊ¨°Â¢ûÂä†„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÂ¶ÇÊûúdataËæÉÂ∞ëÔºåÈÇ£‰πàÂ∞±ÈúÄË¶ÅÊõ¥Â§öÁöÑhand-engineeringÔºåÂØπÂ∑≤ÊúâdataËøõË°åÂ§ÑÁêÜ„ÄÇ hand-engineeringÊòØ‰∏ÄÈ°πÈùûÂ∏∏ÈáçË¶Å‰πüÊØîËæÉÂõ∞ÈöæÁöÑÂ∑•‰Ωú„ÄÇÂæàÂ§öÊó∂ÂÄôÔºåhand-engineeringÂØπÊ®°ÂûãËÆ≠ÁªÉÊïàÊûúÂΩ±ÂìçÂæàÂ§ßÔºåÁâπÂà´ÊòØÂú®Êï∞ÊçÆÈáè‰∏çÂ§öÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ ÂΩì‰Ω†ÊúâÂ∞ëÈáèÁöÑÊï∞ÊçÆÊó∂ÔºåÊúâ‰∏Ä‰ª∂‰∫ãÂØπ‰Ω†ÂæàÊúâÂ∏ÆÂä©ÔºåÈÇ£Â∞±ÊòØËøÅÁßªÂ≠¶‰π†„ÄÇÂú®Âà´‰∫∫ÂÅöÂ•ΩÁöÑÂü∫Á°Ä‰∏äÁ†îÁ©∂ ÊèêÂçáÊÄßËÉΩ * Áî±‰∫éËÆ°ÁÆóÊú∫ËßÜËßâÈóÆÈ¢òÂª∫Á´ãÂú®Â∞èÊï∞ÊçÆÈõÜ‰πã‰∏äÔºåÂÖ∂‰ªñ‰∫∫Â∑≤ÁªèÂÆåÊàê‰∫ÜÂ§ßÈáèÁöÑÁΩëÁªúÊû∂ÊûÑÁöÑÊâãÂ∑•Â∑•Á®ã„ÄÇ‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÂú®Êüê‰∏™ËÆ°ÁÆóÊú∫ËßÜËßâÈóÆÈ¢ò‰∏äÂæàÊúâÊïàÔºå‰ΩÜ‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊòØÂÆÉÈÄöÂ∏∏‰πü‰ºöËß£ÂÜ≥ÂÖ∂‰ªñËÆ°ÁÆóÊú∫ËßÜËßâÈóÆÈ¢ò„ÄÇ ÊâÄ‰ª•ÔºåË¶ÅÊÉ≥Âª∫Á´ã‰∏Ä‰∏™ÂÆûÁî®ÁöÑÁ≥ªÁªüÔºå‰Ω†ÊúÄÂ•ΩÂÖà‰ªéÂÖ∂‰ªñ‰∫∫ÁöÑÁ•ûÁªèÁΩëÁªúÊû∂ÊûÑÂÖ•Êâã„ÄÇÂ¶ÇÊûúÂèØËÉΩÁöÑËØùÔºå‰Ω†ÂèØ‰ª•‰ΩøÁî®ÂºÄÊ∫êÁöÑ‰∏Ä‰∫õÂ∫îÁî®ÔºåÂõ†‰∏∫ÂºÄÊîæÁöÑÊ∫êÁ†ÅÂÆûÁé∞ÂèØËÉΩÂ∑≤ÁªèÊâæÂà∞‰∫ÜÊâÄÊúâÁπÅÁêêÁöÑÁªÜËäÇÔºåÊØîÂ¶ÇÂ≠¶‰π†ÁéáË°∞ÂáèÊñπÂºèÊàñËÄÖË∂ÖÂèÇÊï∞„ÄÇ summary1. CNNÁöÑÂ∏∏ËßÅÁΩëÁªúÁªìÊûÑ ÈáçÁÇπËØ¥‰∫Ü‰∏Ä‰∫õÊÆãÂ∑ÆÁΩëÁªú 2.Êï∞ÊçÆÂ¢ûÂä†ÁöÑÊñπÊ≥ï 3. Â§öÁî®ÂºÄÊ∫êÊ°ÜÊû∂Ôºå‰∏çÁî®‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉ W3 Object detection(ÁõÆÊ†áÊ£ÄÊµã)L1 :Object localization(ÁõÆÊ†áÂÆö‰Ωç)ÁõÆÊ†áÂÆö‰ΩçÂíåÁõÆÊ†áÊ£ÄÊµã Ê®°Âûã ËæìÂÖ•ËøòÂåÖÊã¨‰ΩçÁΩÆ‰ø°ÊÅØ ÊçüÂ§±ÂáΩÊï∞ ÊÉÖÂÜµ‰∏ÄÔºöÊ£ÄÊµãÂà∞‰∫Ü ÊÉÖÂÜµ‰∫åÔºö L2: Landmark detection(ÁâπÂæÅÁÇπÊ£ÄÊµã) ËØ•ÁΩëÁªúÊ®°ÂûãÂÖ±Ê£ÄÊµã‰∫∫ËÑ∏‰∏ä64Â§ÑÁâπÂæÅÁÇπÔºåÂä†‰∏äÊòØÂê¶‰∏∫faceÁöÑÊ†áÂøó‰ΩçÔºåËæìÂá∫labelÂÖ±Êúâ64x2+1=129‰∏™ÂÄº„ÄÇÈÄöËøáÊ£ÄÊµã‰∫∫ËÑ∏ÁâπÂæÅÁÇπÂèØ‰ª•ËøõË°åÊÉÖÁª™ÂàÜÁ±ª‰∏éÂà§Êñ≠ÔºåÊàñËÄÖÂ∫îÁî®‰∫éARÈ¢ÜÂüüÁ≠âÁ≠â„ÄÇ Èô§‰∫Ü‰∫∫ËÑ∏ÁâπÂæÅÁÇπÊ£ÄÊµã‰πãÂ§ñÔºåËøòÂèØ‰ª•Ê£ÄÊµã‰∫∫‰ΩìÂßøÂäøÂä®‰ΩúÔºåÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö L3 :Object detection(ÁõÆÊ†áÊ£ÄÊµã)Â≠¶Ëøá‰∫ÜÂØπË±°ÂÆö‰ΩçÂíåÁâπÂæÅÁÇπÊ£ÄÊµãÔºå‰ªäÂ§©Êàë‰ª¨Êù•ÊûÑÂª∫‰∏Ä‰∏™ÂØπË±°Ê£ÄÊµãÁÆóÊ≥ï„ÄÇËøôËäÇËØæÔºåÊàë‰ª¨Â∞ÜÂ≠¶‰π†Â¶Ç‰ΩïÈÄöËøáÂç∑ÁßØÁΩëÁªúËøõË°åÂØπË±°Ê£ÄÊµãÔºåÈááÁî®ÁöÑÊòØÂü∫‰∫éÊªëÂä®Á™óÂè£ÁöÑÁõÆÊ†áÊ£ÄÊµãÁÆóÊ≥ï„ÄÇ ËÆ≠ÁªÉÂÆåËøô‰∏™Âç∑ÁßØÁΩëÁªúÔºåÂ∞±ÂèØ‰ª•Áî®ÂÆÉÊù•ÂÆûÁé∞ÊªëÂä®Á™óÂè£ÁõÆÊ†áÊ£ÄÊµãÔºåÂÖ∑‰ΩìÊ≠•È™§Â¶Ç‰∏ã„ÄÇ ÈÄâÂÆöÁâπÂÆöÂ§ßÂ∞èÁöÑÁ™óÂè£ÔºåÁ™óÂè£ÂúàÂÆöËæìÂÖ•Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºåÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÂºÄÂßãÈ¢ÑÊµã„ÄÇ ÈáçÂ§ç‰∏äËø∞Êìç‰ΩúÔºå‰∏çËøáËøôÊ¨°Êàë‰ª¨ÈÄâÊã©‰∏Ä‰∏™Êõ¥Â§ßÁöÑÁ™óÂè£ÔºåÊà™ÂèñÊõ¥Â§ßÁöÑÂå∫ÂüüÔºåÂπ∂ËæìÂÖ•ÁªôÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÂ§ÑÁêÜÔºå‰Ω†ÂèØ‰ª•Ê†πÊçÆÂç∑ÁßØÁΩëÁªúÂØπËæìÂÖ•Â§ßÂ∞èË∞ÉÊï¥Ëøô‰∏™Âå∫ÂüüÔºåÁÑ∂ÂêéËæìÂÖ•ÁªôÂç∑ÁßØÁΩëÁªúÔºåËæìÂá∫0Êàñ Â¶ÇÊûú‰Ω†ËøôÊ†∑ÂÅöÔºå‰∏çËÆ∫Ê±ΩËΩ¶Âú®ÂõæÁâáÁöÑ‰ªÄ‰πà‰ΩçÁΩÆÔºåÊÄªÊúâ‰∏Ä‰∏™Á™óÂè£ÂèØ‰ª•Ê£ÄÊµãÂà∞ÂÆÉ„ÄÇ ËøôÁßçÁÆóÊ≥ïÂè´‰ΩúÊªëÂä®Á™óÂè£ÁõÆÊ†áÊ£ÄÊµãÔºåÂõ†‰∏∫Êàë‰ª¨‰ª•Êüê‰∏™Ê≠•ÂπÖÊªëÂä®Ëøô‰∫õÊñπÊ°ÜÁ™óÂè£ÈÅçÂéÜÊï¥Âº†ÂõæÁâáÔºåÂØπËøô‰∫õÊñπÂΩ¢Âå∫ÂüüËøõË°åÂàÜÁ±ªÔºåÂà§Êñ≠ÈáåÈù¢ÊúâÊ≤°ÊúâÊ±ΩËΩ¶„ÄÇ ÊªëÂä®Á™óÁÆóÊ≥ïÁöÑ‰ºòÁÇπÊòØÂéüÁêÜÁÆÄÂçïÔºå‰∏î‰∏çÈúÄË¶Å‰∫∫‰∏∫ÈÄâÂÆöÁõÆÊ†áÂå∫ÂüüÔºàÊ£ÄÊµãÂá∫ÁõÆÊ†áÁöÑÊªëÂä®Á™óÂç≥‰∏∫ÁõÆÊ†áÂå∫ÂüüÔºâ„ÄÇ‰ΩÜÊòØÂÖ∂Áº∫ÁÇπ‰πüÂæàÊòéÊòæÔºåÈ¶ñÂÖàÊªëÂä®Á™óÁöÑÂ§ßÂ∞èÂíåÊ≠•ËøõÈïøÂ∫¶ÈÉΩÈúÄË¶Å‰∫∫‰∏∫Áõ¥ËßÇËÆæÂÆö„ÄÇÊªëÂä®Á™óËøáÂ∞èÊàñËøáÂ§ßÔºåÊ≠•ËøõÈïøÂ∫¶ËøáÂ§ßÂùá‰ºöÈôç‰ΩéÁõÆÊ†áÊ£ÄÊµãÊ≠£Á°ÆÁéá„ÄÇËÄå‰∏îÔºåÊØèÊ¨°ÊªëÂä®Á™óÂå∫ÂüüÈÉΩË¶ÅËøõË°å‰∏ÄÊ¨°CNNÁΩëÁªúËÆ°ÁÆóÔºåÂ¶ÇÊûúÊªëÂä®Á™óÂíåÊ≠•ËøõÈïøÂ∫¶ËæÉÂ∞èÔºåÊï¥‰∏™ÁõÆÊ†áÊ£ÄÊµãÁöÑÁÆóÊ≥ïËøêË°åÊó∂Èó¥‰ºöÂæàÈïø„ÄÇÊâÄ‰ª•ÔºåÊªëÂä®Á™óÁÆóÊ≥ïËôΩÁÑ∂ÁÆÄÂçïÔºå‰ΩÜÊòØÊÄßËÉΩ‰∏ç‰Ω≥Ôºå‰∏çÂ§üÂø´Ôºå‰∏çÂ§üÁÅµÊ¥ª„ÄÇ L 4 : Convolutional implementation of sliding windows(ÊªëÂä®Á™óÂè£ÁöÑÂç∑ÁßØÂÆûÁé∞) ÂÖ®ËøûÊé•Â±ÇËΩ¨Âåñ‰∏∫Âç∑ÁßØÂ±Ç Âçï‰∏™Á™óÂè£Âå∫ÂüüÂç∑ÁßØÁΩëÁªúÁªìÊûÑÂª∫Á´ãÂÆåÊØï‰πãÂêéÔºåÂØπ‰∫éÂæÖÊ£ÄÊµãÂõæÁâáÔºåÂç≥ÂèØ‰ΩøÁî®ËØ•ÁΩëÁªúÂèÇÊï∞ÂíåÁªìÊûÑËøõË°åËøêÁÆó„ÄÇ‰æãÂ¶Ç16 x 16 x 3ÁöÑÂõæÁâáÔºåÊ≠•ËøõÈïøÂ∫¶‰∏∫2ÔºåCNNÁΩëÁªúÂæóÂà∞ÁöÑËæìÂá∫Â±Ç‰∏∫2 x 2 x 4„ÄÇÂÖ∂‰∏≠Ôºå2 x 2Ë°®Á§∫ÂÖ±Êúâ4‰∏™Á™óÂè£ÁªìÊûú„ÄÇÂØπ‰∫éÊõ¥Â§çÊùÇÁöÑ28 x 28 x3ÁöÑÂõæÁâáÔºåCNNÁΩëÁªúÂæóÂà∞ÁöÑËæìÂá∫Â±Ç‰∏∫8 x 8 x 4ÔºåÂÖ±64‰∏™Á™óÂè£ÁªìÊûú„ÄÇ ‰πãÂâçÁöÑÊªëÂä®Á™óÁÆóÊ≥ïÈúÄË¶ÅÂèçÂ§çËøõË°åCNNÊ≠£ÂêëËÆ°ÁÆóÔºå‰æãÂ¶Ç16 x 16 x 3ÁöÑÂõæÁâáÈúÄËøõË°å4Ê¨°Ôºå28 x 28 x3ÁöÑÂõæÁâáÈúÄËøõË°å64Ê¨°„ÄÇËÄåÂà©Áî®Âç∑ÁßØÊìç‰Ωú‰ª£ÊõøÊªëÂä®Á™óÁÆóÊ≥ïÔºåÂàô‰∏çÁÆ°ÂéüÂßãÂõæÁâáÊúâÂ§öÂ§ßÔºåÂè™ÈúÄË¶ÅËøõË°å‰∏ÄÊ¨°CNNÊ≠£ÂêëËÆ°ÁÆóÔºåÂõ†‰∏∫ÂÖ∂‰∏≠ÂÖ±‰∫´‰∫ÜÂæàÂ§öÈáçÂ§çËÆ°ÁÆóÈÉ®ÂàÜÔºåËøôÂ§ßÂ§ßËäÇÁ∫¶‰∫ÜËøêÁÆóÊàêÊú¨„ÄÇÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåÁ™óÂè£Ê≠•ËøõÈïøÂ∫¶‰∏éÈÄâÊã©ÁöÑMAX POOLÂ§ßÂ∞èÊúâÂÖ≥„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÊ≠•ËøõÈïøÂ∫¶‰∏∫4ÔºåÂè™ÈúÄËÆæÁΩÆMAX POOL‰∏∫4 x 4Âç≥ÂèØ„ÄÇ L5 Ôºö Bounding box predictionsÔºàBounding BoxÈ¢ÑÊµãÔºâ YOLOÔºàYou Only Look OnceÔºâÁÆóÊ≥ïÂèØ‰ª•Ëß£ÂÜ≥ËøôÁ±ªÈóÆÈ¢òÔºåÁîüÊàêÊõ¥Âä†ÂáÜÁ°ÆÁöÑÁõÆÊ†áÂå∫ÂüüÔºàÂ¶Ç‰∏äÂõæÁ∫¢Ëâ≤Á™óÂè£Ôºâ„ÄÇ Â¶ÇÊûúÁõÆÊ†á‰∏≠ÂøÉÂùêÊ†á(bx,by)‰∏çÂú®ÂΩìÂâçÁΩëÊ†ºÂÜÖÔºåÂàôÂΩìÂâçÁΩëÊ†ºPc=0ÔºõÁõ∏ÂèçÔºåÂàôÂΩìÂâçÁΩëÊ†ºPc=1ÔºàÂç≥Âè™Áúã‰∏≠ÂøÉÂùêÊ†áÊòØÂê¶Âú®ÂΩìÂâçÁΩëÊ†ºÂÜÖÔºâ„ÄÇÂà§Êñ≠ÊúâÁõÆÊ†áÁöÑÁΩëÊ†º‰∏≠Ôºåbx,by,bh,bwÈôêÂÆö‰∫ÜÁõÆÊ†áÂå∫Âüü„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂΩìÂâçÁΩëÊ†ºÂ∑¶‰∏äËßíÂùêÊ†áËÆæÂÆö‰∏∫(0, 0)ÔºåÂè≥‰∏ãËßíÂùêÊ†áËÆæÂÆö‰∏∫(1, 1)Ôºå(bx,by)ËåÉÂõ¥ÈôêÂÆöÂú®[0,1]‰πãÈó¥Ôºå‰ΩÜÊòØbh,bwÂèØ‰ª•Â§ß‰∫é1„ÄÇÂõ†‰∏∫ÁõÆÊ†áÂèØËÉΩË∂ÖÂá∫ËØ•ÁΩëÊ†ºÔºåÊ®™Ë∑®Â§ö‰∏™Âå∫ÂüüÔºåÂ¶Ç‰∏äÂõæÊâÄÁ§∫„ÄÇÁõÆÊ†áÂç†Âá†‰∏™ÁΩëÊ†ºÊ≤°ÊúâÂÖ≥Á≥ªÔºåÁõÆÊ†á‰∏≠ÂøÉÂùêÊ†áÂøÖÁÑ∂Âú®‰∏Ä‰∏™ÁΩëÊ†º‰πãÂÜÖ„ÄÇ L6 ÔºöIntersection over unionÔºà‰∫§Âπ∂ÊØî) ‰∏ÄËà¨Á∫¶ÂÆöÔºåÂú®ËÆ°ÁÆóÊú∫Ê£ÄÊµã‰ªªÂä°‰∏≠ÔºåÂ¶ÇÊûúlou&gt;=0.5ÔºåÂ∞±ËØ¥Ê£ÄÊµãÊ≠£Á°ÆÔºåÂ¶ÇÊûúÈ¢ÑÊµãÂô®ÂíåÂÆûÈôÖËæπÁïåÊ°ÜÂÆåÁæéÈáçÂè†ÔºåloUÂ∞±ÊòØ1ÔºåÂõ†‰∏∫‰∫§ÈõÜÂ∞±Á≠â‰∫éÂπ∂ÈõÜ„ÄÇ‰ΩÜ‰∏ÄËà¨Êù•ËØ¥Âè™Ë¶Ålou&gt;=0.5ÔºåÈÇ£‰πàÁªìÊûúÊòØÂèØ‰ª•Êé•ÂèóÁöÑÔºåÁúãËµ∑Êù•ËøòÂèØ‰ª•„ÄÇ‰∏ÄËà¨Á∫¶ÂÆöÔºå0.5ÊòØÈòàÂÄºÔºåÁî®Êù•Âà§Êñ≠È¢ÑÊµãÁöÑËæπÁïåÊ°ÜÊòØÂê¶Ê≠£Á°Æ„ÄÇ‰∏ÄËà¨ÊòØËøô‰πàÁ∫¶ÂÆöÔºå‰ΩÜÂ¶ÇÊûú‰Ω†Â∏åÊúõÊõ¥‰∏•Ê†º‰∏ÄÁÇπÔºå‰Ω†ÂèØ‰ª•Â∞ÜloUÂÆöÂæóÊõ¥È´òÔºåÊØîÂ¶ÇËØ¥Â§ß‰∫é0.6ÊàñËÄÖÊõ¥Â§ßÁöÑÊï∞Â≠óÔºå‰ΩÜloUË∂äÈ´òÔºåËæπÁïåÊ°ÜË∂äÁ≤æÁ°Æ„ÄÇ L7: Non-max suppression(ÈùûÊûÅÂ§ßÂÄºÊäëÂà∂)Âà∞ÁõÆÂâç‰∏∫Ê≠¢‰Ω†‰ª¨Â≠¶Âà∞ÁöÑÂØπË±°Ê£ÄÊµã‰∏≠ÁöÑ‰∏Ä‰∏™ÈóÆÈ¢òÊòØÔºå‰Ω†ÁöÑÁÆóÊ≥ïÂèØËÉΩÂØπÂêå‰∏Ä‰∏™ÂØπË±°ÂÅöÂá∫Â§öÊ¨°Ê£ÄÊµãÔºåÊâÄ‰ª•ÁÆóÊ≥ï‰∏çÊòØÂØπÊüê‰∏™ÂØπË±°Ê£ÄÊµãÂá∫‰∏ÄÊ¨°ÔºåËÄåÊòØÊ£ÄÊµãÂá∫Â§öÊ¨°„ÄÇÈùûÊûÅÂ§ßÂÄºÊäëÂà∂Ëøô‰∏™ÊñπÊ≥ïÂèØ‰ª•Á°Æ‰øù‰Ω†ÁöÑÁÆóÊ≥ïÂØπÊØè‰∏™ÂØπË±°Âè™Ê£ÄÊµã‰∏ÄÊ¨°ÔºåÊàë‰ª¨ËÆ≤‰∏Ä‰∏™‰æãÂ≠ê„ÄÇ ÂÅáËÆæ‰Ω†ÈúÄË¶ÅÂú®ËøôÂº†ÂõæÁâáÈáåÊ£ÄÊµãË°å‰∫∫ÂíåÊ±ΩËΩ¶Ôºå‰Ω†ÂèØËÉΩ‰ºöÂú®‰∏äÈù¢Êîæ‰∏™19√ó19ÁΩëÊ†ºÔºåÁêÜËÆ∫‰∏äËøôËæÜËΩ¶Âè™Êúâ‰∏Ä‰∏™‰∏≠ÁÇπÔºåÊâÄ‰ª•ÂÆÉÂ∫îËØ•Âè™Ë¢´ÂàÜÈÖçÂà∞‰∏Ä‰∏™Ê†ºÂ≠êÈáåÔºåÂ∑¶ËæπÁöÑËΩ¶Â≠ê‰πüÂè™Êúâ‰∏Ä‰∏™‰∏≠ÁÇπÔºåÊâÄ‰ª•ÁêÜËÆ∫‰∏äÂ∫îËØ•Âè™Êúâ‰∏Ä‰∏™Ê†ºÂ≠êÂÅöÂá∫ÊúâËΩ¶ÁöÑÈ¢ÑÊµã„ÄÇ ÂÆûÈôÖÊÉÖÂÜµÊòØÊ†ºÂ≠ê1Ôºå2Ôºå3Ôºå4Ôºå5Ôºå6ÈÉΩËÆ§‰∏∫ÈáåÈù¢ÊúâËΩ¶„ÄÇÂõ†‰∏∫‰Ω†Ë¶ÅÂú®361‰∏™Ê†ºÂ≠ê‰∏äÈÉΩËøêË°å‰∏ÄÊ¨°ÂõæÂÉèÊ£ÄÊµãÂíåÂÆö‰ΩçÁÆóÊ≥ïÔºåÈÇ£‰πàÂèØËÉΩÂæàÂ§öÊ†ºÂ≠êÈÉΩ‰ºö‰∏æÊâãËØ¥ÊàëÁöÑpc,ÊàëËøô‰∏™Ê†ºÂ≠êÈáåÊúâËΩ¶ÁöÑÊ¶ÇÁéáÂæàÈ´òÔºåËÄå‰∏çÊòØ361‰∏™Ê†ºÂ≠ê‰∏≠‰ªÖÊúâ‰∏§‰∏™Ê†ºÂ≠ê‰ºöÊä•ÂëäÂÆÉ‰ª¨Ê£ÄÊµãÂá∫‰∏Ä‰∏™ÂØπË±°„ÄÇ ÈùûÊúÄÂ§ßÂÄºÊäëÂà∂ÔºàNon-max SuppressionÔºâÂÅöÊ≥ïÂæàÁÆÄÂçïÔºåÂõæÁ§∫ÊØè‰∏™ÁΩëÊ†ºÁöÑPcÂÄºÂèØ‰ª•Ê±ÇÂá∫ÔºåPcÂÄºÂèçÊò†‰∫ÜËØ•ÁΩëÊ†ºÂåÖÂê´ÁõÆÊ†á‰∏≠ÂøÉÂùêÊ†áÁöÑÂèØ‰ø°Â∫¶„ÄÇÈ¶ñÂÖàÈÄâÂèñPcÊúÄÂ§ßÂÄºÂØπÂ∫îÁöÑÁΩëÊ†ºÂíåÂå∫ÂüüÔºåÁÑ∂ÂêéËÆ°ÁÆóËØ•Âå∫Âüü‰∏éÊâÄÊúâÂÖ∂ÂÆÉÂå∫ÂüüÁöÑIoUÔºåÂâîÈô§ÊéâIoUÂ§ß‰∫éÈòàÂÄºÔºà‰æãÂ¶Ç0.5ÔºâÁöÑÊâÄÊúâÁΩëÊ†ºÂèäÂå∫Âüü„ÄÇËøôÊ†∑Â∞±ËÉΩ‰øùËØÅÂêå‰∏ÄÁõÆÊ†áÂè™Êúâ‰∏Ä‰∏™ÁΩëÊ†º‰∏é‰πãÂØπÂ∫îÔºå‰∏îËØ•ÁΩëÊ†ºPcÊúÄÂ§ßÔºåÊúÄÂèØ‰ø°„ÄÇÊé•ÁùÄÔºåÂÜç‰ªéÂâ©‰∏ãÁöÑÁΩëÊ†º‰∏≠ÈÄâÂèñPcÊúÄÂ§ßÁöÑÁΩëÊ†ºÔºåÈáçÂ§ç‰∏ä‰∏ÄÊ≠•ÁöÑÊìç‰Ωú„ÄÇÊúÄÂêéÔºåÂ∞±ËÉΩ‰ΩøÂæóÊØè‰∏™ÁõÆÊ†áÈÉΩ‰ªÖÁî±‰∏Ä‰∏™ÁΩëÊ†ºÂíåÂå∫ÂüüÂØπÂ∫î„ÄÇÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö ÊÄªÁªì‰∏Ä‰∏ãÈùûÊúÄÂ§ßÂÄºÊäëÂà∂ÁÆóÊ≥ïÁöÑÊµÅÁ®ãÔºö ÂâîÈô§PcÂÄºÂ∞è‰∫éÊüêÈòàÂÄºÔºà‰æãÂ¶Ç0.6ÔºâÁöÑÊâÄÊúâÁΩëÊ†ºÔºõ ÈÄâÂèñPcÂÄºÊúÄÂ§ßÁöÑÁΩëÊ†ºÔºåÂà©Áî®IoUÔºåÊëíÂºÉ‰∏éËØ•ÁΩëÊ†º‰∫§Âè†ËæÉÂ§ßÁöÑÁΩëÊ†ºÔºõ ÂØπÂâ©‰∏ãÁöÑÁΩëÊ†ºÔºåÈáçÂ§çÊ≠•È™§2„ÄÇ Âà∞ÁõÆÂâç‰∏∫Ê≠¢ÔºåÂØπË±°Ê£ÄÊµã‰∏≠Â≠òÂú®ÁöÑ‰∏Ä‰∏™ÈóÆÈ¢òÊòØÊØè‰∏™Ê†ºÂ≠êÂè™ËÉΩÊ£ÄÊµãÂá∫‰∏Ä‰∏™ÂØπË±°ÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥ËÆ©‰∏Ä‰∏™Ê†ºÂ≠êÊ£ÄÊµãÂá∫Â§ö‰∏™ÂØπË±°Ôºå‰Ω†ÂèØ‰ª•Ëøô‰πàÂÅöÔºåÂ∞±ÊòØ‰ΩøÁî®anchor boxËøô‰∏™Ê¶ÇÂøµÔºåÊàë‰ª¨‰ªé‰∏Ä‰∏™‰æãÂ≠êÂºÄÂßãËÆ≤Âêß„ÄÇÊñπÊ≥ïÊòØ‰ΩøÁî®‰∏çÂêåÂΩ¢Áä∂ÁöÑAnchor Boxes„ÄÇ ËøôÂ∞±ÊòØanchor boxÁöÑÊ¶ÇÂøµÔºåÊàë‰ª¨Âª∫Á´ãanchor boxËøô‰∏™Ê¶ÇÂøµÔºåÊòØ‰∏∫‰∫ÜÂ§ÑÁêÜ‰∏§‰∏™ÂØπË±°Âá∫Áé∞Âú®Âêå‰∏Ä‰∏™Ê†ºÂ≠êÁöÑÊÉÖÂÜµÔºåÂÆûË∑µ‰∏≠ËøôÁßçÊÉÖÂÜµÂæàÂ∞ëÂèëÁîü L9 : YOLO ÁÆóÊ≥ïÔºàPutting it together: YOLO algorithmÔºâ ËøôÂ∞±ÊòØYOLOÂØπË±°Ê£ÄÊµãÁÆóÊ≥ïÔºåËøôÂÆûÈôÖ‰∏äÊòØÊúÄÊúâÊïàÁöÑÂØπË±°Ê£ÄÊµãÁÆóÊ≥ï‰πã‰∏ÄÔºåÂåÖÂê´‰∫ÜÊï¥‰∏™ËÆ°ÁÆóÊú∫ËßÜËßâÂØπË±°Ê£ÄÊµãÈ¢ÜÂüüÊñáÁåÆ‰∏≠ÂæàÂ§öÊúÄÁ≤æÂ¶ôÁöÑÊÄùË∑Ø Region proposals (Optional)ÔºàÂÄôÈÄâÂå∫ÂüüÔºàÈÄâ‰øÆÔºâÔºâ‰πãÂâç‰ªãÁªçÁöÑÊªëÂä®Á™óÁÆóÊ≥ï‰ºöÂØπÂéüÂßãÂõæÁâáÁöÑÊØè‰∏™Âå∫ÂüüÈÉΩËøõË°åÊâ´ÊèèÔºåÂç≥‰ΩøÊòØ‰∏Ä‰∫õÁ©∫ÁôΩÁöÑÊàñÊòéÊòæÊ≤°ÊúâÁõÆÊ†áÁöÑÂå∫ÂüüÔºå‰æãÂ¶Ç‰∏ãÂõæÊâÄÁ§∫„ÄÇËøôÊ†∑‰ºöÈôç‰ΩéÁÆóÊ≥ïËøêË°åÊïàÁéáÔºåËÄóË¥πÊó∂Èó¥„ÄÇ ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÂ∞ΩÈáèÈÅøÂÖçÂØπÊó†Áî®Âå∫ÂüüÁöÑÊâ´ÊèèÔºåÂèØ‰ª•‰ΩøÁî®Region ProposalsÁöÑÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÂÅöÊ≥ïÊòØÂÖàÂØπÂéüÂßãÂõæÁâáËøõË°åÂàÜÂâ≤ÁÆóÊ≥ïÂ§ÑÁêÜÔºåÁÑ∂ÂêéÊîØÈòüÂàÜÂâ≤ÂêéÁöÑÂõæÁâá‰∏≠ÁöÑÂùóËøõË°åÁõÆÊ†áÊ£ÄÊµã„ÄÇ Region ProposalsÂÖ±Êúâ‰∏âÁßçÊñπÊ≥ïÔºö R-CNN: ÊªëÂä®Á™óÁöÑÂΩ¢ÂºèÔºå‰∏ÄÊ¨°Âè™ÂØπÂçï‰∏™Âå∫ÂüüÂùóËøõË°åÁõÆÊ†áÊ£ÄÊµãÔºåËøêÁÆóÈÄüÂ∫¶ÊÖ¢„ÄÇ Fast R-CNN: Âà©Áî®Âç∑ÁßØÂÆûÁé∞ÊªëÂä®Á™óÁÆóÊ≥ïÔºåÁ±ª‰ººÁ¨¨4ËäÇÂÅöÊ≥ï„ÄÇ Faster R-CNN: Âà©Áî®Âç∑ÁßØÂØπÂõæÁâáËøõË°åÂàÜÂâ≤ÔºåËøõ‰∏ÄÊ≠•ÊèêÈ´òËøêË°åÈÄüÂ∫¶„ÄÇ W4ÔºöSpecial applications: Face recognition &amp;Neural style transfer( ÁâπÊÆäÂ∫îÁî®Ôºö‰∫∫ËÑ∏ËØÜÂà´ÂíåÁ•ûÁªèÈ£éÊ†ºËΩ¨Êç¢)C1 Ôºö What is face recognition?È¶ñÂÖàÁÆÄÂçï‰ªãÁªç‰∏Ä‰∏ã‰∫∫ËÑ∏È™åËØÅÔºàface verificationÔºâÂíå‰∫∫ËÑ∏ËØÜÂà´Ôºàface recognitionÔºâÁöÑÂå∫Âà´„ÄÇ ‰∫∫ËÑ∏È™åËØÅÔºöËæìÂÖ•‰∏ÄÂº†‰∫∫ËÑ∏ÂõæÁâáÔºåÈ™åËØÅËæìÂá∫‰∏éÊ®°ÊùøÊòØÂê¶‰∏∫Âêå‰∏Ä‰∫∫ÔºåÂç≥‰∏ÄÂØπ‰∏ÄÈóÆÈ¢ò„ÄÇ ‰∫∫ËÑ∏ËØÜÂà´ÔºöËæìÂÖ•‰∏ÄÂº†‰∫∫ËÑ∏ÂõæÁâáÔºåÈ™åËØÅËæìÂá∫ÊòØÂê¶‰∏∫K‰∏™Ê®°Êùø‰∏≠ÁöÑÊüê‰∏Ä‰∏™ÔºåÂç≥‰∏ÄÂØπÂ§öÈóÆÈ¢ò„ÄÇ L2 Ôºö One-shot learningOne-shot learningÂ∞±ÊòØËØ¥Êï∞ÊçÆÂ∫ì‰∏≠ÊØè‰∏™‰∫∫ÁöÑËÆ≠ÁªÉÊ†∑Êú¨Âè™ÂåÖÂê´‰∏ÄÂº†ÁÖßÁâáÔºåÁÑ∂ÂêéËÆ≠ÁªÉ‰∏Ä‰∏™CNNÊ®°ÂûãÊù•ËøõË°å‰∫∫ËÑ∏ËØÜÂà´„ÄÇËã•Êï∞ÊçÆÂ∫ìÊúâK‰∏™‰∫∫ÔºåÂàôCNNÊ®°ÂûãËæìÂá∫softmaxÂ±ÇÂ∞±ÊòØKÁª¥ÁöÑ„ÄÇ ‰ΩÜÊòØOne-shot learningÁöÑÊÄßËÉΩÂπ∂‰∏çÂ•ΩÔºåÂÖ∂ÂåÖÂê´‰∫Ü‰∏§‰∏™Áº∫ÁÇπÔºö ÊØè‰∏™‰∫∫Âè™Êúâ‰∏ÄÂº†ÂõæÁâáÔºåËÆ≠ÁªÉÊ†∑Êú¨Â∞ëÔºåÊûÑÂª∫ÁöÑCNNÁΩëÁªú‰∏çÂ§üÂÅ•Â£Æ„ÄÇ Ëã•Êï∞ÊçÆÂ∫ìÂ¢ûÂä†Âè¶‰∏Ä‰∏™‰∫∫ÔºåËæìÂá∫Â±ÇsoftmaxÁöÑÁª¥Â∫¶Â∞±Ë¶ÅÂèëÁîüÂèòÂåñÔºåÁõ∏ÂΩì‰∫éË¶ÅÈáçÊñ∞ÊûÑÂª∫CNNÁΩëÁªúÔºå‰ΩøÊ®°ÂûãËÆ°ÁÆóÈáèÂ§ßÂ§ßÂ¢ûÂä†Ôºå‰∏çÂ§üÁÅµÊ¥ª„ÄÇ ‰∏∫‰∫ÜËß£ÂÜ≥One-shot learningÁöÑÈóÆÈ¢òÔºåÊàë‰ª¨ÂÖàÊù•‰ªãÁªçÁõ∏‰ººÂáΩÊï∞Ôºàsimilarity functionÔºâ„ÄÇÁõ∏‰ººÂáΩÊï∞Ë°®Á§∫‰∏§Âº†ÂõæÁâáÁöÑÁõ∏‰ººÁ®ãÂ∫¶ÔºåÁî®d(img1,img2)Êù•Ë°®Á§∫„ÄÇËã•d(img1,img2)ËæÉÂ∞èÔºåÂàôË°®Á§∫‰∏§Âº†ÂõæÁâáÁõ∏‰ººÔºõËã•d(img1,img2)ËæÉÂ§ßÔºåÂàôË°®Á§∫‰∏§Âº†ÂõæÁâá‰∏çÊòØÂêå‰∏Ä‰∏™‰∫∫„ÄÇÁõ∏‰ººÂáΩÊï∞ÂèØ‰ª•Âú®‰∫∫ËÑ∏È™åËØÅ‰∏≠‰ΩøÁî®Ôºö d(img1,img2)‚â§œÑ : ‰∏ÄÊ†∑ d(img1,img2)&gt;œÑ : ‰∏ç‰∏ÄÊ†∑ Áé∞Âú®‰Ω†Â∑≤ÁªèÁü•ÈÅìÂáΩÊï∞dÊòØÂ¶Ç‰ΩïÂ∑•‰ΩúÁöÑÔºåÈÄöËøáËæìÂÖ•‰∏§Âº†ÁÖßÁâáÔºåÂÆÉÂ∞ÜËÆ©‰Ω†ËÉΩÂ§üËß£ÂÜ≥‰∏ÄÊ¨°Â≠¶‰π†ÈóÆÈ¢ò„ÄÇÈÇ£‰πàÔºå‰∏ãËäÇËßÜÈ¢ë‰∏≠ÔºåÊàë‰ª¨Â∞Ü‰ºöÂ≠¶‰π†Â¶Ç‰ΩïËÆ≠ÁªÉ‰Ω†ÁöÑÁ•ûÁªèÁΩëÁªúÂ≠¶‰ºöËøô‰∏™ÂáΩÊï∞„ÄÇ L3: Siamese networkÊúÄÂêé‰∏ÄÂ±ÇÂéªÊéâsoftmaxÂçïÂÖÉÂÅöÂàÜÁ±ª Â¶ÇÊûú‰Ω†Ë¶ÅÊØîËæÉ‰∏§‰∏™ÂõæÁâáÁöÑËØùÔºå‰æãÂ¶ÇËøôÈáåÁöÑÁ¨¨‰∏ÄÂº†ÔºàÁºñÂè∑1ÔºâÂíåÁ¨¨‰∫åÂº†ÂõæÁâáÔºàÁºñÂè∑2ÔºâÔºå‰Ω†Ë¶ÅÂÅöÁöÑÂ∞±ÊòØÊääÁ¨¨‰∫åÂº†ÂõæÁâáÂñÇÁªôÊúâÂêåÊ†∑ÂèÇÊï∞ÁöÑÂêåÊ†∑ÁöÑÁ•ûÁªèÁΩëÁªúÔºåÁÑ∂ÂêéÂæóÂà∞‰∏Ä‰∏™‰∏çÂêåÁöÑ128Áª¥ÁöÑÂêëÈáèÔºàÁºñÂè∑3ÔºâÔºåËøô‰∏™ÂêëÈáè‰ª£Ë°®ÊàñËÄÖÁºñÁ†ÅÁ¨¨‰∫å‰∏™ÂõæÁâáÔºåÊàëË¶ÅÊääÁ¨¨‰∫åÂº†ÂõæÁâáÁöÑÁºñÁ†ÅÂè´ÂÅö$f(x^{(2)})$„ÄÇËøôÈáåÊàëÁî®$x^{(1)}$Âíå$x^{(2)}$‰ªÖ‰ªÖ‰ª£Ë°®‰∏§‰∏™ËæìÂÖ•ÂõæÁâá, d(x^{(1)},x^{(2)})=||f(x^{(1)}-f(x^{(2)}||^2‰∏çÂêåÁöÑÂõæÁâáÁöÑCNNÁΩëÁªúÁªìÊûÑÂíåÂèÇÊï∞ÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºåÁõÆÊ†áÂ∞±ÊòØÂà©Áî®Ê¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÔºåË∞ÉÊï¥ÁΩëÁªúÂèÇÊï∞]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Deep Learning Specialization]]></title>
    <url>%2F2019%2F05%2F05%2FDeep%20Learning%20ai_Deep%20Learning%20Specialization%2F</url>
    <content type="text"><![CDATA[C3 Improving Model Performance W1 ML Strategy(1)L01 Improving Model PerformanceÈúÄË¶ÅÊèêÈ´òËÆ≠ÁªÉÁªìÊûúÁöÑË°®Áé∞ÔºåË°®Áé∞ÂæóÊõ¥Â•ΩÁöÑÊé™ÊñΩ Machine Learning Strategy L2 : Orthogonalization(Ê≠£‰∫§Âåñ)ÊâÄË∞ìÊ≠£‰∫§ÔºåÂ∞±ÊòØ‰Ω†ÁöÑÊìçÊéßÊïàÊûúÂ∞ΩÈáèÂè™ÂΩ±Âìç‰∏Ä‰∏™ÊñπÈù¢„ÄÇÊØîÂ¶Ç‰ª•ËÄÅÂºèÁîµËßÜÊú∫‰∏∫‰æãÔºåË∞ÉËäÇÂõæÂÉèÁöÑÂ§ßÂ∞è„ÄÅÂ∑¶Âè≥ÂÅèÁßª„ÄÅ‰∏ä‰∏ãÂÅèÁßª„ÄÇËÄå‰∏çÊòØ‰∏Ä‰∏™ÊåâÈíÆÂèØ‰ª•ÂêåÊó∂Ë∞ÉËäÇÂõæÂÉèÂ§ßÂ∞èÂíåÂ∑¶Âè≥ÂÅèÁßªÔºåÈÇ£Ê†∑‰ºöÂæàÈöæÊìç‰Ωú„ÄÇ ÂÖ∑‰ΩìÂà∞supervised learningÔºåÊúâ‰ª•‰∏ã4‰∏™ÂÅáËÆæÊòØÊ≠£‰∫§ÁöÑÔºü Fit training set well in cost function If it doesn‚Äôt fit well, the use of a bigger neural network or switching to a better optimization algorithm might help. Fit development set well on cost function If it doesn‚Äôt fit well, regularization or using bigger training set might help. Fit test set well on cost function If it doesn‚Äôt fit well, the use of a bigger development set might help Performs well in real world If it doesn‚Äôt perform well, the development test set is not set correctly or the cost function is not evaluating the right thing. Âú®ËÆ≠ÁªÉÈõÜ‰∏äË°®Áé∞Ê¨†‰Ω≥ÔºåÈúÄË¶ÅÂàáÊç¢Âà∞Â•ΩÁöÑ‰ºòÂåñÁÆóÊ≥ï Âú®È™åËØÅÈõÜ‰∏äË°®Áé∞‰∏çÂ•ΩÔºå‰∏ÄÁªÑÊ≠£ÂàôÂåñÊåâÈíÆ Âú®ÊµãËØïÈõÜË°®Áé∞‰∏çÂ•ΩÔºåÈúÄË¶ÅÊõ¥Â•ΩÁöÑÈ™åËØÅÈõÜ Âú®Áî®Êà∑‰ΩìÈ™å‰∏çÂ•ΩÔºåÈúÄË¶ÅÊîπÂèòÊµãËØïÈõÜÂ§ßÂ∞èÊàñËÄÖÊàêÊú¨ÂáΩÊï∞ L3 Single number evaluation metric(Âçï‰∏ÄÊï∞Â≠óËØÑ‰º∞ÊåáÊ†á)classification Precesion ÔºàÊü•ÂáÜÁéáÔºâ recallÔºàÊü•ÂÖ®ÁéáÔºâ F 1=\frac{2}{\frac{1}{P}+\frac{1}{R}}=\frac{2 P R}{P+R}L4 Satisficing and optimizing metrics(Êª°Ë∂≥Âíå‰ºòÂåñÊåáÊ†á)Â¶ÇÊûúÊàë‰ª¨ËøòÊÉ≥Ë¶ÅÂ∞ÜÂàÜÁ±ªÂô®ÁöÑËøêË°åÊó∂Èó¥‰πüÁ∫≥ÂÖ•ËÄÉËôëËåÉÂõ¥ÔºåÂ∞ÜÂÖ∂ÂíåÁ≤æÁ°ÆÁéá„ÄÅÂè¨ÂõûÁéáÁªÑÂêàÊàê‰∏Ä‰∏™ÂçïÂÄºËØÑ‰ª∑ÊåáÊ†áÊòæÁÑ∂‰∏çÈÇ£‰πàÂêàÈÄÇ„ÄÇËøôÊó∂ÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜÊüê‰∫õÊåáÊ†á‰Ωú‰∏∫‰ºòÂåñÊåáÊ†áÔºàOptimizing MatricÔºâÔºåÂØªÊ±ÇÂÆÉ‰ª¨ÁöÑÊúÄ‰ºòÂÄºÔºõËÄåÂ∞ÜÊüê‰∫õÊåáÊ†á‰Ωú‰∏∫Êª°Ë∂≥ÊåáÊ†áÔºàSatisficing MatricÔºâÔºåÂè™Ë¶ÅÂú®‰∏ÄÂÆöÈòàÂÄº‰ª•ÂÜÖÂç≥ÂèØ„ÄÇ Âú®Ëøô‰∏™‰æãÂ≠ê‰∏≠ÔºåÂáÜÁ°ÆÁéáÂ∞±ÊòØ‰∏Ä‰∏™‰ºòÂåñÊåáÊ†áÔºåÂõ†‰∏∫Êàë‰ª¨ÊÉ≥Ë¶ÅÂàÜÁ±ªÂô®Â∞ΩÂèØËÉΩÂÅöÂà∞Ê≠£Á°ÆÂàÜÁ±ªÔºõËÄåËøêË°åÊó∂Èó¥Â∞±ÊòØ‰∏Ä‰∏™Êª°Ë∂≥ÊåáÊ†áÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÂàÜÁ±ªÂô®ÁöÑËøêË°åÊó∂Èó¥‰∏çÂ§ö‰∫éÊüê‰∏™ÈòàÂÄºÔºåÈÇ£ÊúÄÁªàÈÄâÊã©ÁöÑÂàÜÁ±ªÂô®Â∞±Â∫îËØ•ÊòØ‰ª•Ëøô‰∏™ÈòàÂÄº‰∏∫ÁïåÈáåÈù¢ÂáÜÁ°ÆÁéáÊúÄÈ´òÁöÑÈÇ£‰∏™„ÄÇ Â¶ÇÊ≠§ÔºåaccuracyÂ∞±ÂèòÊàê‰∫Üoptimizing metricÔºåËÄårunning timeÂàôÊòØsatisfying metricÔºåstatisfying metricÂè™Ë¶ÅËææÂà∞Ê†áÂáÜÂç≥ÂèØÔºåËÄåoptimizing metricÂàôËøΩÊ±ÇÊõ¥Â•Ω„ÄÇ‰∏ÄËà¨ÁöÑÔºåÈÄâÊã©‰∏ÄÈ°πmetric‰Ωú‰∏∫optimizing metricÔºåÂÖ∂‰ªñÁöÑÂàôËÆæÁΩÆ‰∏∫satisfying metricÔºö L 5: Train/dev/test distributions(ËÆ≠ÁªÉ/ÂºÄÂèë/ÊµãËØïÈõÜÂàíÂàÜ)ÂºÄÂèëÔºàdevÔºâÈõÜ‰πüÂè´ÂÅöÂºÄÂèëÈõÜÔºàdevelopment setÔºâÔºåÊúâÊó∂Áß∞‰∏∫‰øùÁïô‰∫§ÂèâÈ™åËØÅÈõÜÔºàhold out cross validation setÔºâ„ÄÇ Â¶Ç‰ΩïËÆæÁΩÆTrain/dev/testÈõÜÔºåÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂΩ±Âìç‰∫ÜÊú∫Âô®Â≠¶‰π†ÁöÑÈÄüÂ∫¶„ÄÇ Train/dev/testÁöÑÂå∫Âà´ Workflow in machine learning is that you try a lot of ideas, train up different models on the training set, and then use the dev set to evaluate the different ideas and pick one. And, keep innovating to improve dev set performance until, finally, you have one class that you‚Äôre happy with that you then evaluate on your test set. ÂºÄÂèëÈõÜÂêàÂíåÂºÄÂèëÈõÜÂêàÊù•Ëá™Âêå‰∏ÄÂàÜÂ∏ÉÔºåÂ¶ÇÊûúÊòØ‰∏çÂêåÂàÜÂ∏ÉÔºåÁõ∏ÂΩì‰∫éÈù∂ÂøÉÁßªÂä®‰∫Ü L 6: Size of dev and test sets(ÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜÁöÑÂ§ßÂ∞è) L7 : When to change dev/test sets and metrics(‰ªÄ‰πàÊó∂ÂÄôËØ•ÊîπÂèòÂºÄÂèë/ÊµãËØïÈõÜÂíåÊåáÊ†á)Â¶ÇÊûúÂèëÁé∞ËÆæÂÆöÁõÆÊ†áÂíåÂÆûÈôÖÊúüÊúõ‰∏çÁ¨¶ÔºåÈÇ£Â∞±Ë∞ÉÊï¥ÁõÆÊ†á„ÄÇ ‰∏æ‰∏™‰æãÂ≠ê AÂèØËÉΩÊää‰∏Ä‰∫õËâ≤ÊÉÖÁÖßÁâá‰πüÂàÜÁ±ªÊàêÁå´‰∫ÜÔºåÂõ†Ê≠§ÊîπÂèò‰ºòÂåñÊåáÊ†á ÊàëÊÉ≥‰Ω†Â§ÑÁêÜÊú∫Âô®Â≠¶‰π†ÈóÆÈ¢òÊó∂ÔºåÂ∫îËØ•ÊääÂÆÉÂàáÂàÜÊàêÁã¨Á´ãÁöÑÊ≠•È™§„ÄÇ‰∏ÄÊ≠•ÊòØÂºÑÊ∏ÖÊ•öÂ¶Ç‰ΩïÂÆö‰πâ‰∏Ä‰∏™ÊåáÊ†áÊù•Ë°°Èáè‰Ω†ÊÉ≥ÂÅöÁöÑ‰∫ãÊÉÖÁöÑË°®Áé∞ÔºåÁÑ∂ÂêéÊàë‰ª¨ÂèØ‰ª•ÂàÜÂºÄËÄÉËôëÂ¶Ç‰ΩïÊîπÂñÑÁ≥ªÁªüÂú®Ëøô‰∏™ÊåáÊ†á‰∏äÁöÑË°®Áé∞„ÄÇ‰Ω†‰ª¨Ë¶ÅÊääÊú∫Âô®Â≠¶‰π†‰ªªÂä°ÁúãÊàê‰∏§‰∏™Áã¨Á´ãÁöÑÊ≠•È™§ÔºåÁî®ÁõÆÊ†áËøô‰∏™ÊØîÂñªÔºåÁ¨¨‰∏ÄÊ≠•Â∞±ÊòØËÆæÂÆöÁõÆÊ†á„ÄÇÊâÄ‰ª•Ë¶ÅÂÆö‰πâ‰Ω†Ë¶ÅÁûÑÂáÜÁöÑÁõÆÊ†áÔºåËøôÊòØÂÆåÂÖ®Áã¨Á´ãÁöÑ‰∏ÄÊ≠•ÔºåËøôÊòØ‰Ω†ÂèØ‰ª•Ë∞ÉËäÇÁöÑ‰∏Ä‰∏™ÊóãÈíÆ„ÄÇÂ¶Ç‰ΩïËÆæÁ´ãÁõÆÊ†áÊòØ‰∏Ä‰∏™ÂÆåÂÖ®Áã¨Á´ãÁöÑÈóÆÈ¢òÔºåÊääÂÆÉÁúãÊàêÊòØ‰∏Ä‰∏™ÂçïÁã¨ÁöÑÊóãÈíÆÔºåÂèØ‰ª•Ë∞ÉËØïÁÆóÊ≥ïË°®Áé∞ÁöÑÊóãÈíÆÔºåÂ¶Ç‰ΩïÁ≤æÁ°ÆÁûÑÂáÜÔºåÂ¶Ç‰ΩïÂëΩ‰∏≠ÁõÆÊ†áÔºåÂÆö‰πâÊåáÊ†áÊòØÁ¨¨‰∏ÄÊ≠•„ÄÇ ÂêéÁ¨¨‰∫åÊ≠•Ë¶ÅÂÅöÂà´ÁöÑ‰∫ãÊÉÖÔºåÂú®ÈÄºËøëÁõÆÊ†áÁöÑÊó∂ÂÄôÔºå‰πüËÆ∏‰Ω†ÁöÑÂ≠¶‰π†ÁÆóÊ≥ïÈíàÂØπÊüê‰∏™ÈïøËøôÊ†∑ÁöÑÊàêÊú¨ÂáΩÊï∞‰ºòÂåñÔºå$J=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)$‰Ω†Ë¶ÅÊúÄÂ∞èÂåñËÆ≠ÁªÉÈõÜ‰∏äÁöÑÊçüÂ§±„ÄÇ‰Ω†ÂèØ‰ª•ÂÅöÁöÑÂÖ∂‰∏≠‰∏Ä‰ª∂‰∫ãÊòØÔºå‰øÆÊîπËøô‰∏™Ôºå‰∏∫‰∫ÜÂºïÂÖ•Ëøô‰∫õÊùÉÈáçÔºå‰πüËÆ∏ÊúÄÂêéÈúÄË¶Å‰øÆÊîπËøô‰∏™ÂΩí‰∏ÄÂåñÂ∏∏Êï∞Ôºå$J=\frac{1}{\sum w^{(i)}} \sum_{i=1}^{m} w^{(i)} L\left(\hat{y}^{(i)}, y^{(i)}\right)$ ÂÜçÊ¨°ÔºåÂ¶Ç‰ΩïÂÆö‰πâJÂπ∂‰∏çÈáçË¶ÅÔºåÂÖ≥ÈîÆÂú®‰∫éÊ≠£‰∫§ÂåñÁöÑÊÄùË∑ØÔºåÊääËÆæÁ´ãÁõÆÊ†áÂÆö‰∏∫Á¨¨‰∏ÄÊ≠•ÔºåÁÑ∂ÂêéÁûÑÂáÜÂíåÂ∞ÑÂáªÁõÆÊ†áÊòØÁã¨Á´ãÁöÑÁ¨¨‰∫åÊ≠•„ÄÇÊç¢ÁßçËØ¥Ê≥ïÔºåÊàëÈºìÂä±‰Ω†‰ª¨Â∞ÜÂÆö‰πâÊåáÊ†áÁúãÊàê‰∏ÄÊ≠•ÔºåÁÑ∂ÂêéÂú®ÂÆö‰πâ‰∫ÜÊåáÊ†á‰πãÂêéÔºå‰Ω†ÊâçËÉΩÊÉ≥Â¶Ç‰Ωï‰ºòÂåñÁ≥ªÁªüÊù•ÊèêÈ´òËøô‰∏™ÊåáÊ†áËØÑÂàÜ„ÄÇÊØîÂ¶ÇÊîπÂèò‰Ω†Á•ûÁªèÁΩëÁªúË¶Å‰ºòÂåñÁöÑÊàêÊú¨ÂáΩÊï∞J„ÄÇ L8 : Why human-level performance?(‰∏∫‰ªÄ‰πàÊòØ‰∫∫ÁöÑË°®Áé∞Ôºü) ‰∏äÂõæÂ±ïÁ§∫‰∫ÜÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ËøõÔºåÊú∫Âô®Â≠¶‰π†Á≥ªÁªüÂíå‰∫∫ÁöÑË°®Áé∞Ê∞¥Âπ≥ÁöÑÂèòÂåñ„ÄÇ‰∏ÄËà¨ÁöÑÔºåÂΩìÊú∫Âô®Â≠¶‰π†Ë∂ÖËøá‰∫∫ÁöÑË°®Áé∞Ê∞¥Âπ≥ÂêéÔºåÂÆÉÁöÑËøõÊ≠•ÈÄüÂ∫¶ÈÄêÊ∏êÂèòÂæóÁºìÊÖ¢ÔºåÊúÄÁªàÊÄßËÉΩÊó†Ê≥ïË∂ÖËøáÊüê‰∏™ÁêÜËÆ∫‰∏äÈôêÔºåËøô‰∏™‰∏äÈôêË¢´Áß∞‰∏∫Ë¥ùÂè∂ÊñØÊúÄ‰ºòËØØÂ∑ÆÔºàBayes Optimal ErrorÔºâ„ÄÇ ‰πüÂõ†Ê≠§ÔºåÂè™Ë¶ÅÂª∫Á´ãÁöÑÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑË°®Áé∞ËøòÊ≤°ËææÂà∞‰∫∫Á±ªÁöÑË°®Áé∞Ê∞¥Âπ≥Êó∂ÔºåÂ∞±ÂèØ‰ª•ÈÄöËøáÂêÑÁßçÊâãÊÆµÊù•ÊèêÂçáÂÆÉ„ÄÇ‰æãÂ¶ÇÈááÁî®‰∫∫Â∑•Ê†áËÆ∞ËøáÁöÑÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÈÄöËøá‰∫∫Â∑•ËØØÂ∑ÆÂàÜÊûê‰∫ÜËß£‰∏∫‰ªÄ‰πà‰∫∫ËÉΩÂ§üÊ≠£Á°ÆËØÜÂà´ÔºåÊàñËÄÖÊòØËøõË°åÂÅèÂ∑Æ„ÄÅÊñπÂ∑ÆÂàÜÊûê„ÄÇ ÂΩìÊ®°ÂûãÁöÑË°®Áé∞Ë∂ÖËøá‰∫∫Á±ªÂêéÔºåËøô‰∫õÊâãÊÆµËµ∑ÁöÑ‰ΩúÁî®Â∞±ÂæÆ‰πéÂÖ∂ÂæÆ‰∫Ü„ÄÇ L9 : Avoidable bias(ÂèØÈÅøÂÖçÂÅèÂ∑Æ) training error Êàë‰ª¨ÁªèÂ∏∏‰ΩøÁî®Áå´ÂàÜÁ±ªÂô®Êù•ÂÅö‰æãÂ≠êÔºåÊØîÂ¶Ç‰∫∫Á±ªÂÖ∑ÊúâËøë‰πéÂÆåÁæéÁöÑÂáÜÁ°ÆÂ∫¶ÔºåÊâÄ‰ª•‰∫∫Á±ªÊ∞¥Âπ≥ÁöÑÈîôËØØÊòØ1%„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂ¶ÇÊûúÊÇ®ÁöÑÂ≠¶‰π†ÁÆóÊ≥ïËææÂà∞8%ÁöÑËÆ≠ÁªÉÈîôËØØÁéáÂíå10%ÁöÑÂºÄÂèëÈîôËØØÁéáÔºåÈÇ£‰πà‰Ω†‰πüËÆ∏ÊÉ≥Âú®ËÆ≠ÁªÉÈõÜ‰∏äÂæóÂà∞Êõ¥Â•ΩÁöÑÁªìÊûú„ÄÇÊâÄ‰ª•‰∫ãÂÆû‰∏äÔºå‰Ω†ÁöÑÁÆóÊ≥ïÂú®ËÆ≠ÁªÉÈõÜ‰∏äÁöÑË°®Áé∞Âíå‰∫∫Á±ªÊ∞¥Âπ≥ÁöÑË°®Áé∞ÊúâÂæàÂ§ßÂ∑ÆË∑ùÁöÑËØùÔºåËØ¥Êòé‰Ω†ÁöÑÁÆóÊ≥ïÂØπËÆ≠ÁªÉÈõÜÁöÑÊãüÂêàÂπ∂‰∏çÂ•Ω„ÄÇÊâÄ‰ª•‰ªéÂáèÂ∞ëÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÁöÑÂ∑•ÂÖ∑Ëøô‰∏™ËßíÂ∫¶ÁúãÔºåÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÊàë‰ºöÊääÈáçÁÇπÊîæÂú®ÂáèÂ∞ëÂÅèÂ∑Æ‰∏ä„ÄÇ‰Ω†ÈúÄË¶ÅÂÅöÁöÑÊòØÔºåÊØîÂ¶ÇËØ¥ËÆ≠ÁªÉÊõ¥Â§ßÁöÑÁ•ûÁªèÁΩëÁªúÔºåÊàñËÄÖË∑ë‰πÖ‰∏ÄÁÇπÊ¢ØÂ∫¶‰∏ãÈôçÔºåÂ∞±ËØïËØïËÉΩ‰∏çËÉΩÂú®ËÆ≠ÁªÉÈõÜ‰∏äÂÅöÂæóÊõ¥Â•Ω„ÄÇ dev error Ë¥ùÂè∂ÊñØÈîôËØØÁéáÊàñËÄÖÂØπË¥ùÂè∂ÊñØÈîôËØØÁéáÁöÑ‰º∞ËÆ°ÂíåËÆ≠ÁªÉÈîôËØØÁéá‰πãÈó¥ÁöÑÂ∑ÆÂÄºÁß∞‰∏∫ÂèØÈÅøÂÖçÂÅèÂ∑Æ L 10: Understanding human-level performance(ÁêÜËß£‰∫∫ÁöÑË°®Áé∞)ËøòËÆ∞Âæó‰∏ä‰∏™ËßÜÈ¢ë‰∏≠ÔºåÊàë‰ª¨Áî®ËøáËøô‰∏™ËØç‚Äú‰∫∫Á±ªÊ∞¥Âπ≥ÈîôËØØÁéá‚ÄùÁî®Êù•‰º∞ËÆ°Ë¥ùÂè∂ÊñØËØØÂ∑ÆÔºåÈÇ£Â∞±ÊòØÁêÜËÆ∫ÊúÄ‰ΩéÁöÑÈîôËØØÁéáÔºå‰ªª‰ΩïÂáΩÊï∞‰∏çÁÆ°ÊòØÁé∞Âú®ËøòÊòØÂ∞ÜÊù•ÔºåËÉΩÂ§üÂà∞ËææÁöÑÊúÄ‰ΩéÂÄº L11 : Surpassing human- level performance(Ë∂ÖËøá‰∫∫ÁöÑË°®Áé∞)Áé∞Âú®ÔºåÊú∫Âô®Â≠¶‰π†ÊúâÂæàÂ§öÈóÆÈ¢òÂ∑≤ÁªèÂèØ‰ª•Â§ßÂ§ßË∂ÖË∂ä‰∫∫Á±ªÊ∞¥Âπ≥‰∫Ü„ÄÇ L12 : Improving your model performance(ÊîπÂñÑ‰Ω†ÁöÑÊ®°ÂûãÁöÑË°®Áé∞)‰Ω†‰ª¨Â≠¶ËøáÊ≠£‰∫§ÂåñÔºåÂ¶Ç‰ΩïËÆæÁ´ãÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜÔºåÁî®‰∫∫Á±ªÊ∞¥Âπ≥ÈîôËØØÁéáÊù•‰º∞ËÆ°Ë¥ùÂè∂ÊñØÈîôËØØÁéá‰ª•ÂèäÂ¶Ç‰Ωï‰º∞ËÆ°ÂèØÈÅøÂÖçÂÅèÂ∑ÆÂíåÊñπÂ∑Æ„ÄÇÊàë‰ª¨Áé∞Âú®ÊääÂÆÉ‰ª¨ÂÖ®ÈÉ®ÁªÑÂêàËµ∑Êù•ÂÜôÊàê‰∏ÄÂ•óÊåáÂØºÊñπÈíàÔºåÂ¶Ç‰ΩïÊèêÈ´òÂ≠¶‰π†ÁÆóÊ≥ïÊÄßËÉΩÁöÑÊåáÂØºÊñπÈíà„ÄÇ method summaryËøô‰∏ÄÂë®ÁöÑÂÜÖÂÆπ‰∏ªË¶ÅÊòØÊîπÂñÑÊ®°ÂûãÁöÑË°®Áé∞Ôºå‰∏ªË¶ÅÊòØÊåâÁÖßÊ≠£‰∫§ÂåñÔºå‰ΩøÂæóÊõ¥Â•ΩÁöÑÊª°Ë∂≥ 1. ËØÑ‰ª∑ÊåáÊ†á 2. Êï∞ÊçÆÈõÜÁöÑÂàíÂàÜ 3. ‰∫∫ÁöÑË°®Áé∞ÁöÑÈáçË¶ÅÊÄß 4. ÂΩìÂá∫Áé∞Ë°®Áé∞‰∏çÂ•ΩÁöÑÊó∂ÂÄôÔºåÂ¶Ç‰ΩïÊîπÂñÑÂë¢ÔºåÊúâÂì™‰∫õÊñπÊ≥ïÂë¢Ôºü W2 ML Strategy(2)C 1: Carrying out error analysis(ËøõË°åËØØÂ∑ÆÂàÜÊûê)1. simple analysis ÈÄöËøáËßÇÂØüÂèëÁé∞ÁÆóÊ≥ïÂàÜÁ±ªÂá∫ÈîôÁöÑ‰æãÂ≠êÔºåÊòØÊääÁãóÂàÜÊàêÁå´ÔºåÊèêÈ´òÂáÜÁ°ÆÁéáÁöÑÊñπÊ≥ïÂ∞±ÊòØÂ¶Ç‰ΩïÈíàÂØπÁãóÁöÑÂõæÁâá‰ºòÂåñÁÆóÊ≥ï„ÄÇ‰Ω†ÂèØ‰ª•ÈíàÂØπÁãóÔºåÊî∂ÈõÜÊõ¥Â§öÁöÑÁãóÂõæÔºåÊàñËÄÖËÆæËÆ°‰∏Ä‰∫õÂè™Â§ÑÁêÜÁãóÁöÑÁÆóÊ≥ïÂäüËÉΩ‰πãÁ±ªÁöÑÔºå‰∏∫‰∫ÜËÆ©‰Ω†ÁöÑÁå´ÂàÜÁ±ªÂô®Âú®ÁãóÂõæ‰∏äÂÅöÁöÑÊõ¥Â•ΩÔºåËÆ©ÁÆóÊ≥ï‰∏çÂÜçÂ∞ÜÁãóÂàÜÁ±ªÊàêÁå´„ÄÇÁé∞Âú®ËÄÉËôëÁöÑÊòØÂ∫îËØ•‰∏çÂ∫îËØ•Ëøô‰πàÂéªÂÅöÂë¢ÔºüÁªüËÆ°‰∏Ä‰∏ãdev setÈáåÈù¢Â§öÂ∞ëÊòØÈîôËØØÊ†áËÆ∞ÊòØÁãóÁöÑ‰∏™Êï∞ÔºåÂàÜÊûêÂá∫ÂèØ‰ª•ÊîπÂñÑÁöÑÁÆóÊ≥ïÁöÑ‰∏äÈôê„ÄÇ mutiply analysis C2 : Cleaning up Incorrectly labeled data(Ê∏ÖÈô§Ê†áÊ≥®ÈîôËØØÁöÑÊï∞ÊçÆ)incorrct labeltraning setDL algorithms are quite robust to random errors in the traning set so long as your errors or your labeled example to once those errors are not too far from random . distributionÈ¶ñÂÖàÔºåÊàëÈºìÂä±‰Ω†‰∏çÁÆ°Áî®‰ªÄ‰πà‰øÆÊ≠£ÊâãÊÆµÔºåÈÉΩË¶ÅÂêåÊó∂‰ΩúÁî®Âà∞ÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜ‰∏äÔºåÊàë‰ª¨‰πãÂâçËÆ®ËÆ∫Ëøá‰∏∫‰ªÄ‰πàÔºåÂºÄÂèëÂíåÊµãËØïÈõÜÂøÖÈ°ªÊù•Ëá™Áõ∏ÂêåÁöÑÂàÜÂ∏É„ÄÇÂºÄÂèëÈõÜÁ°ÆÂÆö‰∫Ü‰Ω†ÁöÑÁõÆÊ†áÔºåÂΩì‰Ω†Âáª‰∏≠ÁõÆÊ†áÂêéÔºå‰Ω†Â∏åÊúõÁÆóÊ≥ïËÉΩÂ§üÊé®ÂπøÂà∞ÊµãËØïÈõÜ‰∏äÔºåËøôÊ†∑‰Ω†ÁöÑÂõ¢ÈòüËÉΩÂ§üÊõ¥È´òÊïàÁöÑÂú®Êù•Ëá™Âêå‰∏ÄÂàÜÂ∏ÉÁöÑÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜ‰∏äËø≠‰ª£„ÄÇÂ¶ÇÊûú‰Ω†ÊâìÁÆó‰øÆÊ≠£ÂºÄÂèëÈõÜ‰∏äÁöÑÈÉ®ÂàÜÊï∞ÊçÆÔºåÈÇ£‰πàÊúÄÂ•Ω‰πüÂØπÊµãËØïÈõÜÂÅöÂêåÊ†∑ÁöÑ‰øÆÊ≠£‰ª•Á°Æ‰øùÂÆÉ‰ª¨ÁªßÁª≠Êù•Ëá™Áõ∏ÂêåÁöÑÂàÜÂ∏É„ÄÇÊâÄ‰ª•Êàë‰ª¨Èõá‰Ω£‰∫Ü‰∏Ä‰∏™‰∫∫Êù•‰ªîÁªÜÊ£ÄÊü•Ëøô‰∫õÊ†áÁ≠æÔºå‰ΩÜÂøÖÈ°ªÂêåÊó∂Ê£ÄÊü•ÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜ„ÄÇ suggestionÊúÄÂêéÊàëËÆ≤Âá†‰∏™Âª∫ËÆÆÔºö È¶ñÂÖàÔºåÊ∑±Â∫¶Â≠¶‰π†Á†îÁ©∂‰∫∫ÂëòÊúâÊó∂‰ºöÂñúÊ¨¢ËøôÊ†∑ËØ¥Ôºö‚ÄúÊàëÂè™ÊòØÊääÊï∞ÊçÆÊèê‰æõÁªôÁÆóÊ≥ïÔºåÊàëËÆ≠ÁªÉËøá‰∫ÜÔºåÊïàÊûúÊãîÁæ§‚Äù„ÄÇËøôËØùËØ¥Âá∫‰∫ÜÂæàÂ§öÊ∑±Â∫¶Â≠¶‰π†ÈîôËØØÁöÑÁúüÁõ∏ÔºåÊõ¥Â§öÊó∂ÂÄôÔºåÊàë‰ª¨ÊääÊï∞ÊçÆÂñÇÁªôÁÆóÊ≥ïÔºåÁÑ∂ÂêéËÆ≠ÁªÉÂÆÉÔºåÂπ∂ÂáèÂ∞ë‰∫∫Â∑•Âπ≤È¢ÑÔºåÂáèÂ∞ë‰ΩøÁî®‰∫∫Á±ªÁöÑËßÅËß£„ÄÇ‰ΩÜÊàëËÆ§‰∏∫ÔºåÂú®ÊûÑÈÄ†ÂÆûÈôÖÁ≥ªÁªüÊó∂ÔºåÈÄöÂ∏∏ÈúÄË¶ÅÊõ¥Â§öÁöÑ‰∫∫Â∑•ÈîôËØØÂàÜÊûêÔºåÊõ¥Â§öÁöÑ‰∫∫Á±ªËßÅËß£Êù•Êû∂ÊûÑËøô‰∫õÁ≥ªÁªüÔºåÂ∞ΩÁÆ°Ê∑±Â∫¶Â≠¶‰π†ÁöÑÁ†îÁ©∂‰∫∫Âëò‰∏çÊÑøÊÑèÊâøËÆ§ËøôÁÇπ„ÄÇ ÂÖ∂Ê¨°Ôºå‰∏çÁü•ÈÅì‰∏∫‰ªÄ‰πàÔºåÊàëÁúã‰∏Ä‰∫õÂ∑•Á®ãÂ∏àÂíåÁ†îÁ©∂‰∫∫Âëò‰∏çÊÑøÊÑè‰∫≤Ëá™ÂéªÁúãËøô‰∫õÊ†∑Êú¨Ôºå‰πüËÆ∏ÂÅöËøô‰∫õ‰∫ãÊÉÖÂæàÊó†ËÅäÔºåÂùê‰∏ãÊù•Áúã100ÊàñÂá†Áôæ‰∏™Ê†∑Êú¨Êù•ÁªüËÆ°ÈîôËØØÊï∞ÈáèÔºå‰ΩÜÊàëÁªèÂ∏∏‰∫≤Ëá™Ëøô‰πàÂÅö„ÄÇÂΩìÊàëÂ∏¶È¢Ü‰∏Ä‰∏™Êú∫Âô®Â≠¶‰π†Âõ¢ÈòüÊó∂ÔºåÊàëÊÉ≥Áü•ÈÅìÂÆÉÊâÄÁäØÁöÑÈîôËØØÔºåÊàë‰ºö‰∫≤Ëá™ÂéªÁúãÁúãËøô‰∫õÊï∞ÊçÆÔºåÂ∞ùËØïÂíå‰∏ÄÈÉ®ÂàÜÈîôËØØ‰ΩúÊñó‰∫â„ÄÇÊàëÊÉ≥Â∞±Âõ†‰∏∫Ëä±‰∫ÜËøôÂá†ÂàÜÈíüÔºåÊàñËÄÖÂá†‰∏™Â∞èÊó∂Âéª‰∫≤Ëá™ÁªüËÆ°Êï∞ÊçÆÔºåÁúüÁöÑÂèØ‰ª•Â∏Æ‰Ω†ÊâæÂà∞ÈúÄË¶Å‰ºòÂÖàÂ§ÑÁêÜÁöÑ‰ªªÂä°ÔºåÊàëÂèëÁé∞Ëä±Êó∂Èó¥‰∫≤Ëá™Ê£ÄÊü•Êï∞ÊçÆÈùûÂ∏∏ÂÄºÂæóÔºåÊâÄ‰ª•ÊàëÂº∫ÁÉàÂª∫ËÆÆ‰Ω†‰ª¨ËøôÊ†∑ÂÅöÔºåÂ¶ÇÊûú‰Ω†Âú®Êê≠Âª∫‰Ω†ÁöÑÊú∫Âô®Â≠¶‰π†Á≥ªÁªüÁöÑËØùÔºåÁÑ∂Âêé‰Ω†ÊÉ≥Á°ÆÂÆöÂ∫îËØ•‰ºòÂÖàÂ∞ùËØïÂì™‰∫õÊÉ≥Ê≥ïÔºåÊàñËÄÖÂì™‰∫õÊñπÂêë„ÄÇ ËøôÂ∞±ÊòØÈîôËØØÂàÜÊûêËøáÁ®ãÔºåÂú®‰∏ã‰∏Ä‰∏™ËßÜÈ¢ë‰∏≠ÔºåÊàëÊÉ≥ÂàÜ‰∫´‰∏Ä‰∏ãÈîôËØØÂàÜÊûêÊòØÂ¶Ç‰ΩïÂú®ÂêØÂä®Êñ∞ÁöÑÊú∫Âô®Â≠¶‰π†È°πÁõÆ‰∏≠ÂèëÊå•‰ΩúÁî®ÁöÑ„ÄÇ C3: Build your first system quickly, then iterate(Âø´ÈÄüÊê≠Âª∫‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™Á≥ªÁªüÔºåÂπ∂ËøõË°åËø≠‰ª£)1. iterationI recommend that you first quickly set up a definition and metrics so this is really you know deciding where to place your target and you get it wrong you can always move it later we just set up a target somewhere and then I recommend you build an inital machine learning system quickly find the traning set train it and see start to see and understand how well your are doing against your Devon chess setting evaluation metric when you build your initial system you then be able to use bias variance analysis we should talk about earlier as well as error analysis whick we talked about just in last several videos to prioritize the next step in particular if error analysis causes you to realize that a lot of the errors are from the spearker being very far from the mirophone which causes special challenges speech recognitin then that would give you a good reason to focus on techniques to address this it called fast used speech recognition which basically means handling when the speaker is very far from microphone along the value of building this inital system it can be a quick and diry implementation you know do not overthink it but all the value of the inital system is having some learning system having some tranin system allows you lok at bias and variance to do error analysis look at some mistakes to figure out all the different directins you could go in. ÊàëÈºìÂä±‰Ω†‰ª¨Êê≠Âª∫Âø´ÈÄüËÄåÁ≤óÁ≥ôÁöÑÂÆûÁé∞ÔºåÁÑ∂ÂêéÁî®ÂÆÉÂÅöÂÅèÂ∑Æ/ÊñπÂ∑ÆÂàÜÊûêÔºåÁî®ÂÆÉÂÅöÈîôËØØÂàÜÊûêÔºåÁÑ∂ÂêéÁî®ÂàÜÊûêÁªìÊûúÁ°ÆÂÆö‰∏ã‰∏ÄÊ≠•‰ºòÂÖàË¶ÅÂÅöÁöÑÊñπÂêë„ÄÇ C4 : Training and testing on different distributions(‰ΩøÁî®Êù•Ëá™‰∏çÂêåÂàÜÂ∏ÉÁöÑÊï∞ÊçÆÔºåËøõË°åËÆ≠ÁªÉÂíåÊµãËØï)this is resulted in many teams sometimes taking one of the days you can find and just shoving it into the training set . Cat app example ÂÅáËÆæ‰Ω†Âú®ÂºÄÂèë‰∏Ä‰∏™ÊâãÊú∫Â∫îÁî®ÔºåÁî®Êà∑‰ºö‰∏ä‰º†‰ªñ‰ª¨Áî®ÊâãÊú∫ÊãçÊëÑÁöÑÁÖßÁâáÔºå‰Ω†ÊÉ≥ËØÜÂà´Áî®Êà∑‰ªéÂ∫îÁî®‰∏≠‰∏ä‰º†ÁöÑÂõæÁâáÊòØ‰∏çÊòØÁå´„ÄÇÁé∞Âú®‰Ω†Êúâ‰∏§‰∏™Êï∞ÊçÆÊù•Ê∫êÔºå‰∏Ä‰∏™ÊòØ‰Ω†ÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÔºåÊù•Ëá™Â∫îÁî®‰∏ä‰º†ÁöÑÊï∞ÊçÆÔºåÊØîÂ¶ÇÂè≥ËæπÁöÑÂ∫îÁî®ÔºåËøô‰∫õÁÖßÁâá‰∏ÄËà¨Êõ¥‰∏ö‰ΩôÔºåÂèñÊôØ‰∏çÂ§™Â•ΩÔºåÊúâ‰∫õÁîöËá≥ÂæàÊ®°Á≥äÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÈÉΩÊòØ‰∏ö‰ΩôÁî®Êà∑ÊãçÁöÑ„ÄÇÂè¶‰∏Ä‰∏™Êï∞ÊçÆÊù•Ê∫êÂ∞±ÊòØ‰Ω†ÂèØ‰ª•Áî®Áà¨Ëô´Á®ãÂ∫èÊåñÊéòÁΩëÈ°µÁõ¥Êé•‰∏ãËΩΩÔºåÂ∞±Ëøô‰∏™Ê†∑Êú¨ËÄåË®ÄÔºåÂèØ‰ª•‰∏ãËΩΩÂæàÂ§öÂèñÊôØ‰∏ì‰∏ö„ÄÅÈ´òÂàÜËæ®Áéá„ÄÅÊãçÊëÑ‰∏ì‰∏öÁöÑÁå´ÂõæÁâá„ÄÇÂ¶ÇÊûú‰Ω†ÁöÑÂ∫îÁî®Áî®Êà∑Êï∞Ëøò‰∏çÂ§öÔºå‰πüËÆ∏‰Ω†Âè™Êî∂ÈõÜÂà∞10,000Âº†Áî®Êà∑‰∏ä‰º†ÁöÑÁÖßÁâáÔºå‰ΩÜÈÄöËøáÁà¨Ëô´ÊåñÊéòÁΩëÈ°µÔºå‰Ω†ÂèØ‰ª•‰∏ãËΩΩÂà∞Êµ∑ÈáèÁå´ÂõæÔºå‰πüËÆ∏‰Ω†‰ªé‰∫íËÅîÁΩë‰∏ä‰∏ãËΩΩ‰∫ÜË∂ÖËøá20‰∏áÂº†Áå´Âõæ„ÄÇËÄå‰Ω†ÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÁÆóÊ≥ïË°®Áé∞ÊòØ‰Ω†ÁöÑÊúÄÁªàÁ≥ªÁªüÂ§ÑÁêÜÊù•Ëá™Â∫îÁî®Á®ãÂ∫èÁöÑËøô‰∏™ÂõæÁâáÂàÜÂ∏ÉÊó∂ÊïàÊûúÂ•Ω‰∏çÂ•ΩÔºåÂõ†‰∏∫ÊúÄÂêé‰Ω†ÁöÑÁî®Êà∑‰ºö‰∏ä‰º†Á±ª‰ººÂè≥ËæπËøô‰∫õÂõæÁâáÔºå‰Ω†ÁöÑÂàÜÁ±ªÂô®ÂøÖÈ°ªÂú®Ëøô‰∏™‰ªªÂä°‰∏≠Ë°®Áé∞ËâØÂ•Ω„ÄÇÁé∞Âú®‰Ω†Â∞±Èô∑ÂÖ•Âõ∞Â¢É‰∫ÜÔºåÂõ†‰∏∫‰Ω†Êúâ‰∏Ä‰∏™Áõ∏ÂØπÂ∞èÁöÑÊï∞ÊçÆÈõÜÔºåÂè™Êúâ10,000‰∏™Ê†∑Êú¨Êù•Ëá™ÈÇ£‰∏™ÂàÜÂ∏ÉÔºåËÄå‰Ω†ËøòÊúâ‰∏Ä‰∏™Â§ßÂæóÂ§öÁöÑÊï∞ÊçÆÈõÜÊù•Ëá™Âè¶‰∏Ä‰∏™ÂàÜÂ∏ÉÔºåÂõæÁâáÁöÑÂ§ñËßÇÂíå‰Ω†ÁúüÊ≠£ÊÉ≥Ë¶ÅÂ§ÑÁêÜÁöÑÂπ∂‰∏ç‰∏ÄÊ†∑„ÄÇ‰ΩÜ‰Ω†Âèà‰∏çÊÉ≥Áõ¥Êé•Áî®Ëøô10,000Âº†ÂõæÁâáÔºåÂõ†‰∏∫ËøôÊ†∑‰Ω†ÁöÑËÆ≠ÁªÉÈõÜÂ∞±Â§™Â∞è‰∫ÜÔºå‰ΩøÁî®Ëøô20‰∏áÂº†ÂõæÁâá‰ºº‰πéÊúâÂ∏ÆÂä©„ÄÇ‰ΩÜÊòØÔºåÂõ∞Â¢ÉÂú®‰∫éÔºåËøô20‰∏áÂº†ÂõæÁâáÂπ∂‰∏çÂÆåÂÖ®Êù•Ëá™‰Ω†ÊÉ≥Ë¶ÅÁöÑÂàÜÂ∏ÉÔºåÈÇ£‰πà‰Ω†ÂèØ‰ª•ÊÄé‰πàÂÅöÂë¢Ôºü Êàë‰ª¨ÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÊòØÊù•Ëá™ÊâãÊú∫ÊâãÊú∫Êî∂ÈõÜÁöÑÊï∞ÊçÆÔºåËÄå‰∏çÊòØÊù•Ëá™ÁΩëÈ°µ„ÄÇÊñπÊ≥ï‰∏ÄÔºåÈöèÊú∫ÂàÜÈÖçËÆ≠ÁªÉÈõÜ„ÄÅÈ™åËØÅÈõÜ„ÄÅÊµãËØïÈõÜÔºåËøôÊ†∑ÁöÑÂêéÊûúÂ∞±ÊòØËä±‰∫ÜÂ§ßÈáèÊó∂Èó¥Âú®ÂÆûÈôÖ‰∏çÂÖ≥ÂøÉÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÂéª‰ºòÂåñ„ÄÇ ËÆ≠ÁªÉÈõÜ20‰∏áÂº†ÁΩëÁªúÔºå5000ÊâãÊú∫ÔºåÈ™åËØÅÈõÜÂíåÊµãËØïÈõÜÂêÑ2500ÔºåËøôÊ†∑ÂèØ‰ª•‰øùËØÅÈ™åËØÅÈõÜÂíåÊµãËØïÈõÜÊõ¥Êé•ËøëÂÆûÈôÖÂ∫îÁî®Âú∫ÊôØÔºåÊàë‰ª¨ËØïËØïÊê≠Âª∫‰∏Ä‰∏™Â≠¶‰π†Á≥ªÁªüÔºåËÆ©Á≥ªÁªüÂú®Â§ÑÁêÜÊâãÊú∫‰∏ä‰º†ÂõæÁâáÂàÜÂ∏ÉÊó∂ÊïàÊûúËâØÂ•Ω„ÄÇÁº∫ÁÇπÂú®‰∫éÔºåÂΩìÁÑ∂‰∫ÜÔºåÁé∞Âú®‰Ω†ÁöÑËÆ≠ÁªÉÈõÜÂàÜÂ∏ÉÂíå‰Ω†ÁöÑÂºÄÂèëÈõÜ„ÄÅÊµãËØïÈõÜÂàÜÂ∏ÉÂπ∂‰∏ç‰∏ÄÊ†∑„ÄÇ‰ΩÜ‰∫ãÂÆûËØÅÊòéÔºåËøôÊ†∑ÊääÊï∞ÊçÆÂàÜÊàêËÆ≠ÁªÉ„ÄÅÂºÄÂèëÂíåÊµãËØïÈõÜÔºåÂú®ÈïøÊúüËÉΩÁªô‰Ω†Â∏¶Êù•Êõ¥Â•ΩÁöÑÁ≥ªÁªüÊÄßËÉΩ„ÄÇÊàë‰ª¨‰ª•Âêé‰ºöËÆ®ËÆ∫‰∏Ä‰∫õÁâπÊÆäÁöÑÊäÄÂ∑ßÔºåÂèØ‰ª•Â§ÑÁêÜ ËÆ≠ÁªÉÈõÜÁöÑÂàÜÂ∏ÉÂíåÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜÂàÜÂ∏É‰∏ç‰∏ÄÊ†∑ÁöÑÊÉÖÂÜµ„ÄÇ C5: Bias and Variance with mismatched data distributionsÔºàÊï∞ÊçÆÂàÜÂ∏É‰∏çÂåπÈÖçÊó∂ÔºåÂÅèÂ∑Æ‰∏éÊñπÂ∑ÆÁöÑÂàÜÊûêÔºâÈ¶ñÂÖàÁÆóÊ≥ïÂè™ÁúãËøáËÆ≠ÁªÉÈõÜÊï∞ÊçÆÔºåÊ≤°ÁúãËøáÂºÄÂèëÈõÜÊï∞ÊçÆ„ÄÇÁ¨¨‰∫åÔºåÂºÄÂèëÈõÜÊï∞ÊçÆÊù•Ëá™‰∏çÂêåÁöÑÂàÜÂ∏É„ÄÇÂæàÈöæÁ°ÆËÆ§ËøôÂ¢ûÂä†ÁöÑ9%ËØØÂ∑ÆÁéáÊúâÂ§öÂ∞ëÊòØÂõ†‰∏∫ÁÆóÊ≥ïÊ≤°ÁúãÂà∞ÂºÄÂèëÈõÜ‰∏≠ÁöÑÊï∞ÊçÆÂØºËá¥ÁöÑÔºåËøô‰πàËØÑ‰º∞Âë¢ÔºüÂà∞Â∫ïÂì™‰∏™ÂΩ±ÂìçÂÖÉÁ¥†Êõ¥Â§ßÔºå ËØÑ‰º∞ÊñπÊ≥ïÔºåËÆ≠ÁªÉÈõÜÁöÑÂàÜÂ∏ÉÊåñÂá∫Ôºåtraning-dev set : Same distributation as traning set ,but not used for training. Áé∞Âú®ÔºåÊàë‰ª¨Êúâ‰∫ÜËÆ≠ÁªÉÈõÜÈîôËØØÁéá„ÄÅËÆ≠ÁªÉ-È™åËØÅÈõÜÈîôËØØÁéáÔºå‰ª•ÂèäÈ™åËØÅÈõÜÈîôËØØÁéá„ÄÇÂÖ∂‰∏≠ÔºåËÆ≠ÁªÉÈõÜÈîôËØØÁéáÂíåËÆ≠ÁªÉ-È™åËØÅÈõÜÈîôËØØÁéáÁöÑÂ∑ÆÂÄºÂèçÊò†‰∫ÜÊñπÂ∑ÆÔºõËÄåËÆ≠ÁªÉ-È™åËØÅÈõÜÈîôËØØÁéáÂíåÈ™åËØÅÈõÜÈîôËØØÁéáÁöÑÂ∑ÆÂÄºÂèçÊò†‰∫ÜÊ†∑Êú¨ÂàÜÂ∏É‰∏ç‰∏ÄËá¥ÁöÑÈóÆÈ¢òÔºå‰ªéËÄåËØ¥ÊòéÊ®°ÂûãÊìÖÈïøÂ§ÑÁêÜÁöÑÊï∞ÊçÆÂíåÊàë‰ª¨ÂÖ≥ÂøÉÁöÑÊï∞ÊçÆÊù•Ëá™‰∏çÂêåÁöÑÂàÜÂ∏ÉÔºåÊàë‰ª¨Áß∞‰πã‰∏∫Êï∞ÊçÆ‰∏çÂåπÈÖçÔºàData MismatchÔºâÈóÆÈ¢ò„ÄÇ C6: Addressing data mismatchÔºàÂ§ÑÁêÜÊï∞ÊçÆ‰∏çÂåπÈÖçÈóÆÈ¢òÔºâI Data: Artifical data synthesis ÊâÄ‰ª•ÔºåÊÄªËÄåË®Ä‰πãÔºåÂ¶ÇÊûú‰Ω†ËÆ§‰∏∫Â≠òÂú®Êï∞ÊçÆ‰∏çÂåπÈÖçÈóÆÈ¢òÔºåÊàëÂª∫ËÆÆ‰Ω†ÂÅöÈîôËØØÂàÜÊûêÔºåÊàñËÄÖÁúãÁúãËÆ≠ÁªÉÈõÜÔºåÊàñËÄÖÁúãÁúãÂºÄÂèëÈõÜÔºåËØïÂõæÊâæÂá∫ÔºåËØïÂõæ‰∫ÜËß£Ëøô‰∏§‰∏™Êï∞ÊçÆÂàÜÂ∏ÉÂà∞Â∫ïÊúâ‰ªÄ‰πà‰∏çÂêåÔºåÁÑ∂ÂêéÁúãÁúãÊòØÂê¶ÊúâÂäûÊ≥ïÊî∂ÈõÜÊõ¥Â§öÁúãËµ∑Êù•ÂÉèÂºÄÂèëÈõÜÁöÑÊï∞ÊçÆ‰ΩúËÆ≠ÁªÉ„ÄÇ C7: Transfer learningÔºàËøÅÁßªÂ≠¶‰π†ÔºâËøÅÁßªÂ≠¶‰π†ÔºàTranfer LearningÔºâÊòØÈÄöËøáÂ∞ÜÂ∑≤ËÆ≠ÁªÉÂ•ΩÁöÑÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÁöÑ‰∏ÄÈÉ®ÂàÜÁΩëÁªúÁªìÊûÑÂ∫îÁî®Âà∞Âè¶‰∏ÄÊ®°ÂûãÔºåÂ∞Ü‰∏Ä‰∏™Á•ûÁªèÁΩëÁªú‰ªéÊüê‰∏™‰ªªÂä°‰∏≠Â≠¶Âà∞ÁöÑÁü•ËØÜÂíåÁªèÈ™åËøêÁî®Âà∞Âè¶‰∏Ä‰∏™‰ªªÂä°‰∏≠Ôºå‰ª•ÊòæËëóÊèêÈ´òÂ≠¶‰π†‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇ ‰æãÂ¶ÇÔºåÊàë‰ª¨Â∞Ü‰∏∫Áå´ËØÜÂà´Âô®ÊûÑÂª∫ÁöÑÁ•ûÁªèÁΩëÁªúËøÅÁßªÂ∫îÁî®Âà∞ÊîæÂ∞ÑÁßëËØäÊñ≠‰∏≠„ÄÇÂõ†‰∏∫Áå´ËØÜÂà´Âô®ÁöÑÁ•ûÁªèÁΩëÁªúÂ∑≤ÁªèÂ≠¶‰π†Âà∞‰∫ÜÊúâÂÖ≥ÂõæÂÉèÁöÑÁªìÊûÑÂíåÊÄßË¥®Á≠âÊñπÈù¢ÁöÑÁü•ËØÜÔºåÊâÄ‰ª•Âè™Ë¶ÅÂÖàÂà†Èô§Á•ûÁªèÁΩëÁªú‰∏≠ÂéüÊúâÁöÑËæìÂá∫Â±ÇÔºåÂä†ÂÖ•Êñ∞ÁöÑËæìÂá∫Â±ÇÂπ∂ÈöèÊú∫ÂàùÂßãÂåñÊùÉÈáçÁ≥ªÊï∞Ôºà$W[L]$„ÄÅ$b[L]$ÔºâÔºåÈöèÂêéÁî®Êñ∞ÁöÑËÆ≠ÁªÉÈõÜËøõË°åËÆ≠ÁªÉÔºåÂ∞±ÂÆåÊàê‰∫Ü‰ª•‰∏äÁöÑËøÅÁßªÂ≠¶‰π†„ÄÇ Â¶ÇÊûúÊñ∞ÁöÑÊï∞ÊçÆÈõÜÂæàÂ∞èÔºåÂèØËÉΩÂè™ÈúÄË¶ÅÈáçÊñ∞ËÆ≠ÁªÉËæìÂá∫Â±ÇÂâçÁöÑÊúÄÂêé‰∏ÄÂ±ÇÁöÑÊùÉÈáçÔºåÂç≥$W[L]$$„ÄÅb[L]$ÔºåÂπ∂‰øùÊåÅÂÖ∂‰ªñÂèÇÊï∞‰∏çÂèòÔºõËÄåÂ¶ÇÊûúÊúâË∂≥Â§üÂ§öÁöÑÊï∞ÊçÆÔºåÂèØ‰ª•Âè™‰øùÁïôÁΩëÁªúÁªìÊûÑÔºåÈáçÊñ∞ËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªú‰∏≠ÊâÄÊúâÂ±ÇÁöÑÁ≥ªÊï∞„ÄÇËøôÊó∂ÂàùÂßãÊùÉÈáçÁî±‰πãÂâçÁöÑÊ®°ÂûãËÆ≠ÁªÉÂæóÂà∞ÔºåËøô‰∏™ËøáÁ®ãÁß∞‰∏∫È¢ÑËÆ≠ÁªÉÔºàPre-TrainingÔºâÔºå‰πãÂêéÁöÑÊùÉÈáçÊõ¥Êñ∞ËøáÁ®ãÁß∞‰∏∫ÂæÆË∞ÉÔºàFine-TuningÔºâ„ÄÇ Âú®‰∏ãËø∞Âú∫ÂêàËøõË°åËøÅÁßªÂ≠¶‰π†ÊòØÊúâÊÑè‰πâÁöÑÔºö ‰∏§‰∏™‰ªªÂä°ÊúâÂêåÊ†∑ÁöÑËæìÂÖ•ÔºàÊØîÂ¶ÇÈÉΩÊòØÂõæÂÉèÊàñËÄÖÈÉΩÊòØÈü≥È¢ëÔºâÔºõÊã•ÊúâÊõ¥Â§öÊï∞ÊçÆÁöÑ‰ªªÂä°ËøÅÁßªÂà∞Êï∞ÊçÆËæÉÂ∞ëÁöÑ‰ªªÂä°ÔºõÊüê‰∏Ä‰ªªÂä°ÁöÑ‰ΩéÂ±ÇÊ¨°ÁâπÂæÅÔºàÂ∫ïÂ±ÇÁ•ûÁªèÁΩëÁªúÁöÑÊüê‰∫õÂäüËÉΩÔºâÂØπÂè¶‰∏Ä‰∏™‰ªªÂä°ÁöÑÂ≠¶‰π†ÊúâÂ∏ÆÂä©„ÄÇ C8; Multi-task learning ÔºàÂ§ö‰ªªÂä°Â≠¶‰π†ÔºâFor example, autonomous driving example,check cars,stop signs,trfffic lights ,ËæìÂá∫‰πüÊòØ‰∏Ä‰∏™ÂêëÈáèÔºå C9 : What is end-to-end deep learning?(‰ªÄ‰πàÊòØÁ´ØÂà∞Á´ØÁöÑÊ∑±Â∫¶Â≠¶‰π†)Âú®‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†ÂàÜÂùóÊ®°Âûã‰∏≠ÔºåÊØè‰∏Ä‰∏™Ê®°ÂùóÂ§ÑÁêÜ‰∏ÄÁßçËæìÂÖ•ÔºåÁÑ∂ÂêéÂÖ∂ËæìÂá∫‰Ωú‰∏∫‰∏ã‰∏Ä‰∏™Ê®°ÂùóÁöÑËæìÂÖ•ÔºåÊûÑÊàê‰∏ÄÊù°ÊµÅÊ∞¥Á∫ø„ÄÇËÄåÁ´ØÂà∞Á´ØÊ∑±Â∫¶Â≠¶‰π†ÔºàEnd-to-end Deep LearningÔºâÂè™Áî®‰∏Ä‰∏™Âçï‰∏ÄÁöÑÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÊù•ÂÆûÁé∞ÊâÄÊúâÁöÑÂäüËÉΩ„ÄÇÂÆÉÂ∞ÜÊâÄÊúâÊ®°ÂùóÊ∑∑ÂêàÂú®‰∏ÄËµ∑ÔºåÂè™ÂÖ≥ÂøÉËæìÂÖ•ÂíåËæìÂá∫„ÄÇ ‰ºòÁÇπ‰∏éÁº∫ÁÇπÂ∫îÁî®Á´ØÂà∞Á´ØÂ≠¶‰π†ÁöÑ‰ºòÁÇπÔºö Âè™Ë¶ÅÊúâË∂≥Â§üÂ§öÁöÑÊï∞ÊçÆÔºåÂâ©‰∏ãÁöÑÂÖ®ÈÉ®‰∫§Áªô‰∏Ä‰∏™Ë∂≥Â§üÂ§ßÁöÑÁ•ûÁªèÁΩëÁªú„ÄÇÊØîËµ∑‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†ÂàÜÂùóÊ®°ÂûãÔºåÂèØËÉΩÊõ¥ËÉΩÊçïËé∑Êï∞ÊçÆ‰∏≠ÁöÑ‰ªª‰ΩïÁªüËÆ°‰ø°ÊÅØÔºåËÄå‰∏çÈúÄË¶ÅÁî®‰∫∫Á±ªÂõ∫ÊúâÁöÑËÆ§Áü•ÔºàÊàñËÄÖËØ¥ÔºåÊàêËßÅÔºâÊù•ËøõË°åÂàÜÊûêÔºõ ÊâÄÈúÄÊâãÂ∑•ËÆæËÆ°ÁöÑÁªÑ‰ª∂Êõ¥Â∞ëÔºåÁÆÄÂåñËÆæËÆ°Â∑•‰ΩúÊµÅÁ®ãÔºõ Áº∫ÁÇπÔºö ÈúÄË¶ÅÂ§ßÈáèÁöÑÊï∞ÊçÆÔºõ ÊéíÈô§‰∫ÜÂèØËÉΩÊúâÁî®ÁöÑ‰∫∫Â∑•ËÆæËÆ°ÁªÑ‰ª∂Ôºõ Ê†πÊçÆ‰ª•‰∏äÂàÜÊûêÔºåÂÜ≥ÂÆö‰∏Ä‰∏™ÈóÆÈ¢òÊòØÂê¶Â∫îÁî®Á´ØÂà∞Á´ØÂ≠¶‰π†ÁöÑÂÖ≥ÈîÆÁÇπÊòØÔºöÊòØÂê¶ÊúâË∂≥Â§üÁöÑÊï∞ÊçÆÔºåÊîØÊåÅËÉΩÂ§üÁõ¥Êé•Â≠¶‰π†‰ªé x Êò†Â∞ÑÂà∞ y Âπ∂‰∏îË∂≥Â§üÂ§çÊùÇÁöÑÂáΩÊï∞Ôºü Whether to use end-to-end learning?(ÊòØÂê¶Ë¶Å‰ΩøÁî®Á´ØÂà∞Á´ØÁöÑÊ∑±Â∫¶Â≠¶‰π†?)Pros: ‚Äã let the data speak : x-&gt;y ‚Äã less hand-designing of components needed Cons: ‚Äã May need large amount of data ‚Äã excludes potentially useful hand-designed components Key question: Do you hava sufficient data to learn a function of the complexity needed to map x to y? Â¶ÇÊûú‰Ω†ÊÉ≥‰ΩøÁî®Êú∫Âô®Â≠¶‰π†ÊàñËÄÖÊ∑±Â∫¶Â≠¶‰π†Êù•Â≠¶‰π†Êüê‰∫õÂçïÁã¨ÁöÑÁªÑ‰ª∂ÔºåÈÇ£‰πàÂΩì‰Ω†Â∫îÁî®ÁõëÁù£Â≠¶‰π†Êó∂Ôºå‰Ω†Â∫îËØ•‰ªîÁªÜÈÄâÊã©Ë¶ÅÂ≠¶‰π†ÁöÑxÂà∞yÊò†Â∞ÑÁ±ªÂûãÔºåËøôÂèñÂÜ≥‰∫éÈÇ£‰∫õ‰ªªÂä°‰Ω†ÂèØ‰ª•Êî∂ÈõÜÊï∞ÊçÆ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåË∞àËÆ∫Á∫ØÁ´ØÂà∞Á´ØÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÊòØÂæàÊøÄÂä®‰∫∫ÂøÉÁöÑÔºå‰Ω†ËæìÂÖ•ÂõæÂÉèÔºåÁõ¥Êé•ÂæóÂá∫ÊñπÂêëÁõòËΩ¨ËßíÔºå‰ΩÜÊòØÂ∞±ÁõÆÂâçËÉΩÊî∂ÈõÜÂà∞ÁöÑÊï∞ÊçÆËÄåË®ÄÔºåËøòÊúâÊàë‰ª¨‰ªäÂ§©ËÉΩÂ§üÁî®Á•ûÁªèÁΩëÁªúÂ≠¶‰π†ÁöÑÊï∞ÊçÆÁ±ªÂûãËÄåË®ÄÔºåËøôÂÆûÈôÖ‰∏ä‰∏çÊòØÊúÄÊúâÂ∏åÊúõÁöÑÊñπÊ≥ïÔºåÊàñËÄÖËØ¥Ëøô‰∏™ÊñπÊ≥ïÂπ∂‰∏çÊòØÂõ¢ÈòüÊÉ≥Âá∫ÁöÑÊúÄÂ•ΩÁî®ÁöÑÊñπÊ≥ï„ÄÇËÄåÊàëËÆ§‰∏∫ËøôÁßçÁ∫ØÁ≤πÁöÑÁ´ØÂà∞Á´ØÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÔºåÂÖ∂ÂÆûÂâçÊôØ‰∏çÂ¶ÇËøôÊ†∑Êõ¥Â§çÊùÇÁöÑÂ§öÊ≠•ÊñπÊ≥ï„ÄÇÂõ†‰∏∫ÁõÆÂâçËÉΩÊî∂ÈõÜÂà∞ÁöÑÊï∞ÊçÆÔºåËøòÊúâÊàë‰ª¨Áé∞Âú®ËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÁöÑËÉΩÂäõÊòØÊúâÂ±ÄÈôêÁöÑ„ÄÇ SummaryÂ≠¶‰π†Â¶Ç‰ΩïÈÄöËøá‰∏Ä‰∫õÊâãÊÆµÊèêÈ´òÊ®°ÂûãÁöÑË°®Áé∞ÔºåÈ¶ñÂÖà‰∫ÜËß£Ê®°ÂûãÁöÑÊÄßËÉΩÁöÑ‰ΩìÁé∞Ôºåbias„ÄÅvariance„ÄÅË¥ùÂè∂ÊñØËØØÂ∑Æ„ÄÇ‰ª•ÂèäÂ¶Ç‰Ωï‰∏ÄÊ≠•Ê≠•ÁöÑÊîπÂñÑÊÄßËÉΩ„ÄÇÂÖ∑‰ΩìËß£ÂÜ≥‰∫ÜÂ¶Ç‰∏ãÈóÆÈ¢òÔºå1. Êï∞ÊçÆÁöÑÂàíÂàÜ 2. ‰∫∫ÁöÑË°®Áé∞‰∏éÊú∫Âô®ÊÄßËÉΩÁöÑÂÖ≥Á≥ª„ÄÅÂÅèÂ∑Æ„ÄÅÊñπÂ∑Æ 3. ËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜÁöÑÂàÜÂ∏ÉÈóÆÈ¢òÔºåÂΩìÊï∞ÊçÆÊ†∑Êú¨ÂØπ‰∫éËß£ÂÜ≥ÈóÆÈ¢ò‰∏çË∂≥ÁöÑÊó∂ÂÄôÁöÑËß£ÂÜ≥ÂäûÊ≥ïÔºå4. ËøÅÁßªÂ≠¶‰π† 5. Á´ØÂà∞Á´ØÁöÑÂ≠¶‰π† 6. Â§ö‰ªªÂä°Â≠¶‰π†„ÄÇ6. Âú®ÊÄßËÉΩ‰∏çÂ•ΩÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØËÉΩÈúÄË¶ÅÊâãÂä®ÁöÑÂàÜÊûêËØØÂ∑ÆÔºåÂØπÊµãËØïÈõÜÈîôËØØÊ†∑‰æãÂÅöÁªüËÆ°Á≠âÁ≠âÔºå]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[aiai_]]></title>
    <url>%2F2019%2F04%2F17%2FImprovingDeep%20learning.ai_Deep%20Neural%20NetworksHyperparameter%20tuning%2C%20Regularization%20and%20Optimization%2F</url>
    <content type="text"><![CDATA[C2W1L01 : Train/Dev/Test Sets1. processÂ∫îÁî®ÂûãÊú∫Âô®Â≠¶‰π†ÊòØ‰∏Ä‰∏™È´òÂ∫¶Ëø≠‰ª£ÁöÑËøáÁ®ãÔºåÈÄöÂ∏∏Âú®È°πÁõÆÂêØÂä®Êó∂ÔºåÊàë‰ª¨‰ºöÂÖàÊúâ‰∏Ä‰∏™ÂàùÊ≠•ÊÉ≥Ê≥ïÔºåÊØîÂ¶ÇÊûÑÂª∫‰∏Ä‰∏™Âê´ÊúâÁâπÂÆöÂ±ÇÊï∞ÔºåÈöêËóèÂçïÂÖÉÊï∞ÈáèÊàñÊï∞ÊçÆÈõÜ‰∏™Êï∞Á≠âÁ≠âÁöÑÁ•ûÁªèÁΩëÁªúÔºåÁÑ∂ÂêéÁºñÁ†ÅÔºåÂπ∂Â∞ùËØïËøêË°åËøô‰∫õ‰ª£Á†ÅÔºåÈÄöËøáËøêË°åÂíåÊµãËØïÂæóÂà∞ËØ•Á•ûÁªèÁΩëÁªúÊàñËøô‰∫õÈÖçÁΩÆ‰ø°ÊÅØÁöÑËøêË°åÁªìÊûúÔºå‰Ω†ÂèØËÉΩ‰ºöÊ†πÊçÆËæìÂá∫ÁªìÊûúÈáçÊñ∞ÂÆåÂñÑËá™Â∑±ÁöÑÊÉ≥Ê≥ïÔºåÊîπÂèòÁ≠ñÁï•ÔºåÊàñËÄÖ‰∏∫‰∫ÜÊâæÂà∞Êõ¥Â•ΩÁöÑÁ•ûÁªèÁΩëÁªú‰∏çÊñ≠Ëø≠‰ª£Êõ¥Êñ∞Ëá™Â∑±ÁöÑÊñπÊ°à„ÄÇ 2. data split ËÆ≠ÁªÉÈõÜÔºàtrain setÔºâÔºöÁî®ËÆ≠ÁªÉÈõÜÂØπÁÆóÊ≥ïÊàñÊ®°ÂûãËøõË°åËÆ≠ÁªÉËøáÁ®ãÔºõ È™åËØÅÈõÜÔºàdevelopment setÔºâÔºöÂà©Áî®È™åËØÅÈõÜÔºàÂèàÁß∞‰∏∫ÁÆÄÂçï‰∫§ÂèâÈ™åËØÅÈõÜÔºåhold-out cross validation setÔºâËøõË°å‰∫§ÂèâÈ™åËØÅÔºåÈÄâÊã©Âá∫ÊúÄÂ•ΩÁöÑÊ®°ÂûãÊàñËÄÖÈ™åËØÅ‰∏çÂêåÁÆóÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ ÊµãËØïÈõÜÔºàtest setÔºâÔºöÊúÄÂêéÂà©Áî®ÊµãËØïÈõÜÂØπÊ®°ÂûãËøõË°åÊµãËØïÔºåËé∑ÂèñÊ®°ÂûãËøêË°åÁöÑÊó†ÂÅè‰º∞ËÆ°ÔºàÂØπÂ≠¶‰π†ÊñπÊ≥ïËøõË°åËØÑ‰º∞Ôºâ„ÄÇ ÂÅáËÆæËøôÊòØËÆ≠ÁªÉÊï∞ÊçÆÔºåÊàëÁî®‰∏Ä‰∏™ÈïøÊñπÂΩ¢Ë°®Á§∫ÔºåÊàë‰ª¨ÈÄöÂ∏∏‰ºöÂ∞ÜËøô‰∫õÊï∞ÊçÆÂàíÂàÜÊàêÂá†ÈÉ®ÂàÜÔºå‰∏ÄÈÉ®ÂàÜ‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜÔºå‰∏ÄÈÉ®ÂàÜ‰Ωú‰∏∫ÁÆÄÂçï‰∫§ÂèâÈ™åËØÅÈõÜÔºåÊúâÊó∂‰πüÁß∞‰πã‰∏∫È™åËØÅÈõÜÔºåÊñπ‰æøËµ∑ËßÅÔºåÊàëÂ∞±Âè´ÂÆÉÈ™åËØÅÈõÜÔºàdev setÔºâÔºåÂÖ∂ÂÆûÈÉΩÊòØÂêå‰∏Ä‰∏™Ê¶ÇÂøµÔºåÊúÄÂêé‰∏ÄÈÉ®ÂàÜÂàô‰Ωú‰∏∫ÊµãËØïÈõÜ„ÄÇ Âú®Êú∫Âô®Â≠¶‰π†ÂèëÂ±ïÁöÑÂ∞èÊï∞ÊçÆÈáèÊó∂‰ª£ÔºåÂ¶Ç 100„ÄÅ1000„ÄÅ10000 ÁöÑÊï∞ÊçÆÈáèÂ§ßÂ∞èÔºåÂèØ‰ª•Â∞ÜÊï∞ÊçÆÈõÜÊåâÁÖß‰ª•‰∏ãÊØî‰æãËøõË°åÂàíÂàÜÔºö Êó†È™åËØÅÈõÜÁöÑÊÉÖÂÜµÔºö70% / 30%Ôºõ ÊúâÈ™åËØÅÈõÜÁöÑÊÉÖÂÜµÔºö60% / 20% / 20%Ôºõ Âú®Â¶Ç‰ªäÁöÑÂ§ßÊï∞ÊçÆÊó∂‰ª£ÔºåÂØπ‰∫é‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Êã•ÊúâÁöÑÊï∞ÊçÆÈõÜÁöÑËßÑÊ®°ÂèØËÉΩÊòØÁôæ‰∏áÁ∫ßÂà´ÁöÑÔºåÊâÄ‰ª•È™åËØÅÈõÜÂíåÊµãËØïÈõÜÊâÄÂç†ÁöÑÊØîÈáç‰ºöË∂ãÂêë‰∫éÂèòÂæóÊõ¥Â∞è„ÄÇ È™åËØÅÈõÜÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫ÜÈ™åËØÅ‰∏çÂêåÁöÑÁÆóÊ≥ïÂì™ÁßçÊõ¥Âä†ÊúâÊïàÔºåÊâÄ‰ª•È™åËØÅÈõÜÂè™Ë¶ÅË∂≥Â§üÂ§ßÂà∞ËÉΩÂ§üÈ™åËØÅÂ§ßÁ∫¶ 2-10 ÁßçÁÆóÊ≥ïÂì™ÁßçÊõ¥Â•ΩÔºåËÄå‰∏çÈúÄË¶Å‰ΩøÁî® 20% ÁöÑÊï∞ÊçÆ‰Ωú‰∏∫È™åËØÅÈõÜ„ÄÇÂ¶ÇÁôæ‰∏áÊï∞ÊçÆ‰∏≠ÊäΩÂèñ 1 ‰∏áÁöÑÊï∞ÊçÆ‰Ωú‰∏∫È™åËØÅÈõÜÂ∞±ÂèØ‰ª•‰∫Ü„ÄÇ ÊµãËØïÈõÜÁöÑ‰∏ªË¶ÅÁõÆÁöÑÊòØËØÑ‰º∞Ê®°ÂûãÁöÑÊïàÊûúÔºåÂ¶ÇÂú®Âçï‰∏™ÂàÜÁ±ªÂô®‰∏≠ÔºåÂæÄÂæÄÂú®Áôæ‰∏áÁ∫ßÂà´ÁöÑÊï∞ÊçÆ‰∏≠ÔºåÊàë‰ª¨ÈÄâÊã©ÂÖ∂‰∏≠ 1000 Êù°Êï∞ÊçÆË∂≥‰ª•ËØÑ‰º∞Âçï‰∏™Ê®°ÂûãÁöÑÊïàÊûú„ÄÇ 100 ‰∏áÊï∞ÊçÆÈáèÔºö98% / 1% / 1%Ôºõ Ë∂ÖÁôæ‰∏áÊï∞ÊçÆÈáèÔºö99.5% / 0.25% / 0.25%ÔºàÊàñËÄÖ99.5% / 0.4% / 0.1%Ôºâ 3. Âª∫ËÆÆÈ™åËØÅÈõÜË¶ÅÂíåËÆ≠ÁªÉÈõÜÊù•Ëá™‰∫éÂêå‰∏Ä‰∏™ÂàÜÂ∏ÉÔºàÊï∞ÊçÆÊù•Ê∫ê‰∏ÄËá¥ÔºâÔºåÂèØ‰ª•‰ΩøÂæóÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÂèòÂæóÊõ¥Âø´Âπ∂Ëé∑ÂæóÊõ¥Â•ΩÁöÑÊïàÊûú„ÄÇ Â¶ÇÊûú‰∏çÈúÄË¶ÅÁî®Êó†ÂÅè‰º∞ËÆ°Êù•ËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩÔºåÂàôÂèØ‰ª•‰∏çÈúÄË¶ÅÊµãËØïÈõÜ„ÄÇÂ¶ÇÊûúÂè™ÊúâÈ™åËØÅÈõÜÔºåÊ≤°ÊúâÊµãËØïÈõÜÔºåÊàë‰ª¨Ë¶ÅÂÅöÁöÑÂ∞±ÊòØÔºåÂú®ËÆ≠ÁªÉÈõÜ‰∏äËÆ≠ÁªÉÔºåÂ∞ùËØï‰∏çÂêåÁöÑÊ®°ÂûãÊ°ÜÊû∂ÔºåÂú®È™åËØÅÈõÜ‰∏äËØÑ‰º∞Ëøô‰∫õÊ®°ÂûãÔºåÁÑ∂ÂêéËø≠‰ª£Âπ∂ÈÄâÂá∫ÈÄÇÁî®ÁöÑÊ®°Âûã„ÄÇÂõ†‰∏∫È™åËØÅÈõÜ‰∏≠Â∑≤ÁªèÊ∂µÁõñÊµãËØïÈõÜÊï∞ÊçÆÔºåÂÖ∂‰∏çÂÜçÊèê‰æõÊó†ÂÅèÊÄßËÉΩËØÑ‰º∞„ÄÇÂΩìÁÑ∂ÔºåÂ¶ÇÊûú‰Ω†‰∏çÈúÄË¶ÅÊó†ÂÅè‰º∞ËÆ°ÔºåÈÇ£Â∞±ÂÜçÂ•Ω‰∏çËøá‰∫Ü„ÄÇ L02 : Bias/Variance‚ÄúÂÅèÂ∑Æ-ÊñπÂ∑ÆÂàÜËß£‚ÄùÔºàbias-variance decompositionÔºâÊòØËß£ÈáäÂ≠¶‰π†ÁÆóÊ≥ïÊ≥õÂåñÊÄßËÉΩÁöÑ‰∏ÄÁßçÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇ Ê≥õÂåñËØØÂ∑ÆÂèØÂàÜËß£‰∏∫ÂÅèÂ∑Æ„ÄÅÊñπÂ∑Æ‰∏éÂô™Â£∞‰πãÂíåÔºö ÂÅèÂ∑ÆÔºöÂ∫¶Èáè‰∫ÜÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÊúüÊúõÈ¢ÑÊµã‰∏éÁúüÂÆûÁªìÊûúÁöÑÂÅèÁ¶ªÁ®ãÂ∫¶ÔºåÂç≥ÂàªÁîª‰∫ÜÂ≠¶‰π†ÁÆóÊ≥ïÊú¨Ë∫´ÁöÑÊãüÂêàËÉΩÂäõÔºõ ÊñπÂ∑ÆÔºöÂ∫¶Èáè‰∫ÜÂêåÊ†∑Â§ßÂ∞èÁöÑËÆ≠ÁªÉÈõÜÁöÑÂèòÂä®ÊâÄÂØºËá¥ÁöÑÂ≠¶‰π†ÊÄßËÉΩÁöÑÂèòÂåñÔºåÂç≥ÂàªÁîª‰∫ÜÊï∞ÊçÆÊâ∞Âä®ÊâÄÈÄ†ÊàêÁöÑÂΩ±ÂìçÔºõ Âô™Â£∞ÔºöË°®Ëææ‰∫ÜÂú®ÂΩìÂâç‰ªªÂä°‰∏ä‰ªª‰ΩïÂ≠¶‰π†ÁÆóÊ≥ïÊâÄËÉΩÂ§üËææÂà∞ÁöÑÊúüÊúõÊ≥õÂåñËØØÂ∑ÆÁöÑ‰∏ãÁïåÔºåÂç≥ÂàªÁîª‰∫ÜÂ≠¶‰π†ÈóÆÈ¢òÊú¨Ë∫´ÁöÑÈöæÂ∫¶„ÄÇ high bias ,underfitting high variance, overfitting just right 1. example Your algorithms ever on the training set and dev set you can try to diganose whether has problems high barriers or high variances or both or neither. L03 Basic Recipe for Machine learning1. METHOD Training a bigger network almost never hurts. And the main cost of training a neural network that‚Äôs too big is just computational time, so long as you‚Äôre regularizing. ‰ªäÂ§©Êàë‰ª¨ËÆ≤‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÁªÑÁªáÊú∫Âô®Â≠¶‰π†Êù•ËØäÊñ≠ÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÁöÑÂü∫Êú¨ÊñπÊ≥ïÔºåÁÑ∂ÂêéÈÄâÊã©Ëß£ÂÜ≥ÈóÆÈ¢òÁöÑÊ≠£Á°ÆÊìç‰ΩúÔºåÂ∏åÊúõÂ§ßÂÆ∂ÊúâÊâÄ‰∫ÜËß£ÂíåËÆ§ËØÜ„ÄÇÊàëÂú®ËØæ‰∏ä‰∏çÊ≠¢‰∏ÄÊ¨°ÊèêÂà∞‰∫ÜÊ≠£ÂàôÂåñÔºåÂÆÉÊòØ‰∏ÄÁßçÈùûÂ∏∏ÂÆûÁî®ÁöÑÂáèÂ∞ëÊñπÂ∑ÆÁöÑÊñπÊ≥ïÔºåÊ≠£ÂàôÂåñÊó∂‰ºöÂá∫Áé∞ÂÅèÂ∑ÆÊñπÂ∑ÆÊùÉË°°ÈóÆÈ¢òÔºåÂÅèÂ∑ÆÂèØËÉΩÁï•ÊúâÂ¢ûÂä†ÔºåÂ¶ÇÊûúÁΩëÁªúË∂≥Â§üÂ§ßÔºåÂ¢ûÂπÖÈÄöÂ∏∏‰∏ç‰ºöÂ§™È´òÔºåÊàë‰ª¨‰∏ãËäÇËØæÂÜçÁªÜËÆ≤Ôºå‰ª•‰æøÂ§ßÂÆ∂Êõ¥Â•ΩÁêÜËß£Â¶Ç‰ΩïÂÆûÁé∞Á•ûÁªèÁΩëÁªúÁöÑÊ≠£ÂàôÂåñ„ÄÇ Á¨¨‰∏ÄÁÇπÔºåÈ´òÂÅèÂ∑ÆÂíåÈ´òÊñπÂ∑ÆÊòØ‰∏§Áßç‰∏çÂêåÁöÑÊÉÖÂÜµÔºåÊàë‰ª¨ÂêéÁª≠Ë¶ÅÂ∞ùËØïÁöÑÊñπÊ≥ï‰πüÂèØËÉΩÂÆåÂÖ®‰∏çÂêå Âè™Ë¶ÅÊ≠£ÂàôÈÄÇÂ∫¶ÔºåÈÄöÂ∏∏ÊûÑÂª∫‰∏Ä‰∏™Êõ¥Â§ßÁöÑÁΩëÁªú‰æøÂèØ‰ª•ÔºåÂú®‰∏çÂΩ±ÂìçÊñπÂ∑ÆÁöÑÂêåÊó∂ÂáèÂ∞ëÂÅèÂ∑ÆÔºåËÄåÈááÁî®Êõ¥Â§öÊï∞ÊçÆÈÄöÂ∏∏ÂèØ‰ª•Âú®‰∏çËøáÂ§öÂΩ±ÂìçÂÅèÂ∑ÆÁöÑÂêåÊó∂ÂáèÂ∞ëÊñπÂ∑Æ„ÄÇËøô‰∏§Ê≠•ÂÆûÈôÖË¶ÅÂÅöÁöÑÂ∑•‰ΩúÊòØÔºöËÆ≠ÁªÉÁΩëÁªúÔºåÈÄâÊã©ÁΩëÁªúÊàñËÄÖÂáÜÂ§áÊõ¥Â§öÊï∞ÊçÆÔºåÁé∞Âú®Êàë‰ª¨ÊúâÂ∑•ÂÖ∑ÂèØ‰ª•ÂÅöÂà∞Âú®ÂáèÂ∞ëÂÅèÂ∑ÆÊàñÊñπÂ∑ÆÁöÑÂêåÊó∂Ôºå‰∏çÂØπÂè¶‰∏ÄÊñπ‰∫ßÁîüËøáÂ§ö‰∏çËâØÂΩ±Âìç„ÄÇ L041. over fittingregularizationL2 regularization L1 regularizaion: w will be sparse L1 Ê≠£ÂàôÂåñÊúÄÂêéÂæóÂà∞ w ÂêëÈáè‰∏≠Â∞ÜÂ≠òÂú®Â§ßÈáèÁöÑ 0 ‰∏∫‰ªÄ‰πàÂè™Ê≠£ÂàôÂåñÂèÇÊï∞wÔºü‰∏∫‰ªÄ‰πà‰∏çÂÜçÂä†‰∏äÂèÇÊï∞b Âë¢Ôºü‰Ω†ÂèØ‰ª•Ëøô‰πàÂÅöÔºåÂè™ÊòØÊàë‰π†ÊÉØÁúÅÁï•‰∏çÂÜôÔºåÂõ†‰∏∫ÈÄöÂ∏∏wÊòØ‰∏Ä‰∏™È´òÁª¥ÂèÇÊï∞Áü¢ÈáèÔºåwÂ∑≤ÁªèÂèØ‰ª•Ë°®ËææÈ´òÂÅèÂ∑ÆÈóÆÈ¢òÔºåÂèØËÉΩwÂåÖÂê´ÊúâÂæàÂ§öÂèÇÊï∞ÔºåÊàë‰ª¨‰∏çÂèØËÉΩÊãüÂêàÊâÄÊúâÂèÇÊï∞ÔºåËÄåÂè™ÊòØbÂçï‰∏™Êï∞Â≠óÔºåÊâÄ‰ª•wÂá†‰πéÊ∂µÁõñÊâÄÊúâÂèÇÊï∞ÔºåËÄå‰∏çÊòØÔºåÂ¶ÇÊûúÂä†‰∫ÜÂèÇÊï∞bÔºåÂÖ∂ÂÆû‰πüÊ≤°Â§™Â§ßÂΩ±ÂìçÔºåÂõ†‰∏∫bÂè™ÊòØ‰ºóÂ§öÂèÇÊï∞‰∏≠ÁöÑ‰∏Ä‰∏™ÔºåÊâÄ‰ª•ÊàëÈÄöÂ∏∏ÁúÅÁï•‰∏çËÆ°ÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥Âä†‰∏äËøô‰∏™ÂèÇÊï∞ÔºåÂÆåÂÖ®Ê≤°ÈóÆÈ¢ò„ÄÇ 2. Áü©ÈòµËåÉÊï∞Ë¢´Áß∞‰Ωú‚ÄúÂºóÁΩóË¥ùÂ∞º‰πåÊñØËåÉÊï∞‚ÄùÔºåÁî®‰∏ãÊ†áÊ†áÊ≥®F ÂèçÂêë‰º†Êí≠Êó∂ÔºåÂ°´‰∏äÊ≠£ÂàôÂåñÁöÑ‰∏ÄÈ°π Âõ†Ê≠§L2Ê≠£ÂàôÂåñ‰πüË¢´Áß∞‰∏∫‚ÄúÊùÉÈáçË°∞Âáè‚Äù„ÄÇ to get more training data L05 :Why Regularization Reduces OverfittingÊàë‰ª¨Ê∑ªÂä†Ê≠£ÂàôÈ°πÔºåÂÆÉÂèØ‰ª•ÈÅøÂÖçÊï∞ÊçÆÊùÉÂÄºÁü©ÈòµËøáÂ§ßÔºåËøôÂ∞±ÊòØÂºóÁΩóË¥ùÂ∞º‰πåÊñØËåÉÊï∞Ôºå‰∏∫‰ªÄ‰πàÂéãÁº©ËåÉÊï∞ÔºåÊàñËÄÖÂºóÁΩóË¥ùÂ∞º‰πåÊñØËåÉÊï∞ÊàñËÄÖÂèÇÊï∞ÂèØ‰ª•ÂáèÂ∞ëËøáÊãüÂêàÔºüÊàë‰ª¨Â∞ùËØïÊ∂àÈô§ÊàñËá≥Â∞ëÂáèÂ∞ëËÆ∏Â§öÈöêËóèÂçïÂÖÉÁöÑÂΩ±ÂìçÔºåÊúÄÁªàËøô‰∏™ÁΩëÁªú‰ºöÂèòÂæóÊõ¥ÁÆÄÂçï.RegularizationÂÖ∂ÂÆûÊòØËÆ©ÂáΩÊï∞ÂèòÂæóÁÆÄÂåñ„ÄÇ Áõ¥ËßÇ‰∏äÁêÜËß£Â∞±ÊòØÂ¶ÇÊûúÊ≠£ÂàôÂåñËÆæÁΩÆÂæóË∂≥Â§üÂ§ßÔºåÊùÉÈáçÁü©ÈòµË¢´ËÆæÁΩÆ‰∏∫Êé•Ëøë‰∫é0ÁöÑÂÄºÔºåÁõ¥ËßÇÁêÜËß£Â∞±ÊòØÊääÂ§öÈöêËóèÂçïÂÖÉÁöÑÊùÉÈáçËÆæ‰∏∫0Ôºå‰∫éÊòØÂü∫Êú¨‰∏äÊ∂àÈô§‰∫ÜËøô‰∫õÈöêËóèÂçïÂÖÉÁöÑËÆ∏Â§öÂΩ±Âìç„ÄÇÂ¶ÇÊûúÊòØËøôÁßçÊÉÖÂÜµÔºåËøô‰∏™Ë¢´Â§ßÂ§ßÁÆÄÂåñ‰∫ÜÁöÑÁ•ûÁªèÁΩëÁªú‰ºöÂèòÊàê‰∏Ä‰∏™ÂæàÂ∞èÁöÑÁΩëÁªúÔºåÂ∞èÂà∞Â¶ÇÂêå‰∏Ä‰∏™ÈÄªËæëÂõûÂΩíÂçïÂÖÉÔºåÂèØÊòØÊ∑±Â∫¶Âç¥ÂæàÂ§ßÔºåÂÆÉ‰ºö‰ΩøËøô‰∏™ÁΩëÁªú‰ªéËøáÂ∫¶ÊãüÂêàÁöÑÁä∂ÊÄÅÊõ¥Êé•ËøëÂ∑¶ÂõæÁöÑÈ´òÂÅèÂ∑ÆÁä∂ÊÄÅ„ÄÇ ÊÄªÁªì‰∏Ä‰∏ãÔºåÂ¶ÇÊûúÊ≠£ÂàôÂåñÂèÇÊï∞ÂèòÂæóÂæàÂ§ßÔºåwÂèÇÊï∞ÂæàÂ∞èÔºåz‰πü‰ºöÁõ∏ÂØπÂèòÂ∞èÔºåÊ≠§Êó∂ÂøΩÁï•ÁöÑbÂΩ±ÂìçÔºåz‰ºöÁõ∏ÂØπÂèòÂ∞èÔºåÂÆûÈôÖ‰∏äÔºåzÁöÑÂèñÂÄºËåÉÂõ¥ÂæàÂ∞èÔºåËøô‰∏™ÊøÄÊ¥ªÂáΩÊï∞tanhÔºå‰πüÂ∞±ÊòØÊõ≤Á∫øÂáΩÊï∞‰ºöÁõ∏ÂØπÂëàÁ∫øÊÄßÔºåÊï¥‰∏™Á•ûÁªèÁΩëÁªú‰ºöËÆ°ÁÆóÁ¶ªÁ∫øÊÄßÂáΩÊï∞ËøëÁöÑÂÄºÔºåËøô‰∏™Á∫øÊÄßÂáΩÊï∞ÈùûÂ∏∏ÁÆÄÂçïÔºåÂπ∂‰∏çÊòØ‰∏Ä‰∏™ÊûÅÂ§çÊùÇÁöÑÈ´òÂ∫¶ÈùûÁ∫øÊÄßÂáΩÊï∞Ôºå‰∏ç‰ºöÂèëÁîüËøáÊãüÂêà„ÄÇ L2 regularizationÁöÑ‰∏çË∂≥ÔºöË¶ÅÈÄöËøá‰∏çÊñ≠ÁöÑÈÄâÁî®‰∏çÂêåÁöÑŒªËøõË°åÊµãËØïÔºåËÆ°ÁÆóÈáèÂä†Â§ß‰∫Ü„ÄÇ L06 : Dropout Regularization1. Â∑•‰ΩúÂéüÁêÜ Â¶ÇÊûú‰∏äÈù¢ËøôÂπÖÂõæÂ≠òÂú®over fitting„ÄÇÂ§çÂà∂Ëøô‰∏™Á•ûÁªèÁΩëÁªúÔºådropout‰ºöÈÅçÂéÜÁΩëÁªúÁöÑÊØè‰∏ÄÂ±Ç„ÄÇÂÅáËÆæÁΩëÁªú‰∏≠ÁöÑÊØè‰∏ÄÂ±ÇÔºåÊØè‰∏™ËäÇÁÇπÈÉΩ‰ª•ÊäõÁ°¨Â∏ÅÁöÑÊñπÂºèËÆæÁΩÆÊ¶ÇÁéáÔºåÊØè‰∏™ËäÇÁÇπÂæó‰ª•‰øùÁïôÂíåÊ∂àÈô§ÁöÑÊ¶ÇÁéáÈÉΩÊòØ0.5ÔºåËÆæÁΩÆÂÆåËäÇÁÇπÊ¶ÇÁéáÔºåÊàë‰ª¨‰ºöÊ∂àÈô§‰∏Ä‰∫õËäÇÁÇπÔºåÁÑ∂ÂêéÂà†Èô§Êéâ‰ªéËØ•ËäÇÁÇπËøõÂá∫ÁöÑËøûÁ∫øÔºåÊúÄÂêéÂæóÂà∞‰∏Ä‰∏™ËäÇÁÇπÊõ¥Â∞ëÔºåËßÑÊ®°Êõ¥Â∞èÁöÑÁΩëÁªúÔºåÁÑ∂ÂêéÁî®backpropÊñπÊ≥ïËøõË°åËÆ≠ÁªÉ„ÄÇ Êàë‰ª¨ÈíàÂØπÊØè‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ËÆ≠ÁªÉËßÑÊ®°ÊûÅÂ∞èÁöÑÁΩëÁªúÔºåÊúÄÂêé‰Ω†ÂèØËÉΩ‰ºöËÆ§ËØÜÂà∞‰∏∫‰ªÄ‰πàË¶ÅÊ≠£ÂàôÂåñÁΩëÁªúÔºåÂõ†‰∏∫Êàë‰ª¨Âú®ËÆ≠ÁªÉÊûÅÂ∞èÁöÑÁΩëÁªú„ÄÇ 2. inverted dropoutÔºàÂèçÂêëÈöèÊú∫Â§±Ê¥ªÔºâÂØπÁ¨¨L 1234keep_prob = 0.8 # ËÆæÁΩÆÁ•ûÁªèÂÖÉ‰øùÁïôÊ¶ÇÁéádl = np.random.rand(al.shape[0], al.shape[1]) &lt; keep_probal = np.multiply(al, dl)al /= keep_prob ÊúÄÂêé‰∏ÄÊ≠•al /= keep_probÊòØÂõ†‰∏∫ a[l]a[l]‰∏≠ÁöÑ‰∏ÄÈÉ®ÂàÜÂÖÉÁ¥†Â§±Ê¥ªÔºàÁõ∏ÂΩì‰∫éË¢´ÂΩíÈõ∂ÔºâÔºå‰∏∫‰∫ÜÂú®‰∏ã‰∏ÄÂ±ÇËÆ°ÁÆóÊó∂‰∏çÂΩ±Âìç $Z[l+1]=W[l+1]a[l]+b[l+1]$ÁöÑÊúüÊúõÂÄºÔºåÂõ†Ê≠§Èô§‰ª•‰∏Ä‰∏™keep_prob„ÄÇ‰∏æ‰æãËß£ÈáäÊàë‰ª¨ÂÅáËÆæÁ¨¨‰∏âÈöêËóèÂ±Ç‰∏äÊúâ50‰∏™ÂçïÂÖÉÊàñ50‰∏™Á•ûÁªèÂÖÉÔºåÂú®‰∏ÄÁª¥‰∏äÊòØ50ÔºåÊàë‰ª¨ÈÄöËøáÂõ†Â≠êÂàÜËß£Â∞ÜÂÆÉÊãÜÂàÜÊàêÁª¥ÁöÑÔºå‰øùÁïôÂíåÂà†Èô§ÂÆÉ‰ª¨ÁöÑÊ¶ÇÁéáÂàÜÂà´‰∏∫80%Âíå20%ÔºåËøôÊÑèÂë≥ÁùÄÊúÄÂêéË¢´Âà†Èô§ÊàñÂΩíÈõ∂ÁöÑÂçïÂÖÉÂπ≥ÂùáÊúâ10Ôºà50√ó20%=10Ôºâ‰∏™ÔºåÁé∞Âú®Êàë‰ª¨Áúã‰∏ã$z^{[4]}$ÔºåÔºåÊàë‰ª¨ÁöÑÈ¢ÑÊúüÊòØ$z^{[4]}=w^{[4]}a^{[3]}$Ôºå$a^{[3]}$ÂáèÂ∞ë20%Ôºå‰πüÂ∞±ÊòØËØ¥‰∏≠Êúâ$a^{[3]}$20%ÁöÑÂÖÉÁ¥†Ë¢´ÂΩíÈõ∂Ôºå‰∏∫‰∫Ü‰∏çÂΩ±ÂìçÁöÑ$a^{[4]}$ÊúüÊúõÂÄºÔºåÊàë‰ª¨ÈúÄË¶ÅÁî®$w^{[4]}a^{[3]}/keep_prob$ÔºåÂÆÉÂ∞Ü‰ºö‰øÆÊ≠£ÊàñÂº•Ë°•Êàë‰ª¨ÊâÄÈúÄÁöÑÈÇ£20%Ôºå$a^{[3]}$ÁöÑÊúüÊúõÂÄº‰∏ç‰ºöÂèòÔºåÂàíÁ∫øÈÉ®ÂàÜÂ∞±ÊòØÊâÄË∞ìÁöÑdropoutÊñπÊ≥ï„ÄÇ L07 : Understanding DropoutÁõ¥ËßÇ‰∏äÁêÜËß£Ôºö‰∏çË¶Å‰æùËµñ‰∫é‰ªª‰Ωï‰∏Ä‰∏™ÁâπÂæÅÔºåÂõ†‰∏∫ËØ•ÂçïÂÖÉÁöÑËæìÂÖ•ÂèØËÉΩÈöèÊó∂Ë¢´Ê∏ÖÈô§ÔºåÂõ†Ê≠§ËØ•ÂçïÂÖÉÈÄöËøáËøôÁßçÊñπÂºè‰º†Êí≠‰∏ãÂéªÔºåÂπ∂‰∏∫ÂçïÂÖÉÁöÑÂõõ‰∏™ËæìÂÖ•Â¢ûÂä†‰∏ÄÁÇπÊùÉÈáçÔºåÈÄöËøá‰º†Êí≠ÊâÄÊúâÊùÉÈáçÔºådropoutÂ∞Ü‰∫ßÁîüÊî∂Áº©ÊùÉÈáçÁöÑÂπ≥ÊñπËåÉÊï∞ÁöÑÊïàÊûúÔºåÂíå‰πãÂâçËÆ≤ÁöÑL2Ê≠£ÂàôÂåñÁ±ª‰ººÔºõÂÆûÊñΩdropoutÁöÑÁªìÊûúÂÆûÂÆÉ‰ºöÂéãÁº©ÊùÉÈáçÔºåÂπ∂ÂÆåÊàê‰∏Ä‰∫õÈ¢ÑÈò≤ËøáÊãüÂêàÁöÑÂ§ñÂ±ÇÊ≠£ÂàôÂåñÔºõL2ÂØπ‰∏çÂêåÊùÉÈáçÁöÑË°∞ÂáèÊòØ‰∏çÂêåÁöÑÔºåÂÆÉÂèñÂÜ≥‰∫éÊøÄÊ¥ªÂáΩÊï∞ÂÄçÂ¢ûÁöÑÂ§ßÂ∞è„ÄÇ ËÆ°ÁÆóËßÜËßâ‰∏≠ÁöÑËæìÂÖ•ÈáèÈùûÂ∏∏Â§ßÔºåËæìÂÖ•Â§™Â§öÂÉèÁ¥†Ôºå‰ª•Ëá≥‰∫éÊ≤°ÊúâË∂≥Â§üÁöÑÊï∞ÊçÆÔºåÊâÄ‰ª•dropoutÂú®ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠Â∫îÁî®ÂæóÊØîËæÉÈ¢ëÁπÅÔºåÊúâ‰∫õËÆ°ÁÆóÊú∫ËßÜËßâÁ†îÁ©∂‰∫∫ÂëòÈùûÂ∏∏ÂñúÊ¨¢Áî®ÂÆÉÔºåÂá†‰πéÊàê‰∫ÜÈªòËÆ§ÁöÑÈÄâÊã©Ôºå‰ΩÜË¶ÅÁâ¢ËÆ∞‰∏ÄÁÇπÔºådropoutÊòØ‰∏ÄÁßçÊ≠£ÂàôÂåñÊñπÊ≥ïÔºåÂÆÉÊúâÂä©‰∫éÈ¢ÑÈò≤ËøáÊãüÂêàÔºåÂõ†Ê≠§Èô§ÈùûÁÆóÊ≥ïËøáÊãüÂêàÔºå‰∏çÁÑ∂ÊàëÊòØ‰∏ç‰ºö‰ΩøÁî®dropoutÁöÑÔºåÊâÄ‰ª•ÂÆÉÂú®ÂÖ∂ÂÆÉÈ¢ÜÂüüÂ∫îÁî®ÂæóÊØîËæÉÂ∞ëÔºå‰∏ªË¶ÅÂ≠òÂú®‰∫éËÆ°ÁÆóÊú∫ËßÜËßâÈ¢ÜÂüüÔºåÂõ†‰∏∫Êàë‰ª¨ÈÄöÂ∏∏Ê≤°ÊúâË∂≥Â§üÁöÑÊï∞ÊçÆÔºåÊâÄ‰ª•‰∏ÄÁõ¥Â≠òÂú®ËøáÊãüÂêàÔºåËøôÂ∞±ÊòØÊúâ‰∫õËÆ°ÁÆóÊú∫ËßÜËßâÁ†îÁ©∂‰∫∫ÂëòÂ¶ÇÊ≠§ÈíüÊÉÖ‰∫édropoutÂáΩÊï∞ÁöÑÂéüÂõ†„ÄÇÁõ¥ËßÇ‰∏äÊàëËÆ§‰∏∫‰∏çËÉΩÊ¶ÇÊã¨ÂÖ∂ÂÆÉÂ≠¶Áßë„ÄÇdropoutÂ∞Ü‰∫ßÁîüÊî∂Áº©ÊùÉÈáçÁöÑÂπ≥ÊñπËåÉÊï∞ÁöÑÊïàÊûú„ÄÇÂΩìÁÑ∂Ôºå‰∏çÂêåÁöÑÂ±ÇÔºåÂÄºÂèØ‰ª•ËÆæÁΩÆÊàê‰∏çÂêåÔºåÂ¶ÇÊûú‰Ω†ËßâÂæóÊüê‰∏ÄÂ±ÇÂÆπÊòìËøáÊãüÂêàÔºåÊääÂÄºËÆæÁΩÆÂ∞è‰∏ÄÁÇπ„ÄÇ dropout ÁöÑ‰∏ÄÂ§ßÁº∫ÁÇπÊòØÊàêÊú¨ÂáΩÊï∞Êó†Ê≥ïË¢´ÊòéÁ°ÆÂÆö‰πâ„ÄÇÂõ†‰∏∫ÊØèÊ¨°Ëø≠‰ª£ÈÉΩ‰ºöÈöèÊú∫Ê∂àÈô§‰∏Ä‰∫õÁ•ûÁªèÂÖÉÁªìÁÇπÁöÑÂΩ±ÂìçÔºåÂõ†Ê≠§Êó†Ê≥ïÁ°Æ‰øùÊàêÊú¨ÂáΩÊï∞ÂçïË∞ÉÈÄíÂáè„ÄÇÂõ†Ê≠§Ôºå‰ΩøÁî® dropout Êó∂ÔºåÂÖàÂ∞Ükeep_probÂÖ®ÈÉ®ËÆæÁΩÆ‰∏∫ 1.0 ÂêéËøêË°å‰ª£Á†ÅÔºåÁ°Æ‰øù $J(w,b)‚Äã$ÂáΩÊï∞ÂçïË∞ÉÈÄíÂáèÔºåÂÜçÊâìÂºÄ dropout„ÄÇ L08 : Other Regularization Methods Êï∞ÊçÆÊâ©Â¢ûÔºàData AugmentationÔºâÔºöÈÄöËøáÂõæÁâáÁöÑ‰∏Ä‰∫õÂèòÊç¢ÔºàÁøªËΩ¨ÔºåÂ±ÄÈÉ®ÊîæÂ§ßÂêéÂàáÂâ≤Á≠âÔºâÔºåÂæóÂà∞Êõ¥Â§öÁöÑËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜ„ÄÇ Êó©ÂÅúÊ≠¢Ê≥ïÔºàEarly StoppingÔºâÔºöÂ∞ÜËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜËøõË°åÊ¢ØÂ∫¶‰∏ãÈôçÊó∂ÁöÑÊàêÊú¨ÂèòÂåñÊõ≤Á∫øÁîªÂú®Âêå‰∏Ä‰∏™ÂùêÊ†áËΩ¥ÂÜÖÔºåÂΩìËÆ≠ÁªÉÈõÜËØØÂ∑ÆÈôç‰Ωé‰ΩÜÈ™åËØÅÈõÜËØØÂ∑ÆÂçáÈ´òÔºå‰∏§ËÄÖÂºÄÂßãÂèëÁîüËæÉÂ§ßÂÅèÂ∑ÆÊó∂ÂèäÊó∂ÂÅúÊ≠¢Ëø≠‰ª£ÔºåÂπ∂ËøîÂõûÂÖ∑ÊúâÊúÄÂ∞èÈ™åËØÅÈõÜËØØÂ∑ÆÁöÑËøûÊé•ÊùÉÂíåÈòàÂÄºÔºå‰ª•ÈÅøÂÖçËøáÊãüÂêà„ÄÇËøôÁßçÊñπÊ≥ïÁöÑÁº∫ÁÇπÊòØÊó†Ê≥ïÂêåÊó∂ËææÊàêÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÁöÑÊúÄ‰ºò„ÄÇ ‰ΩÜÂØπÊàëÊù•ËØ¥early stoppingÁöÑ‰∏ªË¶ÅÁº∫ÁÇπÂ∞±ÊòØ‰Ω†‰∏çËÉΩÁã¨Á´ãÂú∞Â§ÑÁêÜËøô‰∏§‰∏™ÈóÆÈ¢òÔºåÂõ†‰∏∫ÊèêÊó©ÂÅúÊ≠¢Ê¢ØÂ∫¶‰∏ãÈôçÔºå‰πüÂ∞±ÊòØÂÅúÊ≠¢‰∫Ü‰ºòÂåñ‰ª£‰ª∑ÂáΩÊï∞ÔºåÂõ†‰∏∫Áé∞Âú®‰Ω†‰∏çÂÜçÂ∞ùËØïÈôç‰Ωé‰ª£‰ª∑ÂáΩÊï∞ÔºåÊâÄ‰ª•‰ª£‰ª∑ÂáΩÊï∞ÁöÑÂÄºÂèØËÉΩ‰∏çÂ§üÂ∞èÔºåÂêåÊó∂‰Ω†ÂèàÂ∏åÊúõ‰∏çÂá∫Áé∞ËøáÊãüÂêàÔºå‰Ω†Ê≤°ÊúâÈááÂèñ‰∏çÂêåÁöÑÊñπÂºèÊù•Ëß£ÂÜ≥Ëøô‰∏§‰∏™ÈóÆÈ¢òÔºåËÄåÊòØÁî®‰∏ÄÁßçÊñπÊ≥ïÂêåÊó∂Ëß£ÂÜ≥‰∏§‰∏™ÈóÆÈ¢òÔºåËøôÊ†∑ÂÅöÁöÑÁªìÊûúÊòØÊàëË¶ÅËÄÉËôëÁöÑ‰∏úË•øÂèòÂæóÊõ¥Â§çÊùÇ„ÄÇ Early stoppingÁöÑ‰ºòÁÇπÊòØÔºåÂè™ËøêË°å‰∏ÄÊ¨°Ê¢ØÂ∫¶‰∏ãÈôçÔºå‰Ω†ÂèØ‰ª•ÊâæÂá∫ÁöÑwËæÉÂ∞èÂÄºÔºå‰∏≠Èó¥ÂÄºÂíåËæÉÂ§ßÂÄºÔºåËÄåÊó†ÈúÄÂ∞ùËØïÊ≠£ÂàôÂåñË∂ÖÁ∫ßÂèÇÊï∞ÁöÑÂæàÂ§öÂÄº„ÄÇ L09 Ôºö Normalizing inputs Èõ∂ÂùáÂÄº $u=\frac{1}{m}\sum x^{(i)}$,$x-u$ ÂΩí‰∏ÄÂåñÊñπÂ∑ÆÔºõ $\delta^2=\frac{1}{m}(x^{(i)})^2$,ÊØè‰∏™ÁâπÂæÅÁöÑÊñπÂ∑ÆÔºåÊØè‰∏™ÁâπÂæÅÊï∞ÊçÆÈô§‰ª•ÂÆÉÔºåÂ∞±ÂΩí‰∏ÄÂåñÊñπÂ∑Æ‰∫Ü why Âú®‰∏ç‰ΩøÁî®Ê†áÂáÜÂåñÁöÑÊàêÊú¨ÂáΩÊï∞‰∏≠ÔºåÂ¶ÇÊûúËÆæÁΩÆ‰∏Ä‰∏™ËæÉÂ∞èÁöÑÂ≠¶‰π†ÁéáÔºåÂèØËÉΩÈúÄË¶ÅÂæàÂ§öÊ¨°Ëø≠‰ª£ÊâçËÉΩÂà∞ËææÂÖ®Â±ÄÊúÄ‰ºòËß£ÔºõËÄåÂ¶ÇÊûú‰ΩøÁî®‰∫ÜÊ†áÂáÜÂåñÔºåÈÇ£‰πàÊó†ËÆ∫‰ªéÂì™‰∏™‰ΩçÁΩÆÂºÄÂßãËø≠‰ª£ÔºåÈÉΩËÉΩ‰ª•Áõ∏ÂØπËæÉÂ∞ëÁöÑËø≠‰ª£Ê¨°Êï∞ÊâæÂà∞ÂÖ®Â±ÄÊúÄ‰ºòËß£„ÄÇ L10 : Vanishing /Exploding GradientsËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÔºåÂ∞§ÂÖ∂ÊòØÊ∑±Â∫¶Á•ûÁªèÊâÄÈù¢‰∏¥ÁöÑ‰∏Ä‰∏™ÈóÆÈ¢òÂ∞±ÊòØÊ¢ØÂ∫¶Ê∂àÂ§±ÊàñÊ¢ØÂ∫¶ÁàÜÁÇ∏Ôºå‰πüÂ∞±ÊòØ‰Ω†ËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªúÁöÑÊó∂ÂÄôÔºåÂØºÊï∞ÊàñÂù°Â∫¶ÊúâÊó∂‰ºöÂèòÂæóÈùûÂ∏∏Â§ßÔºåÊàñËÄÖÈùûÂ∏∏Â∞èÔºåÁîöËá≥‰∫é‰ª•ÊåáÊï∞ÊñπÂºèÂèòÂ∞èÔºåËøôÂä†Â§ß‰∫ÜËÆ≠ÁªÉÁöÑÈöæÂ∫¶„ÄÇ Âú®Ê∑±Â∫¶Á•ûÁªèÁΩëÁªú‰∏≠ÔºåÊøÄÊ¥ªÂáΩÊï∞Â∞Ü‰ª•ÊåáÊï∞Á∫ßÈÄíÂáèÔºåËôΩÁÑ∂ÊàëÂè™ÊòØËÆ®ËÆ∫‰∫ÜÊøÄÊ¥ªÂáΩÊï∞‰ª•‰∏éÁõ∏ÂÖ≥ÁöÑÊåáÊï∞Á∫ßÊï∞Â¢ûÈïøÊàñ‰∏ãÈôçÔºåÂÆÉ‰πüÈÄÇÁî®‰∫é‰∏éÂ±ÇÊï∞Áõ∏ÂÖ≥ÁöÑÂØºÊï∞ÊàñÊ¢ØÂ∫¶ÂáΩÊï∞Ôºå‰πüÊòØÂëàÊåáÊï∞Á∫ßÂ¢ûÈïøÊàñÂëàÊåáÊï∞ÈÄíÂáè„ÄÇ ÂÅáÂÆö g(z)=z,b[l]=0g(z)=z,b[l]=0ÔºåÂØπ‰∫éÁõÆÊ†áËæìÂá∫ÊúâÔºö $y^=W[L]W[L‚àí1]‚Ä¶W[2]W[1]X$ ÂØπ‰∫é$ W[l]$ÁöÑÂÄºÂ§ß‰∫é 1 ÁöÑÊÉÖÂÜµÔºåÊøÄÊ¥ªÂáΩÊï∞ÁöÑÂÄºÂ∞Ü‰ª•ÊåáÊï∞Á∫ßÈÄíÂ¢ûÔºõ ÂØπ‰∫é $W[l]$ÁöÑÂÄºÂ∞è‰∫é 1 ÁöÑÊÉÖÂÜµÔºåÊøÄÊ¥ªÂáΩÊï∞ÁöÑÂÄºÂ∞Ü‰ª•ÊåáÊï∞Á∫ßÈÄíÂáè„ÄÇ ÂØπ‰∫éÂØºÊï∞ÂêåÁêÜ„ÄÇÂõ†Ê≠§ÔºåÂú®ËÆ°ÁÆóÊ¢ØÂ∫¶Êó∂ÔºåÊ†πÊçÆ‰∏çÂêåÊÉÖÂÜµÊ¢ØÂ∫¶ÂáΩÊï∞‰ºö‰ª•ÊåáÊï∞Á∫ßÈÄíÂ¢ûÊàñÈÄíÂáèÔºåÂØºËá¥ËÆ≠ÁªÉÂØºÊï∞ÈöæÂ∫¶‰∏äÂçáÔºåÊ¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÁöÑÊ≠•Èïø‰ºöÂèòÂæóÈùûÂ∏∏Â∞èÔºåÈúÄË¶ÅËÆ≠ÁªÉÁöÑÊó∂Èó¥Â∞Ü‰ºöÈùûÂ∏∏Èïø„ÄÇ L11 : Weight initialization in a deep network‰∏∫‰∫ÜÈ¢ÑÈò≤ÂÄºzËøáÂ§ßÊàñËøáÂ∞èÔºå‰Ω†ÂèØ‰ª•ÁúãÂà∞nË∂äÂ§ßÔºå‰Ω†Â∏åÊúõwË∂äÂ∞èÔºåÂõ†‰∏∫zÊòØwx+bÁöÑÂíå,ÊúÄÂêàÁêÜÁöÑÊñπÊ≥ï$w_i=1/n$ Âõ†Ê≠§ÔºåÂÆûÈôÖ‰∏äÔºå‰Ω†Ë¶ÅÂÅöÁöÑÂ∞±ÊòØËÆæÁΩÆÊüêÂ±ÇÊùÉÈáçÁü©Èòµ $w^{[l]}=n p . random. randn (shape) * np.sqrt \left(\frac{1}{n^{[l-1]}}\right)‚Äã$ ÂΩìÂ§ö‰∏™ËäÇÁÇπÊó∂Ôºå‰πü‰∏ÄÊ†∑ÁöÑÁúãÔºå‰ΩøÂæóËøô‰∏™ËäÇÁÇπ$z^{L}$‰∏çË¶ÅÂ§™Â§ßÔºåÂçïÁã¨ÁúãÊØè‰∏™ËäÇÁÇπÊó¢ÂèØ‰ª• relu : var(w(i)) = 2/n or $\frac{2}{n^{[l-1]}*n^{[l]}}$ tanh: var(w(i)) = 1/n ÈÄöËøáËÆæÁΩÆÂàùÂßãÂåñÂåñÊùÉÈáçÁü©ÈòµÔºå‰ΩøÂæó‰∏ç‰ºöÂ¢ûÈïøÂ§™Âø´ÊàñËÄÖÂ§™ÊÖ¢ L12 Ôºö Numerical Approximations of GradientsÂçïËæπËØØÂ∑Æ $f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0} \frac{f(\theta+\varepsilon)-(\theta)}{\varepsilon}$ ËØØÂ∑Æ$O(\varepsilon)$ ÂèåËæπËØØÂ∑Æ $f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0}=\frac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2 \varepsilon}$ $O\left(\varepsilon^{2}\right)$ L 13 Gradient CheckingÊ¢ØÂ∫¶Ê£ÄÈ™åÂ∏ÆÊàë‰ª¨ËäÇÁúÅ‰∫ÜÂæàÂ§öÊó∂Èó¥Ôºå‰πüÂ§öÊ¨°Â∏ÆÊàëÂèëÁé∞backpropÂÆûÊñΩËøáÁ®ã‰∏≠ÁöÑbugÔºåÊé•‰∏ãÊù•ÔºåÊàë‰ª¨ÁúãÁúãÂ¶Ç‰ΩïÂà©Áî®ÂÆÉÊù•Ë∞ÉËØïÊàñÊ£ÄÈ™åbackpropÁöÑÂÆûÊñΩÊòØÂê¶Ê≠£Á°Æ„ÄÇ È¶ñÂÖàË¶ÅÂÅöÁöÑÂ∞±ÊòØÔºåÊääÊâÄÊúâÂèÇÊï∞ËΩ¨Êç¢Êàê‰∏Ä‰∏™Â∑®Â§ßÁöÑÂêëÈáèÊï∞ÊçÆÔºå‰Ω†Ë¶ÅÂÅöÁöÑÂ∞±ÊòØÊääÁü©ÈòµwËΩ¨Êç¢Êàê‰∏Ä‰∏™ÂêëÈáèÔºåÊääÊâÄÊúâÁü©ÈòµwËΩ¨Êç¢ÊàêÂêëÈáè‰πãÂêéÔºåÂÅöËøûÊé•ËøêÁÆóÔºåÂæóÂà∞‰∏Ä‰∏™Â∑®ÂûãÂêëÈáè$\theta$ÔºåËØ•ÂêëÈáèË°®Á§∫‰∏∫ÂèÇÊï∞$\theta$Ôºå‰ª£‰ª∑ÂáΩÊï∞JÊòØÊâÄÊúâWÂíåbÁöÑÂáΩÊï∞ÔºåÁé∞Âú®‰Ω†ÂæóÂà∞‰∫Ü‰∏Ä‰∏™ÁöÑ‰ª£‰ª∑ÂáΩÊï∞ÔºàÂç≥Ôºâ„ÄÇÊé•ÁùÄÔºå‰Ω†ÂæóÂà∞‰∏éÂíåÈ°∫Â∫èÁõ∏ÂêåÁöÑÊï∞ÊçÆÔºå‰Ω†ÂêåÊ†∑ÂèØ‰ª•Êää$dW^{[l]}$,Âíå$db^{[l]}$ ËΩ¨Êç¢Êàê‰∏Ä‰∏™Êñ∞ÁöÑÂêëÈáèÔºåÁî®ÂÆÉ‰ª¨Êù•ÂàùÂßãÂåñÂ§ßÂêëÈáè$d\theta$ÔºåÂÆÉ‰∏é$\theta$ÂÖ∑ÊúâÁõ∏ÂêåÁª¥Â∫¶„ÄÇ Ê¢ØÂ∫¶ÁöÑÈÄºËøëÂÄº d \theta_{\text { approx }}[i]=\frac{J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}+\varepsilon, \ldots\right)-J\left(\theta_{1}, \theta_{2}, \ldots . \theta_{i}-\varepsilon, \ldots\right)}{2 \varepsilon} Áé∞Âú®‰Ω†Â∑≤Áªè‰∫ÜËß£‰∫ÜÊ¢ØÂ∫¶Ê£ÄÈ™åÁöÑÂ∑•‰ΩúÂéüÁêÜÔºåÂÆÉÂ∏ÆÂä©ÊàëÂú®Á•ûÁªèÁΩëÁªúÂÆûÊñΩ‰∏≠ÂèëÁé∞‰∫ÜÂæàÂ§öbugÔºåÂ∏åÊúõÂÆÉÂØπ‰Ω†‰πüÊúâÊâÄÂ∏ÆÂä©„ÄÇ L 14 : Gradient Checking Implementation notes ‰∏çË¶ÅÂú®ËÆ≠ÁªÉ‰∏≠‰ΩøÁî®Ê¢ØÂ∫¶Ê£ÄÈ™åÔºåÂÆÉÂè™Áî®‰∫éË∞ÉËØïÔºàdebugÔºâ„ÄÇ‰ΩøÁî®ÂÆåÊØïÂÖ≥Èó≠Ê¢ØÂ∫¶Ê£ÄÈ™åÁöÑÂäüËÉΩÔºõÂ§™ÊÖ¢‰∫Ü Â¶ÇÊûúÁÆóÊ≥ïÁöÑÊ¢ØÂ∫¶Ê£ÄÈ™åÂ§±Ë¥•ÔºåË¶ÅÊ£ÄÊü•ÊâÄÊúâÈ°πÔºåÂπ∂ËØïÁùÄÊâæÂá∫ bugÔºåÂç≥Á°ÆÂÆöÂì™‰∏™ dŒ∏approx[i] ‰∏é dŒ∏ ÁöÑÂÄºÁõ∏Â∑ÆÊØîËæÉÂ§ßÔºõ ÂΩìÊàêÊú¨ÂáΩÊï∞ÂåÖÂê´Ê≠£ÂàôÈ°πÊó∂Ôºå‰πüÈúÄË¶ÅÂ∏¶‰∏äÊ≠£ÂàôÈ°πËøõË°åÊ£ÄÈ™åÔºõ Ê¢ØÂ∫¶Ê£ÄÈ™å‰∏çËÉΩ‰∏é dropout ÂêåÊó∂‰ΩøÁî®„ÄÇÂõ†‰∏∫ÊØèÊ¨°Ëø≠‰ª£ËøáÁ®ã‰∏≠Ôºådropout ‰ºöÈöèÊú∫Ê∂àÈô§ÈöêËóèÂ±ÇÂçïÂÖÉÁöÑ‰∏çÂêåÂ≠êÈõÜÔºåÈöæ‰ª•ËÆ°ÁÆó dropout Âú®Ê¢ØÂ∫¶‰∏ãÈôç‰∏äÁöÑÊàêÊú¨ÂáΩÊï∞ J„ÄÇÂª∫ËÆÆÂÖ≥Èó≠ dropoutÔºåÁî®Ê¢ØÂ∫¶Ê£ÄÈ™åËøõË°åÂèåÈáçÊ£ÄÊü•ÔºåÁ°ÆÂÆöÂú®Ê≤°Êúâ dropout ÁöÑÊÉÖÂÜµ‰∏ãÁÆóÊ≥ïÊ≠£Á°ÆÔºåÁÑ∂ÂêéÊâìÂºÄ dropoutÔºõ SummaryÂõûÈ°æËøô‰∏ÄÂë®ÔºåÊàë‰ª¨ËÆ≤‰∫ÜÂ¶Ç‰ΩïÈÖçÁΩÆËÆ≠ÁªÉÈõÜÔºåÈ™åËØÅÈõÜÂíåÊµãËØïÈõÜÔºåÂ¶Ç‰ΩïÂàÜÊûêÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÔºåÂ¶Ç‰ΩïÂ§ÑÁêÜÈ´òÂÅèÂ∑ÆÊàñÈ´òÊñπÂ∑Æ‰ª•ÂèäÈ´òÂÅèÂ∑ÆÂíåÈ´òÊñπÂ∑ÆÂπ∂Â≠òÁöÑÈóÆÈ¢òÔºåÂ¶Ç‰ΩïÂú®Á•ûÁªèÁΩëÁªú‰∏≠Â∫îÁî®‰∏çÂêåÂΩ¢ÂºèÁöÑÊ≠£ÂàôÂåñÔºåÂ¶ÇÊ≠£ÂàôÂåñÂíå**dropout**ÔºåËøòÊúâÂä†Âø´Á•ûÁªèÁΩëÁªúËÆ≠ÁªÉÈÄüÂ∫¶ÁöÑÊäÄÂ∑ßÔºåÊúÄÂêéÊòØÊ¢ØÂ∫¶Ê£ÄÈ™å„ÄÇ C2W2 :Optimization AlgorithmL 01 : Mini Batch Gradient Descent Vectorization Mini batch not entire training set bady training set iÔºå$x^{\{i\}}$ mini batch training set mini batch gradient descent L 02 : Understanding Mini-Batch Gradient Decent Â∑¶ÂõæÔºåÈöèÁùÄiterations increased, it should decrease .if it ever goes up on iteration,something is wrong. Âè≥Âõæ : it‚Äôs as if on every iteration you‚Äôre training on a different training set or really training on a different mini batch. It should trend downwards, but it‚Äôs also going to be a little bit noisier.So if you plot J{t}, as you‚Äôre training mini batch in descent it may be over multiple epochs,you might expect to see a curve like this. Choosing your mini-batch size 1. ‰ºòÁº∫ÁÇπ ÈÄöËøáÂáèÂ∞èÂ≠¶‰π†ÁéáÔºåÂô™Â£∞‰ºöË¢´ÊîπÂñÑÊàñÊúâÊâÄÂáèÂ∞èÔºå‰ΩÜÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÁöÑ‰∏ÄÂ§ßÁº∫ÁÇπÊòØÔºå‰Ω†‰ºöÂ§±ÂéªÊâÄÊúâÂêëÈáèÂåñÂ∏¶Áªô‰Ω†ÁöÑÂä†ÈÄüÔºåÂõ†‰∏∫‰∏ÄÊ¨°ÊÄßÂè™Â§ÑÁêÜ‰∫Ü‰∏Ä‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºåËøôÊ†∑ÊïàÁéáËøá‰∫é‰Ωé‰∏ãÔºåÊâÄ‰ª•ÂÆûË∑µ‰∏≠ÊúÄÂ•ΩÈÄâÊã©‰∏çÂ§ß‰∏çÂ∞èÁöÑmini-batchÂ∞∫ÂØ∏ÔºåÂÆûÈôÖ‰∏äÂ≠¶‰π†ÁéáËææÂà∞ÊúÄÂø´„ÄÇ‰Ω†‰ºöÂèëÁé∞‰∏§‰∏™Â•ΩÂ§ÑÔºå‰∏ÄÊñπÈù¢Ôºå‰Ω†ÂæóÂà∞‰∫ÜÂ§ßÈáèÂêëÈáèÂåñÔºå‰∏ä‰∏™ËßÜÈ¢ë‰∏≠Êàë‰ª¨Áî®ËøáÁöÑ‰æãÂ≠ê‰∏≠ÔºåÂ¶ÇÊûúmini-batchÂ§ßÂ∞è‰∏∫1000‰∏™Ê†∑Êú¨Ôºå‰Ω†Â∞±ÂèØ‰ª•ÂØπ1000‰∏™Ê†∑Êú¨ÂêëÈáèÂåñÔºåÊØî‰Ω†‰∏ÄÊ¨°ÊÄßÂ§ÑÁêÜÂ§ö‰∏™Ê†∑Êú¨Âø´ÂæóÂ§ö„ÄÇÂè¶‰∏ÄÊñπÈù¢Ôºå‰Ω†‰∏çÈúÄË¶ÅÁ≠âÂæÖÊï¥‰∏™ËÆ≠ÁªÉÈõÜË¢´Â§ÑÁêÜÂÆåÂ∞±ÂèØ‰ª•ÂºÄÂßãËøõË°åÂêéÁª≠Â∑•‰ΩúÔºåÂÜçÁî®‰∏Ä‰∏ã‰∏ä‰∏™ËßÜÈ¢ëÁöÑÊï∞Â≠óÔºåÊØèÊ¨°ËÆ≠ÁªÉÈõÜÂÖÅËÆ∏Êàë‰ª¨ÈááÂèñ5000‰∏™Ê¢ØÂ∫¶‰∏ãÈôçÊ≠•È™§ÔºåÊâÄ‰ª•ÂÆûÈôÖ‰∏ä‰∏Ä‰∫õ‰Ωç‰∫é‰∏≠Èó¥ÁöÑmini-batchÂ§ßÂ∞èÊïàÊûúÊúÄÂ•Ω„ÄÇ ‰ΩøÁî®batchÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊó∂ÔºåÊØèÊ¨°Ëø≠‰ª£‰Ω†ÈÉΩÈúÄË¶ÅÂéÜÈÅçÊï¥‰∏™ËÆ≠ÁªÉÈõÜÔºåÂèØ‰ª•È¢ÑÊúüÊØèÊ¨°Ëø≠‰ª£ÊàêÊú¨ÈÉΩ‰ºö‰∏ãÈôçÔºåÊâÄ‰ª•Â¶ÇÊûúÊàêÊú¨ÂáΩÊï∞ÊòØËø≠‰ª£Ê¨°Êï∞ÁöÑ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÂÆÉÂ∫îËØ•‰ºöÈöèÁùÄÊØèÊ¨°Ëø≠‰ª£ËÄåÂáèÂ∞ëÔºåÂ¶ÇÊûúÂú®ÊüêÊ¨°Ëø≠‰ª£‰∏≠Â¢ûÂä†‰∫ÜÔºåÈÇ£ËÇØÂÆöÂá∫‰∫ÜÈóÆÈ¢òÔºå‰πüËÆ∏‰Ω†ÁöÑÂ≠¶‰π†ÁéáÂ§™Â§ß„ÄÇ Âú®ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï‰∏≠Ôºå‰ªéÊüê‰∏ÄÁÇπÂºÄÂßãÔºåÊàë‰ª¨ÈáçÊñ∞ÈÄâÂèñ‰∏Ä‰∏™Ëµ∑ÂßãÁÇπÔºåÊØèÊ¨°Ëø≠‰ª£Ôºå‰Ω†Âè™ÂØπ‰∏Ä‰∏™Ê†∑Êú¨ËøõË°åÊ¢ØÂ∫¶‰∏ãÈôçÔºåÂ§ßÈÉ®ÂàÜÊó∂ÂÄô‰Ω†ÂêëÁùÄÂÖ®Â±ÄÊúÄÂ∞èÂÄºÈù†ËøëÔºåÊúâÊó∂ÂÄô‰Ω†‰ºöËøúÁ¶ªÊúÄÂ∞èÂÄºÔºåÂõ†‰∏∫ÈÇ£‰∏™Ê†∑Êú¨ÊÅ∞Â•ΩÁªô‰Ω†ÊåáÁöÑÊñπÂêë‰∏çÂØπÔºåÂõ†Ê≠§ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊòØÊúâÂæàÂ§öÂô™Â£∞ÁöÑÔºåÂπ≥ÂùáÊù•ÁúãÔºåÂÆÉÊúÄÁªà‰ºöÈù†ËøëÊúÄÂ∞èÂÄºÔºå‰∏çËøáÊúâÊó∂ÂÄô‰πü‰ºöÊñπÂêëÈîôËØØÔºåÂõ†‰∏∫ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊ∞∏Ëøú‰∏ç‰ºöÊî∂ÊïõÔºåËÄåÊòØ‰ºö‰∏ÄÁõ¥Âú®ÊúÄÂ∞èÂÄºÈôÑËøëÊ≥¢Âä®Ôºå‰ΩÜÂÆÉÂπ∂‰∏ç‰ºöÂú®ËææÂà∞ÊúÄÂ∞èÂÄºÂπ∂ÂÅúÁïôÂú®Ê≠§„ÄÇ Áî®mini-batchÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºåÊàë‰ª¨‰ªéËøôÈáåÂºÄÂßãÔºå‰∏ÄÊ¨°Ëø≠‰ª£ËøôÊ†∑ÂÅöÔºå‰∏§Ê¨°Ôºå‰∏âÊ¨°ÔºåÂõõÊ¨°ÔºåÂÆÉ‰∏ç‰ºöÊÄªÊúùÂêëÊúÄÂ∞èÂÄºÈù†ËøëÔºå‰ΩÜÂÆÉÊØîÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçË¶ÅÊõ¥ÊåÅÁª≠Âú∞Èù†ËøëÊúÄÂ∞èÂÄºÁöÑÊñπÂêëÔºåÂÆÉ‰πü‰∏ç‰∏ÄÂÆöÂú®ÂæàÂ∞èÁöÑËåÉÂõ¥ÂÜÖÊî∂ÊïõÊàñËÄÖÊ≥¢Âä®ÔºåÂ¶ÇÊûúÂá∫Áé∞Ëøô‰∏™ÈóÆÈ¢òÔºåÂèØ‰ª•ÊÖ¢ÊÖ¢ÂáèÂ∞ëÂ≠¶‰π†ÁéáÔºåÊàë‰ª¨Âú®‰∏ã‰∏™ËßÜÈ¢ë‰ºöËÆ≤Âà∞Â≠¶‰π†ÁéáË°∞ÂáèÔºå‰πüÂ∞±ÊòØÂ¶Ç‰ΩïÂáèÂ∞èÂ≠¶‰π†Áéá„ÄÇ batch : too long,too time ÈöèÊú∫Ôºö lose speeding ,Âô™Â£∞Â§ß mini-batchÂíåstochasticÈÉΩÂ≠òÂú®Âô™Â£∞ÈóÆÈ¢òÔºå‰∏îÂú®local optimaÈôÑËøë‰ºöÂæòÂæä„ÄÇ‰ΩÜËÆæÁΩÆÂêàÈÄÇÂ§ßÂ∞èÁöÑmini-batch sizeÔºåÂô™Â£∞ÂíåÂæòÂæäÈóÆÈ¢òÂèØÊé•ÂèóÁöÑËåÉÂõ¥ÂÜÖ„ÄÇ size=1,ÂèàÂè´ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï stochastic gradient descent howÂ¶Ç‰ΩïÈÄâÊã©mini-batch sizeÔºàËøôÊòØ‰∏Ä‰∏™hyperparameterÔºâÔºö Â∞èÊï∞ÊçÆÈáèÔºåÊØîÂ¶ÇÊÄªÁöÑÊ†∑Êú¨Âè™ÊúâÂá†ÂçÉ‰∏™ÔºåÂÆåÂÖ®ÂèØ‰ª•Áõ¥Êé•Áî®batch gradient descent Â§ßÊï∞ÈáèÔºåmini-batch sizeÂÄæÂêë‰∫éÈÄâÊã©2^n‰∏™ÔºåÊØîÂ¶Ç64, 128, 256Á≠â mini-batch ‰∏éCPU/GPU memoryÁöÑÂÜÖÂ≠òÂÆπÈáè„ÄÇ In practice of course the mini batch size is another hyper parameter that you might do a quick search over to try to figure out which one is most sufficient of reducing the cost function j. ÊåâÁÖß‰∏äÈù¢ÁöÑÊñπÊ≥ï L 03: Exponentially Weighted AveragesIn order to understand those algorithms, you need to be able they use something called exponentially weighted averages. Also called exponentially weighted moving averages in statistics. 1. ÊåáÊï∞Âä†ÊùÉÂπ≥ÂùáÊï∞ÔºàExponentially weighted averagesÔºâ $\theta _i$Ë°®Á§∫ÊØè‰∏ÄÊó•ÁöÑÊ∏©Â∫¶ÂÄºÔºåËìùËâ≤ÁöÑÁÇπÔºå$v_t$Ë°®Á§∫Âä†ÊùÉÂπ≥ÂùáÂêéÁöÑ,Á∫¢Ëâ≤ ÊùÉÂπ≥ÂùáÊñπÊ≥ïÊòØÔºöÊØèÂ§©ÁöÑÊ∏©Â∫¶ÂÄºÂä†ÊùÉÂÄº$vt$ËÆæÁΩÆ‰∏∫Ââç‰∏ÄÂ§©ÁöÑÊ∏©Â∫¶Âä†ÊùÉÂÄº$vt‚àí1$ÂíåÂΩìÂ§©ÁöÑÊ∏©Â∫¶ÂÆûÈôÖÂÄº$Œ∏t$ÂÅöÂä†ÊùÉÂπ≥ÂùáÔºö v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}Áî±‰∫é‰ª•ÂêéÊàë‰ª¨Ë¶ÅËÄÉËôëÁöÑÂéüÂõ†ÔºåÂú®ËÆ°ÁÆóÊó∂ÂèØËßÜ$v_T$Â§ßÊ¶ÇÊòØ$\frac{1}{(1-\beta)}$ÁöÑÊØèÊó•Ê∏©Â∫¶ÁöÑÂä†ÊùÉÂπ≥ÂùáÔºå Â¶ÇÊûúÊòØ$\beta$=0.9ÔºåËøôÊòØÂçÅÂ§©ÁöÑÂπ≥ÂùáÂÄºÔºåÁ∫¢Ëâ≤ Â¶ÇÊûú$\beta$=0.98,ÊòØ50Â§©ÁöÑÁªìÊûúÔºåÁªøËâ≤ Â¶ÇÊûú$beta$=0.5,ÊòØ2dayÁöÑÁªìÊûúÔºåÈªÑËâ≤ Áî±‰∫é‰ªÖÂπ≥Âùá‰∫Ü‰∏§Â§©ÁöÑÊ∏©Â∫¶ÔºåÂπ≥ÂùáÁöÑÊï∞ÊçÆÂ§™Â∞ëÔºåÊâÄ‰ª•ÂæóÂà∞ÁöÑÊõ≤Á∫øÊúâÊõ¥Â§öÁöÑÂô™Â£∞ÔºåÊúâÂèØËÉΩÂá∫Áé∞ÂºÇÂ∏∏ÂÄºÔºå‰ΩÜÊòØËøô‰∏™Êõ≤Á∫øËÉΩÂ§üÊõ¥Âø´ÈÄÇÂ∫îÊ∏©Â∫¶ÂèòÂåñ„ÄÇ ÂΩì $\beta$ËæÉÂ§ßÊó∂ÔºåÊåáÊï∞Âä†ÊùÉÂπ≥ÂùáÂÄºÈÄÇÂ∫îÂú∞Êõ¥ÁºìÊÖ¢‰∏Ä‰∫õ„ÄÇ $ L 04 : Understanding Exponentially Weighted AveragesÂÅáÂ¶ÇŒ≤=0.9ÔºåÊØè‰∏™vÁöÑËÆ°ÁÆóÂ¶Ç‰∏ãÔºö \begin{aligned} v_{100} &=0.9 v_{99}+0.1 \theta_{100} \\ v_{99} &=0.9 v_{98}+0.1 \theta_{99} \\ v_{98} &=0.9 v_{97}+0.1 \theta_{98} \end{aligned}ÈÄíÊé®ÂèØÂæóÔºö v_{100}=0.1 \theta_{100}+0.1 * 0.9 \theta_{99}+0.1 *(0.9)^{2} \theta_{98}+\ldotsÊåáÊï∞ÁöÑË°∞ÂáèËßÑÂæã ‰∏ÄËà¨ÁöÑ v_{t}=\sum_{i=1}^{t}(1-\beta) \beta^{t-i} \theta_{i}Êó†Á©∑Á∫ßÊï∞Ê±ÇÂíåÔºö \sum_{t=1}^{n}(1-\beta) \beta^{t}=1Âõ†Ê≠§ÂèØ‰ª•Ëøë‰ººÁöÑËÆ§‰∏∫ÊâÄÊúâÈ°πÁöÑÁ≥ªÊï∞‰πãÂíåÊ≠£Â•Ω‰∏∫100%„ÄÇ Âç≥Ôºå$vt$ÊòØÂØπtÊó•‰πãÂâçÊâÄÊúâÁöÑÂÆûÈôÖÊ∏©Â∫¶ÁöÑÂä†ÊùÉÂπ≥Âùá,ÊùÉÈáçÊòØÊåáÊï∞ÈÄíÂáèÁöÑ„ÄÇ ÂçÅÂ§©ÂêéÔºåÊõ≤Á∫øÈ´òÂ∫¶‰∏ãÈôçÂà∞‰∫Ü1/3,Ëµã‰∫àÊùÉÈáç$\beta^{t-i}$ 0.9^{10}~=0.35~=1/e‰∏ÄËà¨ËÆ§‰∏∫Ôºå$v_t$Ëøë‰ººÂâç$\frac{1}{1-\beta}$ÁöÑÂä†ÊùÉÂπ≥ÂùáÂÄº L05 : Bias correction in exponentially weighted averagesÊåáÊï∞Âä†ÊùÉÂπ≥ÂùáÁöÑÂÅèÂ∑Æ‰øÆÊ≠£ Áî±‰∫éËÆ°ÁÆó$v1$ÁöÑÊó∂ÂÄôÔºåÂπ∂Ê≤°ÊúâÂéÜÂè≤ÂÄºÂÅöÂä†ÊùÉÔºåËøô‰∏™Êó∂ÂÄô‰ª§ÂÖ∂Ââç‰∏Ä‰∏™Âä†ÊùÉÂÄº$v0=0$ÔºåÂàô‰ºöÂØºËá¥$v_1$ËøúÂ∞è‰∫é$\theta_1$,‰æùÊ¨°Á±ªÊé®ÔºåÂú®Èù†ËøëÂâçÈù¢ÁöÑÂÄº‰ºöÂá∫Áé∞ÊòæËëóÁöÑÂ∞è‰∫éÂÆûÈôÖÂÄºÁöÑÊÉÖÂÜµ Âõ†Ê≠§ÂÅö‰∏Ä‰∏™‰øÆÊ≠£ v_{t}=\frac{\beta v_{t-1}+(1-\beta) \theta_{t}}{1-\beta^{t}}‰Ω†‰ºöÂèëÁé∞ÈöèÁùÄ$\beta^t$Â¢ûÂä†ÔºåÊé•Ëøë‰∫é0ÔºåÊâÄ‰ª•ÂΩìtÂæàÂ§ßÁöÑÊó∂ÂÄôÔºåÂÅèÂ∑Æ‰øÆÊ≠£Âá†‰πéÊ≤°Êúâ‰ΩúÁî®ÔºåÂõ†Ê≠§ÂΩìtËæÉÂ§ßÁöÑÊó∂ÂÄôÔºåÁ¥´Á∫øÂü∫Êú¨ÂíåÁªøÁ∫øÈáçÂêà‰∫Ü„ÄÇ‰∏çËøáÂú®ÂºÄÂßãÂ≠¶‰π†Èò∂ÊÆµÔºå‰Ω†ÊâçÂºÄÂßãÈ¢ÑÊµãÁÉ≠Ë∫´ÁªÉ‰π†ÔºåÂÅèÂ∑Æ‰øÆÊ≠£ÂèØ‰ª•Â∏ÆÂä©‰Ω†Êõ¥Â•ΩÈ¢ÑÊµãÊ∏©Â∫¶ÔºåÂÅèÂ∑Æ‰øÆÊ≠£ÂèØ‰ª•Â∏ÆÂä©‰Ω†‰ΩøÁªìÊûú‰ªéÁ¥´Á∫øÂèòÊàêÁªøÁ∫ø„ÄÇ Âõ†‰∏∫Âú®Machine Learning‰∏≠ÁúãÈáçÁöÑÊòØÂæàÂ§öÊ¨°Ëø≠‰ª£ÂêéÁöÑÁªìÊûúÔºåÂàùÊúüÁöÑÂÅèÂ∑ÆÂΩ±ÂìçÂπ∂‰∏çÂ§ß„ÄÇ L 06 : Gradient Descent With MomentumÂä®ÈáèÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºåËøêË°åÈÄüÂ∫¶Âá†‰πéÊÄªÊòØÂø´‰∫éÊ†áÂáÜÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÁÆóÊ≥ïÔºå ÂΩìÊÖ¢ÊÖ¢‰∏ãÈôçÂà∞ÊúÄÂ∞èÂÄºÔºå‰∏ä‰∏ãÊ≥¢Âä®ÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÁöÑÈÄüÂ∫¶ÂáèÁºìÔºåÊó†Ê≥ï‰ΩøÁî®Êõ¥Â§ßÁöÑÂ≠¶‰π†ÁéáÔºå Âú®Á∫µËΩ¥‰∏äÔºåÂ∏åÊúõÂ≠¶Ê†°ÊÖ¢‰∏ÄÁÇπÔºå‰∏çÈúÄË¶ÅÊëÜÂä®ÔºåÊ®™ÁùÄ‰∏äÔºåÂä†Âø´Â≠¶Ê†°ÔºåÂü∫‰∫éÊ≠§Â∞±Êúâ‰∫ÜGradient descent with momentum„ÄÇ \begin{array}{c}{v_{d W} :=\beta v_{d W}+(1-\beta) d W} \\ {v_{d b} :=\beta v_{d b}+(1-\beta) d b} \\ {w=w-\alpha v_{d W}} \\ {b=b-\alpha v_{d b}}\end{array}ËøôÊ†∑ÔºåÂèØ‰ª•ËÆ©gradientÊõ¥Âπ≥Êªë ÂØπ‰∫é‰∏äÂõæÂûÇÁõ¥ÊñπÂêëÔºåÂéüÊù•ÊòØ‰ºö‰∏ä‰∏ãÈúáËç°Ôºå‰ΩÜÂºïÂÖ•‰∫Üexponentially weighted averageÔºåÁõ∏ÂΩì‰∫éÂØπÂâçÈù¢ÁöÑÈúáËç°ËøõË°å‰∫ÜÂπ≥ÂùáÔºåÁªìÊûúÂ∞±ÊòØ‰∏ä‰∏ãÈúáËç°‰∫íÁõ∏ÊäµÊ∂à‰∫Ü„ÄÇËÄåÊ∞¥Âπ≥ÊñπÂêëÈÉΩÊòØÂêëÂè≥Ê≤°ÊúâÈúáËç°ÔºåÂõ†Ê≠§Âπ≥ÂùáÂêéËøòÊòØÂêëÂè≥„ÄÇÊúÄÁªàÂØºËá¥ÂëàÁé∞‰∏äÂõæÁ∫¢Ëâ≤ÁöÑ‰∏ãÈôçË∑ØÁ∫ø„ÄÇ L 07 : RMSpropRMSprop (Root Mean Square PropagationÔºåÂùáÊñπÊ†π‰º†ÈÄí)Ôºå‰∏émomentum‰∏ÄÊ†∑Ôºå‰πüÊòØÈôç‰ΩéÊ¢ØÂ∫¶ÁöÑÊäñÂä®„ÄÇËÄåÊòØÂπ≥Êäë‰∏çÂêåÂ§ßÂ∞èÊ¢ØÂ∫¶ÁöÑÊõ¥Êñ∞ÈÄüÁéá„ÄÇÂÆûÈôÖ‰∏ä ‰ΩúÁî®Âú®Œ±‰∏äÁöÑ ÂõûÂøÜ‰∏Ä‰∏ãÊàë‰ª¨‰πãÂâçÁöÑ‰æãÂ≠êÔºåÂ¶ÇÊûú‰Ω†ÊâßË°åÊ¢ØÂ∫¶‰∏ãÈôçÔºåËôΩÁÑ∂Ê®™ËΩ¥ÊñπÂêëÊ≠£Âú®Êé®ËøõÔºå‰ΩÜÁ∫µËΩ¥ÊñπÂêë‰ºöÊúâÂ§ßÂπÖÂ∫¶ÊëÜÂä®Ôºå‰∏∫‰∫ÜÂàÜÊûêËøô‰∏™‰æãÂ≠êÔºåÂÅáËÆæbÁ∫µËΩ¥‰ª£Ë°®ÂèÇÊï∞ÔºåÊ®™ËΩ¥‰ª£Ë°®ÂèÇÊï∞WÔºåÂèØËÉΩÊúâw1ÔºåÊàñËÄÖw2ÂÖ∂ÂÆÉÈáçË¶ÅÁöÑÂèÇÊï∞Ôºå‰∏∫‰∫Ü‰æø‰∫éÁêÜËß£ÔºåË¢´Áß∞‰∏∫bÂíåw„ÄÇ Êàë‰ª¨Â∏åÊúõÂ≠¶‰π†ÈÄüÂ∫¶Âø´ÔºåËÄåÂú®ÂûÇÁõ¥ÊñπÂêëÔºå‰πüÂ∞±ÊòØ‰æãÂ≠ê‰∏≠ÁöÑÊñπÂêëÔºåÊàë‰ª¨Â∏åÊúõÂáèÁºìÁ∫µËΩ¥‰∏äÁöÑÊëÜÂä®ÔºåÊâÄ‰ª•Êúâ‰∫Ü$S_{d W} $Âíå$ S_{d b}$ÔºåÊàë‰ª¨Â∏åÊúõ$S_{d W} $‰ºöÁõ∏ÂØπËæÉÂ∞èÔºåÊâÄ‰ª•Êàë‰ª¨Ë¶ÅÈô§‰ª•‰∏Ä‰∏™ËæÉÂ∞èÁöÑÊï∞ÔºåËÄåÂ∏åÊúõ$ S_{d b}$ÂèàËæÉÂ§ßÔºåÊâÄ‰ª•ËøôÈáåÊàë‰ª¨Ë¶ÅÈô§‰ª•ËæÉÂ§ßÁöÑÊï∞Â≠óÔºåËøôÊ†∑Â∞±ÂèØ‰ª•ÂáèÁºìÁ∫µËΩ¥‰∏äÁöÑÂèòÂåñ„ÄÇ Ëøô‰∫õÂæÆÂàÜÔºåÂûÇÁõ¥ÊñπÂêëÁöÑË¶ÅÊØîÊ∞¥Âπ≥ÊñπÂêëÁöÑÂ§ßÂæóÂ§öÔºåÊâÄ‰ª•ÊñúÁéáÂú®ÊñπÂêëÁâπÂà´Â§ßÔºåÊâÄ‰ª•Ëøô‰∫õÂæÆÂàÜ‰∏≠ÔºådbËæÉÂ§ßÔºådwËæÉÂ∞èÔºåÂõ†‰∏∫ÂáΩÊï∞ÁöÑÂÄæÊñúÁ®ãÂ∫¶ÔºåÂú®Á∫µËΩ¥‰∏äÔºå‰πüÂ∞±ÊòØbÊñπÂêë‰∏äË¶ÅÂ§ß‰∫éÂú®Ê®™ËΩ¥‰∏äÔºå‰πüÂ∞±ÊòØÊñπÂêë‰∏äW„ÄÇdbÁöÑÂπ≥ÊñπËæÉÂ§ßÔºåÊâÄ‰ª•$Sdb$‰πü‰ºöËæÉÂ§ßÔºåËÄåÁõ∏ÊØî‰πã‰∏ãÔºådw‰ºöÂ∞è‰∏Ä‰∫õÔºå‰∫¶ÊàñdwÂπ≥Êñπ‰ºöÂ∞è‰∏Ä‰∫õÔºåÂõ†Ê≠§$Sdw$‰ºöÂ∞è‰∏Ä‰∫õÔºåÁªìÊûúÂ∞±ÊòØÁ∫µËΩ¥‰∏äÁöÑÊõ¥Êñ∞Ë¶ÅË¢´‰∏Ä‰∏™ËæÉÂ§ßÁöÑÊï∞Áõ∏Èô§ÔºåÂ∞±ËÉΩÊ∂àÈô§ÊëÜÂä®ÔºåËÄåÊ∞¥Âπ≥ÊñπÂêëÁöÑÊõ¥Êñ∞ÂàôË¢´ËæÉÂ∞èÁöÑÊï∞Áõ∏Èô§„ÄÇ RMSpropÁöÑÂΩ±ÂìçÂ∞±ÊòØ‰Ω†ÁöÑÊõ¥Êñ∞ÊúÄÂêé‰ºöÂèòÊàêËøôÊ†∑ÔºàÁªøËâ≤Á∫øÔºâÔºåÁ∫µËΩ¥ÊñπÂêë‰∏äÊëÜÂä®ËæÉÂ∞èÔºåËÄåÊ®™ËΩ¥ÊñπÂêëÁªßÁª≠Êé®Ëøõ„ÄÇËøòÊúâ‰∏™ÂΩ±ÂìçÂ∞±ÊòØÔºå‰Ω†ÂèØ‰ª•Áî®‰∏Ä‰∏™Êõ¥Â§ßÂ≠¶‰π†ÁéáÔºåÁÑ∂ÂêéÂä†Âø´Â≠¶‰π†ÔºåËÄåÊó†È°ªÂú®Á∫µËΩ¥‰∏äÂûÇÁõ¥ÊñπÂêëÂÅèÁ¶ª„ÄÇ ÂÆûÈôÖ‰∏≠dwÊòØ‰∏Ä‰∏™È´òÁª¥Â∫¶ÁöÑÂèÇÊï∞ÂêëÈáèÔºådb‰πüÊòØ‰∏Ä‰∏™È´òÁª¥Â∫¶ÂèÇÊï∞ÂêëÈáèÔºå‰ΩÜÊòØ‰Ω†ÁöÑÁõ¥ËßâÊòØÔºåÂú®‰Ω†Ë¶ÅÊ∂àÈô§ÊëÜÂä®ÁöÑÁª¥Â∫¶‰∏≠ÔºåÊúÄÁªà‰Ω†Ë¶ÅËÆ°ÁÆó‰∏Ä‰∏™Êõ¥Â§ßÁöÑÂíåÂÄºÔºåËøô‰∏™Âπ≥ÊñπÂíåÂæÆÂàÜÁöÑÂä†ÊùÉÂπ≥ÂùáÂÄºÔºåÊâÄ‰ª•‰Ω†ÊúÄÂêéÂéªÊéâ‰∫ÜÈÇ£‰∫õÊúâÊëÜÂä®ÁöÑÊñπÂêë„ÄÇÊâÄ‰ª•ËøôÂ∞±ÊòØRMSpropÔºåÂÖ®Áß∞ÊòØÂùáÊñπÊ†πÔºåÂõ†‰∏∫‰Ω†Â∞ÜÂæÆÂàÜËøõË°åÂπ≥ÊñπÔºåÁÑ∂ÂêéÊúÄÂêé‰ΩøÁî®Âπ≥ÊñπÊ†π„ÄÇ Ëß£ÈáäÂπ≥ÊñπÔºö ÂûÇÁõ¥ÊñπÂêëÔºåÊØîËæÉÈô°ÔºåÊ¢ØÂ∫¶ÊØîËæÉÂ§ßÔºå‰ΩÜÊàë‰ª¨ÂèàÂ∏åÊúõÂÆÉ‰∏ãÈôçÁöÑÊÖ¢„ÄÇÂõ†Ê≠§ÂØπÊ¢ØÂ∫¶Èô§‰ª•‰∏Ä‰∏™ËæÉÂ§ßÁöÑÂÄºÔºåÊâÄ‰ª•Áî®Ê¢ØÂ∫¶ÁöÑÂπ≥ÊñπÁöÑÂπ≥ÂùáÊù•Ë°®Á§∫„ÄÇËÆ©‰∏çÂêåÁöÑÂèÇÊï∞Êã•Êúâ‰∏çÂêåÁöÑlearning rate„ÄÇ ‰ªéÊüêÁßçËßíÂ∫¶ÁúãÔºåRMSprop‰ºöÊ†πÊçÆÂΩìÂâçÁöÑÊ¢ØÂ∫¶Ëá™Âä®Ë∞ÉÊï¥ÂèÇÊï∞ÁöÑlearning rateÔºåÊ¢ØÂ∫¶Â§ßÈôç‰Ωélearning rateÔºåÊ¢ØÂ∫¶Â∞èÁöÑÊó∂ÂÄôÊèêÈ´òlearning rateÔºå‰ªéËÄå‰∏ÄÊñπÈù¢ÈÅøÂÖç‰∫ÜÈúáËç°ÔºåÂè¶‰∏ÄÊñπÈù¢ÈÅøÂÖçÂú®Âπ≥Âù¶ÁöÑÂú∞ÊñπÂæòÂæäÂ§™‰πÖ„ÄÇ ‰∏∫‰∫ÜÈÅøÂÖçÂá∫Áé∞ÂàÜÊØç‰∏∫0 \begin{array}{c}{s_{d w}=\beta s_{d w}+(1-\beta)(d w)^{2}} \\ {s_{d b}=\beta s_{d b}+(1-\beta)(d b)^{2}} \\ {w :=w-\alpha \frac{d w}{\sqrt{s_{d w}+\varepsilon}}} \\ {b :=b-\alpha \frac{d b}{\sqrt{s_{d b}+\varepsilon}}}\end{array}$\varepsilon$Âèñ$10^{-8}$‰∏çÈîôÁöÑÈÄâÊã©. Ë°•ÂÖÖÔºö RMSPropÁÆóÊ≥ïÂØπÊ¢ØÂ∫¶ËÆ°ÁÆó‰∫ÜÂæÆÂàÜÂπ≥ÊñπÂä†ÊùÉÂπ≥ÂùáÊï∞„ÄÇËøôÁßçÂÅöÊ≥ïÊúâÂà©‰∫éÊ∂àÈô§‰∫ÜÊëÜÂä®ÂπÖÂ∫¶Â§ßÁöÑÊñπÂêëÔºåÁî®Êù•‰øÆÊ≠£ÊëÜÂä®ÂπÖÂ∫¶Ôºå‰ΩøÂæóÂêÑ‰∏™Áª¥Â∫¶ÁöÑÊëÜÂä®ÂπÖÂ∫¶ÈÉΩËæÉÂ∞è„ÄÇÂè¶‰∏ÄÊñπÈù¢‰πü‰ΩøÂæóÁΩëÁªúÂáΩÊï∞Êî∂ÊïõÊõ¥Âø´ L 08 Adam optimization algorithmAdamÔºàAdaptive Moment EstimationÔºåËá™ÈÄÇÂ∫îÁü©‰º∞ËÆ°ÔºâÂ∞±ÊòØmomentumÂíåRMSpropÁöÑÁªìÂêà„ÄÇmomentumË¥üË¥£Âπ≥ÊªëÊ¢ØÂ∫¶ÔºåËÄåRMSpropË¥üË¥£Ë∞ÉËß£learning rate„ÄÇ 1. Adama. ÂºïÂÖ•ÁöÑÂèòÈáèÊúâÔºö $v$ : ËÆ°ÁÆóÂêåmomentumÁÆóÊ≥ïÔºåÂ∞ÜÊ¢ØÂ∫¶ËøõË°åÊåáÊï∞Âä†ÊùÉÂπ≥Âùá $s$: ËÆ°ÁÆóÂêåRMSpropÔºåÂ∞ÜÊ¢ØÂ∫¶ÁöÑÂπ≥ÊñπËøõË°åÊåáÊï∞Âä†ÊùÉÂπ≥Âùá $Œ≤1$ : ËÆ°ÁÆóvvÁöÑÂä†ÊùÉÂèÇÊï∞ $Œ≤2$ : ËÆ°ÁÆóssÁöÑÂä†ÊùÉÂèÇÊï∞ b. Âú®Ëø≠‰ª£ÂâçÔºåÂàùÂßãÂåñÂèÇÊï∞vÂíås v_{d W}=0, s_{d W}=0, v_{d b}=0, s_{d b}=0c. ÂØπÁ¨¨tÊ¨°Ê¢ØÂ∫¶‰∏ãÈôçÁöÑËø≠‰ª£ a. È¶ñÂÖàËÆ°ÁÆódwÂíådbÁöÑvÂíås \begin{array}{c}{v_{d W}=\beta_{1} v_{d W}+\left(1-\beta_{1}\right) d W} \\ {s_{d W}=\beta_{2} s_{d W}+\left(1-\beta_{2}\right)(d W)^{2}} \\ {v_{d b}=\beta_{1} v_{d b}+\left(1-\beta_{1}\right) d b} \\ {s_{d b}=\beta_{2} s_{d b}+\left(1-\beta_{2}\right)(d b)^{2}}\end{array}d. ‰øÆÊ≠£ v_{d W}^{\text {corrected}}=\frac{v_{d W}}{1-\left(\beta_{1}\right)^{t}}\\ \begin{aligned} s_{d W}^{\text {corrected}} &=\frac{s_{d W}}{1-\left(\beta_{2}\right)^{t}} \\ v_{d b}^{\text {corrected}} &=\frac{v_{d b}}{1-\left(\beta_{1}\right)^{t}} \\ s_{d b}^{\text {corrected}} &=\frac{s_{d b}}{1-\left(\beta_{2}\right)^{t}} \end{aligned}e. ÊúÄÂêéÊõ¥Êñ∞ÂèÇÊï∞WÂíåb W=W-\alpha \frac{v_{d W}^{\text {corrected}}}{\sqrt{s_{d W}^{\text { corrected }}+\varepsilon}}\\ b=b-\alpha \frac{v_{d b}^{\text {corrected}}}{\sqrt{s_{d b}^{\text { corrected }}+\varepsilon}}Ë∂ÖÂèÇÁöÑÈÄâÊã©Ôºö Œ±ÔºöÈúÄË¶ÅË∞É‰ºò Œ≤1: ÈÄöÂ∏∏ÈÄâÊã©‰∏∫0.9 Œ≤2: ÈÄöÂ∏∏ÈÄâÊã©‰∏∫0.999 Œµ: ‰∏ÄËà¨‰∏çÈúÄË¶ÅË∞É‰ºòÔºåÈÄâÊã©‰∏Ä‰∏™Â∞èÊï∞ÔºåÊØîÂ¶Ç10‚àí8 ‰Ω†ÂèØ‰ª•Â∞ùËØï‰∏ÄÁ≥ªÂàóÂÄºŒ±ÔºåÁÑ∂ÂêéÁúãÂì™‰∏™ÊúâÊïà L09 : Learning Rate Decay why ‰∏∫‰ªÄ‰πàË¶ÅÂÅölearning rate decayÔºü ËæÉÂ§ßÁöÑlearning rateËôΩÁÑ∂Âú®ÁÆóÊ≥ïÂºÄÂßãÈò∂ÊÆµ‰ºöÂä†Âø´Êî∂ÊïõÈÄüÂ∫¶Ôºå‰ΩÜÂú®Êî∂ÊïõÊé•ËøëÂà∞‰ºòÂåñÁÇπÁöÑÊó∂ÂÄôÔºåÁÆóÊ≥ï‰ºöÂú®‰ºòÂåñÁÇπÈôÑËøëÈúáËç°ÔºåÂ¶Ç‰∏ãÂõæÔºö 2.Â¶Ç‰ΩïÂÅölearning rate decayÔºü ÊÄùË∑ØÂæàÁÆÄÂçïÔºåÂ∞±ÊòØÂºïÂÖ•‰∏Ä‰∏™ÂáΩÊï∞ÔºåËÆ©Œ±ÈöèÁùÄËø≠‰ª£ÔºàÊØîÂ¶Çmin-batchÁöÑepochÔºâÈÄíÂáè„ÄÇ‰∏∫Ê≠§ÂèØ‰ª•ÈááÁî®ÁöÑdecayÂáΩÊï∞ÊúâÔºö ÂÄíÊï∞Ôºö \alpha :=\frac{1}{1+\text { decay rate * epoch num}} \alpha L 10: The Problem of local Optima‰∫ãÂÆû‰∏äÔºåÂ¶ÇÊûú‰Ω†Ë¶ÅÂàõÂª∫‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÔºåÈÄöÂ∏∏Ê¢ØÂ∫¶‰∏∫Èõ∂ÁöÑÁÇπÂπ∂‰∏çÊòØËøô‰∏™Âõæ‰∏≠ÁöÑÂ±ÄÈÉ®ÊúÄ‰ºòÁÇπÔºåÂÆûÈôÖ‰∏äÊàêÊú¨ÂáΩÊï∞ÁöÑÈõ∂Ê¢ØÂ∫¶ÁÇπÔºåÈÄöÂ∏∏ÊòØÈûçÁÇπ„ÄÇ ‰ΩÜÊòØ‰∏Ä‰∏™ÂÖ∑ÊúâÈ´òÁª¥Â∫¶Á©∫Èó¥ÁöÑÂáΩÊï∞ÔºåÂ¶ÇÊûúÊ¢ØÂ∫¶‰∏∫0ÔºåÈÇ£‰πàÂú®ÊØè‰∏™ÊñπÂêëÔºåÂÆÉÂèØËÉΩÊòØÂá∏ÂáΩÊï∞Ôºå‰πüÂèØËÉΩÊòØÂáπÂáΩÊï∞„ÄÇÂ¶ÇÊûú‰Ω†Âú®2‰∏áÁª¥Á©∫Èó¥‰∏≠ÔºåÈÇ£‰πàÊÉ≥Ë¶ÅÂæóÂà∞Â±ÄÈÉ®ÊúÄ‰ºòÔºåÊâÄÊúâÁöÑ2‰∏á‰∏™ÊñπÂêëÈÉΩÈúÄË¶ÅÊòØËøôÊ†∑Ôºå‰ΩÜÂèëÁîüÁöÑÊú∫Áéá‰πüËÆ∏ÂæàÂ∞èÔºå‰πüËÆ∏ÊòØ$2^{-20000}$ÔºåÂõ†Ê≠§Êõ¥ÊúâÂèØËÉΩÈÅáÂà∞Êúâ‰∫õÊñπÂêëÁöÑÊõ≤Á∫ø‰ºöËøôÊ†∑Âêë‰∏äÂºØÊõ≤ÔºåÂè¶‰∏Ä‰∫õÊñπÂêëÊõ≤Á∫øÂêë‰∏ãÂºØÔºåËÄå‰∏çÊòØÊâÄÊúâÁöÑÈÉΩÂêë‰∏äÂºØÊõ≤ÔºåÂõ†Ê≠§Âú®È´òÁª¥Â∫¶Á©∫Èó¥Ôºå‰Ω†Êõ¥ÂèØËÉΩÁ¢∞Âà∞ÈûçÁÇπ„ÄÇÊâÄÊúâÔºåÊãÖÂøÉÊî∂ÊïõÂà∞local optimaÔºåÁúüÊòØ‰∫∫‰ª¨ÊÉ≥Â§ö‰∫ÜÔºåÂÆûÈôÖ‰∏äÂπ∂Ê≤°ÊúâÊÉ≥Ë±°ÁöÑÈÇ£‰πàÂ§ölocal optima„ÄÇÂú®È´òÁª¥Á©∫Èó¥ÔºåÂá†‰πé‰∏çÂ§™ÂèØËÉΩË¢´Âõ∞Âú®‰∏Ä‰∏™local optimaÔºåËøôÊòØ‰∏Ä‰∏™Â•ΩÊ∂àÊÅØ„ÄÇ Âõ†Ê≠§ÔºåÂú®È´òÁª¥Á©∫Èó¥ÈÅáÂà∞ÁöÑÈóÆÈ¢òÊòØÈ´òÂéüÈóÆÈ¢òÔºàProblem of plateausÔºâ AdamÁÆóÊ≥ïÂèØ‰ª•Âä†ÈÄüÂ≠¶‰π† W3 Hyperparameter tuningL01 Tuning process Âà∞ÁõÆÂâç‰∏∫Ê≠¢ÔºåÊàë‰ª¨Êé•Ëß¶Âà∞ÁöÑhyperparameterÊúâÔºö learning rate: Œ±Œ± momentum ÂèÇÊï∞: Œ≤Œ≤ AdamÂèÇÊï∞: Œ≤1Œ≤1Âíå Œ≤2Œ≤2‰ª•ÂèäŒµŒµ Á•ûÁªèÁΩëÁªúÂ±ÇÊï∞: L Á•ûÁªèÁΩëÁªúÈöêËóèÂ±ÇneuronÊï∞Ôºön[l]n[l] learning rate decayÂèÇÊï∞ min-batch size Ëøô‰∫õhyperparameterÈáçË¶ÅÊÄßÊéíÂ∫èÔºö ÊúÄÈáçË¶ÅÁöÑÔºö learning rate: Œ±Œ± ÊØîËæÉÈáçË¶ÅÁöÑÔºö momentum ÂèÇÊï∞: Œ≤Œ≤ Á•ûÁªèÁΩëÁªúÂ±ÇÊï∞: L Á•ûÁªèÁΩëÁªúÈöêËóèÂ±ÇneuronÊï∞Ôºön[l]n[l] Ê¨°ÈáçË¶ÅÁöÑÔºö Á•ûÁªèÁΩëÁªúÈöêËóèÂ±ÇneuronÊï∞ learning rate decayÂèÇÊï∞ Âü∫Êú¨‰∏çÈúÄË∞ÉÊï¥ÁöÑ Œ≤1Œ≤1Âíå Œ≤2Œ≤2‰ª•ÂèäŒµ 1. Try random values : Don‚Äôt use a grid why: ‰∏æ‰∏™‰æãÂ≠êÔºåÂÅáËÆæË∂ÖÂèÇÊï∞1ÊòØÔºàÂ≠¶‰π†ÈÄüÁéáÔºâÔºåÂèñ‰∏Ä‰∏™ÊûÅÁ´ØÁöÑ‰æãÂ≠êÔºåÂÅáËÆæË∂ÖÂèÇÊï∞2ÊòØAdamÁÆóÊ≥ï‰∏≠ÔºåÂàÜÊØç‰∏≠ÁöÑ$\varepsilon$„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåaÁöÑÂèñÂÄºÂæàÈáçË¶ÅÔºåËÄå$\varepsilon$ÂèñÂÄºÂàôÊó†ÂÖ≥Á¥ßË¶Å„ÄÇÂ¶ÇÊûú‰Ω†Âú®ÁΩëÊ†º‰∏≠ÂèñÁÇπÔºåÊé•ÁùÄÔºå‰Ω†ËØïÈ™å‰∫ÜaÁöÑ5‰∏™ÂèñÂÄºÔºåÈÇ£‰Ω†‰ºöÂèëÁé∞ÔºåÊó†ËÆ∫$\varepsilon$Âèñ‰ΩïÂÄºÔºåÁªìÊûúÂü∫Êú¨‰∏äÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇÊâÄ‰ª•Ôºå‰Ω†Áü•ÈÅìÂÖ±Êúâ25ÁßçÊ®°ÂûãÔºå‰ΩÜËøõË°åËØïÈ™åÁöÑÂÄºÂè™Êúâ5‰∏™ÔºåÊàëËÆ§‰∏∫ËøôÊòØÂæàÈáçË¶ÅÁöÑ„ÄÇ ÂØπÊØîËÄåË®ÄÔºåÂ¶ÇÊûú‰Ω†ÈöèÊú∫ÂèñÂÄºÔºå‰Ω†‰ºöËØïÈ™å25‰∏™Áã¨Á´ãÁöÑa,$\varepsilon$Ôºå‰ºº‰πé‰Ω†Êõ¥ÊúâÂèØËÉΩÂèëÁé∞ÊïàÊûúÂÅöÂ•ΩÁöÑÈÇ£‰∏™„ÄÇ 2. Áî±Á≤óÁ≥ôÂà∞Á≤æÁªÜÁöÑÁ≠ñÁï• L 02: Using an Appropriate Scale to pick hyperparameters$a$ÂèñÂÄº0.0001,1,Â¶ÇÊûú‰Ω†Áîª‰∏ÄÊù°‰ªé0.0001Âà∞1ÁöÑÊï∞ËΩ¥ÔºåÊ≤øÂÖ∂ÈöèÊú∫ÂùáÂåÄÂèñÂÄºÔºåÈÇ£90%ÁöÑÊï∞ÂÄºÂ∞Ü‰ºöËêΩÂú®0.1Âà∞1‰πãÈó¥ÔºåÁªìÊûúÂ∞±ÊòØÔºåÂú®0.1Âà∞1‰πãÈó¥ÔºåÂ∫îÁî®‰∫Ü90%ÁöÑËµÑÊ∫êÔºåËÄåÂú®0.0001Âà∞0.1‰πãÈó¥ÔºåÂè™Êúâ10%ÁöÑÊêúÁ¥¢ËµÑÊ∫êÔºåËøôÁúã‰∏äÂéª‰∏çÂ§™ÂØπ„ÄÇ ÂêåÊó∂Âú®ËåÉÂõ¥ÂÜÖÊêúÁ¥¢Ôºå‰πü‰∏çÊòØÂùáÂåÄÂàÜÂ∏ÉÔºàuniformly randomÔºâÁöÑÔºåÈÄöÂ∏∏ÊúâËøô‰∏™ÂèÇÊï∞ÁöÑscaleÔºåÊØîÂ¶ÇÂØπÊï∞scale„ÄÇ ÂèçËÄåÔºåÁî®ÂØπÊï∞Ê†áÂ∞∫ÊêúÁ¥¢Ë∂ÖÂèÇÊï∞ÁöÑÊñπÂºè‰ºöÊõ¥ÂêàÁêÜÔºåÂõ†Ê≠§ËøôÈáå‰∏ç‰ΩøÁî®Á∫øÊÄßËΩ¥ÔºåÂàÜÂà´‰æùÊ¨°Âèñ0.0001Ôºå0.001Ôºå0.01Ôºå0.1Ôºå1ÔºåÂú®ÂØπÊï∞ËΩ¥‰∏äÂùáÂåÄÈöèÊú∫ÂèñÁÇπÔºåËøôÊ†∑ÔºåÂú®0.0001Âà∞0.001‰πãÈó¥ÔºåÂ∞±‰ºöÊúâÊõ¥Â§öÁöÑÊêúÁ¥¢ËµÑÊ∫êÂèØÁî®ÔºåËøòÊúâÂú®0.001Âà∞0.01‰πãÈó¥Á≠âÁ≠â„ÄÇ L 03 : Hyperparameter tuning i practice ‰∏çÂêåÁöÑÁÆóÊ≥ïÂíåÂú∫ÊôØÔºåÂØπË∂ÖÂèÇÁöÑscaleÊïèÊÑüÊÄßÂèØËÉΩ‰∏ç‰∏ÄÊ†∑. Ê†πÊçÆËÆ°ÁÆóËµÑÊ∫êÂíåÊï∞ÊçÆÈáèÔºåÂèØ‰ª•ÈááÂèñ‰∏§ÁßçÁ≠ñÁï•Êù•Ë∞ÉÂèÇ PandaÔºàÁÜäÁå´Á≠ñÁï•ÔºâÔºöÂØπ‰∏Ä‰∏™Ê®°ÂûãÂÖàÂêé‰øÆÊîπÂèÇÊï∞ÔºåÊü•ÁúãÂÖ∂Ë°®Áé∞ÔºåÊúÄÁªàÈÄâÊã©ÊúÄÂ•ΩÁöÑÂèÇÊï∞„ÄÇÂ∞±ÂÉèÁÜäÁå´‰∏ÄÊ†∑Ôºå‰∏ÄÊ¨°Âè™ÊäöÂÖª‰∏Ä‰∏™Âêé‰ª£„ÄÇ CaviarÔºàÈ±ºÂ≠êÈÖ±Á≠ñÁï•ÔºâÔºöËÆ°ÁÆóËµÑÊ∫êË∂≥Â§üÔºåÂèØ‰ª•ÂêåÊó∂ËøêË°åÂæàÂ§öÊ®°ÂûãÂÆû‰æãÔºåÈááÁî®‰∏çÂêåÁöÑÂèÇÊï∞ÔºåÁÑ∂ÂêéÊúÄÁªàÈÄâÊã©‰∏Ä‰∏™Â•ΩÁöÑ„ÄÇÁ±ª‰ººÈ±ºÁ±ªÔºå‰∏ÄÊ¨°‰∏ãÂæàÂ§öÂçµÔºåËá™Âä®Á´û‰∫âÊàêÊ¥ª„ÄÇ L 04: Normalizing Activations in a Network1. Implementing Batch NormalizingBatchÂΩí‰∏ÄÂåñ,BatchÂΩí‰∏ÄÂåñ‰ºö‰Ωø‰Ω†ÁöÑÂèÇÊï∞ÊêúÁ¥¢ÈóÆÈ¢òÂèòÂæóÂæàÂÆπÊòìÔºå‰ΩøÁ•ûÁªèÁΩëÁªúÂØπË∂ÖÂèÇÊï∞ÁöÑÈÄâÊã©Êõ¥Âä†Á®≥ÂÆöÔºåË∂ÖÂèÇÊï∞ÁöÑËåÉÂõ¥‰ºöÊõ¥Âä†Â∫ûÂ§ßÔºåÂ∑•‰ΩúÊïàÊûú‰πüÂæàÂ•ΩÔºå‰πü‰ºöÊòØ‰Ω†ÁöÑËÆ≠ÁªÉÊõ¥Âä†ÂÆπÊòìÔºåÁîöËá≥ÊòØÊ∑±Â±ÇÁΩëÁªú„ÄÇ ÂèØ‰ª•normalize $a^{[l]},z^{[l]}$,ÈÄâÊã©$z^{[L]}$ ËÆæÁΩÆ Œ≥ Âíå Œ≤ ÁöÑÂéüÂõ†ÊòØÔºåÂ¶ÇÊûúÂêÑÈöêËóèÂ±ÇÁöÑËæìÂÖ•ÂùáÂÄºÂú®Èù†Ëøë 0 ÁöÑÂå∫ÂüüÔºåÂç≥Â§Ñ‰∫éÊøÄÊ¥ªÂáΩÊï∞ÁöÑÁ∫øÊÄßÂå∫ÂüüÔºå‰∏çÂà©‰∫éËÆ≠ÁªÉÈùûÁ∫øÊÄßÁ•ûÁªèÁΩëÁªúÔºå‰ªéËÄåÂæóÂà∞ÊïàÊûúËæÉÂ∑ÆÁöÑÊ®°Âûã„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶ÅÁî® Œ≥ Âíå Œ≤ ÂØπÊ†áÂáÜÂåñÂêéÁöÑÁªìÊûúÂÅöËøõ‰∏ÄÊ≠•Â§ÑÁêÜ„ÄÇ ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåŒ≤ÂíåŒ≥‰∏çÊòØË∂ÖÂèÇÔºåËÄåÊòØÊ¢ØÂ∫¶‰∏ãÈôçÈúÄÂ≠¶‰π†ÁöÑÂèÇÊï∞„ÄÇ L 05 : Fitting Batch Norm Into Neural Networks Ê≥®ÊÑè ÂÖàÂâçÊàëËØ¥ËøáÊØèÂ±ÇÁöÑÂèÇÊï∞ÊòØ$w^{[l]}$Âíå$b^{[l]}$ÔºåËøòÊúâ$\beta^{[l]}$Âíå$b^{[l]}$ÔºåËØ∑Ê≥®ÊÑèËÆ°ÁÆóÁöÑÊñπÂºèÂ¶Ç‰∏ãÔºå$z^{[l]}=w^{[l]} a^{[l-1]}+b^{[l]}$Ôºå‰ΩÜBatchÂΩí‰∏ÄÂåñÂÅöÁöÑÊòØÔºåÂÆÉË¶ÅÁúãËøô‰∏™mini-batchÔºåÂÖàÂ∞Ü$z^{[l]}$ÂΩí‰∏ÄÂåñÔºåÁªìÊûú‰∏∫ÂùáÂÄº0ÂíåÊ†áÂáÜÊñπÂ∑ÆÔºåÂÜçÁî±$\beta$ÂíåbÈáçÁº©ÊîæÔºå‰ΩÜËøôÊÑèÂë≥ÁùÄÔºåÊó†ËÆ∫$b^{[l]}$ÁöÑÂÄºÊòØÂ§öÂ∞ëÔºåÈÉΩÊòØË¶ÅË¢´ÂáèÂéªÁöÑÔºåÂõ†‰∏∫Âú®BatchÂΩí‰∏ÄÂåñÁöÑËøáÁ®ã‰∏≠Ôºå‰Ω†Ë¶ÅËÆ°ÁÆóÁöÑ$z^{[l]}$ÂùáÂÄºÔºåÂÜçÂáèÂéªÂπ≥ÂùáÂÄºÔºåÂú®Ê≠§‰æã‰∏≠ÁöÑmini-batch‰∏≠Â¢ûÂä†‰ªª‰ΩïÂ∏∏Êï∞ÔºåÊï∞ÂÄºÈÉΩ‰∏ç‰ºöÊîπÂèòÔºåÂõ†‰∏∫Âä†‰∏äÁöÑ‰ªª‰ΩïÂ∏∏Êï∞ÈÉΩÂ∞Ü‰ºöË¢´ÂùáÂÄºÂáèÂéªÊâÄÊäµÊ∂à. ÊúÄÂêéÔºåËØ∑ËÆ∞‰ΩèÁöÑÁª¥$z^{[l]}$ÔºåÂõ†‰∏∫Âú®Ëøô‰∏™‰æãÂ≠ê‰∏≠ÔºåÁª¥Êï∞‰ºöÊòØ$\left(n^{[l]}, 1\right)$ÔºåÁöÑÂ∞∫ÂØ∏‰∏∫ÔºåÂ¶ÇÊûúÊòØlÂ±ÇÈöêËóèÂçïÂÖÉÁöÑÊï∞ÈáèÔºåÈÇ£$ \beta^{[l]}$Âíå$ \gamma^{[l]}$ÁöÑÁª¥Â∫¶‰πüÊòØ$\left(n^{[l]}, 1\right)$ÔºåÂõ†‰∏∫ËøôÊòØ‰Ω†ÈöêËóèÂ±ÇÁöÑÊï∞ÈáèÔºå‰Ω†ÊúâÈöêËóèÂçïÂÖÉÔºåÊâÄ‰ª•$\gamma^{[l]}$Âíå$ \beta^{[l]}‚Äã$Áî®Êù•Â∞ÜÊØè‰∏™ÈöêËóèÂ±ÇÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÁº©Êîæ‰∏∫ÁΩëÁªúÊÉ≥Ë¶ÅÁöÑÂÄº„ÄÇ L 06 Why Doest Batch Norm Work? È¶ñÂÖàÔºåËµ∑Âà∞‰∫ÜnormalizationÁöÑ‰ΩúÁî®ÔºåÂêåÂØπËæìÂÖ•Êï∞ÊçÆXÁöÑnormalization‰ΩúÁî®Á±ª‰ºº„ÄÇ ËÆ©ÊØè‰∏ÄÂ±ÇÁöÑÂ≠¶‰π†Ôºå‰∏ÄÂÆöÁ®ãÂ∫¶Ëß£ËÄ¶‰∫ÜÂâçÂ±ÇÂèÇÊï∞ÂíåÂêéÂ±ÇÂèÇÊï∞ÔºåËÆ©ÂêÑÂ±ÇÊõ¥Âä†Áã¨Á´ãÁöÑÂ≠¶‰π†„ÄÇÊó†ËÆ∫Ââç‰∏ÄÂ±ÇÂ¶Ç‰ΩïÂèòÔºåÊú¨Â±ÇËæìÂÖ•ÁöÑÊï∞ÊçÆÊÄªÊòØ‰øùÊåÅÁ®≥ÂÆöÁöÑÂùáÂÄºÂíåÊñπÂ∑Æ„ÄÇÔºà‰∏ªË¶ÅÂéüÂõ†Ôºâ ÊâÄ‰ª•‰Ωø‰Ω†Êï∞ÊçÆÊîπÂèòÂàÜÂ∏ÉÁöÑËøô‰∏™ÊÉ≥Ê≥ïÔºåÊúâ‰∏™ÊúâÁÇπÊÄ™ÁöÑÂêçÂ≠ó‚ÄúCovariate shift‚ÄùÔºåÊÉ≥Ê≥ïÊòØËøôÊ†∑ÁöÑÔºåÂ¶ÇÊûú‰Ω†Â∑≤ÁªèÂ≠¶‰π†‰∫ÜÂà∞ ÁöÑÊò†Â∞ÑÔºåÂ¶ÇÊûú ÁöÑÂàÜÂ∏ÉÊîπÂèò‰∫ÜÔºåÈÇ£‰πà‰Ω†ÂèØËÉΩÈúÄË¶ÅÈáçÊñ∞ËÆ≠ÁªÉ‰Ω†ÁöÑÂ≠¶‰π†ÁÆóÊ≥ï„ÄÇËøôÁßçÂÅöÊ≥ïÂêåÊ†∑ÈÄÇÁî®‰∫éÔºåÂ¶ÇÊûúÁúüÂÆûÂáΩÊï∞Áî± Âà∞ Êò†Â∞Ñ‰øùÊåÅ‰∏çÂèòÔºåÊ≠£Â¶ÇÊ≠§‰æã‰∏≠ÔºåÂõ†‰∏∫ÁúüÂÆûÂáΩÊï∞ÊòØÊ≠§ÂõæÁâáÊòØÂê¶ÊòØ‰∏ÄÂè™Áå´ÔºåËÆ≠ÁªÉ‰Ω†ÁöÑÂáΩÊï∞ÁöÑÈúÄË¶ÅÂèòÂæóÊõ¥Âä†Ëø´ÂàáÔºåÂ¶ÇÊûúÁúüÂÆûÂáΩÊï∞‰πüÊîπÂèòÔºåÊÉÖÂÜµÂ∞±Êõ¥Á≥ü‰∫Ü„ÄÇ ÂÖ≥‰∫éÁ¨¨‰∫åÁÇπÔºåÂ¶ÇÊûúÂÆûÈôÖÂ∫îÁî®Ê†∑Êú¨ÂíåËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊï∞ÊçÆÂàÜÂ∏É‰∏çÂêåÔºà‰æãÂ¶ÇÔºåÊ©òÁå´ÂõæÁâáÂíåÈªëÁå´ÂõæÁâáÔºâÔºåÊàë‰ª¨Áß∞ÂèëÁîü‰∫Ü‚ÄúCovariate Shift‚Äù„ÄÇËøôÁßçÊÉÖÂÜµ‰∏ãÔºå‰∏ÄËà¨Ë¶ÅÂØπÊ®°ÂûãËøõË°åÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇBatch Normalization ÁöÑ‰ΩúÁî®Â∞±ÊòØÂáèÂ∞è Covariate Shift ÊâÄÂ∏¶Êù•ÁöÑÂΩ±ÂìçÔºåËÆ©Ê®°ÂûãÂèòÂæóÊõ¥Âä†ÂÅ•Â£ÆÔºåÈ≤ÅÊ£íÊÄßÔºàRobustnessÔºâÊõ¥Âº∫„ÄÇ Âç≥‰ΩøËæìÂÖ•ÁöÑÂÄºÊîπÂèò‰∫ÜÔºåÁî±‰∫é Batch Normalization ÁöÑ‰ΩúÁî®Ôºå‰ΩøÂæóÂùáÂÄºÂíåÊñπÂ∑Æ‰øùÊåÅ‰∏çÂèòÔºàÁî± Œ≥ Âíå Œ≤ ÂÜ≥ÂÆöÔºâÔºåÈôêÂà∂‰∫ÜÂú®ÂâçÂ±ÇÁöÑÂèÇÊï∞Êõ¥Êñ∞ÂØπÊï∞ÂÄºÂàÜÂ∏ÉÁöÑÂΩ±ÂìçÁ®ãÂ∫¶ÔºåÂõ†Ê≠§ÂêéÂ±ÇÁöÑÂ≠¶‰π†ÂèòÂæóÊõ¥ÂÆπÊòì‰∏Ä‰∫õ„ÄÇBatch Normalization ÂáèÂ∞ë‰∫ÜÂêÑÂ±Ç W Âíå b ‰πãÈó¥ÁöÑËÄ¶ÂêàÊÄßÔºåËÆ©ÂêÑÂ±ÇÊõ¥Âä†Áã¨Á´ãÔºåÂÆûÁé∞Ëá™ÊàëËÆ≠ÁªÉÂ≠¶‰π†ÁöÑÊïàÊûú„ÄÇ Âè¶Â§ñÔºåBatch Normalization ‰πüËµ∑Âà∞ÂæÆÂº±ÁöÑÊ≠£ÂàôÂåñÔºàregularizationÔºâÊïàÊûú„ÄÇÂõ†‰∏∫Âú®ÊØè‰∏™ mini-batch ËÄåÈùûÊï¥‰∏™Êï∞ÊçÆÈõÜ‰∏äËÆ°ÁÆóÂùáÂÄºÂíåÊñπÂ∑ÆÔºåÂè™Áî±Ëøô‰∏ÄÂ∞èÈÉ®ÂàÜÊï∞ÊçÆ‰º∞ËÆ°ÂæóÂá∫ÁöÑÂùáÂÄºÂíåÊñπÂ∑Æ‰ºöÊúâ‰∏Ä‰∫õÂô™Â£∞ÔºåÂõ†Ê≠§ÊúÄÁªàËÆ°ÁÆóÂá∫ÁöÑ z~(i)z~(i)‰πüÊúâ‰∏ÄÂÆöÂô™Â£∞„ÄÇÁ±ª‰ºº‰∫é dropoutÔºåËøôÁßçÂô™Â£∞‰ºö‰ΩøÂæóÁ•ûÁªèÂÖÉ‰∏ç‰ºöÂÜçÁâπÂà´‰æùËµñ‰∫é‰ªª‰Ωï‰∏Ä‰∏™ËæìÂÖ•ÁâπÂæÅ„ÄÇ Âõ†‰∏∫ Batch Normalization Âè™ÊúâÂæÆÂº±ÁöÑÊ≠£ÂàôÂåñÊïàÊûúÔºåÂõ†Ê≠§ÂèØ‰ª•Âíå dropout ‰∏ÄËµ∑‰ΩøÁî®Ôºå‰ª•Ëé∑ÂæóÊõ¥Âº∫Â§ßÁöÑÊ≠£ÂàôÂåñÊïàÊûú„ÄÇÈÄöËøáÂ∫îÁî®Êõ¥Â§ßÁöÑ mini-batch Â§ßÂ∞èÔºåÂèØ‰ª•ÂáèÂ∞ëÂô™Â£∞Ôºå‰ªéËÄåÂáèÂ∞ëËøôÁßçÊ≠£ÂàôÂåñÊïàÊûú„ÄÇ ÊúÄÂêéÔºå‰∏çË¶ÅÂ∞Ü Batch Normalization ‰Ωú‰∏∫Ê≠£ÂàôÂåñÁöÑÊâãÊÆµÔºåËÄåÊòØÂΩì‰ΩúÂä†ÈÄüÂ≠¶‰π†ÁöÑÊñπÂºè„ÄÇÊ≠£ÂàôÂåñÂè™ÊòØ‰∏ÄÁßçÈùûÊúüÊúõÁöÑÂâØ‰ΩúÁî®ÔºåBatch Normalization Ëß£ÂÜ≥ÁöÑËøòÊòØÂèçÂêë‰º†Êí≠ËøáÁ®ã‰∏≠ÁöÑÊ¢ØÂ∫¶ÈóÆÈ¢òÔºàÊ¢ØÂ∫¶Ê∂àÂ§±ÂíåÁàÜÁÇ∏Ôºâ„ÄÇ L 07 : Batch Norm At Test TimeÈóÆÈ¢òÔºöBNÁÆóÊ≥ïÂú®ËÆ≠ÁªÉÊó∂ÊòØ‰∏Ä‰∏™ÊâπÊ¨°ÁöÑÊï∞ÊçÆËÆ≠ÁªÉÔºåËÉΩÁÆóÂá∫ÊØè‰∏ÄÂ±ÇZÁöÑÂùáÂÄºÂíåÊñπÂ∑ÆÔºõËÄåÂú®ÊµãËØïÊó∂ÔºåËæìÂÖ•ÁöÑÂàôÊòØÂçï‰∏™Êï∞ÊçÆÔºåÂçïÊù°Êï∞ÊçÆÊ≤°Ê≥ïÂÅöÂùáÂÄºÂíåÊñπÂ∑ÆÁöÑËÆ°ÁÆóÔºåÊÄé‰πàÂú®ÊµãËØïÊúüËæìÂÖ•ÂùáÂÄºÂíåÊñπÂ∑ÆÂë¢? ÂÆûÈôÖÂ∫îÁî®‰∏≠‰∏ÄËà¨‰∏ç‰ΩøÁî®ËøôÁßçÊñπÊ≥ïÔºåËÄåÊòØ‰ΩøÁî®‰πãÂâçÂ≠¶‰π†ËøáÁöÑÊåáÊï∞Âä†ÊùÉÂπ≥ÂùáÁöÑÊñπÊ≥ïÊù•È¢ÑÊµãÊµãËØïËøáÁ®ãÂçï‰∏™Ê†∑Êú¨ÁöÑ Œº Âíå $œÉ^2$ ËÆ°ÁÆó$z_{\text { norm }}^{(\hat{2})}$ÔºåÁî®$\mu$ Âíå$ \sigma^{2}$ÁöÑÊåáÊï∞Âä†ÊùÉÂπ≥ÂùáÔºåÁî®‰Ω†ÊâãÂ§¥ÁöÑÊúÄÊñ∞Êï∞ÂÄºÊù•ÂÅöË∞ÉÊï¥ÔºåÁÑ∂Âêé‰Ω†ÂèØ‰ª•Áî®Â∑¶ËæπÊàë‰ª¨ÂàöÁÆóÂá∫Êù•ÁöÑÂíå‰Ω†Âú®Á•ûÁªèÁΩëÁªúËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂæóÂà∞ÁöÑ$\beta$Âíå$\sigma$ÂèÇÊï∞Êù•ËÆ°ÁÆó‰Ω†ÈÇ£‰∏™ÊµãËØïÊ†∑Êú¨ÁöÑzÂÄº„ÄÇ L 08 : Softmax Regression1. [Multi-class classification] ÊúÄÂêé‰∏ÄÂ±ÇÊòØÊ¶ÇÁéáÔºå‰πãÂíå‰∏∫1ÔºåË¶ÅÁî®Âà∞SoftmaxÂ±ÇÔºåSoftmaxÊøÄÊ¥ªÂáΩÊï∞ÁöÑÁâπÊÆä‰πãÂ§ÑÂú®‰∫éÔºåÂõ†‰∏∫ÈúÄË¶ÅÂ∞ÜÊâÄÊúâÂèØËÉΩÁöÑËæìÂá∫ÂΩí‰∏ÄÂåñÔºåÂ∞±ÈúÄË¶ÅËæìÂÖ•‰∏Ä‰∏™ÂêëÈáèÔºåÊúÄÂêéËæìÂá∫‰∏Ä‰∏™ÂêëÈáè„ÄÇ 2. Softmax exampleÊ≤°ÊúâÈöêËóèÂ±ÇÁöÑsoftmax,‰ª£Ë°®‰∏Ä‰∫õÂÜ≥Á≠ñËæπÁïå L 09 Training SoftMax classifier SoftmaxËøô‰∏™ÂêçÁß∞ÁöÑÊù•Ê∫êÊòØ‰∏éÊâÄË∞ìhardmaxÂØπÊØî,SoftmaxÂõûÂΩíÊàñSoftmaxÊøÄÊ¥ªÂáΩÊï∞Â∞ÜlogisticÊøÄÊ¥ªÂáΩÊï∞Êé®ÂπøÂà∞Á±ªÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØ‰∏§Á±ªÔºåÁªìÊûúÂ∞±ÊòØÂ¶ÇÊûúC=2ÔºåÈÇ£‰πàC=2ÁöÑSoftmaxÂÆûÈôÖ‰∏äÂèòÂõû‰∫ÜlogisticÂõûÂΩíÔºå Loss Function J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right) 3. Gradient descent with softmax ÊúÄÂêé‰∏ÄÂ±ÇÊ±ÇÂØºÔºåsoftmaxÊøÄÊ¥ªÂáΩÊï∞ J\left(w^{[1]}, b^{[1]}, \ldots \ldots\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)L11 TensorFlow1. Âü∫Êú¨ÊµÅÁ®ã‰ΩøÁî®tensorflowÔºåÂè™Ë¶ÅÂëäËØâtensorflow forward propÔºåÂÆÉËá™Â∑±Â∞±‰ºöÂÅöbackpropÔºåÂõ†Ê≠§‰∏çÁî®Ëá™Â∑±ÂÆûÁé∞backprop 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport tensorflow as tf#ÂØºÂÖ•TensorFlow‚Äãw = tf.Variable(0,dtype = tf.float32)#Êé•‰∏ãÊù•ÔºåËÆ©Êàë‰ª¨ÂÆö‰πâÂèÇÊï∞wÔºåÂú®TensorFlow‰∏≠Ôºå‰Ω†Ë¶ÅÁî®tf.Variable()Êù•ÂÆö‰πâÂèÇÊï∞‚Äã#ÁÑ∂ÂêéÊàë‰ª¨ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞Ôºö‚Äãcost = tf.add(tf.add(w**2,tf.multiply(- 10.,w)),25)#ÁÑ∂ÂêéÊàë‰ª¨ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞JÁÑ∂ÂêéÊàë‰ª¨ÂÜçÂÜôÔºö‚Äãtrain = tf.train.GradientDescentOptimizer(0.01).minimize(cost)#(ËÆ©Êàë‰ª¨Áî®0.01ÁöÑÂ≠¶‰π†ÁéáÔºåÁõÆÊ†áÊòØÊúÄÂ∞èÂåñÊçüÂ§±)„ÄÇ‚Äã#ÊúÄÂêé‰∏ãÈù¢ÁöÑÂá†Ë°åÊòØÊÉØÁî®Ë°®ËææÂºè:‚Äãinit = tf.global_variables_initializer()‚Äãsession = tf.Session()#ËøôÊ†∑Â∞±ÂºÄÂêØ‰∫Ü‰∏Ä‰∏™TensorFlow session„ÄÇ‚Äãsession.run(init)#Êù•ÂàùÂßãÂåñÂÖ®Â±ÄÂèòÈáè„ÄÇ‚Äã#ÁÑ∂ÂêéËÆ©TensorFlowËØÑ‰º∞‰∏Ä‰∏™ÂèòÈáèÔºåÊàë‰ª¨Ë¶ÅÁî®Âà∞:‚Äãsession.run(w)‚Äã#‰∏äÈù¢ÁöÑËøô‰∏ÄË°åÂ∞ÜwÂàùÂßãÂåñ‰∏∫0ÔºåÂπ∂ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞ÔºåÊàë‰ª¨ÂÆö‰πâtrain‰∏∫Â≠¶‰π†ÁÆóÊ≥ïÔºåÂÆÉÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï‰ºòÂåñÂô®‰ΩøÊçüÂ§±ÂáΩÊï∞ÊúÄÂ∞èÂåñÔºå‰ΩÜÂÆûÈôÖ‰∏äÊàë‰ª¨ËøòÊ≤°ÊúâËøêË°åÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊâÄ‰ª•#‰∏äÈù¢ÁöÑËøô‰∏ÄË°åÂ∞ÜwÂàùÂßãÂåñ‰∏∫0ÔºåÂπ∂ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞ÔºåÊàë‰ª¨ÂÆö‰πâtrain‰∏∫Â≠¶‰π†ÁÆóÊ≥ïÔºåÂÆÉÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï‰ºòÂåñÂô®‰ΩøÊçüÂ§±ÂáΩÊï∞ÊúÄÂ∞èÂåñÔºå‰ΩÜÂÆûÈôÖ‰∏äÊàë‰ª¨ËøòÊ≤°ÊúâËøêË°åÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊâÄ‰ª•session.run(w)ËØÑ‰º∞‰∫ÜwÔºåËÆ©ÊàëÔºöÔºö‚Äãprint(session.run(w))‚ÄãÊâÄ‰ª•Â¶ÇÊûúÊàë‰ª¨ËøêË°åËøô‰∏™ÔºåÂÆÉËØÑ‰º∞Á≠â‰∫é0ÔºåÂõ†‰∏∫Êàë‰ª¨‰ªÄ‰πàÈÉΩËøòÊ≤°ËøêË°å„ÄÇ#Áé∞Âú®ËÆ©Êàë‰ª¨ËæìÂÖ•Ôºö‚Äã$session.run(train)ÔºåÂÆÉÊâÄÂÅöÁöÑÂ∞±ÊòØËøêË°å‰∏ÄÊ≠•Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï„ÄÇ#Êé•‰∏ãÊù•Âú®ËøêË°å‰∫Ü‰∏ÄÊ≠•Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂêéÔºåËÆ©Êàë‰ª¨ËØÑ‰º∞‰∏Ä‰∏ãwÁöÑÂÄºÔºåÂÜçprintÔºö‚Äãprint(session.run(w))#Âú®‰∏ÄÊ≠•Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï‰πãÂêéÔºåwÁé∞Âú®ÊòØ0.1„ÄÇ 2. Â¶Ç‰ΩïÁî®ËÆ≠ÁªÉÊï∞ÊçÆplaceholder Âú®ÂÆûÈôÖÁöÑËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåË¶ÅÁî®‰∏çÂêåÁöÑÊ†∑Êú¨ÂèçÂ§çÊîæÂà∞‰∏Ä‰∏™ÂæÖ‰ºòÂåñÂáΩÊï∞‰∏≠ÁöÑÔºåËøô‰∏™Êó∂ÂÄôÂ∞±ÂèØ‰ª•Áî®tensorflowÁöÑplaceholderÂÆûÁé∞,Âú®runÁöÑÊó∂ÂÄôÔºåÂØπÂ∫îÁªôÂá∫feed_dictÔºåË°®ÂêçÂç†‰ΩçÁ¨¶xÁöÑÂÆûÈôÖÂÄº„ÄÇ 123456789101112131415161718192021import numpy as npimport tensorflow as tf # ÂØºÂÖ•Tensorflowcoefficient = np.array([[2.],[-10.],[25.]])w = tf.Variable(0, dtype=tf.float32)x = tf.placeholder(tf.float32, [3,1]) # 3x1Â§ßÂ∞èÁöÑplaceholdercost = w**x[0][0] - x[1][0]*w + x[2][0] # Ë¶Å‰ºòÂåñÁöÑcost functionÔºàÂç≥forward propÁöÑÂΩ¢ÂºèÔºâtrain = tf.train.GradientDescentOptimizer(0.01).minimize(cost) init = tf.global_variables_initializer()session = tf.Session()session.run(init)print(session.run(w))session.run(train, feed_dict=&#123;x:coefficient&#125;) # xÂç†‰ΩçÁ¨¶ÊõøÊç¢‰∏∫coefficientprint(session.run(w))for i in range(1000): session.run(train, feed_dict=&#123;x:coefficient&#125;) # # xÂç†‰ΩçÁ¨¶ÊõøÊç¢‰∏∫coefficientprint(session.run(w)) 3. ËÆ°ÁÆóÊµÅTensorFlowÁ®ãÂ∫èÁöÑÊ†∏ÂøÉÊòØËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞ÔºåÁÑ∂ÂêéTensorFlowËá™Âä®ËÆ°ÁÆóÂá∫ÂØºÊï∞Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÊúÄÂ∞èÂåñÊçüÂ§±ÔºåÂõ†Ê≠§Ëøô‰∏™Á≠âÂºèÊàñËÄÖËøôË°å‰ª£Á†ÅÊâÄÂÅöÁöÑÂ∞±ÊòØËÆ©TensorFlowÂª∫Á´ãËÆ°ÁÆóÂõæÔºå with ËØ≠Âè•ÈÄÇÁî®‰∫éÂØπËµÑÊ∫êËøõË°åËÆøÈóÆÁöÑÂú∫ÂêàÔºåÁ°Æ‰øù‰∏çÁÆ°‰ΩøÁî®ËøáÁ®ã‰∏≠ÊòØÂê¶ÂèëÁîüÂºÇÂ∏∏ÈÉΩ‰ºöÊâßË°åÂøÖË¶ÅÁöÑ‚ÄúÊ∏ÖÁêÜ‚ÄùÊìç‰ΩúÔºåÈáäÊîæËµÑÊ∫êÔºåÊØîÂ¶ÇÊñá‰ª∂‰ΩøÁî®ÂêéËá™Âä®ÂÖ≥Èó≠„ÄÅÁ∫øÁ®ã‰∏≠ÈîÅÁöÑËá™Âä®Ëé∑ÂèñÂíåÈáäÊîæÁ≠â„ÄÇÂª∫Á´ãËÆ°ÁÆóÊµÅÁöÑËøáÁ®ãÔºåÂâçÂêë‰º†Êí≠ÁöÑËøáÁ®ãÔºåoperation Summaryhow to systematically organize the hyper parameter search process and batch normalization and framework http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2 http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3 http://dl-notes.imshuai.com/#/c2w1?id=_4-heros-of-deep-learning-yoshua-bengio-interview https://www.youtube.com/watch?v=4Ct3Yujl1dk&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=14]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂΩ©ÈìÖDailyLifeStyle]]></title>
    <url>%2F2019%2F04%2F16%2F%E5%BD%A9%E9%93%85DailyLifeStyle%2F</url>
    <content type="text"><![CDATA[Day one 1. Â∑•ÂÖ∑ÁÆÄÂçï‰ªãÁªç ÂΩ©ÈìÖÊ∞¥Ê∫∂ÊÄßÔºåÊ≤πÊÄß ÂΩ©ÈìÖÁ∫∏ ÈìÖÁ¨î B2B&lt;4BÈªëÁöÑÁ®ãÂ∫¶ H4H&lt;8HËΩØÂ∫¶ Ê©°ÁöÆ ËΩØÊ©°ÁöÆ Á°¨Ê©°ÁöÆ ÁîµÂä®Ê©°ÁöÆÊì¶ ÈìÖÁ¨îÂàÄ ÂèØË∑≥Ê°£Á±ªÂûã ÂãæÁ∫øÁ¨î ÈíàÁÆ°Á¨î ÔºàÊ®±Ëä±Ôºâ Á¨îÂ•ó È´òÂÖâÁ¨î ÂèØ‰ª•Áî®‰øÆÊ≠£Ê∂≤ÊõøÊç¢Ôºà‰∏âÊ£±) Á∫∏Êì¶Á¨î Áéõ‰∏Ω Âà∑Â≠ê ÁîªÊùø ÈÄüÂÜôÊùø2. È¢úËâ≤‰∏âÂéüËâ≤Ôºö Á∫¢ ÈªÑ Ëìù Ëâ≤Áõ∏È¢úËâ≤ È•±ÂíåÂ∫¶ È≤úËâ≥Á®ãÂ∫¶ ÊòéÂ∫¶ ÊòéÊöóÁ®ãÂ∫¶ ÈÇªËøëËâ≤ ÂØπÊØîËâ≤ Á∫¢-Áªø Ëìù-Ê©ô Á¥´-ÈªÑ ÊöñËâ≤ÂíåÂÜ∑Ëâ≤ 3. ÊéíÁ∫ø ‰∏Ä‰∏™ÊñπÂêëÂæÄÂêå‰∏Ä‰∏™ÊñπÂêëÊéíÔºåÊó†ËøûÊé• Êù•ÂõûÁõ∏ËøûÊé•Ôºå‰∏ÄÊù°Á∫ø ‰∏çÂêåÊñπÂêëÊéíÂàóÊ≥®ÊÑèÔºöÂäõÂ∫¶ÂíåÈó¥Ë∑ù4. Âπ≥Ê∂ÇÂäõÂ∫¶‰∏ÄËá¥5. Ê∏êÂèòÂäõÂ∫¶‰∏ç‰∏ÄËá¥]]></content>
      <categories>
        <category>Â®±‰πêÁîüÊ¥ª</category>
      </categories>
      <tags>
        <tag>ÂΩ©ÈìÖ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F04%2F13%2Fmachine%20learning%2F</url>
    <content type="text"><![CDATA[Ëã±‰ºüËææ:ËäØÁâáÔºåGPU ÂºÄÂèëÊ°ÜÊû∂ÔºötensorflowÔºåpytorch caffe ÁõëÁù£Â≠¶‰π†Â≠¶‰π†ÁõÆÁöÑÊòØÂ≠¶‰π†‰∏Ä‰∏™ËæìÂÖ•Âà∞ËæìÂá∫ÁöÑÊò†Â∞ÑÔºåÁß∞‰∏∫Ê®°Âûã„ÄÇÊ®°ÂûãÁöÑÈõÜÂêàÂ∞±ÊòØÂÅáËÆæÁ©∫Èó¥„ÄÇ Ê®°ÂûãÔºöÊ¶ÇÁéáÊ®°ÂûãÔºõÈùûÊ¶ÇÁéáÊ®°ÂûãÔºõ Â≠¶‰π†ËøáÁ®ãÔºöÊêúÁ¥¢ËøáÁ®ã]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F04%2F13%2Fmachine%20learning%20test%2F</url>
    <content type="text"><![CDATA[Logistics Regression Â¶Ç‰ΩïÂá∏Êòæ‰Ω†ÊòØ‰∏Ä‰∏™ÂØπÈÄªËæëÂõûÂΩíÂ∑≤ÁªèÈùûÂ∏∏‰∫ÜËß£ÁöÑ‰∫∫Âë¢„ÄÇÈÇ£Â∞±ÊòØÁî®‰∏ÄÂè•ËØùÊ¶ÇÊã¨ÂÆÉÔºÅÈÄªËæëÂõûÂΩíÂÅáËÆæÊï∞ÊçÆÊúç‰ªé‰ºØÂä™Âà©ÂàÜÂ∏É,ÈÄöËøáÊûÅÂ§ßÂåñ‰ººÁÑ∂ÂáΩÊï∞ÁöÑÊñπÊ≥ïÔºåËøêÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊù•Ê±ÇËß£ÂèÇÊï∞ÔºåÊù•ËææÂà∞Â∞ÜÊï∞ÊçÆ‰∫åÂàÜÁ±ªÁöÑÁõÆÁöÑ„ÄÇ ‚Äã ËøôÈáåÈù¢ÂÖ∂ÂÆûÂåÖÂê´‰∫Ü5‰∏™ÁÇπ 1ÔºöÈÄªËæëÂõûÂΩíÁöÑÂÅáËÆæÔºå2ÔºöÈÄªËæëÂõûÂΩíÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå3ÔºöÈÄªËæëÂõûÂΩíÁöÑÊ±ÇËß£ÊñπÊ≥ïÔºå4ÔºöÈÄªËæëÂõûÂΩíÁöÑÁõÆÁöÑÔºå5:ÈÄªËæëÂõûÂΩíÂ¶Ç‰ΩïÂàÜÁ±ª„ÄÇËøô‰∫õÈóÆÈ¢òÊòØËÄÉÊ†∏‰Ω†ÂØπÈÄªËæëÂõûÂΩíÁöÑÂü∫Êú¨‰∫ÜËß£„ÄÇ ÈÄªËæëÂõûÂΩíÁöÑÂü∫Êú¨ÂÅáËÆæ ‰ªª‰ΩïÁöÑÊ®°ÂûãÈÉΩÊòØÊúâËá™Â∑±ÁöÑÂÅáËÆæÔºåÂú®Ëøô‰∏™ÂÅáËÆæ‰∏ãÊ®°ÂûãÊâçÊòØÈÄÇÁî®ÁöÑ„ÄÇÈÄªËæëÂõûÂΩíÁöÑ Á¨¨‰∏Ä‰∏™ Âü∫Êú¨ÂÅáËÆæÊòØ ÂÅáËÆæÊï∞ÊçÆÊúç‰ªé‰ºØÂä™Âà©ÂàÜÂ∏É„ÄÇ ‰ºØÂä™Âà©ÂàÜÂ∏ÉÊúâ‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰æãÂ≠êÊòØÊäõÁ°¨Â∏ÅÔºåÊäõ‰∏≠‰∏∫Ê≠£Èù¢ÁöÑÊ¶ÇÁéáÊòØp,Êäõ‰∏≠‰∏∫Ë¥üÈù¢ÁöÑÊ¶ÇÁéáÊòØ 1-p,Âú®ÈÄªËæëÂõûÂΩíËøô‰∏™Ê®°ÂûãÈáåÈù¢ÊòØÂÅáËÆæ $h_Œ∏(x)$‰∏∫Ê†∑Êú¨‰∏∫Ê≠£ÁöÑÊ¶ÇÁéáÔºå $1‚àíh_Œ∏(x)$‰∏∫Ê†∑Êú¨‰∏∫Ë¥üÁöÑÊ¶ÇÁéá„ÄÇÈÇ£‰πàÊï¥‰∏™Ê®°ÂûãÂèØ‰ª•ÊèèËø∞‰∏∫$h_Œ∏(x;Œ∏)=p$ ÈÄªËæëÂõûÂΩíÁöÑÁ¨¨‰∫å‰∏™ÂÅáËÆæÊòØÂÅáËÆæÊ†∑Êú¨‰∏∫Ê≠£ÁöÑÊ¶ÇÁéáÊòØ $p=\frac{1}{1+e^{w^Tx}}$ ÊâÄ‰ª•ÈÄªËæëÂõûÂΩíÁöÑÊúÄÁªàÂΩ¢Âºè $h_Œ∏(x;Œ∏)=\frac{1}{1+e^{w^Tx}}$ ÈÄªËæëÂõûÂΩíÁöÑÊçüÂ§±ÂáΩÊï∞ ÈÄªËæëÂõûÂΩíÁöÑÊçüÂ§±ÂáΩÊï∞ÊòØÂÆÉÁöÑÊûÅÂ§ß‰ººÁÑ∂ÂáΩÊï∞ $LŒ∏(x)=\pi_{i=1}^{m}h_Œ∏(xi;Œ∏)^y_i‚àó(1‚àíh_Œ∏(xi;Œ∏))^{1‚àíy_i}$ ÈÄªËæëÂõûÂΩíÁöÑÊ±ÇËß£ÊñπÊ≥ï Áî±‰∫éËØ•ÊûÅÂ§ß‰ººÁÑ∂ÂáΩÊï∞Êó†Ê≥ïÁõ¥Êé•Ê±ÇËß£ÔºåÊàë‰ª¨‰∏ÄËà¨ÈÄöËøáÂØπËØ•ÂáΩÊï∞ËøõË°åÊ¢ØÂ∫¶‰∏ãÈôçÊù•‰∏çÊñ≠ÈÄºÊÄ•ÊúÄ‰ºòËß£„ÄÇÂú®Ëøô‰∏™Âú∞ÊñπÂÖ∂ÂÆû‰ºöÊúâ‰∏™Âä†ÂàÜÁöÑÈ°πÔºåËÄÉÂØü‰Ω†ÂØπÂÖ∂‰ªñ‰ºòÂåñÊñπÊ≥ïÁöÑ‰∫ÜËß£„ÄÇÂõ†‰∏∫Â∞±Ê¢ØÂ∫¶‰∏ãÈôçÊú¨Ë∫´Êù•ÁúãÁöÑËØùÂ∞±ÊúâÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÔºåÊâπÊ¢ØÂ∫¶‰∏ãÈôçÔºåsmall batch Ê¢ØÂ∫¶‰∏ãÈôç‰∏âÁßçÊñπÂºèÔºåÈù¢ËØïÂÆòÂèØËÉΩ‰ºöÈóÆËøô‰∏âÁßçÊñπÂºèÁöÑ‰ºòÂä£‰ª•ÂèäÂ¶Ç‰ΩïÈÄâÊã©ÊúÄÂêàÈÄÇÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÊñπÂºè„ÄÇ ÁÆÄÂçïÊù•ËØ¥ ÊâπÊ¢ØÂ∫¶‰∏ãÈôç‰ºöËé∑ÂæóÂÖ®Â±ÄÊúÄ‰ºòËß£ÔºåÁº∫ÁÇπÊòØÂú®Êõ¥Êñ∞ÊØè‰∏™ÂèÇÊï∞ÁöÑÊó∂ÂÄôÈúÄË¶ÅÈÅçÂéÜÊâÄÊúâÁöÑÊï∞ÊçÆÔºåËÆ°ÁÆóÈáè‰ºöÂæàÂ§ßÔºåÂπ∂‰∏î‰ºöÊúâÂæàÂ§öÁöÑÂÜó‰ΩôËÆ°ÁÆóÔºåÂØºËá¥ÁöÑÁªìÊûúÊòØÂΩìÊï∞ÊçÆÈáèÂ§ßÁöÑÊó∂ÂÄôÔºåÊØè‰∏™ÂèÇÊï∞ÁöÑÊõ¥Êñ∞ÈÉΩ‰ºöÂæàÊÖ¢„ÄÇ ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊòØ‰ª•È´òÊñπÂ∑ÆÈ¢ëÁπÅÊõ¥Êñ∞Ôºå‰ºòÁÇπÊòØ‰ΩøÂæósgd‰ºöË∑≥Âà∞Êñ∞ÁöÑÂíåÊΩúÂú®Êõ¥Â•ΩÁöÑÂ±ÄÈÉ®ÊúÄ‰ºòËß£ÔºåÁº∫ÁÇπÊòØ‰ΩøÂæóÊî∂ÊïõÂà∞Â±ÄÈÉ®ÊúÄ‰ºòËß£ÁöÑËøáÁ®ãÊõ¥Âä†ÁöÑÂ§çÊùÇ„ÄÇ Â∞èÊâπÈáèÊ¢ØÂ∫¶‰∏ãÈôçÁªìÂêà‰∫ÜsgdÂíåbatch gdÁöÑ‰ºòÁÇπÔºåÊØèÊ¨°Êõ¥Êñ∞ÁöÑÊó∂ÂÄô‰ΩøÁî®n‰∏™Ê†∑Êú¨„ÄÇÂáèÂ∞ë‰∫ÜÂèÇÊï∞Êõ¥Êñ∞ÁöÑÊ¨°Êï∞ÔºåÂèØ‰ª•ËææÂà∞Êõ¥Âä†Á®≥ÂÆöÊî∂ÊïõÁªìÊûúÔºå‰∏ÄËà¨Âú®Ê∑±Â∫¶Â≠¶‰π†ÂΩì‰∏≠Êàë‰ª¨ÈááÁî®ËøôÁßçÊñπÊ≥ï„ÄÇ ÂÖ∂ÂÆûËøôÈáåËøòÊúâ‰∏Ä‰∏™ÈöêËóèÁöÑÊõ¥Âä†Ê∑±ÁöÑÂä†ÂàÜÈ°πÔºåÁúã‰Ω†‰∫Ü‰∏ç‰∫ÜËß£ËØ∏Â¶ÇAdamÔºåÂä®ÈáèÊ≥ïÁ≠â‰ºòÂåñÊñπÊ≥ï„ÄÇÂõ†‰∏∫‰∏äËø∞ÊñπÊ≥ïÂÖ∂ÂÆûËøòÊúâ‰∏§‰∏™Ëá¥ÂëΩÁöÑÈóÆÈ¢ò„ÄÇ Á¨¨‰∏Ä‰∏™ÊòØÂ¶Ç‰ΩïÂØπÊ®°ÂûãÈÄâÊã©ÂêàÈÄÇÁöÑÂ≠¶‰π†Áéá„ÄÇËá™ÂßãËá≥Áªà‰øùÊåÅÂêåÊ†∑ÁöÑÂ≠¶‰π†ÁéáÂÖ∂ÂÆû‰∏çÂ§™ÂêàÈÄÇ„ÄÇÂõ†‰∏∫‰∏ÄÂºÄÂßãÂèÇÊï∞ÂàöÂàöÂºÄÂßãÂ≠¶‰π†ÁöÑÊó∂ÂÄôÔºåÊ≠§Êó∂ÁöÑÂèÇÊï∞ÂíåÊúÄ‰ºòËß£ÈöîÁöÑÊØîËæÉËøúÔºåÈúÄË¶Å‰øùÊåÅ‰∏Ä‰∏™ËæÉÂ§ßÁöÑÂ≠¶‰π†ÁéáÂ∞ΩÂø´ÈÄºËøëÊúÄ‰ºòËß£„ÄÇ‰ΩÜÊòØÂ≠¶‰π†Âà∞ÂêéÈù¢ÁöÑÊó∂ÂÄôÔºåÂèÇÊï∞ÂíåÊúÄ‰ºòËß£Â∑≤ÁªèÈöîÁöÑÊØîËæÉËøë‰∫ÜÔºå‰Ω†Ëøò‰øùÊåÅÊúÄÂàùÁöÑÂ≠¶‰π†ÁéáÔºåÂÆπÊòìË∂äËøáÊúÄ‰ºòÁÇπÔºåÂú®ÊúÄ‰ºòÁÇπÈôÑËøëÊù•ÂõûÊåØËç°ÔºåÈÄö‰øó‰∏ÄÁÇπËØ¥ÔºåÂ∞±ÂæàÂÆπÊòìÂ≠¶ËøáÂ§¥‰∫ÜÔºåË∑ëÂÅè‰∫Ü„ÄÇ Á¨¨‰∫å‰∏™ÊòØÂ¶Ç‰ΩïÂØπÂèÇÊï∞ÈÄâÊã©ÂêàÈÄÇÁöÑÂ≠¶‰π†Áéá„ÄÇÂú®ÂÆûË∑µ‰∏≠ÔºåÂØπÊØè‰∏™ÂèÇÊï∞ÈÉΩ‰øùÊåÅÁöÑÂêåÊ†∑ÁöÑÂ≠¶‰π†Áéá‰πüÊòØÂæà‰∏çÂêàÁêÜÁöÑ„ÄÇÊúâ‰∫õÂèÇÊï∞Êõ¥Êñ∞È¢ëÁπÅÔºåÈÇ£‰πàÂ≠¶‰π†ÁéáÂèØ‰ª•ÈÄÇÂΩìÂ∞è‰∏ÄÁÇπ„ÄÇÊúâ‰∫õÂèÇÊï∞Êõ¥Êñ∞ÁºìÊÖ¢ÔºåÈÇ£‰πàÂ≠¶‰π†ÁéáÂ∞±Â∫îËØ•Â§ß‰∏ÄÁÇπ„ÄÇËøôÈáåÊàë‰ª¨‰∏çÂ±ïÂºÄÔºåÊúâÁ©∫Êàë‰ºö‰∏ìÈó®Âá∫‰∏Ä‰∏™‰∏ìÈ¢ò‰ªãÁªç„ÄÇ ÈÄªËæëÂõûÂΩíÁöÑÁõÆÁöÑ ËØ•ÂáΩÊï∞ÁöÑÁõÆÁöÑ‰æøÊòØÂ∞ÜÊï∞ÊçÆ‰∫åÂàÜÁ±ªÔºåÊèêÈ´òÂáÜÁ°ÆÁéá„ÄÇ ÈÄªËæëÂõûÂΩíÂ¶Ç‰ΩïÂàÜÁ±ª ÈÄªËæëÂõûÂΩí‰Ωú‰∏∫‰∏Ä‰∏™ÂõûÂΩí(‰πüÂ∞±ÊòØyÂÄºÊòØËøûÁª≠ÁöÑ)ÔºåÂ¶Ç‰ΩïÂ∫îÁî®Âà∞ÂàÜÁ±ª‰∏äÂéªÂë¢„ÄÇyÂÄºÁ°ÆÂÆûÊòØ‰∏Ä‰∏™ËøûÁª≠ÁöÑÂèòÈáè„ÄÇÈÄªËæëÂõûÂΩíÁöÑÂÅöÊ≥ïÊòØÂàíÂÆö‰∏Ä‰∏™ÈòàÂÄºÔºåyÂÄºÂ§ß‰∫éËøô‰∏™ÈòàÂÄºÁöÑÊòØ‰∏ÄÁ±ªÔºåyÂÄºÂ∞è‰∫éËøô‰∏™ÈòàÂÄºÁöÑÊòØÂè¶Â§ñ‰∏ÄÁ±ª„ÄÇÈòàÂÄºÂÖ∑‰ΩìÂ¶Ç‰ΩïË∞ÉÊï¥Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµÈÄâÊã©„ÄÇ‰∏ÄËà¨‰ºöÈÄâÊã©0.5ÂÅö‰∏∫ÈòàÂÄºÊù•ÂàíÂàÜ„ÄÇ 3.ÂØπÈÄªËæëÂõûÂΩíÁöÑËøõ‰∏ÄÊ≠•ÊèêÈóÆ‚Äã ÈÄªËæëÂõûÂΩíËôΩÁÑ∂‰ªéÂΩ¢Âºè‰∏äÈùûÂ∏∏ÁöÑÁÆÄÂçïÔºå‰ΩÜÊòØÂÖ∂ÂÜÖÊ∂µÊòØÈùûÂ∏∏ÁöÑ‰∏∞ÂØå„ÄÇÊúâÂæàÂ§öÈóÆÈ¢òÊòØÂèØ‰ª•ËøõË°åÊÄùËÄÉÁöÑ ÈÄªËæëÂõûÂΩíÁöÑÊçüÂ§±ÂáΩÊï∞‰∏∫‰ªÄ‰πàË¶Å‰ΩøÁî®ÊûÅÂ§ß‰ººÁÑ∂ÂáΩÊï∞‰Ωú‰∏∫ÊçüÂ§±ÂáΩÊï∞Ôºü ÊçüÂ§±ÂáΩÊï∞‰∏ÄËà¨ÊúâÂõõÁßçÔºåÂπ≥ÊñπÊçüÂ§±ÂáΩÊï∞ÔºåÂØπÊï∞ÊçüÂ§±ÂáΩÊï∞ÔºåHingeLoss0-1ÊçüÂ§±ÂáΩÊï∞ÔºåÁªùÂØπÂÄºÊçüÂ§±ÂáΩÊï∞„ÄÇÂ∞ÜÊûÅÂ§ß‰ººÁÑ∂ÂáΩÊï∞ÂèñÂØπÊï∞‰ª•ÂêéÁ≠âÂêå‰∫éÂØπÊï∞ÊçüÂ§±ÂáΩÊï∞„ÄÇÂú®ÈÄªËæëÂõûÂΩíËøô‰∏™Ê®°Âûã‰∏ãÔºåÂØπÊï∞ÊçüÂ§±ÂáΩÊï∞ÁöÑËÆ≠ÁªÉÊ±ÇËß£ÂèÇÊï∞ÁöÑÈÄüÂ∫¶ÊòØÊØîËæÉÂø´ÁöÑ„ÄÇËá≥‰∫éÂéüÂõ†Â§ßÂÆ∂ÂèØ‰ª•Ê±ÇÂá∫Ëøô‰∏™ÂºèÂ≠êÁöÑÊ¢ØÂ∫¶Êõ¥Êñ∞ $w_j=w_j‚àí(y^i‚àíh_w(x^i;w))‚àóx_j^i\theta‚Äã$ Ëøô‰∏™ÂºèÂ≠êÁöÑÊõ¥Êñ∞ÈÄüÂ∫¶Âè™Âíå$x_j,y_j Áõ∏ÂÖ≥„ÄÇÂíåsigmodÂáΩÊï∞Êú¨Ë∫´ÁöÑÊ¢ØÂ∫¶ÊòØÊó†ÂÖ≥ÁöÑ„ÄÇËøôÊ†∑Êõ¥Êñ∞ÁöÑÈÄüÂ∫¶ÊòØÂèØ‰ª•Ëá™ÂßãËá≥ÁªàÈÉΩÊØîËæÉÁöÑÁ®≥ÂÆö„ÄÇ ‰∏∫‰ªÄ‰πà‰∏çÈÄâÂπ≥ÊñπÊçüÂ§±ÂáΩÊï∞ÁöÑÂë¢ÔºüÂÖ∂‰∏ÄÊòØÂõ†‰∏∫Â¶ÇÊûú‰Ω†‰ΩøÁî®Âπ≥ÊñπÊçüÂ§±ÂáΩÊï∞Ôºå‰Ω†‰ºöÂèëÁé∞Ê¢ØÂ∫¶Êõ¥Êñ∞ÁöÑÈÄüÂ∫¶ÂíåsigmodÂáΩÊï∞Êú¨Ë∫´ÁöÑÊ¢ØÂ∫¶ÊòØÂæàÁõ∏ÂÖ≥ÁöÑ„ÄÇsigmodÂáΩÊï∞Âú®ÂÆÉÂú®ÂÆö‰πâÂüüÂÜÖÁöÑÊ¢ØÂ∫¶ÈÉΩ‰∏çÂ§ß‰∫é0.25„ÄÇËøôÊ†∑ËÆ≠ÁªÉ‰ºöÈùûÂ∏∏ÁöÑÊÖ¢„ÄÇ ÈÄªËæëÂõûÂΩíÂú®ËÆ≠ÁªÉÁöÑËøáÁ®ãÂΩì‰∏≠ÔºåÂ¶ÇÊûúÊúâÂæàÂ§öÁöÑÁâπÂæÅÈ´òÂ∫¶Áõ∏ÂÖ≥ÊàñËÄÖËØ¥Êúâ‰∏Ä‰∏™ÁâπÂæÅÈáçÂ§ç‰∫Ü100ÈÅçÔºå‰ºöÈÄ†ÊàêÊÄéÊ†∑ÁöÑÂΩ±ÂìçÔºü ÂÖàËØ¥ÁªìËÆ∫ÔºåÂ¶ÇÊûúÂú®ÊçüÂ§±ÂáΩÊï∞ÊúÄÁªàÊî∂ÊïõÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÖ∂ÂÆûÂ∞±ÁÆóÊúâÂæàÂ§öÁâπÂæÅÈ´òÂ∫¶Áõ∏ÂÖ≥‰πü‰∏ç‰ºöÂΩ±ÂìçÂàÜÁ±ªÂô®ÁöÑÊïàÊûú„ÄÇ ‰ΩÜÊòØÂØπÁâπÂæÅÊú¨Ë∫´Êù•ËØ¥ÁöÑËØùÔºåÂÅáËÆæÂè™Êúâ‰∏Ä‰∏™ÁâπÂæÅÔºåÂú®‰∏çËÄÉËôëÈááÊ†∑ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰Ω†Áé∞Âú®Â∞ÜÂÆÉÈáçÂ§ç100ÈÅç„ÄÇËÆ≠ÁªÉ‰ª•ÂêéÂÆå‰ª•ÂêéÔºåÊï∞ÊçÆËøòÊòØËøô‰πàÂ§öÔºå‰ΩÜÊòØËøô‰∏™ÁâπÂæÅÊú¨Ë∫´ÈáçÂ§ç‰∫Ü100ÈÅçÔºåÂÆûË¥®‰∏äÂ∞ÜÂéüÊù•ÁöÑÁâπÂæÅÂàÜÊàê‰∫Ü100‰ªΩÔºåÊØè‰∏Ä‰∏™ÁâπÂæÅÈÉΩÊòØÂéüÊù•ÁâπÂæÅÊùÉÈáçÂÄºÁöÑÁôæÂàÜ‰πã‰∏Ä„ÄÇ Â¶ÇÊûúÂú®ÈöèÊú∫ÈááÊ†∑ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÖ∂ÂÆûËÆ≠ÁªÉÊî∂ÊïõÂÆå‰ª•ÂêéÔºåËøòÊòØÂèØ‰ª•ËÆ§‰∏∫Ëøô100‰∏™ÁâπÂæÅÂíåÂéüÊù•ÈÇ£‰∏Ä‰∏™ÁâπÂæÅÊâÆÊºîÁöÑÊïàÊûú‰∏ÄÊ†∑ÔºåÂè™ÊòØÂèØËÉΩ‰∏≠Èó¥ÂæàÂ§öÁâπÂæÅÁöÑÂÄºÊ≠£Ë¥üÁõ∏Ê∂à‰∫Ü„ÄÇ ‰∏∫‰ªÄ‰πàÊàë‰ª¨ËøòÊòØ‰ºöÂú®ËÆ≠ÁªÉÁöÑËøáÁ®ãÂΩì‰∏≠Â∞ÜÈ´òÂ∫¶Áõ∏ÂÖ≥ÁöÑÁâπÂæÅÂéªÊéâÔºü ÂéªÊéâÈ´òÂ∫¶Áõ∏ÂÖ≥ÁöÑÁâπÂæÅ‰ºöËÆ©Ê®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÊõ¥Â•Ω ÂèØ‰ª•Â§ßÂ§ßÊèêÈ´òËÆ≠ÁªÉÁöÑÈÄüÂ∫¶„ÄÇÂ¶ÇÊûúÊ®°ÂûãÂΩì‰∏≠ÊúâÂæàÂ§öÁâπÂæÅÈ´òÂ∫¶Áõ∏ÂÖ≥ÁöÑËØùÔºåÂ∞±ÁÆóÊçüÂ§±ÂáΩÊï∞Êú¨Ë∫´Êî∂Êïõ‰∫ÜÔºå‰ΩÜÂÆûÈôÖ‰∏äÂèÇÊï∞ÊòØÊ≤°ÊúâÊî∂ÊïõÁöÑÔºåËøôÊ†∑‰ºöÊãâ‰ΩéËÆ≠ÁªÉÁöÑÈÄüÂ∫¶„ÄÇÂÖ∂Ê¨°ÊòØÁâπÂæÅÂ§ö‰∫ÜÔºåÊú¨Ë∫´Â∞±‰ºöÂ¢ûÂ§ßËÆ≠ÁªÉÁöÑÊó∂Èó¥„ÄÇ 4.ÈÄªËæëÂõûÂΩíÁöÑ‰ºòÁº∫ÁÇπÊÄªÁªì‚Äã Èù¢ËØïÁöÑÊó∂ÂÄôÔºåÂà´‰∫∫‰πüÁªèÂ∏∏‰ºöÈóÆÂà∞Ôºå‰Ω†Âú®‰ΩøÁî®ÈÄªËæëÂõûÂΩíÁöÑÊó∂ÂÄôÊúâÂì™‰∫õÊÑüÂèó„ÄÇËßâÂæóÂÆÉÊúâÂì™‰∫õ‰ºòÁº∫ÁÇπ„ÄÇ ‚Äã Âú®ËøôÈáåÊàë‰ª¨ÊÄªÁªì‰∫ÜÈÄªËæëÂõûÂΩíÂ∫îÁî®Âà∞Â∑•‰∏öÁïåÂΩì‰∏≠‰∏Ä‰∫õ‰ºòÁÇπÔºö ÂΩ¢ÂºèÁÆÄÂçïÔºåÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÈùûÂ∏∏Â•Ω„ÄÇ‰ªéÁâπÂæÅÁöÑÊùÉÈáçÂèØ‰ª•ÁúãÂà∞‰∏çÂêåÁöÑÁâπÂæÅÂØπÊúÄÂêéÁªìÊûúÁöÑÂΩ±ÂìçÔºåÊüê‰∏™ÁâπÂæÅÁöÑÊùÉÈáçÂÄºÊØîËæÉÈ´òÔºåÈÇ£‰πàËøô‰∏™ÁâπÂæÅÊúÄÂêéÂØπÁªìÊûúÁöÑÂΩ±Âìç‰ºöÊØîËæÉÂ§ß„ÄÇ Ê®°ÂûãÊïàÊûú‰∏çÈîô„ÄÇÂú®Â∑•Á®ã‰∏äÊòØÂèØ‰ª•Êé•ÂèóÁöÑÔºà‰Ωú‰∏∫baseline)ÔºåÂ¶ÇÊûúÁâπÂæÅÂ∑•Á®ãÂÅöÁöÑÂ•ΩÔºåÊïàÊûú‰∏ç‰ºöÂ§™Â∑ÆÔºåÂπ∂‰∏îÁâπÂæÅÂ∑•Á®ãÂèØ‰ª•Â§ßÂÆ∂Âπ∂Ë°åÂºÄÂèëÔºåÂ§ßÂ§ßÂä†Âø´ÂºÄÂèëÁöÑÈÄüÂ∫¶„ÄÇ ËÆ≠ÁªÉÈÄüÂ∫¶ËæÉÂø´„ÄÇÂàÜÁ±ªÁöÑÊó∂ÂÄôÔºåËÆ°ÁÆóÈáè‰ªÖ‰ªÖÂè™ÂíåÁâπÂæÅÁöÑÊï∞ÁõÆÁõ∏ÂÖ≥„ÄÇÂπ∂‰∏îÈÄªËæëÂõûÂΩíÁöÑÂàÜÂ∏ÉÂºè‰ºòÂåñsgdÂèëÂ±ïÊØîËæÉÊàêÁÜüÔºåËÆ≠ÁªÉÁöÑÈÄüÂ∫¶ÂèØ‰ª•ÈÄöËøáÂ†ÜÊú∫Âô®Ëøõ‰∏ÄÊ≠•ÊèêÈ´òÔºåËøôÊ†∑Êàë‰ª¨ÂèØ‰ª•Âú®Áü≠Êó∂Èó¥ÂÜÖËø≠‰ª£Â•ΩÂá†‰∏™ÁâàÊú¨ÁöÑÊ®°Âûã„ÄÇ ËµÑÊ∫êÂç†Áî®Â∞è,Â∞§ÂÖ∂ÊòØÂÜÖÂ≠ò„ÄÇÂõ†‰∏∫Âè™ÈúÄË¶ÅÂ≠òÂÇ®ÂêÑ‰∏™Áª¥Â∫¶ÁöÑÁâπÂæÅÂÄºÔºå„ÄÇ Êñπ‰æøËæìÂá∫ÁªìÊûúË∞ÉÊï¥„ÄÇÈÄªËæëÂõûÂΩíÂèØ‰ª•ÂæàÊñπ‰æøÁöÑÂæóÂà∞ÊúÄÂêéÁöÑÂàÜÁ±ªÁªìÊûúÔºåÂõ†‰∏∫ËæìÂá∫ÁöÑÊòØÊØè‰∏™Ê†∑Êú¨ÁöÑÊ¶ÇÁéáÂàÜÊï∞ÔºåÊàë‰ª¨ÂèØ‰ª•ÂæàÂÆπÊòìÁöÑÂØπËøô‰∫õÊ¶ÇÁéáÂàÜÊï∞ËøõË°åcutoffÔºå‰πüÂ∞±ÊòØÂàíÂàÜÈòàÂÄº(Â§ß‰∫éÊüê‰∏™ÈòàÂÄºÁöÑÊòØ‰∏ÄÁ±ªÔºåÂ∞è‰∫éÊüê‰∏™ÈòàÂÄºÁöÑÊòØ‰∏ÄÁ±ª)„ÄÇ ‚Äã ‰ΩÜÊòØÈÄªËæëÂõûÂΩíÊú¨Ë∫´‰πüÊúâËÆ∏Â§öÁöÑÁº∫ÁÇπ: ÂáÜÁ°ÆÁéáÂπ∂‰∏çÊòØÂæàÈ´ò„ÄÇÂõ†‰∏∫ÂΩ¢ÂºèÈùûÂ∏∏ÁöÑÁÆÄÂçï(ÈùûÂ∏∏Á±ª‰ººÁ∫øÊÄßÊ®°Âûã)ÔºåÂæàÈöæÂéªÊãüÂêàÊï∞ÊçÆÁöÑÁúüÂÆûÂàÜÂ∏É„ÄÇ ÂæàÈöæÂ§ÑÁêÜÊï∞ÊçÆ‰∏çÂπ≥Ë°°ÁöÑÈóÆÈ¢ò„ÄÇ‰∏æ‰∏™‰æãÂ≠êÔºöÂ¶ÇÊûúÊàë‰ª¨ÂØπ‰∫é‰∏Ä‰∏™Ê≠£Ë¥üÊ†∑Êú¨ÈùûÂ∏∏‰∏çÂπ≥Ë°°ÁöÑÈóÆÈ¢òÊØîÂ¶ÇÊ≠£Ë¥üÊ†∑Êú¨ÊØî 10000:1.Êàë‰ª¨ÊääÊâÄÊúâÊ†∑Êú¨ÈÉΩÈ¢ÑÊµã‰∏∫Ê≠£‰πüËÉΩ‰ΩøÊçüÂ§±ÂáΩÊï∞ÁöÑÂÄºÊØîËæÉÂ∞è„ÄÇ‰ΩÜÊòØ‰Ωú‰∏∫‰∏Ä‰∏™ÂàÜÁ±ªÂô®ÔºåÂÆÉÂØπÊ≠£Ë¥üÊ†∑Êú¨ÁöÑÂå∫ÂàÜËÉΩÂäõ‰∏ç‰ºöÂæàÂ•Ω„ÄÇ Â§ÑÁêÜÈùûÁ∫øÊÄßÊï∞ÊçÆËæÉÈ∫ªÁÉ¶„ÄÇÈÄªËæëÂõûÂΩíÂú®‰∏çÂºïÂÖ•ÂÖ∂‰ªñÊñπÊ≥ïÁöÑÊÉÖÂÜµ‰∏ãÔºåÂè™ËÉΩÂ§ÑÁêÜÁ∫øÊÄßÂèØÂàÜÁöÑÊï∞ÊçÆÔºåÊàñËÄÖËøõ‰∏ÄÊ≠•ËØ¥ÔºåÂ§ÑÁêÜ‰∫åÂàÜÁ±ªÁöÑÈóÆÈ¢ò „ÄÇ ÈÄªËæëÂõûÂΩíÊú¨Ë∫´Êó†Ê≥ïÁ≠õÈÄâÁâπÂæÅ„ÄÇÊúâÊó∂ÂÄôÔºåÊàë‰ª¨‰ºöÁî®gbdtÊù•Á≠õÈÄâÁâπÂæÅÔºåÁÑ∂ÂêéÂÜç‰∏äÈÄªËæëÂõûÂΩí„ÄÇ Ê®°Âûã„ÄÅÁ≠ñÁï•„ÄÅÁÆóÊ≥ïCodings]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow]]></title>
    <url>%2F2019%2F04%2F11%2Ftensorflow%2F</url>
    <content type="text"><![CDATA[official definition What is tensorflowflow of tensors ‚ÄúTensorFlow is an open source software library for numerical computation using dataflow graphs. Nodes in the graph represents mathematical operations, while graph edges represent multi-dimensional data arrays (aka tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.‚Äù* A major difference between numpy and TensorFlow is that TensorFlow follows a lazy programming paradigm. It first builds a graph of all the operation to be done, and then when a ‚Äúsession‚Äù is called, it ‚Äúruns‚Äù the graph. It‚Äôs built to be scalable, by changing internal data representation to tensors (aka multi-dimensional arrays). Building a computational graph can be considered as the main ingredient of TensorFlow. It‚Äôs easy to classify TensorFlow as a neural network library, but it‚Äôs not just that. Yes, it was designed to be a powerful neural network library. But it has the power to do much more than that. You can build other machine learning algorithms on it such as decision trees or k-Nearest Neighbors. You can literally do everything you normally would do in numpy! It‚Äôs aptly called ‚Äúnumpy on steroids‚Äù The advantages of using TensorFlow are: It has an intuitive construct, because as the name suggests it has ‚Äúflow of tensors‚Äù. You can easily visualize each and every part of the graph. Easily train on cpu/gpu for distributed computing Platform flexibility. You can run the models wherever you want, whether it is on mobile, server or PC. scikit-learn 123456# define hyperparamters of ML algorithmclf = svm.SVC(gamma=0.001, C=100.)# train clf.fit(X, y)# test clf.predict(X_test) The usual workflow of running a program in TensorFlow is as follows: Build a computational graph, this can be any mathematical operation TensorFlow supports. Initialize variables, to compile the variables defined previously Create session(‰ºöËØùÔºâ, this is where the magic starts! Run graph in session, the compiled graph is passed to the session, which starts its execution. Close session, shutdown the session. Lets write a small program to add two numbers! 12345678910111213141516171819# import tensorflowimport tensorflow as tf# build computational grapha = tf.placeholder(tf.int16)b = tf.placeholder(tf.int16)addition = tf.add(a, b)# initialize variablesinit = tf.initialize_all_variables()# create session and run the graphwith tf.Session() as sess: sess.run(init) print "Addition: %i" % sess.run(addition, feed_dict=&#123;a: 2, b: 3&#125;)# close sessionsess.close() A typical implementation of Neural Network would be as follows: Define Neural Network architecture to be compiled Transfer data to your model Under the hood, the data is first divided into batches, so that it can be ingested. The batches are first preprocessed, augmented and then fed into Neural Network for training The model then gets trained incrementally Display the accuracy for a specific number of timesteps After training save the model for future use Test the model on a new data and check how it performs ‰∏âÁ±ªÈùûÂ∏∏ÈáçË¶ÅÁöÑÂèòÈáèÂç†‰ΩçÁ¨¶tensorFlow‰∏≠Êé•Êî∂ÂÄºÁöÑÊñπÂºè‰∏∫Âç†‰ΩçÁ¨¶(placeholder)ÔºåÂàõÂª∫placeholder 123- # b = tf.placeholder(tf.float32, [None, 1], name='b')Á¨¨‰∫å‰∏™ÂèÇÊï∞ÂÄº‰∏∫[None, 1]ÔºåÂÖ∂‰∏≠NoneË°®Á§∫‰∏çÁ°ÆÂÆöÔºåÂç≥‰∏çÁ°ÆÂÆöÁ¨¨‰∏Ä‰∏™Áª¥Â∫¶ÁöÑÂ§ßÂ∞èÔºåÁ¨¨‰∏ÄÁª¥ÂèØ‰ª•ÊòØ‰ªªÊÑèÂ§ßÂ∞è„ÄÇÁâπÂà´ÂØπÂ∫îtensorÊï∞Èáè(ÊàñËÄÖÊ†∑Êú¨Êï∞Èáè)ÔºåËæìÂÖ•ÁöÑtensorÊï∞ÁõÆÂèØ‰ª•ÊòØ32„ÄÅ64‚Ä¶ placeholder: A way to feed data into the graphsfeed_dict: A dictionary to pass numeric values to computational graph Â∏∏Èáètf.constant()`ÂÆö‰πâÂ∏∏Èáè 1const = tf.constant(2.0, name='const') ÂèòÈáè ‚Äã ‰ΩøÁî®tf.Variable()ÂÆö‰πâÂèòÈáè 1c = tf.Variable(1.0, dtype=tf.float32, name='c') TensorFlow‰∏≠ÊâÄÊúâÁöÑÂèòÈáèÂøÖÈ°ªÁªèËøáÂàùÂßãÂåñÊâçËÉΩ‰ΩøÁî®Ôºå**ÂàùÂßãÂåñÊñπÂºèÂàÜ‰∏§Ê≠•Ôºö ÂÆö‰πâÂàùÂßãÂåñoperation 12# 1. ÂÆö‰πâinit operationinit_op = tf.global_variables_initializer() ËøêË°åÂàùÂßãÂåñoperation 12# 2. ËøêË°åinit operation sess.run(init_op) reference https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/ https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial https://github.com/aymericdamien/TensorFlow-Examples video:https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/ course: https://classroom.udacity.com/courses/ud187 tensorflow: GOOGLE ÂºÄÊ∫ê„ÄÅDeep learning ÁªÉÊï∞ÊàêÈáëC1 tensorboard Ôºöa tool;visual network;debug alter dir of jupyter È°∫‰æøÊîπ‰∫Ü‰∏ãÊñ∞‰∏ãËΩΩÁöÑË∑ØÂæÑÔºàGOODÔºâ CPU or GPU C2graphs ‰ª£Ë°®ËÆ°ÁÆó‰ªªÂä°ÔºåËäÇÁÇπÔºàop)Ôºå‰∏Ä‰∏™opÂèØ‰ª•Ëé∑Âæóo‰∏™ÊàñËÄÖÂ§ö‰∏™tensor,ËæìÂá∫1‰∏™ÊàñËÄÖÂ§ö‰∏™tensor Session(‰ºöËØù)ÁöÑ‰∏ä‰∏ãÊñáÔºàcontext)‰∏≠ÊâßË°å tensorË°®Á§∫Êï∞ÊçÆ,nÁª¥Êï∞ÁªÑ C3 ÁÆÄÂçïÁöÑÂõûÂΩíÁ•ûÁªèÁΩëÁªúÔºàÊãüÂêà‰∫åÊ¨°ÂáΩÊï∞ÔºâÔºåË≤å‰ººÂ≠¶‰∫ÜÁêÜËÆ∫Ê≤°ÊúâÂÆûË∑µÔºåËøòÁúüÊòØÂøòÂæóÂø´Âïä ÊâãÂÜô‰ΩìÂàÜÁ±ª„ÄÅSoftmaxÂáΩÊï∞ softmaxÂáΩÊï∞ÂèØ‰ª•Áªô‰∏çÂêåÁöÑÂØπË±°ÂàÜÈÖçÊ¶ÇÁéáÔºåsoftmax($x_i$)=$\frac{exp(x_i)}{\sum_j{exp(x_j)}}$ Â¶ÇËæìÂá∫[1,2,5] ,$p1=\frac{exp(1)}{exp(1)+exp(2)+exp(5)}$,$p2=\frac{exp(2)}{exp(1)+exp(2)+exp(5)}$,$p1=\frac{exp(5)}{exp(1)+exp(2)+exp(5)}$ Keras ÂÆâË£Ö backend Âü∫‰∫é‰ªÄ‰πàÂÅöËøêÁÆóÔºàtensorflow or theano) import keras Êü•Áúã Â∫ïÂ±ÇÊê≠Âª∫ aÔºâ /.keras/keras.json Áõ∏ÂÖ≥ÁöÑÈÖçÁΩÆ‰ø°ÊÅØ b) ÁªàÁ´ØÊîπÔºåÂçïÊ¨° import os os.environ[‚ÄòKERAS_BACKEND‚Äô]= ‚Äòtensorflow‚Äô import keras For example model :Sequential layer : Dense activation ËÆ≠ÁªÉÁÆóÊ≥ïÔºömodel.compile(ÂèÇÊï∞optimizer=‚ÄôÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÁöÑÂèòÁßç‚Äô , loss=‚Äôrms/‚Äò) ËÆ≠ÁªÉÔºömodel. fit (x,y) model.train_on_batch evaluate:model.evaluate prediction: model.predict(x_test, batch_size=128) https://github.com/MorvanZhou/tutorials/blob/master/kerasTUT/5-classifier_example.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 4 - Regressor exampleimport numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.models import Sequential # ÊåâÈ°∫Â∫èÂª∫Á´ãfrom keras.layers import Dense # ÂÖ®ËøûÊé•Â±Çimport matplotlib.pyplot as plt# create some dataX = np.linspace(-1, 1, 200)np.random.shuffle(X) # randomize the dataY = 0.5 * X + 2 + np.random.normal(0, 0.05, (200, ))# plot dataplt.scatter(X, Y)plt.show()X_train, Y_train = X[:160], Y[:160] # first 160 data pointsX_test, Y_test = X[160:], Y[160:] # last 40 data points# build a neural network from the 1st layer to the last layermodel = Sequential()model.add(Dense(units=1, input_dim=1)) # choose loss function and optimizing methodmodel.compile(loss='mse', optimizer='sgd')# trainingprint('Training -----------')for step in range(301): cost = model.train_on_batch(X_train, Y_train) if step % 100 == 0: print('train cost: ', cost)# testprint('\nTesting ------------')cost = model.evaluate(X_test, Y_test, batch_size=40)print('test cost:', cost)W, b = model.layers[0].get_weights()print('Weights=', W, '\nbiases=', b)# plotting the predictionY_pred = model.predict(X_test)plt.scatter(X_test, Y_test)plt.plot(X_test, Y_pred)plt.show() 51234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556"""To know more or get code samples, please visit my website:https://morvanzhou.github.io/tutorials/Or search: Ëé´ÁÉ¶PythonThank you for supporting!"""# please note, all tutorial code are running under python3.5.# If you use the version like python2.7, please modify the code accordingly# 5 - Classifier exampleimport numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Activationfrom keras.optimizers import RMSprop# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called# X shape (60,000 28x28), y shape (10,000, )(X_train, y_train), (X_test, y_test) = mnist.load_data()# data pre-processingX_train = X_train.reshape(X_train.shape[0], -1) / 255. # normalizeX_test = X_test.reshape(X_test.shape[0], -1) / 255. # normalizey_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10)# Another way to build your neural netmodel = Sequential([ Dense(32, input_dim=784), Activation('relu'), Dense(10), Activation('softmax'),])# Another way to define your optimizerrmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)# We add metrics to get more results you want to seemodel.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy'])print('Training ------------')# Another way to train the modelmodel.fit(X_train, y_train, epochs=2, batch_size=32)print('\nTesting ------------')# Evaluate the model with the metrics we defined earlierloss, accuracy = model.evaluate(X_test, y_test)print('test loss: ', loss)print('test accuracy: ', accuracy) 6 CNNÂç∑ÁßØÁ•ûÁªèÁΩëÁªú‰∏çÊòØÂØπ https://www.cnblogs.com/skyfsm/p/6790245.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990"""To know more or get code samples, please visit my website:https://morvanzhou.github.io/tutorials/Or search: Ëé´ÁÉ¶PythonThank you for supporting!"""# please note, all tutorial code are running under python3.5.# If you use the version like python2.7, please modify the code accordingly# 6 - CNN example# to try tensorflow, un-comment following two lines# import os# os.environ['KERAS_BACKEND']='tensorflow'import numpy as npnp.random.seed(1337) # for reproducibilityfrom keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.models import Sequentialfrom keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flattenfrom keras.optimizers import Adam# download the mnist to the path '~/.keras/datasets/' if it is the first time to be called# training X shape (60000, 28x28), Y shape (60000, ). test X shape (10000, 28x28), Y shape (10000, )(X_train, y_train), (X_test, y_test) = mnist.load_data()# data pre-processingX_train = X_train.reshape(-1, 1,28, 28)/255.X_test = X_test.reshape(-1, 1,28, 28)/255.y_train = np_utils.to_categorical(y_train, num_classes=10)y_test = np_utils.to_categorical(y_test, num_classes=10)# Another way to build your CNNmodel = Sequential()# Conv layer 1 output shape (32, 28, 28)model.add(Convolution2D( batch_input_shape=(None, 1, 28, 28), filters=32, kernel_size=5, strides=1, padding='same', # Padding method data_format='channels_first',))model.add(Activation('relu'))# Pooling layer 1 (max pooling) output shape (32, 14, 14)model.add(MaxPooling2D( pool_size=2, strides=2, padding='same', # Padding method data_format='channels_first',))# Conv layer 2 output shape (64, 14, 14)model.add(Convolution2D(64, 5, strides=1, padding='same', data_format='channels_first'))model.add(Activation('relu'))# Pooling layer 2 (max pooling) output shape (64, 7, 7)model.add(MaxPooling2D(2, 2, 'same', data_format='channels_first'))# Fully connected layer 1 input shape (64 * 7 * 7) = (3136), output shape (1024)model.add(Flatten())model.add(Dense(1024))model.add(Activation('relu'))# Fully connected layer 2 to shape (10) for 10 classesmodel.add(Dense(10))model.add(Activation('softmax'))# Another way to define your optimizeradam = Adam(lr=1e-4)# We add metrics to get more results you want to seemodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])print('Training ------------')# Another way to train the modelmodel.fit(X_train, y_train, epochs=1, batch_size=64,)print('\nTesting ------------')# Evaluate the model with the metrics we defined earlierloss, accuracy = model.evaluate(X_test, y_test)print('\ntest loss: ', loss)print('\ntest accuracy: ', accuracy)]]></content>
      <categories>
        <category>ÁºñÁ®ãËØ≠Ë®Ä</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>tensorlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Neural Network and Deep Learning]]></title>
    <url>%2F2019%2F04%2F11%2FDeep%20Learning.ai_Neural%20Networks%20and%20Deep%20Learning%2F</url>
    <content type="text"><![CDATA[Course one : Neural Networks and Deep Learning(Course 1 of the Deep Learning Specialization)C1W1C1W1L01: WelcomeAI is the new Electricity! Course 1: Neural Networks and Deep Learning Course 2: Improving Deep Neural Networks: Hyperparameter tuning,Regularization and Optimization Course 3: Structuring your Machine Learning project Course 4: Convolutional Neural Networks Course 5: Natural Langurge Processing: Building sequence models C1W1L02 : What is Neural NetworkDeep Learning = training (very large) neural network For example of house prize prediction : the simplest neural networkÂ¶ÇÊûúÁé∞Âú®ÊúâÂÖ≠Ê†ãÊàøÂ≠êÁöÑ‰ø°ÊÅØÔºåÂàÜÂà´ÊòØÊàøÂ≠êÁöÑÂ§ßÂ∞è(size of house)ÂíåÂØπÂ∫îÁöÑ‰ª∑Ê†º(prize),ÁªòÂà∂Âá∫Â¶Ç‰∏ãÁöÑ„ÄÇËá™ÁÑ∂ÁöÑÊÉ≥Ê≥ïÔºöÁ∫øÊÄßÂõûÂΩíÔºåÂæóÂà∞ÊãüÂêàÁöÑÁõ¥Á∫ø„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàø‰ª∑‰∏çÂèØËÉΩÊòØË¥üÊï∞ÂêßÔºÅÂõ†Ê≠§‰∏ãÂõæ‰∏≠ËìùËâ≤ÁöÑÁ∫øÔºåÂ§ßËá¥Â∞±ÊòØÊàë‰ª¨ÊâÄÈúÄË¶ÅÁöÑÂáΩÊï∞„ÄÇËøô‰∏™ÂØπÂ∫î‰∏Ä‰∏™ÊúÄÁÆÄÂçïÁ•ûÁªèÁΩëÁªúÔºàneural networkÔºâ ‰∏äËø∞ÊòØ‰∏Ä‰∏™tiny little neural networkÔºåÊõ¥Â§ßÁöÑÔºåÊõ¥Â§çÊùÇÁöÑÁ•ûÁªèÁΩëÁªúÊòØ ÊääÂæàÂ§öÊúÄÁÆÄÂçïÁöÑsingle neuralÂ†ÜÁßØ(stacking)Âà∞‰∏ÄËµ∑„ÄÇ For example of house prize prediction : stacking the neural‰∏äÈù¢Ëøô‰∏™‰æãÂ≠êÔºå‰ªÖ‰ªÖËÄÉËôëÁâπÂæÅÊòØsize,ÂÆûÈôÖÊÉÖÂÜµ‰∏äÔºå‰∏éÊàøÂ±ãÁõ∏ÂÖ≥ÁöÑÁâπÂæÅËøòÊúânumber of bedrooms„ÄÅzip code„ÄÅwealth, number of bedrooms and size affect family size. The zip code is a feature that tells you you know walkability. The wealth tells you how good is the school quality hidden layer Áî®ËæìÂÖ•Â±ÇËÆ°ÁÆóÂæóÂà∞ÔºåÂõ†Ê≠§ËØ¥ËæìÂÖ•Â±Ç‰∏é‰∏≠Èó¥Â±ÇÁ¥ßÂØÜËøûÊé•Ëµ∑Êù•‰∫Ü The actual application of neural networkshidden layer ‰∏é‰∏ä‰∏ÄÂ±ÇÁöÑËøûÊé•ÊÉÖÂÜµÂπ∂‰∏çÊòØÊâãÂ∑•Á°ÆÂÆöÔºåÊØè‰∏ÄÂ±ÇÈÉΩÊòØ‰∏ä‰∏ÄÂ±ÇÊâÄÊúâÁöÑËæìÂÖ•ÂáΩÊï∞ÔºåÊâÄ‰ª•Âª∫Á´ãÁöÑÁ•ûÁªèÁΩëÁªúÂ¶Ç‰∏ãÔºö The remarkable thing about neural network Given enough data about X&amp;Y (x,y) which good at freaking out functions :map x to y Most powerful in supervised learning C2W1CL03 : Supervised Learning with Neural NetworkÂ∏∏ËßÅÁöÑÁõëÁù£Â≠¶‰π†Êà™Ê≠¢Âà∞ÁõÆÂâçÔºåNeural NetworkÁöÑÊàêÂäüÂ∫îÁî®Âü∫Êú¨ÈÉΩÂú®Supervised Learning„ÄÇÊØîÂ¶ÇÔºöAdÔºåImages vision, Audio to Text, Machine translation, Autonomous Driving Â∏∏ËßÅÁöÑÁ•ûÁªèÁΩëÁªúÁöÑËÆæËÆ° Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºöConvolutional Neural Network (CNN) ÈÄöÂ∏∏ÊúâÁî®ÂõæÂÉèÊï∞ÊçÆ ÈÄíÂΩíÁ•ûÁªèÁΩëÁªúÔºö Recurrent Neural Network (RNN) ÈÄöÂ∏∏Áî®‰∫étime series ÂØπÂ∫îÂ§çÊùÇÁöÑÂ∫îÁî®‰∏≠ÔºåÂÆöÂà∂‰∏Ä‰∫õÂ§çÊùÇÁöÑÊ∑∑ÂêàÁöÑÁ•ûÁªèÁΩëÁªúÁªìÊûÑ ÁªìÊûÑÂåñÂíåÈùûÁªìÊûÑÂåñÊï∞ÊçÆ Â§ÑÁêÜÈùûÁªìÊûÑÂåñÊï∞ÊçÆÊòØÂæàÈöæÁöÑÔºå‰∏éÁªìÊûÑÂåñÊï∞ÊçÆÊØîËæÉÔºåËÆ©ËÆ°ÁÆóÊú∫ÁêÜËß£ÈùûÁªìÊûÑÂåñÊï∞ÊçÆÂæàÈöæ C1W1L04: Why is deep learning taking offAnswer: scale If you want to hit this very high level of performance ,firstly, you need to be able train a big enough neural network in order to take advantage of the huge amount of data and second you need to be out here on the x axes you do need a lot of data. If you do not have a lot training data is often up to your skill at hand engineering features that determines the foreman.Âú®Ëøô‰∏™Â∞èÁöÑËÆ≠ÁªÉÈõÜ‰∏≠ÔºåÂêÑÁßçÁÆóÊ≥ïÁöÑ‰ºòÂÖàÁ∫ß‰∫ãÂÆû‰∏äÂÆö‰πâÁöÑ‰πü‰∏çÊòØÂæàÊòéÁ°ÆÔºåÊâÄ‰ª•Â¶ÇÊûú‰Ω†Ê≤°ÊúâÂ§ßÈáèÁöÑËÆ≠ÁªÉÈõÜÔºåÈÇ£ÊïàÊûú‰ºöÂèñÂÜ≥‰∫é‰Ω†ÁöÑÁâπÂæÅÂ∑•Á®ãËÉΩÂäõÔºåÈÇ£Â∞ÜÂÜ≥ÂÆöÊúÄÁªàÁöÑÊÄßËÉΩ„ÄÇ Ëøô‰∏™ÂõæÂΩ¢Âå∫ÂüüÁöÑÂ∑¶ËæπÔºåÂêÑÁßçÁÆóÊ≥ï‰πãÈó¥ÁöÑ‰ºòÂÖàÁ∫ßÂπ∂‰∏çÊòØÂÆö‰πâÁöÑÂæàÊòéÁ°ÆÔºåÊúÄÁªàÁöÑÊÄßËÉΩÊõ¥Â§öÁöÑÊòØÂèñÂÜ≥‰∫é‰Ω†Âú®Áî®Â∑•Á®ãÈÄâÊã©ÁâπÂæÅÊñπÈù¢ÁöÑËÉΩÂäõ‰ª•ÂèäÁÆóÊ≥ïÂ§ÑÁêÜÊñπÈù¢ÁöÑ‰∏Ä‰∫õÁªÜËäÇ. Âè™ÊòØÂú®Êüê‰∫õÂ§ßÊï∞ÊçÆËßÑÊ®°ÈùûÂ∏∏Â∫ûÂ§ßÁöÑËÆ≠ÁªÉÈõÜÔºå‰πüÂ∞±ÊòØÂú®Âè≥ËæπËøô‰∏™‰ºöÈùûÂ∏∏ÁöÑÂ§ßÊó∂ÔºåÊàë‰ª¨ËÉΩÊõ¥Âä†ÊåÅÁª≠Âú∞ÁúãÂà∞Êõ¥Â§ßÁöÑÁî±Á•ûÁªèÁΩëÁªúÊéßÂà∂ÂÖ∂ÂÆÉÊñπÊ≥ï. The Reason the scale of data the speed of computation such as GPUS innovation of algorithm ËÆ∏Â§öÁÆóÊ≥ïÊñπÈù¢ÁöÑÂàõÊñ∞Ôºå‰∏ÄÁõ¥ÊòØÂú®Â∞ùËØïÁùÄ‰ΩøÂæóÁ•ûÁªèÁΩëÁªúËøêË°åÁöÑÊõ¥Âø´ switch sigmoid function to relu function Âú®Ëøô‰∏™Âå∫ÂüüÔºå‰πüÂ∞±ÊòØËøô‰∏™sigmoidÂáΩÊï∞ÁöÑÊ¢ØÂ∫¶‰ºöÊé•ËøëÈõ∂ÔºåÊâÄ‰ª•Â≠¶‰π†ÁöÑÈÄüÂ∫¶‰ºöÂèòÂæóÈùûÂ∏∏ÁºìÊÖ¢ÔºåÂõ†‰∏∫ÂΩì‰Ω†ÂÆûÁé∞Ê¢ØÂ∫¶‰∏ãÈôç‰ª•ÂèäÊ¢ØÂ∫¶Êé•ËøëÈõ∂ÁöÑÊó∂ÂÄôÔºåÂèÇÊï∞‰ºöÊõ¥Êñ∞ÁöÑÂæàÊÖ¢ÔºåÊâÄ‰ª•Â≠¶‰π†ÁöÑÈÄüÁéá‰πü‰ºöÂèòÁöÑÂæàÊÖ¢ÔºåËÄåÈÄöËøáÊîπÂèòËøô‰∏™Ë¢´Âè´ÂÅöÊøÄÊ¥ªÂáΩÊï∞ÁöÑ‰∏úË•øÔºåÁ•ûÁªèÁΩëÁªúÊç¢Áî®Ëøô‰∏Ä‰∏™ÂáΩÊï∞ÔºåÂè´ÂÅöReLUÁöÑÂáΩÊï∞Ôºà‰øÆÊ≠£Á∫øÊÄßÂçïÂÖÉÔºâÔºåReLUÂÆÉÁöÑÊ¢ØÂ∫¶ÂØπ‰∫éÊâÄÊúâËæìÂÖ•ÁöÑË¥üÂÄºÈÉΩÊòØÈõ∂ÔºåÂõ†Ê≠§Ê¢ØÂ∫¶Êõ¥Âä†‰∏ç‰ºöË∂ãÂêëÈÄêÊ∏êÂáèÂ∞ëÂà∞Èõ∂„ÄÇ ËÆ≠ÁªÉ‰Ω†ÁöÑÁ•ûÁªèÁΩëÁªúÁöÑËøáÁ®ãÔºåÂæàÂ§öÊó∂ÂÄôÊòØÂá≠ÂÄüÁõ¥ËßâÁöÑÔºåÂæÄÂæÄ‰Ω†ÂØπÁ•ûÁªèÁΩëÁªúÊû∂ÊûÑÊúâ‰∫Ü‰∏Ä‰∏™ÊÉ≥Ê≥ïÔºå‰∫éÊòØ‰Ω†Â∞ùËØïÂÜô‰ª£Á†ÅÂÆûÁé∞‰Ω†ÁöÑÊÉ≥Ê≥ïÔºåÁÑ∂ÂêéËÆ©‰Ω†ËøêË°å‰∏Ä‰∏™ËØïÈ™åÁéØÂ¢ÉÊù•ÂëäËØâ‰Ω†Ôºå‰Ω†ÁöÑÁ•ûÁªèÁΩëÁªúÊïàÊûúÊúâÂ§öÂ•ΩÔºåÈÄöËøáÂèÇËÄÉËøô‰∏™ÁªìÊûúÂÜçËøîÂõûÂéª‰øÆÊîπ‰Ω†ÁöÑÁ•ûÁªèÁΩëÁªúÈáåÈù¢ÁöÑ‰∏Ä‰∫õÁªÜËäÇÔºåÁÑ∂Âêé‰Ω†‰∏çÊñ≠ÁöÑÈáçÂ§ç‰∏äÈù¢ÁöÑÊìç‰ΩúÔºåÂΩì‰Ω†ÁöÑÁ•ûÁªèÁΩëÁªúÈúÄË¶ÅÂæàÈïøÊó∂Èó¥ÂéªËÆ≠ÁªÉÔºåÈúÄË¶ÅÂæàÈïøÊó∂Èó¥ÈáçÂ§çËøô‰∏ÄÂæ™ÁéØÔºåÂú®ËøôÈáåÂ∞±ÊúâÂæàÂ§ßÁöÑÂå∫Âà´ÔºåÊ†πÊçÆ‰Ω†ÁöÑÁîü‰∫ßÊïàÁéáÂéªÊûÑÂª∫Êõ¥È´òÊïàÁöÑÁ•ûÁªèÁΩëÁªú„ÄÇÂΩì‰Ω†ËÉΩÂ§üÊúâ‰∏Ä‰∏™ÊÉ≥Ê≥ïÔºåËØï‰∏ÄËØïÔºåÁúãÊïàÊûúÂ¶Ç‰Ωï„ÄÇÂú®10ÂàÜÈíüÂÜÖÔºåÊàñËÄÖ‰πüËÆ∏Ë¶ÅËä±‰∏ä‰∏ÄÊï¥Â§©ÔºåÂ¶ÇÊûú‰Ω†ËÆ≠ÁªÉ‰Ω†ÁöÑÁ•ûÁªèÁΩëÁªúÁî®‰∫Ü‰∏Ä‰∏™ÊúàÁöÑÊó∂Èó¥ÔºåÊúâÊó∂ÂÄôÂèëÁîüËøôÊ†∑ÁöÑ‰∫ãÊÉÖÔºå‰πüÊòØÂÄºÂæóÁöÑÔºåÂõ†‰∏∫‰Ω†ÂæàÂø´ÂæóÂà∞‰∫Ü‰∏Ä‰∏™ÁªìÊûú„ÄÇÂú®10ÂàÜÈíüÂÜÖÊàñËÄÖ‰∏ÄÂ§©ÂÜÖÔºå‰Ω†Â∫îËØ•Â∞ùËØïÊõ¥Â§öÁöÑÊÉ≥Ê≥ïÔºåÈÇ£ÊûÅÊúâÂèØËÉΩ‰ΩøÂæó‰Ω†ÁöÑÁ•ûÁªèÁΩëÁªúÂú®‰Ω†ÁöÑÂ∫îÁî®ÊñπÈù¢Â∑•‰ΩúÁöÑÊõ¥Â•Ω„ÄÅÊõ¥Âø´ÁöÑËÆ°ÁÆóÔºåÂú®ÊèêÈ´òÈÄüÂ∫¶ÊñπÈù¢ÁúüÁöÑÊúâÂ∏ÆÂä©ÔºåÈÇ£Ê†∑‰Ω†Â∞±ËÉΩÊõ¥Âø´Âú∞ÂæóÂà∞‰Ω†ÁöÑÂÆûÈ™åÁªìÊûú„ÄÇ SummaryÊó©‰∏äËä±‰∫Ü2hÂ∞èÊó∂Â≠¶‰π†Á¨¨‰∏ÄÂë®ÁöÑËßÜÈ¢ëÔºåÂÖàÁúã‰∏ÄÈÅçËßÜÈ¢ëÁöÑÂ≠óÂπïÔºåÈÄêÂ≠óÈÄêÂè•ÁöÑÁêÜËß£ÔºåËôΩÁÑ∂ÂæàÂ§öÊó∂ÂÄôÈÉΩÊòØËá™Â∑±‰π±ÁåúÁöÑÔºåÂ§ßÊ¶ÇÊ∏ÖÊ•öËÆ≤ÁöÑ‰ªÄ‰πàÔºÅÁÑ∂ÂêéÂÜçÁúãÂ§ßÁâõÁöÑÁ¨îËÆ∞ÔºåÁÑ∂ÂêéÂÜçÁúã‰∏ÄÁØáÁªìÂêàPPT„ÄÇ‰∏ãÂçà‰πüÁúã‰∫ÜÂçä‰∏™Â§öÂ∞èÊó∂„ÄÇÈóÆÈ¢òÔºö1. Ëá™Â∑±ÁöÑËã±ÊñáÊ∞¥Âπ≥‰∏çÂ§üÔºåËøô‰∏™ÈúÄË¶ÅÂ§ßÂ§ßÁöÑÊèêÈ´òËÆ∑„ÄÇ2. ÂÖ∂ÂÆûÂè™Ë¶ÅÁúãÂà´‰∫∫ÁöÑÁ¨îËÆ∞Â∞±ÂèØ‰ª•Áü•ÈÅìÂÜÖÂÆπÔºå‰ΩÜÊòØËøòÊòØÊÉ≥Âê¨andow ngÁöÑËÆ≤Ëß£„ÄÇ3. ËßÜÈ¢ëÈÉΩÊØîËæÉÁü≠ÔºåÊØè‰∏™ËßÜÈ¢ëËÆæËÆ°ÁöÑÁü•ËØÜÁÇπÊàñËÄÖÂÜÖÂÆπ‰∏çÂ§öÔºå1Âà∞3‰∏™ÔºåÂàÜÊàêÁü•ËØÜÁÇπÂÅöÁ¨îËÆ∞ËøòÊòØ‰∏çÈîôÁöÑ Ëøô‰∏ÄÂë®ÁöÑÂÜÖÂÆπÔºå‰πüÂ∞±ÊòØ‰ªäÂ§©ÊàëÂ≠¶‰π†ÁöÑÁü•ËØÜÁÆÄÂçïÂíåÂÆπÊòìÁêÜËß£„ÄÇÂ≠¶‰π†‰∫ÜÁ•ûÁªèÁΩëÁªúÁöÑÂ§ßËá¥ÁªìÊûÑÔºåÁ•ûÁªèÁΩëÁªúÁöÑÂ∫îÁî®È¢ÜÂüüÔºåÊ∑±Â∫¶Â≠¶‰π†‰∏∫‰ªÄ‰πàÂèñÂæóÂø´ÈÄüÁöÑÂèëÂ±ïÁöÑ‰∏âÁÇπÂéüÂõ†ÔºåÂ∞§ÂÖ∂ÊòØÊï∞ÊçÆscale‰∏éÂÖ∂‰ªñÊñπÊ≥ïÂíåÁ•ûÁªèÁΩëÁªúËßÑÊ®°ÁöÑÂ§ßËá¥ÊÄßËÉΩÂÖ≥Á≥ª C1W2C1W2L01: Binary ClassificationIn this week, we‚Äôre going to go over the basics of neural network programming. We are going to study handle data without for loop. forward password for propagation backward pass or what‚Äôs called a backward propagation step Why the computations in learning in a neural network can be organized in this board propagation and a separate backward propagation by using logistic regression to convey(‰º†Ëææ) theses ideas. Binary ClassificationInputÔºõ an image . three separate matrices corresponding red green and blue color channels of this image. Â¶ÇÊûú‰Ω†ÁöÑÂõæÁâáÂ§ßÂ∞è‰∏∫64x64ÂÉèÁ¥†ÔºåÈÇ£‰πà‰Ω†Â∞±Êúâ‰∏â‰∏™ËßÑÊ®°‰∏∫64x64ÁöÑÁü©ÈòµÔºåÂàÜÂà´ÂØπÂ∫îÂõæÁâá‰∏≠Á∫¢„ÄÅÁªø„ÄÅËìù‰∏âÁßçÂÉèÁ¥†ÁöÑÂº∫Â∫¶ÂÄº unroll all of these pixel intensity values into a feature vector pixel intensity values of this image notation (x,y)Ôºö a pair X comma Y $M_{train}$: M subscript train ÊØèÊù°ÊµãËØïÈõÜÂú®Áü©Èòµ‰∏≠ÈÉΩÊòØ‰ª•ÂàóÂêëÈáèÁöÑÂΩ¢ÂºèÂ≠òÂú® Matrix capital Model : hypothesis Function :Logistic RegressionSo given an input X and the parameters W and b, how do we generate the output Y hat? Well, one thing you could try, that doesn‚Äôt work, would be to have Y hat be w transpose X plus B, kind of a linear function of the input X. And in fact, this is what you use if you were doing linear regression. But this isn‚Äôt a very good algorithm for binary classification because you want Y hat to be the chance that Y is equal to one. So,Y hat should really be between zero and one. This is what the sigmoid function looks like. sigmoid function \sigma(z) = \frac{1}{1+e^{-z}}Âõ†‰∏∫‰Ω†ÊÉ≥ËÆ©$\hat{y}$Ë°®Á§∫ÂÆûÈôÖÂÄº$y$Á≠â‰∫é1ÁöÑÊú∫ÁéáÁöÑËØùÔºå Â∫îËØ•Âú®0Âà∞1‰πãÈó¥„ÄÇËøôÊòØ‰∏Ä‰∏™ÈúÄË¶ÅËß£ÂÜ≥ÁöÑÈóÆÈ¢òÔºåÂõ†‰∏∫ÂèØËÉΩÊØî1Ë¶ÅÂ§ßÂæóÂ§öÔºåÊàñËÄÖÁîöËá≥‰∏∫‰∏Ä‰∏™Ë¥üÂÄº„ÄÇÂØπ‰∫é‰Ω†ÊÉ≥Ë¶ÅÁöÑÂú®0Âíå1‰πãÈó¥ÁöÑÊ¶ÇÁéáÊù•ËØ¥ÂÆÉÊòØÊ≤°ÊúâÊÑè‰πâÁöÑÔºåÂõ†Ê≠§Âú®ÈÄªËæëÂõûÂΩí‰∏≠ÔºåÊàë‰ª¨ÁöÑËæìÂá∫Â∫îËØ•ÊòØÁ≠â‰∫éÁî±‰∏äÈù¢ÂæóÂà∞ÁöÑÁ∫øÊÄßÂáΩÊï∞ÂºèÂ≠ê‰Ωú‰∏∫Ëá™ÂèòÈáèÁöÑsigmoidÂáΩÊï∞‰∏≠ÔºåÂÖ¨ÂºèÂ¶Ç‰∏äÂõæÊúÄ‰∏ãÈù¢ÊâÄÁ§∫ÔºåÂ∞ÜÁ∫øÊÄßÂáΩÊï∞ËΩ¨Êç¢‰∏∫ÈùûÁ∫øÊÄßÂáΩÊï∞„ÄÇ Ê≥®ÊÑèÔºöÂéüÊù•$w,b$ÊòØÂàÜÂºÄÂú®ÔºåËøôÈáåÂ∞±ÂêàÂπ∂ÔºåÂºïÂÖ•ÂèòÈáè$x_0=1$,ÂØπÂ∫îÂÅèÁΩÆ$b$, StrategyÔºöCost functionFirstly : Loss function L(\hat{y},y)=\frac{1}{2}\sum{(y_i-\hat{y_i})^2}Ëøô‰∏™‰ºòÂåñÈóÆÈ¢ò‰∏çÊòØÂá∏‰ºòÂåñÈóÆÈ¢ò(non-convex)ÔºåÂõ†Ê≠§‰∏çÈÄâÁî®Ëøô‰∏™ SecondlyÔºå L(y,\hat{y})=-(ylog^{\hat{y}}+(1-y)log^{1-\hat{y}}) Algorithm: Gradient Descent Gradient DescentÁÆóÊ≥ïÊ≠•È™§Ôºö Initialize $w$, $b$ to zero repeatÔºö $w :=w‚àí\alpha \frac{‚àÇJ(w,b)}{‚àÇw}‚Äã$ $b :=b-\alpha \frac{‚àÇJ(w,b)}{‚àÇb}$ C1W2L05 &amp; C1W2L06 DerivativesÊ±ÇÂØºÔºåËøô‰∏™ÊòØÂæÆÁßØÂàÜÁöÑÂÜÖÂÆπÔºå‰∏çÁî®ÂÜô‰∫ÜÔºÅ C1W2L07Ôºö Computation GraphC1W2L08 : Derivatives with compution graphsÈìæÂºèÊ≥ïÂàô \frac{\partial L}{\partial v}=\frac{\partial L}{\partial u}\frac{\partial u}{\partial v}C1W2L09 : Logistic Regression Gradient Descentsingle training exampleYou‚Äôve seen the loss function that measures how well you‚Äôre doing on the single training example. You‚Äôve also seen the cost function that measures how well your parameters w and b are doing on your entire training set. You‚Äôve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives. \frac{\partial L}{\partial w}=\frac{\partial L}{\partial \alpha }\frac{\partial \alpha }{\partial z}\frac{\partial z}{\partial w} \\=-(\frac{y}{a}+\frac{1-y}{1-a})a(1-a)x=(a-y)xC1W2L10 Gradient Descent on m example \min L(w,b)=\sum_{i=1}^{m}L(\alpha_i,y_i)/m\\ \frac{\partial L }{\partial w}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial w})/m=(\sum_{i=1}^{m}(a-y_i)x_i)/m\\ \frac{\partial L }{\partial b}=(\sum_{i=1}^{m}\frac{\partial L(a_i,y_i)}{\partial b})/m=(\sum_{i=1}^{m}(a-1)x_i)/m ‰∏äÈù¢ÁöÑ‰º™‰ª£Á†ÅÂëäËØâÊàë‰ª¨ÔºåÈúÄË¶ÅÂ§öÊ¨°for loopÂÆåÊàê‰ª£Á†ÅÔºå‰ΩÜÊòØËøô‰ºöÈÄ†ÊàêËøêÁÆóÈÄüÂ∫¶‰∏ãÈôçÔºÅÂõ†‰∏∫Êàë‰ª¨Ë∂äÊù•Ë∂äÂ§öÂú∞ËÆ≠ÁªÉÈùûÂ∏∏Â§ßÁöÑÊï∞ÊçÆÈõÜÔºåÂõ†Ê≠§‰Ω†ÁúüÁöÑÈúÄË¶Å‰Ω†ÁöÑ‰ª£Á†ÅÂèòÂæóÈùûÂ∏∏È´òÊïà„ÄÇÊâÄ‰ª•Âú®Êé•‰∏ãÊù•ÁöÑÂá†‰∏™ËßÜÈ¢ë‰∏≠ÔºåÊàë‰ª¨‰ºöË∞àÂà∞ÂêëÈáèÂåñÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂ∫îÁî®ÂêëÈáèÂåñËÄåËøû‰∏Ä‰∏™forÂæ™ÁéØÈÉΩ‰∏ç‰ΩøÁî®„ÄÇÊâÄ‰ª•Â≠¶‰π†‰∫ÜËøô‰∫õÔºåÊàëÂ∏åÊúõ‰Ω†ÊúâÂÖ≥‰∫éÂ¶Ç‰ΩïÂ∫îÁî®ÈÄªËæëÂõûÂΩíÔºåÊàñÊòØÁî®‰∫éÈÄªËæëÂõûÂΩíÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÔºå‰∫ãÊÉÖ‰ºöÂèòÂæóÊõ¥Âä†Ê∏ÖÊô∞ summary‰ªäÂ§©‰∏ªË¶ÅÂ≠¶‰π†‰∫Ü‰ª•logistics regression ‰∏∫‰æãÔºåÂ¶Ç‰ΩïÈÄöËøáÈìæÂºèÊ±ÇÂØºÁöÑËøáÁ®ãÔºåÁÆÄÂçïÁöÑÁªÉ‰π†‰∏Ä‰∏ãÔºå‰ª•ÂèäÂÜçÊ¨°‰∫ÜËß£‰ªÄ‰πàÊòØÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºå‰ª•ÂèäËÆ≠ÁªÉÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÈúÄË¶Å‰∏Ä‰∏™ÊçüÂ§±ÂáΩÊï∞ÔºåËÆ≠ÁªÉÁöÑËøáÁ®ãÂ∞±ÊòØÊ±ÇÊçüÂ§±ÂáΩÊï∞ÊúÄ‰ºòÂÄºÁöÑËøáÁ®ã C1W2L11: Vectorization1. ‰ªÄ‰πàÊòØVectorizationÔºöÂ∞Ü for loop Â∞ΩÂèØËÉΩËΩ¨Êç¢‰∏∫Áü©ÈòµËøêÁÆóÈÄöËøánumpyÂÜÖÁΩÆÂáΩÊï∞ÂíåÈÅøÂºÄÊòæÂºèÁöÑÂæ™ÁéØ(loop)ÁöÑÊñπÂºèËøõË°åÂêëÈáèÂåñÔºå‰ªéËÄåÊúâÊïàÊèêÈ´ò‰ª£Á†ÅÈÄüÂ∫¶„ÄÇ 123np.dot(a,b)Â¶ÇÊûúa,bÊòØ‰∏ÄÁª¥Êï∞ÁªÑÔºåÂàôËÆ°ÁÆóÁÇπÁßØÂ¶ÇÊûúa,bÊòØÂ§öÁª¥Êï∞ÊçÆÔºåÂàôÁü©Èòµ‰πòÊ≥ï 2. An example of vectorizationvectorizationÁöÑÂ•ΩÂ§ÑÔºöconciser code, but faster execution ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂØπÊØîÂÆûÈ™åÔºö1,000,000Â§ßÂ∞èÁöÑ‰∏§‰∏™ÂêëÈáèÂÜÖÁßØËÆ°ÁÆóÔºåfor loopË¶ÅÊØîVectorizationÂø´300ÂÄç„ÄÇ Âú®Deep LearningÊó∂‰ª£ÔºåvectorizationÊòØ‰∏ÄÈ°πÈáçË¶ÅÁöÑÊäÄËÉΩ„ÄÇ 123456789101112131415161718192021import numpy as np #ÂØºÂÖ•numpyÂ∫ìa = np.array([1,2,3,4]) #ÂàõÂª∫‰∏Ä‰∏™Êï∞ÊçÆaprint(a)# [1 2 3 4]import time #ÂØºÂÖ•Êó∂Èó¥Â∫ìa = np.random.rand(1000000)b = np.random.rand(1000000) #ÈÄöËøároundÈöèÊú∫ÂæóÂà∞‰∏§‰∏™‰∏ÄÁôæ‰∏áÁª¥Â∫¶ÁöÑÊï∞ÁªÑtic = time.time() #Áé∞Âú®ÊµãÈáè‰∏Ä‰∏ãÂΩìÂâçÊó∂Èó¥#ÂêëÈáèÂåñÁöÑÁâàÊú¨c = np.dot(a,b)toc = time.time()print(‚ÄúVectorized version:‚Äù + str(1000*(toc-tic)) +‚Äùms‚Äù) #ÊâìÂç∞‰∏Ä‰∏ãÂêëÈáèÂåñÁöÑÁâàÊú¨ÁöÑÊó∂Èó¥‚Äã#ÁªßÁª≠Â¢ûÂä†ÈùûÂêëÈáèÂåñÁöÑÁâàÊú¨c = 0tic = time.time()for i in range(1000000): c += a[i]*b[i]toc = time.time()print(c)print(‚ÄúFor loop:‚Äù + str(1000*(toc-tic)) + ‚Äúms‚Äù)#ÊâìÂç∞forÂæ™ÁéØÁöÑÁâàÊú¨ÁöÑÊó∂Èó¥ 3. GPU or CPU Â§ßËßÑÊ®°ÁöÑÊ∑±Â∫¶Â≠¶‰π†ÂÜçGPUÊàñËÄÖÂõæÂÉèÂ§ÑÁêÜÂçïÂÖÉËøêË°å‚ÄùÔºåCPUÂíåGPUÈÉΩÊúâÂπ∂Ë°åÂåñÁöÑÊåá‰ª§Ôºå‰ªñ‰ª¨ÊúâÊó∂ÂÄô‰ºöÂè´ÂÅöSIMDÊåá‰ª§ÔºåËøô‰∏™‰ª£Ë°®‰∫Ü‰∏Ä‰∏™ÂçïÁã¨Êåá‰ª§Â§öÁª¥Êï∞ÊçÆÔºåËøô‰∏™ÁöÑÂü∫Á°ÄÊÑè‰πâÊòØÔºåÂ¶ÇÊûú‰Ω†‰ΩøÁî®‰∫Übuilt-inÂáΩÊï∞,ÂÉènp.functionÊàñËÄÖÂπ∂‰∏çË¶ÅÊ±Ç‰Ω†ÂÆûÁé∞Âæ™ÁéØÁöÑÂáΩÊï∞ÔºåÂÆÉÂèØ‰ª•ËÆ©pythonÁöÑÂÖÖÂàÜÂà©Áî®Âπ∂Ë°åÂåñËÆ°ÁÆó„ÄÇ Âè™ÊòØÂú®GPUÂíåCPU‰∏äÈù¢ËÆ°ÁÆóÔºåGPUÊõ¥Âä†ÊìÖÈïøSIMDËÆ°ÁÆóÔºå‰ΩÜÊòØCPU‰∫ãÂÆû‰∏ä‰πü‰∏çÊòØÂ§™Â∑ÆÔºåÂèØËÉΩÊ≤°ÊúâGPUÈÇ£‰πàÊìÖÈïøÂêß„ÄÇSIMD Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data) C12L12 Ôºö More Vectorization ExampleÁü©ÈòµÂíåÂêëÈáè‰πòÊ≥ï ÂêëÈáèÂáΩÊï∞ ÂéüÂàôÔºöwhenever possible, avoid explict for-loops ‰ΩøÁî®Element wisedÁöÑÁü©ÈòµËøêÁÆóÔºåÂ∞ÜÂáΩÊï∞‰ΩúÁî®Âú®ÊØè‰∏™Áü©ÈòµÂÖÉÁ¥†‰∏äÔºåÊØîÂ¶ÇÔºö np.exp() np.log() np.abs() np.maxium() 1/v v**2 C1W2L13: Vectorizing Logistic Regression1. ÂâçÂêë‰º†Êí≠ \hat{y}=œÉ(w^TX+b)=(a(1),a(2),...,a(m‚àí1),a(m))=\\ (\alpha(z_1),\alpha(z_m),...,\alpha(z_m))=\\ (\alpha(w^Tx_1+b),\alpha(w^Tx_2+b),...,\alpha(w^Tx_m+b))=1234import numpy as npz=np.dot(W^T,X)+b# zËøôÈáåÂ∞±ÊòØpython Â∑ßÂ¶ôÁöÑÂú∞ÊñπÔºåbÊòØÂÆûÊï∞Ôºå‰ΩÜÊòØÂêëÈáèÂä†‰∏äÂÆûÊï∞ÂêéÔºåbÊâ©Â±ïÊàêÂêëÈáèÔºåË¢´Áß∞‰∏∫ÂπøÊí≠ÔºàbrosdcastingÔºâ ‰∏™‰∫∫ÁªèÈ™åÔºö È¶ñÂÖàÔºåÁÜüÊÇâÊØè‰∏™ÂèòÈáèÁöÑËÆ∞Âè∑ÂíåÁª¥Â∫¶ÔºåÂøÖË¶ÅÁöÑËØùÔºåÂèØ‰ª•ÁîªÂá∫Êù•ÔºåÊõ¥Áõ¥ËßÇ„ÄÇ ÂÖà‰ªé‰∏Ä‰∏™Ê†∑Êú¨ÂÅöÂêëÈáèÂåñÔºåÂÜçÊääm‰∏™Ê†∑Êú¨ÁöÑÊìç‰ΩúÂêëÈáèÂåñ„ÄÇ for-loopÈáåÈù¢ÊòØÂæ™ÁéØ‰πòÊ≥ïÔºåÂàôÂêëÈáèÂåñ‰∏ÄÂÆöÊòØ‰∏Ä‰∏™‰πòÊ≥ïÂΩ¢ÂºèÔºåËã•ÂØπ‰∫é‰∏çÁ°ÆÂÆö‰πòÊ≥ïÁöÑÂ∑¶Âè≥ÂÖ≥Á≥ªÔºåÊòØÂê¶ÈúÄËΩ¨ÁΩÆÔºåÂèØ‰ª•Ê†πÊçÆÁõÆÊ†áÂèòÈáèÁöÑÁª¥Â∫¶Êé®Êµã„ÄÇÊàñËÄÖÂÖà‰πòËµ∑Êù•ÔºåÂÜçÊ†πÊçÆÁõÆÊ†áÂèòÈáèÁúãÊòØÂê¶Ë¶ÅËΩ¨ÁΩÆ„ÄÇ C1W2L14 : Vectorzing Logistic Regression‚Äôs Gradient Compution backforwd \frac{‚àÇJ}{‚àÇw}=\frac{1}{m}X(A‚àíY)T\\ \frac{‚àÇJ}{‚àÇb}=\frac{1}{m}(a(i)‚àíy(i)) ÈáçË¶ÅÁöÑÊòØÂºÑÊ∏ÖÊ•öÔºåÈáåÈù¢ÁöÑË°åÂàóÂÖ≥Á≥ªÔºå‰ª£Ë°®ÁöÑÊÑèÊÄùÔºåËøêÁÆóÊó∂ÂÄôÔºåÂÖàËá™Â∑±ÁêÜÊ∏ÖÊ•ö„ÄÇËøòÊúâÁÇπÁßØ„ÄÅÁ≠âÁ≠âËøêÁÆóÊÄßË¥®ÂØπÂ∫îÁöÑÊìç‰ΩúÔºåÊàñËÄÖÂØπÂ∫îÁöÑÂÜÖÁΩÆÂáΩÊï∞ C1W2L15: Broadcasting in PythonOne Example A.sum(axis = 0)‰∏≠ÁöÑÂèÇÊï∞axis„ÄÇaxisÁî®Êù•ÊåáÊòéÂ∞ÜË¶ÅËøõË°åÁöÑËøêÁÆóÊòØÊ≤øÁùÄÂì™‰∏™ËΩ¥ÊâßË°åÔºåÂú®numpy‰∏≠Ôºå0ËΩ¥ÊòØÂûÇÁõ¥ÁöÑÔºå‰πüÂ∞±ÊòØÂàóÔºåËÄå1ËΩ¥ÊòØÊ∞¥Âπ≥ÁöÑÔºå‰πüÂ∞±ÊòØË°å„ÄÇ Á¨¨‰∫å‰∏™A/cal.reshape(1,4)Êåá‰ª§ÂàôË∞ÉÁî®‰∫Ünumpy‰∏≠ÁöÑÂπøÊí≠Êú∫Âà∂„ÄÇËøôÈáå‰ΩøÁî® 3 by 4ÁöÑÁü©ÈòµÈô§‰ª•1 by 4 ÁöÑÁü©Èòµ„ÄÇÊäÄÊúØ‰∏äÊù•ËÆ≤ÔºåÂÖ∂ÂÆûÂπ∂‰∏çÈúÄË¶ÅÂÜçÂ∞ÜÁü©Èòµ reshape(ÈáçÂ°ë)Êàê ÔºåÂõ†‰∏∫Áü©ÈòµÊú¨Ë∫´Â∑≤ÁªèÊòØ ‰∫Ü„ÄÇ‰ΩÜÊòØÂΩìÊàë‰ª¨ÂÜô‰ª£Á†ÅÊó∂‰∏çÁ°ÆÂÆöÁü©ÈòµÁª¥Â∫¶ÁöÑÊó∂ÂÄôÔºåÈÄöÂ∏∏‰ºöÂØπÁü©ÈòµËøõË°åÈáçÂ°ëÊù•Á°Æ‰øùÂæóÂà∞Êàë‰ª¨ÊÉ≥Ë¶ÅÁöÑÂàóÂêëÈáèÊàñË°åÂêëÈáè„ÄÇÈáçÂ°ëÊìç‰ΩúreshapeÊòØ‰∏Ä‰∏™Â∏∏ÈáèÊó∂Èó¥ÁöÑÊìç‰ΩúÔºåÊó∂Èó¥Â§çÊùÇÂ∫¶ÊòØÔºåÂÆÉÁöÑË∞ÉÁî®‰ª£‰ª∑ÊûÅ‰Ωé„ÄÇ Secondly Example pythonÁöÑÂπøÊí≠Êú∫Âà∂‰ºöÂ∞ÜÂ∏∏Êï∞Êâ©Â±ïÊàê4by 1ÁöÑÂàóÂêëÈáè ÂÖ∂ÂÆûÊòØÂ∞Ü1by*n ÁöÑÁü©ÈòµÂ§çÂà∂Êàê‰∏∫mbynÁöÑÁü©Èòµ ÂπøÊí≠Êú∫Âà∂ÁöÑ‰∏æ‰æã axisË°•ÂÖÖÔºönumpy‰∏≠ÔºåÁ±ª‰ººsumÁöÑÂáΩÊï∞ÔºåÁªèÂ∏∏Ê∂âÂèäaxisÂèÇÊï∞ÔºåÂèØ‰ª•ÂèñÂÄº‰∏∫0Êàñ1ÔºåÁîöËá≥ÂÖ∂‰ªñ„ÄÇÁªèÂ∏∏ËÆ∞‰∏ç‰ΩèÔºåËøôÈáåÊàëÊü•‰∫Ü‰∫Ü‰∏Ä‰∏ãÔºåÊòØËøôÊ†∑ÁöÑÔºàÂéüÊñáÔºâÔºö axisÁöÑÊï∞Â≠óÔºåÂíåÊï∞ÁªÑÁöÑshapeÂèÇÊï∞ÁöÑÁ¥¢ÂºïÊòØÂØπÂ∫îÁöÑ„ÄÇÊØîÂ¶Ç‰∏Ä‰∏™Êï∞ÁªÑÁöÑshapeÊòØ(5,6)ÔºåÂàô‰ª£Ë°®5‰∏™rowÔºå6‰∏™column„ÄÇÂç≥Âú®shape‰∏≠ÔºårowÂíåcolumnÁöÑ‰∏™Êï∞ÁöÑÁ¥¢ÂºïÊòØ0Âíå1„ÄÇ‰πüÂ∞±Á¨¨1‰∏™ÂùêÊ†áÔºåÂú®shape‰∏≠ÁöÑÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†ÔºåÁ¥¢ÂºïÊòØ0Ôºå‰ª£Ë°®rowÁöÑÊñπÂêëÔºõÁ¨¨2‰∏™ÂùêÊ†áÔºåÂú®shape‰∏≠ÁöÑÁ¨¨2‰∏™ÂÖÉÁ¥†ÔºåÁ¥¢ÂºïÊòØ1Ôºå‰ª£Ë°®rowÁöÑÊñπÂêë„ÄÇ ÂØπ‰∫ésumÂáΩÊï∞ÔºåaxisÊåáÁöÑÊòØsum‚ÄúÊ≤øÁùÄ‚ÄùÁöÑÊñπÂêëÔºåÁªèËøáËÆ°ÁÆóÔºåËøô‰∏™ÊñπÂêëÁöÑÁª¥Â∫¶Âõ†‰∏∫Ê±ÇÂíåÂêéÂ∞±Ê∂àÂ§±‰∫ÜÔºåÊØîÂ¶Çsum(axis=0)‰ª£Ë°®ÊòØÊ≤øÁùÄ‚Äúrow‚ÄùÊñπÂêëËøõË°åÊ±ÇÂíåÔºå ÂΩìÁÑ∂axisÂèØ‰ª•ÊòØ‰∏Ä‰∏™tupeÔºåÈÇ£Â∞±Áõ∏ÂΩì‰∫éÊ≤øÁùÄÂ§ö‰∏™Â§ö‰∏™ÊñπÂêëÊ±ÇÂíå„ÄÇ sumÂ¶ÇÊûú‰∏ç‰º†ÂÖ•axisÂèÇÊï∞ÔºåÈªòËÆ§ÊòØÂØπÊâÄÊúâÁª¥Â∫¶Ê±ÇÂíå„ÄÇ broadcasting ÂΩì‰∏§‰∏™Êï∞ÁªÑÁöÑÂΩ¢Áä∂Âπ∂‰∏çÁõ∏ÂêåÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÊâ©Â±ïÊï∞ÁªÑÁöÑÊñπÊ≥ïÊù•ÂÆûÁé∞Áõ∏Âä†„ÄÅÁõ∏Âáè„ÄÅÁõ∏‰πòÁ≠âÊìç‰ΩúÔºåËøôÁßçÊú∫Âà∂Âè´ÂÅöÂπøÊí≠ÔºàbroadcastingÔºâ„ÄÇ ‰∏âÁßçÂπøÊí≠ÊÉÖÂÜµ C1W2L16 A Note on Python/numpy vectorsÊú¨ËäÇ‰∏ªË¶ÅËÆ≤Python‰∏≠ÁöÑnumpy‰∏ÄÁª¥Êï∞ÁªÑÁöÑÁâπÊÄßÔºå‰ª•Âèä‰∏éË°åÂêëÈáèÊàñÂàóÂêëÈáèÁöÑÂå∫Âà´ 1. ‰∏ÄÁª¥Êï∞ÁªÑÁöÑÁâπÊÄßÈ¶ñÂÖàËÆæÁΩÆa = np.array.random.randn(5)ÔºåËøôÊ†∑‰ºöÁîüÊàêÂ≠òÂÇ®Âú®Êï∞ÁªÑa‰∏≠ÁöÑ5‰∏™È´òÊñØÈöèÊú∫Êï∞ÂèòÈáè„ÄÇ‰πãÂêéËæìÂá∫ Ôºå‰ªéÂ±èÂπï‰∏äÂèØ‰ª•ÂæóÁü•ÔºåÊ≠§Êó∂a ÁöÑshapeÔºàÂΩ¢Áä∂ÔºâÊòØ‰∏Ä‰∏™ÁöÑÁªìÊûÑ„ÄÇËøôÂú®Python‰∏≠Ë¢´Áß∞‰Ωú‰∏Ä‰∏™‰∏ÄÁª¥Êï∞ÁªÑ„ÄÇÂÆÉÊó¢‰∏çÊòØ‰∏Ä‰∏™Ë°åÂêëÈáè‰πü‰∏çÊòØ‰∏Ä‰∏™ÂàóÂêëÈáèÔºåËøô‰πüÂØºËá¥ÂÆÉÊúâ‰∏Ä‰∫õ‰∏çÊòØÂæàÁõ¥ËßÇÁöÑÊïàÊûú„ÄÇ‰∏æ‰∏™‰æãÂ≠êÔºåÂ¶ÇÊûúÊàëËæìÂá∫‰∏Ä‰∏™ËΩ¨ÁΩÆÈòµÔºåÊúÄÁªàÁªìÊûúÂÆÉ‰ºöÂíåÁúãËµ∑Êù•‰∏ÄÊ†∑ÔºåÊâÄ‰ª•ÂíåÁöÑËΩ¨ÁΩÆÈòµÊúÄÁªàÁªìÊûúÁúãËµ∑Êù•‰∏ÄÊ†∑„ÄÇËÄåÂ¶ÇÊûúÊàëËæìÂá∫ÂíåÁöÑËΩ¨ÁΩÆÈòµÁöÑÂÜÖÁßØÔºå‰Ω†ÂèØËÉΩ‰ºöÊÉ≥Ôºö‰πò‰ª•ÁöÑËΩ¨ÁΩÆËøîÂõûÁªô‰Ω†ÁöÑÂèØËÉΩ‰ºöÊòØ‰∏Ä‰∏™Áü©Èòµ„ÄÇ‰ΩÜÊòØÂ¶ÇÊûúÊàëËøôÊ†∑ÂÅöÔºå‰Ω†Âè™‰ºöÂæóÂà∞‰∏Ä‰∏™Êï∞„ÄÇ ÊâÄ‰ª•ÊàëÂª∫ËÆÆÂΩì‰Ω†ÁºñÂÜôÁ•ûÁªèÁΩëÁªúÊó∂Ôºå‰∏çË¶ÅÂú®‰ΩøÁî®ÁöÑshape(5,1)ÊòØËøòÊòØ(n,)ÊàñËÄÖ‰∏ÄÁª¥Êï∞ÁªÑ„ÄÇÁõ∏ÂèçÔºåÂ¶ÇÊûú‰Ω†ËÆæÁΩÆ(5,1)ÔºåÈÇ£‰πàËøôÂ∞±ÊòØ5Ë°å1ÂàóÂêëÈáè„ÄÇÂú®ÂÖàÂâçÁöÑÊìç‰ΩúÈáåaÂíåaÁöÑËΩ¨ÁΩÆÁúãËµ∑Êù•‰∏ÄÊ†∑ÔºåËÄåÁé∞Âú®ËøôÊ†∑ÁöÑ aÂèòÊàê‰∏Ä‰∏™Êñ∞ÁöÑa ÁöÑËΩ¨ÁΩÆÔºåÂπ∂‰∏îÂÆÉÊòØ‰∏Ä‰∏™Ë°åÂêëÈáè„ÄÇËØ∑Ê≥®ÊÑè‰∏Ä‰∏™ÁªÜÂæÆÁöÑÂ∑ÆÂà´ÔºåÂú®ËøôÁßçÊï∞ÊçÆÁªìÊûÑ‰∏≠ÔºåÂΩìÊàë‰ª¨ËæìÂá∫a ÁöÑËΩ¨ÁΩÆÊó∂Êúâ‰∏§ÂØπÊñπÊã¨Âè∑ÔºåËÄå‰πãÂâçÂè™Êúâ‰∏ÄÂØπÊñπÊã¨Âè∑ÔºåÊâÄ‰ª•ËøôÂ∞±ÊòØ1Ë°å5ÂàóÁöÑÁü©ÈòµÂíå‰∏ÄÁª¥Êï∞ÁªÑÁöÑÂ∑ÆÂà´„ÄÇ 2. Ë°åÂêëÈáèÂíåÂàóÂêëÈáèrank 1 arrayÈóÆÈ¢òÔºöshapeÊòØ(x,)ÁöÑÊï∞ÁªÑÔºåÊó¢‰∏çÊòØË°åÂêëÈáèÔºå‰πü‰∏çÊòØÂàóÂêëÈáèÔºåÊ≤°Ê≥ïÂèÇ‰∏éÊ≠£Â∏∏ÁöÑÁü©ÈòµËøêÁÆóÔºåÂ∫îËØ•ÊÄªÊòØ‰ΩøÁî®(x,1)Êàñ(1,x)ÁöÑshapeÊù•Ë°®Á§∫ÂêëÈáè„ÄÇ‰ΩÜÂèØ‰ª•ÈÄöËøáreshapeÊñπÊ≥ïÂ∞Ürank 1 arrayËΩ¨Êç¢‰∏∫Ë°åÂêëÈáèÊàñÂàóÂêëÈáè„ÄÇÔºà‰ªÄ‰πàÊòØrankÔºåÂ∞±ÊòØ‰∏Ä‰∏™Êï∞ÁªÑÁöÑÁª¥Â∫¶Ôºâ‰∏ÄÁª¥ÁöÑÊï∞ÁªÑÊó¢‰∏çÊòØË°åÂêëÈáè‰πü‰∏çÊòØÂàóÂêëÈáèÔºåËΩ¨ÁΩÆÂêéÔºå‰æùÁÑ∂ÊòØÊú¨Ë∫´„ÄÇ 3. Ëß£ÂÜ≥ÊñπÊ≥ï12assert(a.shape=Ôºà5Ôºå1)Ôºâ# ‰∏∫‰∫ÜÁ°Æ‰øù‰Ω†ÁöÑÁü©ÈòµÊàñÂêëÈáèÊâÄÈúÄË¶ÅÁöÑÁª¥Êï∞Êó∂Ôºå‰∏çË¶ÅÁæû‰∫é reshape Êìç‰Ωú C1W2L18 ÔºöQuick Tour of Jupyter/iPython NotebooksC1W2L18: Explanation of Logistic Regression Cost FunctionÂØπÂ∫îlogistic regressionÔºåËæìÂá∫$\hat{y}=p(y=1|x)$,ÈÇ£‰πà$p(y=0|x)=1-\hat{y}$ ÁªºÂêà‰∏äÈù¢ p(y|x)= \hat{y}^y*(1-\hat{y})^{1-y}ÂØπ‰∫éÊï¥‰∏™ËÆ≠ÁªÉÈõÜÔºå ÂÅáËÆæÊâÄÊúâÁöÑËÆ≠ÁªÉÊ†∑Êú¨Êúç‰ªéÂêå‰∏ÄÂàÜÂ∏É‰∏îÁõ∏‰∫íÁã¨Á´ãÔºå‰πüÂç≥Áã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑÔºåÊâÄÊúâËøô‰∫õÊ†∑Êú¨ÁöÑËÅîÂêàÊ¶ÇÁéáÂ∞±ÊòØÊØè‰∏™Ê†∑Êú¨Ê¶ÇÁéáÁöÑ‰πòÁßØ: p(labels \ in\ training\ set)=\Pi_{i=1}^mp(y_i|x_i)Â¶ÇÊûúÂà©Áî®ÊûÅÂ§ß‰ººÁÑ∂Ê≥ïÂÅöÔºåÊâæÂà∞‰∏ÄÁªÑÂèÇÊï∞Ôºå‰ΩøÂæóÊ†∑Êú¨ËßÇÊµãÂÄºÊ¶ÇÁéáÊúÄÂ§ß \max log p(label \ in \ training \ set)=log \Pi_{i=1}^mp(y_i|x_i)=\sum -L(\hat{y^i},y^i) \min cost J(w,b)=\frac{1}{m}L(\hat{y^i},y^i)ÊÄªÁªì‰∏Ä‰∏ãÔºå‰∏∫‰∫ÜÊúÄÂ∞èÂåñÊàêÊú¨ÂáΩÊï∞ÔºåÊàë‰ª¨‰ªélogisticÂõûÂΩíÊ®°ÂûãÁöÑÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÁöÑËßíÂ∫¶Âá∫ÂèëÔºåÂÅáËÆæËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÊ†∑Êú¨ÈÉΩÊòØÁã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑÊù°‰ª∂‰∏ã Day3 : summary‰∏ªË¶ÅÂ≠¶‰π†‰∫ÜpythonÁºñÁ®ãÁöÑÂ¶Ç‰ΩïÊâçËÉΩÈ´òÊïàÁéáÔºåÂÜÖÁΩÆÂáΩÊï∞ÁöÑÂÖ∑ÊúâÂπ∂Ë°åÊÄßÔºåsimdÊåá‰ª§Ôºå‰ª•Âèä‰∏ÄÁª¥Êï∞ÁªÑÁöÑ‰ΩøÁî®Ê≥®ÊÑè‰∫ãÈ°πÔºålogistic regressionÁöÑlost functionÁöÑÂéüÁêÜËØÅÊòé C1W3C1W3L01 : Neural Network Overview ËÆ∏Â§ösigmoidÂçïÂÖÉÂ†ÜÂè†Ëµ∑Êù•ÂΩ¢Êàê‰∏Ä‰∏™Á•ûÁªèÁΩëÁªú„ÄÇ Ê≠£Âêë‰º†Êí≠ÔºöËæìÂÖ•Â±ÇÂà∞layer one \left.\begin{array}{c}{x} \\ {W^{[1]}} \\ {b^{[1]}}\end{array}\right\} \Longrightarrow z^{[1]}=W^{[1]} x+b^{[1]} \Longrightarrow a^{[1]}=\sigma\left(z^{[1)}\right)layer one Âà∞layer two \left.\begin{array}{r}{a^{(1]}=\sigma\left(z^{[1]}\right)} \\ {W^{[2]}} \\ {b^{[2]}}\end{array}\right\}\begin{array}{l}{\Longrightarrow z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \Longrightarrow a^{[2]}=\sigma\left(z^{[2]}\right)} \\ {\Longrightarrow L\left(a^{[2]}, y\right)}\end{array}ÂèçÂêë‰º†Êí≠ \left.\begin{array}{r}{d a^{[1]}=d \sigma\left(z^{[1]}\right)} \\ {d W^{[2]}} \\ {d b^{[2]}}\end{array}\right\}\begin{array}{l}{\Longleftarrow d z^{[2]}=d\left(W^{[2]} \alpha^{[1]}+b^{[2]}\right) \Longleftarrow d a^{[2]}=d \sigma\left(z^{[2]}\right)} \\ {\Longleftarrow d L\left(a^{[2]}, y\right)}\end{array} $W$ÁöÑË°åÊï∞ÊòØÊú¨Ê¨°ÁªìÁÇπ‰∏™Êï∞ÔºåÂàóÊï∞ÊòØ‰∏äÂ±ÇËäÇÁÇπ‰∏™Êï∞ C1W3L02 : Nerual Network RepresentationsÁ¨¶Âè∑ËØ¥Êòé C1W3L03Ôºö Computation Neural Network OutputA simple training examples ÂÖ∂‰∏≠ÔºåxË°®Á§∫ËæìÂÖ•ÁâπÂæÅÔºåaË°®Á§∫ÊØè‰∏™Á•ûÁªèÂÖÉÁöÑËæìÂá∫ÔºåWË°®Á§∫ÁâπÂæÅÁöÑÊùÉÈáçÔºå‰∏äÊ†áË°®Á§∫Á•ûÁªèÁΩëÁªúÁöÑÂ±ÇÊï∞ÔºàÈöêËóèÂ±Ç‰∏∫1ÔºâÔºå‰∏ãÊ†áË°®Á§∫ËØ•Â±ÇÁöÑÁ¨¨Âá†‰∏™Á•ûÁªèÂÖÉ„ÄÇËøôÊòØÁ•ûÁªèÁΩëÁªúÁöÑÁ¨¶Âè∑ÊÉØ‰æãÔºå‰∏ãÂêå„ÄÇ Á•ûÁªèÁΩëÁªúÁöÑËÆ°ÁÆó ÂÖ≥‰∫éÁ•ûÁªèÁΩëÁªúÊòØÊÄé‰πàËÆ°ÁÆóÁöÑÔºå‰ªéÊàë‰ª¨‰πãÂâçÊèêÂèäÁöÑÈÄªËæëÂõûÂΩíÂºÄÂßãÔºåÂ¶Ç‰∏ãÂõæÊâÄÁ§∫„ÄÇÁî®ÂúÜÂúàË°®Á§∫Á•ûÁªèÁΩëÁªúÁöÑËÆ°ÁÆóÂçïÂÖÉÔºåÈÄªËæëÂõûÂΩíÁöÑËÆ°ÁÆóÊúâ‰∏§‰∏™Ê≠•È™§ÔºåÈ¶ñÂÖà‰Ω†ÊåâÊ≠•È™§ËÆ°ÁÆóÂá∫ÔºåÁÑ∂ÂêéÂú®Á¨¨‰∫åÊ≠•‰∏≠‰Ω†‰ª•sigmoidÂáΩÊï∞‰∏∫ÊøÄÊ¥ªÂáΩÊï∞ËÆ°ÁÆóÔºàÂæóÂá∫ÔºâÔºå‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÂè™ÊòØËøôÊ†∑Â≠êÂÅö‰∫ÜÂ•ΩÂ§öÊ¨°ÈáçÂ§çËÆ°ÁÆó„ÄÇ ËØ¥ÊòéÔºö$w_i^{[1]}$Âíå$W^{[1]}$ÁöÑÂÖ≥Á≥ªÔºå‰∏Ä‰∏™ÊåâÁÖßlogistic regression Ôºå‰∏Ä‰∏™ÊòØÁü©ÈòµË°®Á§∫„ÄÇ ÂêëÈáèÂåñËÆ°ÁÆó Â¶ÇÊûú‰Ω†ÊâßË°åÁ•ûÁªèÁΩëÁªúÁöÑÁ®ãÂ∫èÔºåÁî®forÂæ™ÁéØÊù•ÂÅöËøô‰∫õÁúãËµ∑Êù•ÁúüÁöÑÂæà‰ΩéÊïà„ÄÇÊâÄ‰ª•Êé•‰∏ãÊù•Êàë‰ª¨Ë¶ÅÂÅöÁöÑÂ∞±ÊòØÊääËøôÂõõ‰∏™Á≠âÂºèÂêëÈáèÂåñ„ÄÇÂêëÈáèÂåñÁöÑËøáÁ®ãÊòØÂ∞ÜÁ•ûÁªèÁΩëÁªú‰∏≠ÁöÑ‰∏ÄÂ±ÇÁ•ûÁªèÂÖÉÂèÇÊï∞Á∫µÂêëÂ†ÜÁßØËµ∑Êù•Ôºå‰æãÂ¶ÇÈöêËóèÂ±Ç‰∏≠ÁöÑÁ∫µÂêëÂ†ÜÁßØËµ∑Êù•ÂèòÊàê‰∏Ä‰∏™(4,3)ÁöÑÁü©ÈòµÔºåÁî®Á¨¶Âè∑$W^{[1]}$Ë°®Á§∫„ÄÇÂè¶‰∏Ä‰∏™ÁúãÂæÖËøô‰∏™ÁöÑÊñπÊ≥ïÊòØÊàë‰ª¨ÊúâÂõõ‰∏™ÈÄªËæëÂõûÂΩíÂçïÂÖÉÔºå‰∏îÊØè‰∏Ä‰∏™ÈÄªËæëÂõûÂΩíÂçïÂÖÉÈÉΩÊúâÁõ∏ÂØπÂ∫îÁöÑÂèÇÊï∞‚Äî‚ÄîÂêëÈáèÔºåÊääËøôÂõõ‰∏™ÂêëÈáèÂ†ÜÁßØÂú®‰∏ÄËµ∑Ôºå‰Ω†‰ºöÂæóÂá∫Ëøô4√ó3ÁöÑÁü©Èòµ„ÄÇ z^{[n]}=W^{[n]}X+b^{[n]} a^{[1]}=\left[ \begin{array}{c}{a_{1}^{[1]}} \\ {a_{2}^{[1]}} \\ {a_{3}^{[1]}} \\ {a_{4}^{[1]}}\end{array}\right]=\sigma\left(z^{[1]}\right) Given input XÔºàa single training set) \begin{array}{c}{z^{[1]}=W^{[1]} a^{[0]}+b^{[1]}} \\ {a^{[1]}=\sigma\left(z^{[1]}\right)} \\ {z^{[2]}=W^{[2]} a^{[1]}+b^{[2]}} \\ {a^{[2]}=\sigma\left(z^{[2]}\right)}\end{array}ËØ¥ÊòéÔºö $W$ÁöÑÁ¨¨$i$Ë°åË°®Á§∫ÔºåÂΩìÂâçÂ±ÇÂà∞‰∏ä‰∏ÄÂ±ÇÁöÑÊùÉÈáçË°åÂêëÈáèÔºåÂÜçËÆ°ÁÆóÂçï‰∏™ÁöÑÊó∂ÂÄôÔºåÁî±‰∫éÊòØÊåâÁÖßlogristics regressionÁöÑÊñπÂºèÔºåÊâÄ‰ª•ËÆ§‰∏∫$w_i$ÊòØÂàóÂêëÈáèÔºåÊâÄ‰ª•ËΩ¨ÁΩÆÊàêË°åÂêëÈáè„ÄÇ‰∏äÈù¢ÁöÑÂõæ‰πüËØ¥Êòé‰∫ÜÔºöÂ¶Ç‰Ωï‰ªéÂçï‰∏™Êìç‰ΩúÂà∞Áü©ÈòµÊìç‰ΩúÔºåÊùÉÈáçÁü©ÈòµÊòØÊÄé‰πàÊûÑÈÄ†ÔºåÊÄé‰πàË°®Á§∫ÁöÑ„ÄÇ bÊòØÂàóÂêëÈáè„ÄÇ C1W3L04: Vectorizing Across Mutilple ExampleDifferent training examples in different columns of the matrix for loop vectorizing : stacking training set in columns x=\left[ \begin{array}{cccc}{\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {x^{(1)}} & {x^{(2)}} & {\dots} & {x} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots}\end{array}\right] Â∞±Êúâ \left\{\begin{array}{l}{A^{[1]}=\sigma\left(z^{[1]}\right)} \\ {z^{[2]}=W^{[2]} A^{[1]}+b^{[2]}} \\ {A^{[2]}=\sigma\left(z^{[2]}\right)}\end{array}\right. ÂΩìÂûÇÁõ¥Êâ´ÊèèÔºåÊòØÁ¥¢ÂºïÂà∞ÈöêËóèÂçï‰ΩçÁöÑÊï∞Â≠ó„ÄÇÂΩìÊ∞¥Âπ≥Êâ´ÊèèÔºåÂ∞Ü‰ªéÁ¨¨‰∏Ä‰∏™ËÆ≠ÁªÉÁ§∫‰æã‰∏≠‰ªéÁ¨¨‰∏Ä‰∏™ÈöêËóèÁöÑÂçïÂÖÉÂà∞Á¨¨‰∫å‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºåÁ¨¨‰∏â‰∏™ËÆ≠ÁªÉÊ†∑Êú¨‚Ä¶‚Ä¶Áõ¥Âà∞ËäÇÁÇπÂØπÂ∫î‰∫éÁ¨¨‰∏Ä‰∏™ÈöêËóèÂçïÂÖÉÁöÑÊøÄÊ¥ªÂÄºÔºå‰∏îËøô‰∏™ÈöêËóèÂçïÂÖÉÊòØ‰Ωç‰∫éËøô‰∏™ËÆ≠ÁªÉÊ†∑Êú¨‰∏≠ÁöÑÊúÄÁªàËÆ≠ÁªÉÊ†∑Êú¨„ÄÇ ‰ªéÊ∞¥Âπ≥‰∏äÁúãÔºåÁü©Èòµ‰ª£Ë°®‰∫ÜÂêÑ‰∏™ËÆ≠ÁªÉÊ†∑Êú¨„ÄÇ‰ªéÁ´ñÁõ¥‰∏äÁúãÔºåÁü©ÈòµÁöÑ‰∏çÂêåÁöÑÁ¥¢ÂºïÂØπÂ∫î‰∫é‰∏çÂêåÁöÑÈöêËóèÂçïÂÖÉ„ÄÇ C1W3L05 : Explanation for vectorized implement C1W3L06 : Activation FunctionÂú®ËÆ®ËÆ∫‰ºòÂåñÁÆóÊ≥ïÊó∂ÔºåÊúâ‰∏ÄÁÇπË¶ÅËØ¥ÊòéÔºöÂü∫Êú¨Â∑≤Áªè‰∏çÁî®sigmoidÊøÄÊ¥ªÂáΩÊï∞‰∫ÜÔºåtanhÂáΩÊï∞Âú®ÊâÄÊúâÂú∫ÂêàÈÉΩ‰ºò‰∫ésigmoidÂáΩÊï∞„ÄÇ sigmoidÂáΩÊï∞ÂíåtanhÂáΩÊï∞‰∏§ËÄÖÂÖ±ÂêåÁöÑÁº∫ÁÇπÊòØÔºåÂú®zÁâπÂà´Â§ßÊàñËÄÖÁâπÂà´Â∞èÁöÑÊÉÖÂÜµ‰∏ãÔºåÂØºÊï∞ÁöÑÊ¢ØÂ∫¶ÊàñËÄÖÂáΩÊï∞ÁöÑÊñúÁéá‰ºöÂèòÂæóÁâπÂà´Â∞èÔºåÊúÄÂêéÂ∞±‰ºöÊé•Ëøë‰∫é0ÔºåÂØºËá¥Èôç‰ΩéÊ¢ØÂ∫¶‰∏ãÈôçÁöÑÈÄüÂ∫¶„ÄÇ Âú®Êú∫Âô®Â≠¶‰π†Âè¶‰∏Ä‰∏™ÂæàÊµÅË°åÁöÑÂáΩÊï∞ÊòØÔºö‰øÆÊ≠£Á∫øÊÄßÂçïÂÖÉÁöÑÂáΩÊï∞ÔºàReLuÔºâÔºåReLuÂáΩÊï∞ÂõæÂÉèÊòØÂ¶Ç‰∏ãÂõæ„ÄÇ$ a = max(0,z)$Ôºö ÊâÄ‰ª•ÔºåÂè™Ë¶ÅÊòØÊ≠£ÂÄºÁöÑÊÉÖÂÜµ‰∏ãÔºåÂØºÊï∞ÊÅíÁ≠â‰∫é1ÔºåÂΩìÊòØË¥üÂÄºÁöÑÊó∂ÂÄôÔºåÂØºÊï∞ÊÅíÁ≠â‰∫é0„ÄÇ‰ªéÂÆûÈôÖ‰∏äÊù•ËØ¥ÔºåÂΩì‰ΩøÁî®ÁöÑÂØºÊï∞Êó∂Ôºå=0ÁöÑÂØºÊï∞ÊòØÊ≤°ÊúâÂÆö‰πâÁöÑ„ÄÇ‰ΩÜÊòØÂΩìÁºñÁ®ãÂÆûÁé∞ÁöÑÊó∂ÂÄôÔºåÁöÑÂèñÂÄºÂàöÂ•ΩÁ≠â‰∫é0.00000001ÔºåËøô‰∏™ÂÄºÁõ∏ÂΩìÂ∞èÔºåÊâÄ‰ª•ÔºåÂú®ÂÆûË∑µ‰∏≠Ôºå‰∏çÈúÄË¶ÅÊãÖÂøÉËøô‰∏™ÂÄºÔºåÊòØÁ≠â‰∫é0ÁöÑÊó∂ÂÄôÔºåÂÅáËÆæ‰∏Ä‰∏™ÂØºÊï∞ÊòØ1ÊàñËÄÖ0ÊïàÊûúÈÉΩÂèØ‰ª•„ÄÇ Â¶ÇÊûúËæìÂá∫ÊòØ0„ÄÅ1ÂÄºÔºà‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºâÔºåÂàôËæìÂá∫Â±ÇÈÄâÊã©sigmoidÂáΩÊï∞ÔºåÁÑ∂ÂêéÂÖ∂ÂÆÉÁöÑÊâÄÊúâÂçïÂÖÉÈÉΩÈÄâÊã©ReluÂáΩÊï∞„ÄÇ ËøôÊòØÂæàÂ§öÊøÄÊ¥ªÂáΩÊï∞ÁöÑÈªòËÆ§ÈÄâÊã©ÔºåÂ¶ÇÊûúÂú®ÈöêËóèÂ±Ç‰∏ä‰∏çÁ°ÆÂÆö‰ΩøÁî®Âì™‰∏™ÊøÄÊ¥ªÂáΩÊï∞ÔºåÈÇ£‰πàÈÄöÂ∏∏‰ºö‰ΩøÁî®ReluÊøÄÊ¥ªÂáΩÊï∞„ÄÇÊúâÊó∂Ôºå‰πü‰ºö‰ΩøÁî®tanhÊøÄÊ¥ªÂáΩÊï∞Ôºå‰ΩÜReluÁöÑ‰∏Ä‰∏™Áº∫ÁÇπÊòØÔºöÂΩìzÊòØË¥üÂÄºÁöÑÊó∂ÂÄôÔºåÂØºÊï∞Á≠â‰∫é0„ÄÇ ËøôÈáå‰πüÊúâÂè¶‰∏Ä‰∏™ÁâàÊú¨ÁöÑReluË¢´Áß∞‰∏∫Leaky Relu„ÄÇ ÂΩìÊòØË¥üÂÄºÊó∂ÔºåËøô‰∏™ÂáΩÊï∞ÁöÑÂÄº‰∏çÊòØÁ≠â‰∫é0ÔºåËÄåÊòØËΩªÂæÆÁöÑÂÄæÊñú„ÄÇ ‰∏§ËÄÖÁöÑ‰ºòÁÇπÊòØÔºö Á¨¨‰∏ÄÔºåÂú®ÁöÑÂå∫Èó¥ÂèòÂä®ÂæàÂ§ßÁöÑÊÉÖÂÜµ‰∏ãÔºåÊøÄÊ¥ªÂáΩÊï∞ÁöÑÂØºÊï∞ÊàñËÄÖÊøÄÊ¥ªÂáΩÊï∞ÁöÑÊñúÁéáÈÉΩ‰ºöËøúÂ§ß‰∫é0ÔºåÂú®Á®ãÂ∫èÂÆûÁé∞Â∞±ÊòØ‰∏Ä‰∏™if-elseËØ≠Âè•ÔºåËÄåsigmoidÂáΩÊï∞ÈúÄË¶ÅËøõË°åÊµÆÁÇπÂõõÂàôËøêÁÆóÔºåÂú®ÂÆûË∑µ‰∏≠Ôºå‰ΩøÁî®ReLuÊøÄÊ¥ªÂáΩÊï∞Á•ûÁªèÁΩëÁªúÈÄöÂ∏∏‰ºöÊØî‰ΩøÁî®sigmoidÊàñËÄÖtanhÊøÄÊ¥ªÂáΩÊï∞Â≠¶‰π†ÁöÑÊõ¥Âø´„ÄÇ Á¨¨‰∫åÔºåsigmoidÂíåtanhÂáΩÊï∞ÁöÑÂØºÊï∞Âú®Ê≠£Ë¥üÈ•±ÂíåÂå∫ÁöÑÊ¢ØÂ∫¶ÈÉΩ‰ºöÊé•Ëøë‰∫é0ÔºåËøô‰ºöÈÄ†ÊàêÊ¢ØÂ∫¶Âº•Êï£ÔºåËÄåReluÂíåLeaky ReLuÂáΩÊï∞Â§ß‰∫é0ÈÉ®ÂàÜÈÉΩ‰∏∫Â∏∏Êï∞Ôºå‰∏ç‰ºö‰∫ßÁîüÊ¢ØÂ∫¶Âº•Êï£Áé∞Ë±°„ÄÇ(ÂêåÊó∂Â∫îËØ•Ê≥®ÊÑèÂà∞ÁöÑÊòØÔºåReluËøõÂÖ•Ë¥üÂçäÂå∫ÁöÑÊó∂ÂÄôÔºåÊ¢ØÂ∫¶‰∏∫0ÔºåÁ•ûÁªèÂÖÉÊ≠§Êó∂‰∏ç‰ºöËÆ≠ÁªÉÔºå‰∫ßÁîüÊâÄË∞ìÁöÑÁ®ÄÁñèÊÄßÔºåËÄåLeaky ReLu‰∏ç‰ºöÊúâËøôÈóÆÈ¢ò) Âú®ReLuÁöÑÊ¢ØÂ∫¶‰∏ÄÂçäÈÉΩÊòØ0Ôºå‰ΩÜÊòØÔºåÊúâË∂≥Â§üÁöÑÈöêËóèÂ±Ç‰ΩøÂæózÂÄºÂ§ß‰∫é0ÔºåÊâÄ‰ª•ÂØπÂ§ßÂ§öÊï∞ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÊù•ËØ¥Â≠¶‰π†ËøáÁ®ã‰ªçÁÑ∂ÂèØ‰ª•ÂæàÂø´„ÄÇ sigmoidÊøÄÊ¥ªÂáΩÊï∞ÔºöÈô§‰∫ÜËæìÂá∫Â±ÇÊòØ‰∏Ä‰∏™‰∫åÂàÜÁ±ªÈóÆÈ¢òÂü∫Êú¨‰∏ç‰ºöÁî®ÂÆÉ„ÄÇ tanhÊøÄÊ¥ªÂáΩÊï∞ÔºötanhÊòØÈùûÂ∏∏‰ºòÁßÄÁöÑÔºåÂá†‰πéÈÄÇÂêàÊâÄÊúâÂú∫Âêà„ÄÇ ReLuÊøÄÊ¥ªÂáΩÊï∞ÔºöÊúÄÂ∏∏Áî®ÁöÑÈªòËÆ§ÂáΩÊï∞ÔºåÔºåÂ¶ÇÊûú‰∏çÁ°ÆÂÆöÁî®Âì™‰∏™ÊøÄÊ¥ªÂáΩÊï∞ÔºåÂ∞±‰ΩøÁî®ReLuÊàñËÄÖLeaky ReLu„ÄÇ ÈÄöÂ∏∏ÁöÑÂª∫ËÆÆÊòØÔºöÂ¶ÇÊûú‰∏çÁ°ÆÂÆöÂì™‰∏Ä‰∏™ÊøÄÊ¥ªÂáΩÊï∞ÊïàÊûúÊõ¥Â•ΩÔºåÂèØ‰ª•ÊääÂÆÉ‰ª¨ÈÉΩËØïËØïÔºåÁÑ∂ÂêéÂú®È™åËØÅÈõÜÊàñËÄÖÂèëÂ±ïÈõÜ‰∏äËøõË°åËØÑ‰ª∑„ÄÇÁÑ∂ÂêéÁúãÂì™‰∏ÄÁßçË°®Áé∞ÁöÑÊõ¥Â•ΩÔºåÂ∞±Âéª‰ΩøÁî®ÂÆÉ„ÄÇ C1W3L07 : Why non-linear activation Functions ÈÄöËøáÊé®ÂØºÂèØ‰ª•ÂæóÂá∫ÔºåÂ¶ÇÊûú‰ΩøÁî®Á∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÔºåÁõ∏ÂΩì‰∫éÊ≤°ÊúâÈöêËóèÂ±Ç„ÄÇÊó†ËÆ∫‰Ω†ÁöÑÁ•ûÁªèÁΩëÁªúÊúâÂ§öÂ∞ëÂ±Ç‰∏ÄÁõ¥Âú®ÂÅöÁöÑÂè™ÊòØËÆ°ÁÆóÁ∫øÊÄßÂáΩÊï∞ÔºåÊâÄ‰ª•‰∏çÂ¶ÇÁõ¥Êé•ÂéªÊéâÂÖ®ÈÉ®ÈöêËóèÂ±Ç„ÄÇÂΩìÂΩìÁÑ∂ÔºåÂú®output layerÊòØÂèØ‰ª•‰∏çÁî®activation functionÔºåÊàñËÄÖÁî®linear activation functionÔºõËøôÁßçÊÉÖÂÜµ‰∏ÄËà¨ÊòØË¶ÅÊ±ÇËæìÂá∫ÂÆûÊï∞ÈõÜÁªìÊûúÔºàÊØîÂ¶ÇÈ¢ÑÊµãÊàø‰ª∑Ôºâ„ÄÇÂç≥‰æøÂ¶ÇÊ≠§ÔºåÂú®hidden layerËøòÊòØË¶ÅÁî®non-linear activation function„ÄÇ sigmoid activation function \frac{d}{d z} g(z)=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)=g(z)(1-g(z))tanh activation function g(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{x}+e^{-z}} \frac{d}{d z} g(z)=1-(\tanh (z))^{2}Rectified linear unit(RelU) g(z)^{\prime}=\left\{\begin{array}{ll}{0} & {\text { if } z0} \\ {\text {undefined}} & {\text { if } z=0}\end{array}\right.Ê≥®ÔºöÈÄöÂ∏∏Âú®z= 0ÁöÑÊó∂ÂÄôÁªôÂÆöÂÖ∂ÂØºÊï∞1,0ÔºõÂΩìÁÑ∂=0ÁöÑÊÉÖÂÜµÂæàÂ∞ë Leaky linear unit (Leaky ReLU) g(z)=\max (0.01 z, z) g(z)^{\prime}=\left\{\begin{array}{ll}{0.01} & {\text { if } z0} \\ {\text {undefined}} & {\text { if } z=0}\end{array}\right.Ê≥®ÔºöÈÄöÂ∏∏Âú®ÁöÑz=0Êó∂ÂÄôÁªôÂÆöÂÖ∂ÂØºÊï∞1,0.01ÔºõÂΩìÁÑ∂ÁöÑÊÉÖÂÜµÂæàÂ∞ë„ÄÇ C1W3L09 : Gradient Descent For Neural Networks gradient descentÁöÑÂÖ≥ÈîÆÊòØÊ±Çcost functionÂØπÂèÇÊï∞ÁöÑÂÅèÂØºÊï∞ Ê±ÇÂØºËøáÁ®ã‰ΩøÁî®ÁöÑÊòØBackpropagation È¶ñÂÖàÂÅöforward propagationÔºåÊ±ÇËß£Âá∫ÊØè‰∏ÄÂ±ÇÁöÑËæìÂá∫A (1) z^{[1]}=W^{[1]} x+b^{[1]}\\ (2) a^{[1]}=\sigma\left(z^{[1]}\right)\\(3) z^{[2]}=W^{[2]}=W^{[2]} a^{[1]}+b^{[2]}\\(4) a^{[2]}=g^{[2]}\left(z^{[z]}\right)=\sigma\left(z^{[2]}\right) ÁÑ∂ÂêéÂêëÂêéÔºåÈÄêÂ±ÇÊ±ÇËß£ÂØπÊØè‰∏ÄÂ±ÇÂèÇÊï∞ÁöÑÂÅèÂØºÊï∞ sumÔºåkeepdimsÊòØÈò≤Ê≠¢pythonËæìÂá∫ÈÇ£‰∫õÂè§ÊÄ™ÁöÑÁß©Êï∞(n,)ÔºåÂä†‰∏äËøô‰∏™Á°Æ‰øùÈòµÁü©ÈòµËøô‰∏™ÂêëÈáèËæìÂá∫ÁöÑÁª¥Â∫¶‰∏∫(n,1ÔºâËøôÊ†∑Ê†áÂáÜÁöÑÂΩ¢Âºè„ÄÇ C1WL10: Backpropagation intuition (optional) ÂÆûÁé∞ÂêéÂêë‰º†Êí≠Êúâ‰∏™ÊäÄÂ∑ßÔºåÂ∞±ÊòØË¶Å‰øùËØÅÁü©ÈòµÁöÑÁª¥Â∫¶Áõ∏‰∫íÂåπÈÖç ÂÖ∂ÂÆûÔºåÂØπ‰∫é‰∏Ä‰∏™Á•ûÁªèÂÖÉÔºåËæìÂÖ•ÈÉ®ÂàÜÔºöÊòØÊùÉÈáçÂíå‰∏ä‰∏ÄÂ±ÇËæìÂá∫ÁöÑÁ∫øÊÄßÁªÑÂêàÔºõËæìÂá∫ÔºöÊøÄÊ¥ªÂáΩÊï∞‰ΩúÁî®‰∫éËæìÂÖ•ÔºåÂõ†Ê≠§ÂØπ$W$Ê±ÇÂÅèÂØºÊó∂ÔºåÂØπÊøÄÊ¥ªÂáΩÊï∞Ê±Ç‰∏ÄÊ¨°ÔºåÂÜçÂØπÁ∫øÊÄßÁªÑÂêàÊ±Ç‰∏ÄÊ¨°„ÄÇÂØπ$b$Ê±ÇÂÅèÂØºÊòØÔºåÂØπÁ∫øÊÄßÈÉ®ÂàÜÊ±ÇÂÅèÂØºÊòØ1,ËøôÈáåÁî®Ê±ÇÂíå„ÄÇ C1W3L11: Random Initialization` ‰∏élogistic regression‰∏çÂêåÔºåÂàùÂßãÂåñÂèÇÊï∞‰∏çÂèØÂõ∫ÂÆö‰∏∫0ÔºåËÄåÊòØÊØè‰∏™ÂèÇÊï∞ÈÉΩË¶ÅÈöèÊú∫ÂàùÂßãÂåñ„ÄÇ ‰∏ªË¶ÅÂéüÂõ†ÊòØÔºöÂ¶ÇÊûúÊØè‰∏™ÂèÇÊï∞wÂíåbÈÉΩÊòØ0ÔºåÂàôÂêå‰∏ÄÂ±ÇÁöÑÊØè‰∏™neuronËÆ°ÁÆóÁªìÊûúÂÆåÂÖ®‰∏ÄÊ†∑ÔºàËæìÂÖ•‰∏ÄÊ†∑aÔºåÂèÇÊï∞‰∏ÄÊ†∑wÔºåÂàôz‰∏ÄÊ†∑,symmetry breaking problemÔºâÔºõÊé•‰∏ãÊù•ÂèçÂêë‰º†Êí≠Êó∂ÁöÑÂÅèÂØºÊï∞‰πü‰∏ÄÊ†∑Ôºå‰∏ã‰∏ÄËΩÆËø≠‰ª£Âêå‰∏ÄÂ±ÇÁöÑÊØè‰∏™neuronÁöÑwÂèàÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇËøôÊ†∑Êï¥‰∏™neural Network‰∏äÊØè‰∏ÄÂ±ÇÁöÑneuronÊòØÂêåË¥®ÁöÑÔºåËá™ÁÑ∂‰∏ç‰ºöÊúâÂ•ΩÁöÑperformance„ÄÇ .png) ‰∏çËøáÔºåÂØπbÂèÇÊï∞ÔºåÂèØ‰ª•ÈÉΩÂàùÂßãÂåñ‰∏∫0„ÄÇ Âè¶Â§ñÈúÄË¶ÅÊ≥®ÊÑèÔºåËôΩÁÑ∂wÊòØÈöèÊú∫ÂàùÂßãÂåñÔºå‰ΩÜÊúÄÂ•Ω‰ΩøÁî®ËæÉÂ∞èÁöÑÈöèÊú∫Êï∞„ÄÇ‰∏ªË¶ÅÊòØÈÅøÂÖçËÆ©zÁöÑËÆ°ÁÆóÂÄºËøáÂ§ßÔºåÂØºËá¥activation functionÂØπzÁöÑÂÅèÂØºÊï∞Ë∂ã‰∫é0ÔºåÂØºËá¥Gradient descent‰∏ãÈôçËæÉÊÖ¢„ÄÇ ÈÄöÂ∏∏ÁöÑÂÅöÊ≥ïÊòØÂØπrandomÁöÑÂÄº‰πò‰ª•‰∏Ä‰∏™ÊØîÁéáÔºåÊØîÂ¶Ç0.01Ôºà‰ΩÜÂÖ∑‰ΩìÊÄé‰πàÈÄâËøô‰∏™ÊØîÁéáÔºå‰πüË¶ÅÊ†πÊçÆÊÉÖÂÜµËÄåÂÆöÔºåËøôÂ∫îËØ•ÂèàÊòØ‰∏Ä‰∏™Ë∂ÖÂèÇ‰∫ÜÔºâÔºö $W[1]=np.random.randn((2,2))‚àó0.01$ Âõ†‰∏∫Â¶ÇÊûú‰Ω†Áî®tanhÊàñËÄÖsigmoidÊøÄÊ¥ªÂáΩÊï∞ÔºåÊàñËÄÖËØ¥Âè™Âú®ËæìÂá∫Â±ÇÊúâ‰∏Ä‰∏™SigmoidÔºåÂ¶ÇÊûúÔºàÊï∞ÂÄºÔºâÊ≥¢Âä®Â§™Â§ßÔºåÂΩì‰Ω†ËÆ°ÁÆóÊøÄÊ¥ªÂÄºÊó∂Â¶ÇÊûúÂæàÂ§ßÔºåÂ∞±‰ºöÂæàÂ§ßÊàñËÄÖÂæàÂ∞èÔºåÂõ†Ê≠§ËøôÁßçÊÉÖÂÜµ‰∏ã‰Ω†ÂæàÂèØËÉΩÂÅúÂú®tanh/sigmoidÂáΩÊï∞ÁöÑÂπ≥Âù¶ÁöÑÂú∞ÊñπÔºåËøô‰∫õÂú∞ÊñπÊ¢ØÂ∫¶ÂæàÂ∞è‰πüÂ∞±ÊÑèÂë≥ÁùÄÊ¢ØÂ∫¶‰∏ãÈôç‰ºöÂæàÊÖ¢ÔºåÂõ†Ê≠§Â≠¶‰π†‰πüÂ∞±ÂæàÊÖ¢„ÄÇ ‰∫ãÂÆû‰∏äÊúâÊó∂ÊúâÊØî0.01Êõ¥Â•ΩÁöÑÂ∏∏Êï∞ÔºåÂΩì‰Ω†ËÆ≠ÁªÉ‰∏Ä‰∏™Âè™Êúâ‰∏ÄÂ±ÇÈöêËóèÂ±ÇÁöÑÁΩëÁªúÊó∂ÔºàËøôÊòØÁõ∏ÂØπÊµÖÁöÑÁ•ûÁªèÁΩëÁªúÔºåÊ≤°ÊúâÂ§™Â§öÁöÑÈöêËóèÂ±ÇÔºâÔºåËÆæ‰∏∫0.01ÂèØËÉΩ‰πüÂèØ‰ª•„ÄÇ‰ΩÜÂΩì‰Ω†ËÆ≠ÁªÉ‰∏Ä‰∏™ÈùûÂ∏∏ÈùûÂ∏∏Ê∑±ÁöÑÁ•ûÁªèÁΩëÁªúÔºå‰Ω†ÂèØËÉΩË¶ÅËØïËØï0.01‰ª•Â§ñÁöÑÂ∏∏Êï∞„ÄÇ summaryÂ¶Ç‰ΩïÂª∫Á´ã‰∏Ä‰∏™‰∏ÄÂ±ÇÁöÑÁ•ûÁªèÁΩëÁªú‰∫ÜÔºåÂàùÂßãÂåñÂèÇÊï∞ÔºåÁî®ÂâçÂêë‰º†Êí≠È¢ÑÊµãÔºåËøòÊúâËÆ°ÁÆóÂØºÊï∞ÔºåÁªìÂêàÂèçÂêë‰º†Êí≠Áî®Âú®Ê¢ØÂ∫¶‰∏ãÈôç‰∏≠„ÄÇ 1. Define the neural network structure ( # of input units, # of hidden units, etc). 2. Initialize the model's parameters 1. Loop: - Implement forward propagation - Compute loss - Implement backward propagation to get the gradients - Update parameters (gradient descent) C1W4C1W4L01 Deep-layer neural network1. logistics regression and shallow neural network and deep-layer neural network 2. notationÁ•ûÁªèÁΩëÁªúÊ®°Âûã \begin{array}{l}{X \in \mathbb{R}^{n_{x} \times m}} ‰ª£Ë°®ËæìÂÖ•ÁöÑÁü©Èòµ\\{x^{(i)} \in \mathbb{R}^{n_{x}}} ‰ª£Ë°®Á¨¨ i ‰∏™Ê†∑Êú¨ÁöÑÂàóÂêëÈáè\\ {Y \in \mathbb{R}^{n_{y} \times n}} Ê†áËÆ∞Áü©Èòµ\\ {y^{(i)} \in \mathbb{R}^{n_{v}}}ÊòØÁ¨¨iÊ†∑Êú¨ÁöÑËæìÂá∫Ê†áÁ≠æ\\ W^{[l]} \in \mathbb{R}^{l \times(l-1)}‰ª£Ë°®Á¨¨[l]Â±ÇÁöÑÊùÉÈáçÁü©Èòµ\\ b^{[l]} \in \mathbb{R}^{l}‰ª£Ë°®Á¨¨[l]Â±ÇÁöÑÂÅèÂ∑ÆÁü©Èòµ\\ {\hat{y}^{(i)} \in \mathbb{R}^{n_{v}}}ÊòØÈ¢ÑÊµãËæìÂá∫ÂêëÈáè\end{array} ÈÄöÁî®ÊøÄÊ¥ªÂÖ¨ÂºèÔºö a_{j}^{[l]}=g^{[l]}\left(z_{j}^{[l]}\right)=g^{ | l ]}\left(\sum_{k} w_{j k}^{[l]} a_{k}^{[l-1]}+b_{j}^{[l]}\right)C1W4L02Ôºö Forward and Backward propagation1. forward propagation 2. backward propagation \begin{array}{l}{d z^{[l]}=d a^{[l]} * g^{[l]}\left(z^{l l}\right)} \\ {d w^{[l]}=d z^{[l]} \cdot a^{[l-1]}}\\d b^{[l]}=d z^{[l]}\\ d a^{[l-1]}=w^{[l]} \cdot d z^{[l]}\\ d z^{[l]}=w^{[l+1] T} d z^{[l+1]} \cdot g^{[l]^{\prime}}\left(z^{[l]}\right)\end{array}ÂêëÈáèÂåñ \begin{array}{l}{d Z^{[l]}=d A^{[l]} * g^{[l]}\left(Z^{[l]}\right)} \\ {d W^{[l]}=\frac{1}{m} d Z^{[l]} \cdot A^{[l-1] T}}\\ \begin{array}{l}{d b^{[l]}=\frac{1}{m} n p \cdot \operatorname{sum}\left(d z^{[l]}, \text { axis }=1, \text {keepdims}=\text {True}\right)} \\ {d A^{[l-1]}=W^{[l] T} \cdot d Z^{[l]}}\end{array}\end{array}summary C1W4L03 : Forward Propagation in d deep network ËøôÈáåÂè™ËÉΩÁî®‰∏Ä‰∏™ÊòæÂºèforÂæ™ÁéØÔºå‰ªé1Âà∞ÔºåÁÑ∂Âêé‰∏ÄÂ±ÇÊé•ÁùÄ‰∏ÄÂ±ÇÂéªËÆ°ÁÆó„ÄÇ C1W4L04 Getting matrix dimension rightÂΩìÂÆûÁé∞Ê∑±Â∫¶Á•ûÁªèÁΩëÁªúÁöÑÊó∂ÂÄôÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Â∏∏Áî®ÁöÑÊ£ÄÊü•‰ª£Á†ÅÊòØÂê¶ÊúâÈîôÁöÑÊñπÊ≥ïÂ∞±ÊòØÊãøÂá∫‰∏ÄÂº†Á∫∏Ëøá‰∏ÄÈÅçÁÆóÊ≥ï‰∏≠Áü©ÈòµÁöÑÁª¥Êï∞„ÄÇ $d_w^{[l]}$Âíå$w^{[l]}$Áª¥Â∫¶Áõ∏ÂêåÔºå$db^{[l]}$Âíå$b^{[l]}$Áª¥Â∫¶Áõ∏ÂêåÔºå‰∏îwÂíåbÂêëÈáèÂåñÁª¥Â∫¶‰∏çÂèòÔºå‰ΩÜz,a‰ª•ÂèäxÁöÑÁª¥Â∫¶‰ºöÂêëÈáèÂåñÂêéÂèëÁîüÂèòÂåñ„ÄÇ ÂèçÂêë‰º†Êí≠ÁöÑÁª¥Êï∞Ê£ÄÊü• Âú®‰Ω†ÂÅöÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÁöÑÂèçÂêë‰º†Êí≠Êó∂Ôºå‰∏ÄÂÆöË¶ÅÁ°ÆËÆ§ÊâÄÊúâÁöÑÁü©ÈòµÁª¥Êï∞ÊòØÂâçÂêé‰∏ÄËá¥ÁöÑÔºåÂèØ‰ª•Â§ßÂ§ßÊèêÈ´ò‰ª£Á†ÅÈÄöËøáÁéá„ÄÇ C1W4L05 Why deep representations?Á•ûÁªèÁΩëÁªú‰∏çÈúÄË¶ÅÂæàÂ§ßÔºå‰ΩÜÊòØÂæóÊúâÊ∑±Â∫¶Ôºå‰πüÂ∞±ÊòØÈöêËóèÂ±ÇÈúÄË¶ÅÂæàÂ§öÔºå 1. for example of face detector C1W4L06 :Building blocks of a deep neural network ÂèØ‰ª•ÁúãÂæóÂá∫ÔºåÂÜçÂèçÂêë‰º†Êí≠ÁöÑÊó∂ÂÄôÔºåÈúÄË¶ÅÁî®Âà∞$Z^{[L]},W^{[L]},b^{[L]}$,Âõ†Ê≠§cash them Ê≠£Âêë‰º†Êí≠Ôºö$Z^{[1]},A^{[1]}‚Ä¶‚Ä¶‚Ä¶‚Ä¶$,ÂèçÂêë‰º†Êí≠Ôºö$dA^{[L]},dZ{[L]},dW^{[L]}dB^{[L]},dA^{[L-1]}$ C1W4L07ÔºöParameters vs Hyperparameters1 What 2 How Idea‚ÄîCode‚ÄîExperiment‚ÄîIdeaËøô‰∏™Âæ™ÁéØÔºåÂ∞ùËØïÂêÑÁßç‰∏çÂêåÁöÑÂèÇÊï∞ÔºåÂÆûÁé∞Ê®°ÂûãÂπ∂ËßÇÂØüÊòØÂê¶ÊàêÂäüÔºåÁÑ∂ÂêéÂÜçËø≠‰ª£ ‰ªäÂ§©ÁöÑÊ∑±Â∫¶Â≠¶‰π†Â∫îÁî®È¢ÜÂüüÔºåËøòÊòØÂæàÁªèÈ™åÊÄßÁöÑËøáÁ®ãÔºåÈÄöÂ∏∏‰Ω†Êúâ‰∏™ÊÉ≥Ê≥ïÔºåÊØîÂ¶Ç‰Ω†ÂèØËÉΩÂ§ßËá¥Áü•ÈÅì‰∏Ä‰∏™ÊúÄÂ•ΩÁöÑÂ≠¶‰π†ÁéáÂÄºÔºåÂèØËÉΩËØ¥ÊúÄÂ•ΩÔºåÊàë‰ºöÊÉ≥ÂÖàËØïËØïÁúãÔºåÁÑ∂Âêé‰Ω†ÂèØ‰ª•ÂÆûÈôÖËØï‰∏Ä‰∏ãÔºåËÆ≠ÁªÉ‰∏Ä‰∏ãÁúãÁúãÊïàÊûúÂ¶Ç‰Ωï„ÄÇÁÑ∂ÂêéÂü∫‰∫éÂ∞ùËØïÁöÑÁªìÊûú‰Ω†‰ºöÂèëÁé∞Ôºå‰Ω†ËßâÂæóÂ≠¶‰π†ÁéáËÆæÂÆöÂÜçÊèêÈ´òÂà∞0.05‰ºöÊØîËæÉÂ•Ω„ÄÇÂ¶ÇÊûú‰Ω†‰∏çÁ°ÆÂÆö‰ªÄ‰πàÂÄºÊòØÊúÄÂ•ΩÁöÑÔºå‰Ω†Â§ßÂèØ‰ª•ÂÖàËØïËØï‰∏Ä‰∏™Â≠¶‰π†ÁéáÔºåÂÜçÁúãÁúãÊçüÂ§±ÂáΩÊï∞JÁöÑÂÄºÊúâÊ≤°Êúâ‰∏ãÈôç„ÄÇÁÑ∂Âêé‰Ω†ÂèØ‰ª•ËØï‰∏ÄËØïÂ§ß‰∏Ä‰∫õÁöÑÂÄºÔºåÁÑ∂ÂêéÂèëÁé∞ÊçüÂ§±ÂáΩÊï∞ÁöÑÂÄºÂ¢ûÂä†Âπ∂ÂèëÊï£‰∫Ü„ÄÇÁÑ∂ÂêéÂèØËÉΩËØïËØïÂÖ∂‰ªñÊï∞ÔºåÁúãÁªìÊûúÊòØÂê¶‰∏ãÈôçÁöÑÂæàÂø´ÊàñËÄÖÊî∂ÊïõÂà∞Âú®Êõ¥È´òÁöÑ‰ΩçÁΩÆ„ÄÇ‰Ω†ÂèØËÉΩÂ∞ùËØï‰∏çÂêåÁöÑÂπ∂ËßÇÂØüÊçüÂ§±ÂáΩÊï∞Ëøô‰πàÂèò‰∫ÜÔºåËØïËØï‰∏ÄÁªÑÂÄºÔºåÁÑ∂ÂêéÂèØËÉΩÊçüÂ§±ÂáΩÊï∞ÂèòÊàêËøôÊ†∑ÔºåËøô‰∏™ÂÄº‰ºöÂä†Âø´Â≠¶‰π†ËøáÁ®ãÔºåÂπ∂‰∏îÊî∂ÊïõÂú®Êõ¥‰ΩéÁöÑÊçüÂ§±ÂáΩÊï∞ÂÄº‰∏äÔºàÁÆ≠Â§¥Ê†áËØÜÔºâÔºåÊàëÂ∞±Áî®Ëøô‰∏™ÂÄº‰∫Ü„ÄÇ Âú®ÂâçÈù¢Âá†È°µ‰∏≠ÔºåËøòÊúâÂæàÂ§ö‰∏çÂêåÁöÑË∂ÖÂèÇÊï∞„ÄÇÁÑ∂ËÄåÔºåÂΩì‰Ω†ÂºÄÂßãÂºÄÂèëÊñ∞Â∫îÁî®Êó∂ÔºåÈ¢ÑÂÖàÂæàÈöæÁ°ÆÂàáÁü•ÈÅìÔºåÁ©∂Á´üË∂ÖÂèÇÊï∞ÁöÑÊúÄ‰ºòÂÄºÂ∫îËØ•ÊòØ‰ªÄ‰πà„ÄÇÊâÄ‰ª•ÈÄöÂ∏∏Ôºå‰Ω†ÂøÖÈ°ªÂ∞ùËØïÂæàÂ§ö‰∏çÂêåÁöÑÂÄºÔºåÂπ∂Ëµ∞Ëøô‰∏™Âæ™ÁéØÔºåËØïËØïÂêÑÁßçÂèÇÊï∞„ÄÇËØïËØïÁúã5‰∏™ÈöêËóèÂ±ÇÔºåËøô‰∏™Êï∞ÁõÆÁöÑÈöêËóèÂçïÂÖÉÔºåÂÆûÁé∞Ê®°ÂûãÂπ∂ËßÇÂØüÊòØÂê¶ÊàêÂäüÔºåÁÑ∂ÂêéÂÜçËø≠‰ª£„ÄÇËøôÈ°µÁöÑÊ†áÈ¢òÊòØÔºåÂ∫îÁî®Ê∑±Â∫¶Â≠¶‰π†È¢ÜÂüüÔºå‰∏Ä‰∏™ÂæàÂ§ßÁ®ãÂ∫¶Âü∫‰∫éÁªèÈ™åÁöÑËøáÁ®ãÔºåÂá≠ÁªèÈ™åÁöÑËøáÁ®ãÈÄö‰øóÊù•ËØ¥ÔºåÂ∞±ÊòØËØïÁõ¥Âà∞‰Ω†ÊâæÂà∞ÂêàÈÄÇÁöÑÊï∞ÂÄº„ÄÇ ÊâÄ‰ª•ÊàëÁªèÂ∏∏Âª∫ËÆÆ‰∫∫‰ª¨ÔºåÁâπÂà´ÊòØÂàöÂºÄÂßãÂ∫îÁî®‰∫éÊñ∞ÈóÆÈ¢òÁöÑ‰∫∫‰ª¨ÔºåÂéªËØï‰∏ÄÂÆöËåÉÂõ¥ÁöÑÂÄºÁúãÁúãÁªìÊûúÂ¶Ç‰Ωï„ÄÇÁÑ∂Âêé‰∏ã‰∏ÄÈó®ËØæÁ®ãÔºåÊàë‰ª¨‰ºöÁî®Êõ¥Á≥ªÁªüÁöÑÊñπÊ≥ïÔºåÁî®Á≥ªÁªüÊÄßÁöÑÂ∞ùËØïÂêÑÁßçË∂ÖÂèÇÊï∞ÂèñÂÄº„ÄÇÁÑ∂ÂêéÂÖ∂Ê¨°ÔºåÁîöËá≥ÊòØ‰Ω†Â∑≤ÁªèÁî®‰∫ÜÂæà‰πÖÁöÑÊ®°ÂûãÔºåÂèØËÉΩ‰Ω†Âú®ÂÅöÁΩëÁªúÂπøÂëäÂ∫îÁî®ÔºåÂú®‰Ω†ÂºÄÂèëÈÄî‰∏≠ÔºåÂæàÊúâÂèØËÉΩÂ≠¶‰π†ÁéáÁöÑÊúÄ‰ºòÊï∞ÂÄºÊàñÊòØÂÖ∂‰ªñË∂ÖÂèÇÊï∞ÁöÑÊúÄ‰ºòÂÄºÊòØ‰ºöÂèòÁöÑÔºåÊâÄ‰ª•Âç≥‰Ωø‰Ω†ÊØèÂ§©ÈÉΩÂú®Áî®ÂΩìÂâçÊúÄ‰ºòÁöÑÂèÇÊï∞Ë∞ÉËØï‰Ω†ÁöÑÁ≥ªÁªüÔºå‰Ω†ËøòÊòØ‰ºöÂèëÁé∞ÔºåÊúÄ‰ºòÂÄºËøá‰∏ÄÂπ¥Â∞±‰ºöÂèòÂåñÔºåÂõ†‰∏∫ÁîµËÑëÁöÑÂü∫Á°ÄËÆæÊñΩÔºåCPUÊàñÊòØGPUÂèØËÉΩ‰ºöÂèòÂåñÂæàÂ§ß„ÄÇÊâÄ‰ª•Êúâ‰∏ÄÊù°ÁªèÈ™åËßÑÂæãÂèØËÉΩÊØèÂá†‰∏™ÊúàÂ∞±‰ºöÂèò„ÄÇÂ¶ÇÊûú‰Ω†ÊâÄËß£ÂÜ≥ÁöÑÈóÆÈ¢òÈúÄË¶ÅÂæàÂ§öÂπ¥Êó∂Èó¥ÔºåÂè™Ë¶ÅÁªèÂ∏∏ËØïËØï‰∏çÂêåÁöÑË∂ÖÂèÇÊï∞ÔºåÂã§‰∫éÊ£ÄÈ™åÁªìÊûúÔºåÁúãÁúãÊúâÊ≤°ÊúâÊõ¥Â•ΩÁöÑË∂ÖÂèÇÊï∞Êï∞ÂÄºÔºåÁõ∏‰ø°‰Ω†ÊÖ¢ÊÖ¢‰ºöÂæóÂà∞ËÆæÂÆöË∂ÖÂèÇÊï∞ÁöÑÁõ¥ËßâÔºåÁü•ÈÅì‰Ω†ÁöÑÈóÆÈ¢òÊúÄÂ•ΩÁî®‰ªÄ‰πàÊï∞ÂÄº„ÄÇ Êúâ‰∏ÄÊù°ÁªèÈ™åËßÑÂæãÔºöÁªèÂ∏∏ËØïËØï‰∏çÂêåÁöÑË∂ÖÂèÇÊï∞ÔºåÂã§‰∫éÊ£ÄÊü•ÁªìÊûúÔºåÁúãÁúãÊúâÊ≤°ÊúâÊõ¥Â•ΩÁöÑË∂ÖÂèÇÊï∞ÂèñÂÄºÔºå‰Ω†Â∞Ü‰ºöÂæóÂà∞ËÆæÂÆöË∂ÖÂèÇÊï∞ÁöÑÁõ¥Ëßâ„ÄÇ ÊÄªÁªìÔºöË∂ÖÂèÇÊï∞ÁöÑËÆæÂÆöÔºåÈù†ÁªèÈ™åÔºåÂ∞ùËØïÔºåÂπ∂Ë∞ÉÔºåÊ†πÊçÆÁªìÊûúË∞ÉÔºå C1W4L08 : What does this have to do with the brain?# summary : forward prop and back prop1. logistics regression,shallow neural network and deep neural networklogistics regression Z = W^TX+B\\ A = \frac{1}{1+e^{-Z}}\\ L(A,Y)=-\frac{1}{m}(Ylog^A+(1-Y)log^{1-A}\\ \frac{\partial L}{\partial Z}=(A-Y)\\ \frac{\partial L}{\partial W}=X(A-Y)\\ËØ¥ÊòéÔºöXÊòØÊ†∑Êú¨ÊåâÂàóÂ†ÜÁßØÔºåWÊòØÂàóÂêëÈáè shallow neural network ‰ª•‰∫åÂàÜÈóÆÈ¢ò‰∏∫‰æã Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}\\ A^{[1]}=g^{[1]}(Z^{[1]})\\ \ \\ Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\ A^{[2]}=g^{[2]}(Z^{[2]})\\ \ \ \\ \ \\ L(A^{[2]},Y)=-\frac{1}{m}(Ylog^{A}+(1-Y)log^{1-A})\\ \frac{\partial L}{\partial Z^{[2]}}=(A^{[2]}-Y)\\ \frac{\partial L}{\partial W^{[2]}}=(A^{[2]}-Y)A^{[1]^T}\\ \frac{\partial L}{\partial b^{[2]}}=(A^{[2]}-Y)1_{1*m}^T\\ \frac{\partial L}{\partial a^{[1]}}=W^{[2]^T}(A^{[2]}-Y)\\ \ \\ \frac{\partial L}{\partial Z^{[1]}}=W^{[2]^T}(A^{[2]}-Y)* g^{'[1]}(Z^{[1]})\\ËØ¥ÊòéÔºöWÊòØÊåâÂàóÊéí$W^{[L]}$ÊòØ$n^{[L]}*n^{[L-1]}$Áü©ÈòµÔºåA,ZÊòØÊåâÂàóÂ†ÜÁßØÔºåËÆ∞ÂæóÊ£ÄÊü•Áü©ÈòµÁª¥Êï∞Â∞±Â•Ω‰∫Ü deep neural network Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\ A^{[l]}=g^{[l]}(Z^{[l]})\\ \ \ \\ \ \\ \frac{\partial L}{\partial Z^{[l]}}=\partial A^*g^{'[l]}(Z^{l})\\ \frac{\partial L}{\partial W^{[l]}}=\partial Z^{[l]} A^{[1-1]^T}\\ \frac{\partial L}{\partial b^{[l]}}=\partial Z^{[l]}\\ \frac{\partial L}{\partial a^{[l-1]}}=W^{[l]^T}\partial Z^{[l]}\\ \ \\ \frac{\partial L}{\partial Z^{[l-1]}}=W^{[l]^T}\partial Z^{[l]}* g^{'[l-1]}(Z^{[l-1]})\\2. vectorization Êé®ÂØºÁöÑÊó∂ÂÄôË¶ÅÂêëÈáèÂåñÔºåÊ≥®ÊÑèÁü©ÈòµÁª¥Êï∞Ë°®Á§∫ÔºåÂèØ‰ª•‰ªéÂçï‰∏™Êé®ÂØºÂà∞mutli ÂÖÖÂàÜÂà©Áî®pythonÁöÑÂπøÊí≠Â±ûÊÄßÔºåÂíåÂÜÖÁΩÆÂáΩÊï∞ÁöÑÂπ∂Ë°åÂåñ python‰∏ÄÁª¥Ôºå‰∫åÁª¥Êï∞ÁªÑÁöÑÁâπÊÄß 3. Áü•ËØÜÁªìÊûÑ]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deep_learning.aiÊ∑±Â∫¶Â≠¶‰π†Á¨îËÆ∞]]></title>
    <url>%2F2019%2F04%2F11%2Fdeep-learning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[C5: Sequence ModelsW1 : Recurrent Neural Networks (Âæ™ÁéØÂ∫èÂàóÊ®°Âûã)L1 Ôºö Why Sequence Models?Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºàRNNÔºâ‰πãÁ±ªÁöÑÊ®°ÂûãÂú®ËØ≠Èü≥ËØÜÂà´„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåÂÖ∂‰ªñÈ¢ÜÂüü‰∏≠ÂºïËµ∑ÂèòÈù©„ÄÇ Â∫èÂàóÊ®°ÂûãÁöÑÂàóÂ≠ê L2 : Notation Êï∞Â≠¶Á¨¶Âè∑NLP Êàë‰ª¨Áî®$X^{(i)}$Êù•Ë°®Á§∫Á¨¨‰∏™iËÆ≠ÁªÉÊ†∑Êú¨ÔºåÊâÄ‰ª•‰∏∫‰∫ÜÊåá‰ª£Á¨¨‰∏™tÂÖÉÁ¥†ÔºåÊàñËÄÖËØ¥ÊòØËÆ≠ÁªÉÊ†∑Êú¨iÁöÑÂ∫èÂàó‰∏≠Á¨¨t‰∏™ÂÖÉÁ¥†Áî®$X^{(i)}$Ëøô‰∏™Á¨¶Âè∑Êù•Ë°®Á§∫„ÄÇÂ¶ÇÊûúÊòØÂ∫èÂàóÈïøÂ∫¶$T_x$ÔºåÈÇ£‰πà‰Ω†ÁöÑËÆ≠ÁªÉÈõÜÈáå‰∏çÂêåÁöÑËÆ≠ÁªÉÊ†∑Êú¨Â∞±‰ºöÊúâ‰∏çÂêåÁöÑÈïøÂ∫¶ÔºåÊâÄ‰ª•$T_x^{(i)}$Â∞±‰ª£Ë°®Á¨¨‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑËæìÂÖ•Â∫èÂàóÈïøÂ∫¶„ÄÇÂêåÊ†∑$y^{(i)}$‰ª£Ë°®Á¨¨i‰∏™ËÆ≠ÁªÉÊ†∑Êú¨‰∏≠Á¨¨t‰∏™ÂÖÉÁ¥†Ôºå$T_y^{(i)}$Â∞±ÊòØÁ¨¨i‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑËæìÂá∫Â∫èÂàóÁöÑÈïøÂ∫¶„ÄÇ È¢ÑÂÖàÊúâ‰∏Ä‰∏™ËØçÂÖ∏ L3 : Recurrent Neural Network Model (Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÊ®°Âûã)Áé∞Âú®Êàë‰ª¨ËÆ®ËÆ∫‰∏Ä‰∏ãÊÄéÊ†∑ÊâçËÉΩÂª∫Á´ã‰∏Ä‰∏™Ê®°ÂûãÔºåÂª∫Á´ã‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÊù•Â≠¶‰π†XÂà∞YÁöÑÊò†Â∞Ñ $a^{}$ÈÄöÂ∏∏ ÊòØÈõ∂ÂêëÈáè NÊ®°ÂûãÂåÖÂê´‰∏âÁ±ªÊùÉÈáçÁ≥ªÊï∞ÔºåÂàÜÂà´ÊòØWaxÔºåWaaÔºåWya„ÄÇ‰∏î‰∏çÂêåÂÖÉÁ¥†‰πãÈó¥Âêå‰∏Ä‰ΩçÁΩÆÂÖ±‰∫´Âêå‰∏ÄÊùÉÈáçÁ≥ªÊï∞„ÄÇ RNNÁöÑÊ≠£Âêë‰º†Êí≠ÔºàForward PropagationÔºâËøáÁ®ã‰∏∫Ôºö Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÁî®ÁöÑÊøÄÊ¥ªÂáΩÊï∞ÁªèÂ∏∏ÊòØtanhÔºå‰∏çËøáÊúâÊó∂ÂÄô‰πü‰ºöÁî®ReLUÔºå‰ΩÜÊòØtanhÊòØÊõ¥ÈÄöÂ∏∏ÁöÑÈÄâÊã©ÔºåÊàë‰ª¨ÊúâÂÖ∂‰ªñÊñπÊ≥ïÊù•ÈÅøÂÖçÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÔºåÊàë‰ª¨Â∞ÜÂú®‰πãÂêéËøõË°åËÆ≤Ëø∞„ÄÇÈÄâÁî®Âì™‰∏™ÊøÄÊ¥ªÂáΩÊï∞ÊòØÂèñÂÜ≥‰∫é‰Ω†ÁöÑËæìÂá∫yÔºåÂ¶ÇÊûúÂÆÉÊòØ‰∏Ä‰∏™‰∫åÂàÜÈóÆÈ¢òÔºåÈÇ£‰πàÊàëÁåú‰Ω†‰ºöÁî®sigmoidÂáΩÊï∞‰Ωú‰∏∫ÊøÄÊ¥ªÂáΩÊï∞ÔºåÂ¶ÇÊûúÊòØkÁ±ªÂà´ÂàÜÁ±ªÈóÆÈ¢òÁöÑËØùÔºåÈÇ£‰πàÂèØ‰ª•ÈÄâÁî®softmax‰Ωú‰∏∫ÊøÄÊ¥ªÂáΩÊï∞„ÄÇ‰∏çËøáËøôÈáåÊøÄÊ¥ªÂáΩÊï∞ÁöÑÁ±ªÂûãÂèñÂÜ≥‰∫é‰Ω†Êúâ‰ªÄ‰πàÊ†∑Á±ªÂûãÁöÑËæìÂá∫yÔºåÂØπ‰∫éÂëΩÂêçÂÆû‰ΩìËØÜÂà´Êù•ËØ¥yÂè™ÂèØËÉΩÊòØ0ÊàñËÄÖ1ÔºåÈÇ£ÊàëÁåúËøôÈáåÁ¨¨‰∫å‰∏™ÊøÄÊ¥ªÂáΩÊï∞gÂèØ‰ª•ÊòØsigmoidÊøÄÊ¥ªÂáΩÊï∞„ÄÇ c4: Backpropagation through time ( ÈÄöËøáÊó∂Èó¥ÁöÑÂèçÂêë‰º†Êí≠) ÂèÇÊï∞ÁöÑÂÖ≥Á≥ª* Âçï‰∏™ÂÖÉÁ¥†ÁöÑLoss function: ËØ•Ê†∑Êú¨ÊâÄÊúâÂÖÉÁ¥†ÁöÑLoss function‰∏∫Ôºö ÁÑ∂ÂêéÔºåÂèçÂêë‰º†Êí≠ÔºàBackpropagationÔºâËøáÁ®ãÂ∞±ÊòØ‰ªéÂè≥Âà∞Â∑¶ÂàÜÂà´ËÆ°ÁÆóL(y^,y)ÂØπÂèÇÊï∞WaÔºåWyÔºåbaÔºåbyÁöÑÂÅèÂØºÊï∞„ÄÇÊÄùË∑Ø‰∏éÂÅöÊ≥ï‰∏éÊ†áÂáÜÁöÑÁ•ûÁªèÁΩëÁªúÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ‰∏ÄËà¨ÂèØ‰ª•ÈÄöËøáÊàêÁÜüÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂Ëá™Âä®Ê±ÇÂØºÔºå‰æãÂ¶ÇPyTorch„ÄÅTensorflowÁ≠â„ÄÇËøôÁßç‰ªéÂè≥Âà∞Â∑¶ÁöÑÊ±ÇÂØºËøáÁ®ãË¢´Áß∞‰∏∫Backpropagation through time L5: Different types of RNNs (‰∏çÂêåÁ±ªÂûãÁöÑÂæ™ÁéØÁ•ûÁªèÁΩëÁªú) L6 : Language model and sequence generation (ËØ≠Ë®ÄÊ®°ÂûãÂíåÂ∫èÂàóÁîüÊàê) L7 : Sampling novel sequences (ÂØπÊñ∞Â∫èÂàóÈááÊ†∑) Vanishing gradients with RNNs (Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÁöÑÊ¢ØÂ∫¶Ê∂àÂ§±)È¶ñÂÖà‰ªéÂ∑¶Âà∞Âè≥ÂâçÂêë‰º†Êí≠ÔºåÁÑ∂ÂêéÂèçÂêë‰º†Êí≠„ÄÇ‰ΩÜÊòØÂèçÂêë‰º†Êí≠‰ºöÂæàÂõ∞ÈöæÔºåÂõ†‰∏∫ÂêåÊ†∑ÁöÑÊ¢ØÂ∫¶Ê∂àÂ§±ÁöÑÈóÆÈ¢òÔºåÂêéÈù¢Â±ÇÁöÑËæìÂá∫ËØØÂ∑ÆÔºà‰∏äÂõæÁºñÂè∑6ÊâÄÁ§∫ÔºâÂæàÈöæÂΩ±ÂìçÂâçÈù¢Â±ÇÔºà‰∏äÂõæÁºñÂè∑7ÊâÄÁ§∫ÁöÑÂ±ÇÔºâÁöÑËÆ°ÁÆó„ÄÇËøôÂ∞±ÊÑèÂë≥ÁùÄÔºåÂÆûÈôÖ‰∏äÂæàÈöæËÆ©‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúËÉΩÂ§üÊÑèËØÜÂà∞ÂÆÉË¶ÅËÆ∞‰ΩèÁúãÂà∞ÁöÑÊòØÂçïÊï∞ÂêçËØçËøòÊòØÂ§çÊï∞ÂêçËØçÔºåÁÑ∂ÂêéÂú®Â∫èÂàóÂêéÈù¢ÁîüÊàê‰æùËµñÂçïÂ§çÊï∞ÂΩ¢ÂºèÁöÑwasÊàñËÄÖwere„ÄÇËÄå‰∏îÂú®Ëã±ËØ≠ÈáåÈù¢ÔºåËøô‰∏≠Èó¥ÁöÑÂÜÖÂÆπÔºà‰∏äÂõæÁºñÂè∑8ÊâÄÁ§∫ÔºâÂèØ‰ª•‰ªªÊÑèÈïøÔºåÂØπÂêßÔºüÊâÄ‰ª•‰Ω†ÈúÄË¶ÅÈïøÊó∂Èó¥ËÆ∞‰ΩèÂçïËØçÊòØÂçïÊï∞ËøòÊòØÂ§çÊï∞ÔºåËøôÊ†∑ÂêéÈù¢ÁöÑÂè•Â≠êÊâçËÉΩÁî®Âà∞Ëøô‰∫õ‰ø°ÊÅØ„ÄÇ‰πüÊ≠£ÊòØËøô‰∏™ÂéüÂõ†ÔºåÊâÄ‰ª•Âü∫Êú¨ÁöÑRNNÊ®°Âûã‰ºöÊúâÂæàÂ§öÂ±ÄÈÉ®ÂΩ±Âìç http://www.ai-start.com/dl2017/html/lesson5-week1.html https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484029&amp;idx=1&amp;sn=c93b5eddec33dc29dc172a5ea0d76822&amp;chksm=976fa7e0a0182ef61e36d1c32aa0706c4e81e1762a7ee2554165beecde929b72cf026c5b7a64&amp;scene=21#wechat_redirect]]></content>
      <categories>
        <category>Â≠¶‰π†„ÅÆÂéÜÁ®ã(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>ÊàëÁöÑËØª‰π¶Á¨îËÆ∞</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F07%2F%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[: [TOC] Âà©Áî®Âê¨ÂäõÊùêÊñôÂ≠¶Ëã±ËØ≠ÊúÄÈ´òÊïàÊúâÊïàËæìÂá∫Â∫îËØ•ÊòØÁ®çÈ´ò‰∫éÁé∞ÊúâÊ∞¥Âπ≥ÁöÑ„ÄÅÊõ¥ÂáÜÁ°Æ„ÄÅÊõ¥Âæó‰ΩìÁöÑË°®ËææËæìÂá∫ ÂÖàËÆ∞‰Ωè‰ª•‰∏ã‰∏§ÁÇπÔºå‰∏ãÈù¢Êàë‰ª¨ÂÜç‰∏Ä‰∏ÄËß£ÊûêÂ∫îËØ•Ë¶ÅÊÄé‰πàÂÅöÂà∞„ÄÇ 1. Â≠¶‰π†ÈúÄË¶ÅÂèçÈ¶àÔºåÊù•ÂëäËØâÊàë‰ª¨ËøôËæìÂá∫ÊòØÊ≠£Á°ÆÁöÑÔºåÂèØ‰ª•ÁªßÁª≠ÔºõÊàñËÄÖÊòØÈîôËØØÁöÑÔºåÈúÄË¶Å‰øÆÊ≠£„ÄÇ 2. Â∞ÜËæìÂÖ•ÂÜÖÂåñÔºåÂèòÊàêËá™Â∑±ÁöÑÁü•ËØÜ„ÄÇ Êàë‰∏ÄËà¨‰ºöÂê¨ÂõõÈÅçÔºå‰ΩÜ‰∏çÊòØÂÖ®Âê¨ÂéüÊñá„ÄÇ Á¨¨‰∏ÄÈÅçÔºöÊ≥õÂê¨ÂéüÊñáÔºå‰∏çË¶ÅÁúãÂ≠óÂπïÊàñËÑöÊú¨ÔºåÊ∏ÖÊ•öÂΩïÈü≥ÁöÑÂÜÖÂÆπ„ÄÇÂê¨Á¨¨‰∏ÄÈÅçÁöÑÊó∂ÂÄôÊàëÈÄöÂ∏∏ËøûÁ¨îËÆ∞ÈÉΩ‰∏çÂÅöÔºåÁõÆÁöÑÊòØ‰∏∫‰∫ÜËÆ©Ëá™Â∑±ÊµÅÁïÖÁöÑÂê¨ÂÆåÔºåÂØπÂê¨ÂäõÁöÑÂÜÖÂÆπÊúâ‰∏Ä‰∏™Êï¥‰ΩìÁöÑÊéåÊè°„ÄÇ Á¨¨‰∫åÈÅçÔºöÂÖàÂê¨ÂéüÊñáÔºåÊ†πÊçÆÂÜÖÂÆπÊÆµËêΩÔºåÂºÄÂßãÂ§çËø∞ÂÜÖÂÆπÔºåÂπ∂ÂΩïÈü≥„ÄÇËøô‰∏ÄÊ≠•ÁöÑÁõÆÁöÑÊòØÂº∫Ëø´Ëá™Â∑±Ë∞ÉÁî®Â∑≤ÁªèÂ≠¶ËøáÁöÑÁü•ËØÜÔºåÁªÑÁªáËØ≠Ë®ÄÂíåËøõË°åÁªÉ‰π†„ÄÇ Á¨¨‰∏âÈÅçÔºåÂê¨Ëá™Â∑±ÁöÑÂΩïÈü≥ÔºåÁÑ∂ÂêéÂÅöÂá∫‰øÆÊ≠£„ÄÇËøô‰∏ÄÊ≠•ÂæàÈáçË¶ÅÔºåÂèØ‰ª•ËÆ©‰Ω†‰∫ÜËß£Ëá™Â∑±ÁöÑÂèëÈü≥ÈóÆÈ¢òÂíåËØ≠Ê≥ïÈóÆÈ¢òÔºåÂπ∂ÊääÂèØÂê¨Âá∫Êù•ÁöÑËØ≠Ê≥ïÈóÆÈ¢òËøõË°å‰øÆÊîπ„ÄÇÈÄöËøáËøô‰∏ÄÊ≠•ÔºåÊàë‰ª¨Â∞±ÂèØ‰ª•ÊääÁÆÄÂçïÈáçÂ§çËæìÂÖ•ÁöÑËØ≠Ë®ÄÊùêÊñôÔºåËΩ¨Âåñ‰∏∫ÊúâÊïàËæìÂá∫„ÄÇ Á¨¨ÂõõÈÅçÔºåÂê¨ÂéüÊñáÁúãÂ≠óÂπïÂíåËÑöÊú¨ÔºåÁúãÊääÂê¨‰∏çÊáÇÁöÑÂú∞ÊñπÊ†áÊ≥®ÔºåËØ¥Êòé‰∏∫‰ªÄ‰πàÂê¨‰∏çÊáÇÔºàÊØîÂ¶ÇÊòØÂõ†‰∏∫Ëá™Â∑±ÂèëÈü≥‰∏çÂáÜÂØºËá¥ÁöÑÂê¨‰∏çÂá∫ÔºåÊàñËÄÖÂ∞±ÊòØÂõ†‰∏∫Ëøô‰∏™ËØçÊ≤°ËÉåËøá„ÄÅ‰∏çÁÜüÊÇâÔºâ„ÄÇ‰∏çÁÜüÊÇâÁöÑÁî®Ê≥ïÂíåËá™Â∑±Áî®ÈîôÁöÑÂú∞ÊñπÊÄªÁªìÔºåËÉå‰∏ãÊù•Ôºå‰∏ãÊ¨°ËØïÁùÄÁî®„ÄÇ TipsÔºö ‰∏çË¶ÅÈÄâÊã©Â§™ÈöæÁöÑÊùêÊñôÔºåÂ§™ÈöæÁöÑÊùêÊñôÂÆπÊòì‰ΩøËá™Â∑±‰∏ßÂ§±Â≠¶‰π†ÂÖ¥Ë∂£„ÄÇ ‰∏ÄÂºÄÂßãÔºå‰∏çË¶ÅÈÄâÊã©Â§™ÈïøÁöÑÂê¨ÂäõÊùêÊñô„ÄÇ10ÂàÜÈíüÂ∑¶Âè≥ÊúÄ‰Ω≥„ÄÇÂú®ËøôÈáåÊé®ËçêtedÔºåÂèØ‰ª•ÈÄâÊã©ÊúâÂ≠óÂπïÊàñÂÖ≥Èó≠Â≠óÂπï„ÄÇ Âú®‰∏Ä‰∏™Áõ∏ËøëÁöÑÊó∂Èó¥ÊÆµÂÜÖÔºåÈÄâÊã©Áõ∏ËøëÈ¢òÊùêÁöÑÊùêÊñô„ÄÇÊØîÂ¶ÇÊàë‰ºöÂú®‰∏§‰∏™ÊòüÊúüÂÜÖÈÄâÊã©‚ÄúÂøÉÁêÜ‚ÄùÈ¢òÊùêÁöÑÂΩïÈü≥„ÄÇËøôÊ†∑ÊàëÂ∞±‰ºöÊúâÊõ¥Â§ßÁöÑÂá†ÁéáÁî®‰∏äÂàöÂ≠¶ËøáÁöÑÁªìÊûÑÂíåËØçÊ±á„ÄÇ ÂèäÊó∂ÊÄªÁªìÔºåÂèäÊó∂Â§ç‰π†Â∑≤ËÉåËøáÁöÑÊùêÊñôÔºåÂ§ç‰π†ÁöÑÈáçË¶ÅÊÄßÂ§ßÂÆ∂ÈÉΩÊáÇÔºåËøôÈáåÂ∞±‰∏çÂ§öËØ¥‰∫Ü„ÄÇ ÁúüÈ¢òÂê¨ÂÜô ÊùêÊñôÈÄâÊã© ËÉΩÂ§üÂê¨ÂæóÊáÇ 70%ÁöÑÊùêÊñô 2 ÂÖ∑‰ΩìÊâßË°åÊñπÊ≥ï ÂÖàÊ≥õÂê¨‰∏ÄÁØá ÂÜçÂæ™ÁéØÂê¨Âá†ÈÅç ÂÜçÈÄêÂè•ÈÄêÂè•ÁöÑÂê¨ ÁæéÂâßÁ≤æÂê¨ ÂÖàÁúã‰∏≠ÊñáÂê¨ Ëã±ÊñáÔºåÊü• Âê¨Êâæ Âè∞ËØçÔºåË∑üËØª ÈáçÂ§ç‰∏âÂõõËá≥Â∞ë10]]></content>
  </entry>
  <entry>
    <title><![CDATA[deeplearningvideo]]></title>
    <url>%2F2019%2F04%2F03%2Fdeeplearningvideo%2F</url>
    <content type="text"><![CDATA[CourseraÊ∑±Â∫¶Â≠¶‰π†ÊïôÁ®ã‰∏≠ÊñáÁ¨îËÆ∞ ËØæÁ®ãÊ¶ÇËø∞ https://mooc.study.163.com/university/deeplearning_ai#/c Ëøô‰∫õËØæÁ®ã‰∏ì‰∏∫Â∑≤Êúâ‰∏ÄÂÆöÂü∫Á°ÄÔºàÂü∫Êú¨ÁöÑÁºñÁ®ãÁü•ËØÜÔºåÁÜüÊÇâPython„ÄÅÂØπÊú∫Âô®Â≠¶‰π†ÊúâÂü∫Êú¨‰∫ÜËß£ÔºâÔºåÊÉ≥Ë¶ÅÂ∞ùËØïËøõÂÖ•‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÁöÑËÆ°ÁÆóÊú∫‰∏ì‰∏ö‰∫∫Â£´ÂáÜÂ§á„ÄÇ‰ªãÁªçÊòæÁ§∫Ôºö‚ÄúÊ∑±Â∫¶Â≠¶‰π†ÊòØÁßëÊäÄ‰∏öÊúÄÁÉ≠Èó®ÁöÑÊäÄËÉΩ‰πã‰∏ÄÔºåÊú¨ËØæÁ®ãÂ∞ÜÂ∏Æ‰Ω†ÊéåÊè°Ê∑±Â∫¶Â≠¶‰π†„ÄÇ‚Äù Âú®Ëøô5Â†ÇËØæ‰∏≠ÔºåÂ≠¶ÁîüÂ∞ÜÂèØ‰ª•Â≠¶‰π†Âà∞Ê∑±Â∫¶Â≠¶‰π†ÁöÑÂü∫Á°ÄÔºåÂ≠¶‰ºöÊûÑÂª∫Á•ûÁªèÁΩëÁªúÔºåÂπ∂Áî®Âú®ÂåÖÊã¨Âê¥ÊÅ©ËææÊú¨‰∫∫Âú®ÂÜÖÁöÑÂ§ö‰Ωç‰∏öÁïåÈ°∂Â∞ñ‰∏ìÂÆ∂ÊåáÂØº‰∏ãÂàõÂª∫Ëá™Â∑±ÁöÑÊú∫Âô®Â≠¶‰π†È°πÁõÆ„ÄÇDeep Learning SpecializationÂØπÂç∑ÁßØÁ•ûÁªèÁΩëÁªú (CNN)„ÄÅÈÄíÂΩíÁ•ûÁªèÁΩëÁªú (RNN)„ÄÅÈïøÁü≠ÊúüËÆ∞ÂøÜ (LSTM) Á≠âÊ∑±Â∫¶Â≠¶‰π†Â∏∏Áî®ÁöÑÁΩëÁªúÁªìÊûÑ„ÄÅÂ∑•ÂÖ∑ÂíåÁü•ËØÜÈÉΩÊúâÊ∂âÂèä„ÄÇ Á¨îËÆ∞ÊòØÊ†πÊçÆËßÜÈ¢ëÂíåÂ≠óÂπïÂÜôÁöÑÔºåÊ≤°ÊúâÊäÄÊúØÂê´ÈáèÔºåÂè™ÈúÄË¶Å‰∏ìÊ≥®Âíå‰∏•Ë∞®„ÄÇ 2018-04-14 Êú¨ËØæÁ®ãËßÜÈ¢ëÊïôÁ®ãÂú∞ÂùÄÔºöhttps://mooc.study.163.com/university/deeplearning_ai#/c ÔºàËØ•ËßÜÈ¢ë‰ªéwww.deeplearning.ai ÁΩëÁ´ô‰∏ãËΩΩÔºåÂõ†‰ºóÊâÄÂë®Áü•ÁöÑÂéüÂõ†ÔºåÂõΩÂÜÖÁî®Êà∑ËßÇÁúãÊüê‰∫õÂú®Á∫øËßÜÈ¢ëÈùûÂ∏∏‰∏çÂÆπÊòìÔºåÊïÖ‰∏Ä‰∫õÂ≠¶ËÄÖ‰∏ÄËµ∑Âà∂‰Ωú‰∫ÜÁ¶ªÁ∫øËßÜÈ¢ëÔºåÊó®Âú®Êñπ‰æøÂõΩÂÜÖÁî®Êà∑‰∏™‰∫∫Â≠¶‰π†‰ΩøÁî®ÔºåËØ∑ÂãøÁî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇËßÜÈ¢ëÂÜÖÂµå‰∏≠Ëã±ÊñáÂ≠óÂπïÔºåÊé®Ëçê‰ΩøÁî®potplayerÊí≠Êîæ„ÄÇÁâàÊùÉÂ±û‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏àÊâÄÊúâÔºåËã•Âú®Á∫øËßÜÈ¢ëÊµÅÁïÖÔºåËØ∑Âà∞ÂÆòÊñπÁΩëÁ´ôËßÇÁúã„ÄÇÔºâ Á¨îËÆ∞ÁΩëÁ´ô(ÈÄÇÂêàÊâãÊú∫ÈòÖËØª) Âê¥ÊÅ©ËææËÄÅÂ∏àÁöÑÊú∫Âô®Â≠¶‰π†ËØæÁ®ãÁ¨îËÆ∞ÂíåËßÜÈ¢ëÔºöhttps://github.com/fengdu78/Coursera-ML-AndrewNg-Notes Ê∑±Â∫¶Â≠¶‰π†Á¨îËÆ∞ÁõÆÂΩïÁ¨¨‰∏ÄÈó®ËØæ Á•ûÁªèÁΩëÁªúÂíåÊ∑±Â∫¶Â≠¶‰π†(Neural Networks and Deep Learning)Á¨¨‰∏ÄÂë®ÔºöÊ∑±Â∫¶Â≠¶‰π†ÂºïË®Ä(Introduction to Deep Learning) 1.1 Ê¨¢Ëøé(Welcome) 1.2 ‰ªÄ‰πàÊòØÁ•ûÁªèÁΩëÁªúÔºü(What is a Neural Network) 1.3 Á•ûÁªèÁΩëÁªúÁöÑÁõëÁù£Â≠¶‰π†(Supervised Learning with Neural Networks) 1.4 ‰∏∫‰ªÄ‰πàÁ•ûÁªèÁΩëÁªú‰ºöÊµÅË°åÔºü(Why is Deep Learning taking off?) 1.5 ÂÖ≥‰∫éÊú¨ËØæÁ®ã(About this Course) 1.6 ËØæÁ®ãËµÑÊ∫ê(Course Resources) 1.7 Geoffery Hinton ‰∏ìËÆø(Geoffery Hinton interview) Á¨¨‰∫åÂë®ÔºöÁ•ûÁªèÁΩëÁªúÁöÑÁºñÁ®ãÂü∫Á°Ä(Basics of Neural Network programming) 2.1 ‰∫åÂàÜÁ±ª(Binary Classification) 2.2 ÈÄªËæëÂõûÂΩí(Logistic Regression) 2.3 ÈÄªËæëÂõûÂΩíÁöÑ‰ª£‰ª∑ÂáΩÊï∞ÔºàLogistic Regression Cost FunctionÔºâ 2.4 Ê¢ØÂ∫¶‰∏ãÈôçÔºàGradient DescentÔºâ 2.5 ÂØºÊï∞ÔºàDerivativesÔºâ 2.6 Êõ¥Â§öÁöÑÂØºÊï∞‰æãÂ≠êÔºàMore Derivative ExamplesÔºâ 2.7 ËÆ°ÁÆóÂõæÔºàComputation GraphÔºâ 2.8 ËÆ°ÁÆóÂõæÂØºÊï∞ÔºàDerivatives with a Computation GraphÔºâ 2.9 ÈÄªËæëÂõûÂΩíÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÔºàLogistic Regression Gradient DescentÔºâ 2.10 Ê¢ØÂ∫¶‰∏ãÈôçÁöÑ‰æãÂ≠ê(Gradient Descent on m Examples) 2.11 ÂêëÈáèÂåñ(Vectorization) 2.12 Êõ¥Â§öÁöÑÂêëÈáèÂåñ‰æãÂ≠êÔºàMore Examples of VectorizationÔºâ 2.13 ÂêëÈáèÂåñÈÄªËæëÂõûÂΩí(Vectorizing Logistic Regression) 2.14 ÂêëÈáèÂåñÈÄªËæëÂõûÂΩíÁöÑÊ¢ØÂ∫¶ËÆ°ÁÆóÔºàVectorizing Logistic Regression‚Äôs GradientÔºâ 2.15 Python‰∏≠ÁöÑÂπøÊí≠Êú∫Âà∂ÔºàBroadcasting in PythonÔºâ 2.16 ÂÖ≥‰∫é Python‰∏énumpyÂêëÈáèÁöÑ‰ΩøÁî®ÔºàA note on python or numpy vectorsÔºâ 2.17 Jupyter/iPython NotebooksÂø´ÈÄüÂÖ•Èó®ÔºàQuick tour of Jupyter/iPython NotebooksÔºâ 2.18 ÈÄªËæëÂõûÂΩíÊçüÂ§±ÂáΩÊï∞ËØ¶Ëß£ÔºàExplanation of logistic regression cost functionÔºâ Á¨¨‰∏âÂë®ÔºöÊµÖÂ±ÇÁ•ûÁªèÁΩëÁªú(Shallow neural networks) 3.1 Á•ûÁªèÁΩëÁªúÊ¶ÇËø∞ÔºàNeural Network OverviewÔºâ 3.2 Á•ûÁªèÁΩëÁªúÁöÑË°®Á§∫ÔºàNeural Network RepresentationÔºâ 3.3 ËÆ°ÁÆó‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÁöÑËæìÂá∫ÔºàComputing a Neural Network‚Äôs outputÔºâ 3.4 Â§öÊ†∑Êú¨ÂêëÈáèÂåñÔºàVectorizing across multiple examplesÔºâ 3.5 ÂêëÈáèÂåñÂÆûÁé∞ÁöÑËß£ÈáäÔºàJustification for vectorized implementationÔºâ 3.6 ÊøÄÊ¥ªÂáΩÊï∞ÔºàActivation functionsÔºâ 3.7 ‰∏∫‰ªÄ‰πàÈúÄË¶ÅÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÔºüÔºàwhy need a nonlinear activation function?Ôºâ 3.8 ÊøÄÊ¥ªÂáΩÊï∞ÁöÑÂØºÊï∞ÔºàDerivatives of activation functionsÔºâ 3.9 Á•ûÁªèÁΩëÁªúÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÔºàGradient descent for neural networksÔºâ 3.10ÔºàÈÄâ‰øÆÔºâÁõ¥ËßÇÁêÜËß£ÂèçÂêë‰º†Êí≠ÔºàBackpropagation intuitionÔºâ 3.11 ÈöèÊú∫ÂàùÂßãÂåñÔºàRandom+InitializationÔºâ Á¨¨ÂõõÂë®ÔºöÊ∑±Â±ÇÁ•ûÁªèÁΩëÁªú(Deep Neural Networks) 4.1 Ê∑±Â±ÇÁ•ûÁªèÁΩëÁªúÔºàDeep L-layer neural networkÔºâ 4.2 ÂâçÂêë‰º†Êí≠ÂíåÂèçÂêë‰º†Êí≠ÔºàForward and backward propagationÔºâ 4.3 Ê∑±Â±ÇÁΩëÁªú‰∏≠ÁöÑÂâçÂêëÂíåÂèçÂêë‰º†Êí≠ÔºàForward propagation in a Deep NetworkÔºâ 4.4 Ê†∏ÂØπÁü©ÈòµÁöÑÁª¥Êï∞ÔºàGetting your matrix dimensions rightÔºâ 4.5 ‰∏∫‰ªÄ‰πà‰ΩøÁî®Ê∑±Â±ÇË°®Á§∫ÔºüÔºàWhy deep representations?Ôºâ 4.6 Êê≠Âª∫Á•ûÁªèÁΩëÁªúÂùóÔºàBuilding blocks of deep neural networksÔºâ 4.7 ÂèÇÊï∞VSË∂ÖÂèÇÊï∞ÔºàParameters vs HyperparametersÔºâ 4.8 Ê∑±Â∫¶Â≠¶‰π†ÂíåÂ§ßËÑëÁöÑÂÖ≥ËÅîÊÄßÔºàWhat does this have to do with the brain?Ôºâ Á¨¨‰∫åÈó®ËØæ ÊîπÂñÑÊ∑±Â±ÇÁ•ûÁªèÁΩëÁªúÔºöË∂ÖÂèÇÊï∞Ë∞ÉËØï„ÄÅÊ≠£ÂàôÂåñ‰ª•Âèä‰ºòÂåñ(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)Á¨¨‰∏ÄÂë®ÔºöÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂÆûÁî®Â±ÇÈù¢(Practical aspects of Deep Learning) 1.1 ËÆ≠ÁªÉÔºåÈ™åËØÅÔºåÊµãËØïÈõÜÔºàTrain / Dev / Test setsÔºâ 1.2 ÂÅèÂ∑ÆÔºåÊñπÂ∑ÆÔºàBias /VarianceÔºâ 1.3 Êú∫Âô®Â≠¶‰π†Âü∫Á°ÄÔºàBasic Recipe for Machine LearningÔºâ 1.4 Ê≠£ÂàôÂåñÔºàRegularizationÔºâ 1.5 ‰∏∫‰ªÄ‰πàÊ≠£ÂàôÂåñÊúâÂà©‰∫éÈ¢ÑÈò≤ËøáÊãüÂêàÂë¢ÔºüÔºàWhy regularization reduces overfitting?Ôºâ 1.6 dropout Ê≠£ÂàôÂåñÔºàDropout RegularizationÔºâ 1.7 ÁêÜËß£ dropoutÔºàUnderstanding DropoutÔºâ 1.8 ÂÖ∂‰ªñÊ≠£ÂàôÂåñÊñπÊ≥ïÔºàOther regularization methodsÔºâ 1.9 Ê†áÂáÜÂåñËæìÂÖ•ÔºàNormalizing inputsÔºâ 1.10 Ê¢ØÂ∫¶Ê∂àÂ§±/Ê¢ØÂ∫¶ÁàÜÁÇ∏ÔºàVanishing / Exploding gradientsÔºâ 1.11 Á•ûÁªèÁΩëÁªúÁöÑÊùÉÈáçÂàùÂßãÂåñÔºàWeight Initialization for Deep NetworksVanishing /Exploding gradientsÔºâ 1.12 Ê¢ØÂ∫¶ÁöÑÊï∞ÂÄºÈÄºËøëÔºàNumerical approximation of gradientsÔºâ 1.13 Ê¢ØÂ∫¶Ê£ÄÈ™åÔºàGradient checkingÔºâ 1.14 Ê¢ØÂ∫¶Ê£ÄÈ™åÂ∫îÁî®ÁöÑÊ≥®ÊÑè‰∫ãÈ°πÔºàGradient Checking Implementation NotesÔºâ Á¨¨‰∫åÂë®Ôºö‰ºòÂåñÁÆóÊ≥ï (Optimization algorithms) 2.1 Mini-batch Ê¢ØÂ∫¶‰∏ãÈôçÔºàMini-batch gradient descentÔºâ 2.2 ÁêÜËß£Mini-batch Ê¢ØÂ∫¶‰∏ãÈôçÔºàUnderstanding Mini-batch gradient descentÔºâ 2.3 ÊåáÊï∞Âä†ÊùÉÂπ≥ÂùáÔºàExponentially weighted averagesÔºâ 2.4 ÁêÜËß£ÊåáÊï∞Âä†ÊùÉÂπ≥ÂùáÔºàUnderstanding Exponentially weighted averagesÔºâ 2.5 ÊåáÊï∞Âä†ÊùÉÂπ≥ÂùáÁöÑÂÅèÂ∑Æ‰øÆÊ≠£ÔºàBias correction in exponentially weighted averagesÔºâ 2.6 momentumÊ¢ØÂ∫¶‰∏ãÈôçÔºàGradient descent with momentumÔºâ 2.7 RMSprop‚Äî‚Äîroot mean square propÔºàRMSpropÔºâ 2.8 Adam‰ºòÂåñÁÆóÊ≥ïÔºàAdam optimization algorithmÔºâ 2.9 Â≠¶‰π†ÁéáË°∞ÂáèÔºàLearning rate decayÔºâ 2.10 Â±ÄÈÉ®ÊúÄ‰ºòÈóÆÈ¢òÔºàThe problem of local optimaÔºâ Á¨¨‰∏âÂë®Ë∂ÖÂèÇÊï∞Ë∞ÉËØïÔºåbatchÊ≠£ÂàôÂåñÂíåÁ®ãÂ∫èÊ°ÜÊû∂ÔºàHyperparameter tuning, Batch Normalization and Programming Frameworks) 3.1 Ë∞ÉËØïÂ§ÑÁêÜÔºàTuning processÔºâ 3.2 ‰∏∫Ë∂ÖÂèÇÊï∞ÈÄâÊã©ÂíåÈÄÇÂêàËåÉÂõ¥ÔºàUsing an appropriate scale to pick hyperparametersÔºâ 3.3 Ë∂ÖÂèÇÊï∞ËÆ≠ÁªÉÁöÑÂÆûË∑µÔºöPandas vs. CaviarÔºàHyperparameters tuning in practice: Pandas vs. CaviarÔºâ 3.4 ÁΩëÁªú‰∏≠ÁöÑÊ≠£ÂàôÂåñÊøÄÊ¥ªÂáΩÊï∞ÔºàNormalizing activations in a networkÔºâ 3.5 Â∞Ü Batch NormÊãüÂêàËøõÁ•ûÁªèÁΩëÁªúÔºàFitting Batch Norm into a neural networkÔºâ 3.6 ‰∏∫‰ªÄ‰πàBatch NormÂ•èÊïàÔºüÔºàWhy does Batch Norm work?Ôºâ 3.7 ÊµãËØïÊó∂ÁöÑBatch NormÔºàBatch Norm at test timeÔºâ 3.8 Softmax ÂõûÂΩíÔºàSoftmax RegressionÔºâ 3.9 ËÆ≠ÁªÉ‰∏Ä‰∏™Softmax ÂàÜÁ±ªÂô®ÔºàTraining a softmax classifierÔºâ 3.10 Ê∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂ÔºàDeep learning frameworksÔºâ 3.11 TensorFlowÔºàTensorFlowÔºâ Á¨¨‰∏âÈó®ËØæ ÁªìÊûÑÂåñÊú∫Âô®Â≠¶‰π†È°πÁõÆ (Structuring Machine Learning Projects)Á¨¨‰∏ÄÂë®ÔºöÊú∫Âô®Â≠¶‰π†Á≠ñÁï•Ôºà1Ôºâ(ML Strategy (1)) 1.1 ‰∏∫‰ªÄ‰πàÊòØMLÁ≠ñÁï•Ôºü (Why ML Strategy) 1.2 Ê≠£‰∫§Âåñ(Orthogonalization) 1.3 Âçï‰∏ÄÊï∞Â≠óËØÑ‰º∞ÊåáÊ†á(Single number evaluation metric) 1.4 Êª°Ë∂≥Âíå‰ºòÂåñÊåáÊ†á (Satisficing and Optimizing metric) 1.5 ËÆ≠ÁªÉÈõÜ„ÄÅÂºÄÂèëÈõÜ„ÄÅÊµãËØïÈõÜÁöÑÂàíÂàÜ(Train/dev/test distributions) 1.6 ÂºÄÂèëÈõÜÂíåÊµãËØïÈõÜÁöÑÂ§ßÂ∞è (Size of the dev and test sets) 1.7 ‰ªÄ‰πàÊó∂ÂÄôÊîπÂèòÂºÄÂèëÈõÜ/ÊµãËØïÈõÜÂíåËØÑ‰º∞ÊåáÊ†á(When to change dev/test sets and metrics) 1.8 ‰∏∫‰ªÄ‰πàÊòØ‰∫∫ÁöÑË°®Áé∞ (Why human-level performance?) 1.9 ÂèØÈÅøÂÖçÂÅèÂ∑Æ(Avoidable bias) 1.10 ÁêÜËß£‰∫∫Á±ªÁöÑË°®Áé∞ (Understanding human-level performance) 1.11 Ë∂ÖËøá‰∫∫Á±ªÁöÑË°®Áé∞(Surpassing human-level performance) 1.12 ÊîπÂñÑ‰Ω†ÁöÑÊ®°ÂûãË°®Áé∞ (Improving your model performance) Á¨¨‰∫åÂë®ÔºöÊú∫Âô®Â≠¶‰π†Á≠ñÁï•Ôºà2Ôºâ(ML Strategy (2)) 2.1 ËØØÂ∑ÆÂàÜÊûê (Carrying out error analysis) 2.2 Ê∏ÖÈô§Ê†áÊ≥®ÈîôËØØÁöÑÊï∞ÊçÆ(Cleaning up incorrectly labeled data) 2.3 Âø´ÈÄüÊê≠Âª∫‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™Á≥ªÁªüÔºåÂπ∂ËøõË°åËø≠‰ª£(Build your first system quickly, then iterate) 2.4 Âú®‰∏çÂêåÁöÑÂàÜÂ∏É‰∏äÁöÑËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ (Training and testing on different distributions) 2.5 Êï∞ÊçÆÂàÜÂ∏É‰∏çÂåπÈÖçÁöÑÂÅèÂ∑Æ‰∏éÊñπÂ∑ÆÂàÜÊûê (Bias and Variance with mismatched data distributions) 2.6 Â§ÑÁêÜÊï∞ÊçÆ‰∏çÂåπÈÖçÈóÆÈ¢ò(Addressing data mismatch) 2.7 ËøÅÁßªÂ≠¶‰π† (Transfer learning) 2.8 Â§ö‰ªªÂä°Â≠¶‰π†(Multi-task learning) 2.9 ‰ªÄ‰πàÊòØÁ´ØÂà∞Á´ØÁöÑÊ∑±Â∫¶Â≠¶‰π†Ôºü (What is end-to-end deep learning?) 2.10 ÊòØÂê¶‰ΩøÁî®Á´ØÂà∞Á´ØÁöÑÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ï (Whether to use end-to-end deep learning) Á¨¨ÂõõÈó®ËØæ Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàConvolutional Neural NetworksÔºâÁ¨¨‰∏ÄÂë® Âç∑ÁßØÁ•ûÁªèÁΩëÁªú(Foundations of Convolutional Neural Networks) 1.1 ËÆ°ÁÆóÊú∫ËßÜËßâÔºàComputer visionÔºâ 1.2 ËæπÁºòÊ£ÄÊµãÁ§∫‰æãÔºàEdge detection exampleÔºâ 1.3 Êõ¥Â§öËæπÁºòÊ£ÄÊµãÂÜÖÂÆπÔºàMore edge detectionÔºâ 1.4 Padding 1.5 Âç∑ÁßØÊ≠•ÈïøÔºàStrided convolutionsÔºâ 1.6 ‰∏âÁª¥Âç∑ÁßØÔºàConvolutions over volumesÔºâ 1.7 ÂçïÂ±ÇÂç∑ÁßØÁΩëÁªúÔºàOne layer of a convolutional networkÔºâ 1.8 ÁÆÄÂçïÂç∑ÁßØÁΩëÁªúÁ§∫‰æãÔºàA simple convolution network exampleÔºâ 1.9 Ê±†ÂåñÂ±ÇÔºàPooling layersÔºâ 1.10 Âç∑ÁßØÁ•ûÁªèÁΩëÁªúÁ§∫‰æãÔºàConvolutional neural network exampleÔºâ 1.11 ‰∏∫‰ªÄ‰πà‰ΩøÁî®Âç∑ÁßØÔºüÔºàWhy convolutions?Ôºâ Á¨¨‰∫åÂë® Ê∑±Â∫¶Âç∑ÁßØÁΩëÁªúÔºöÂÆû‰æãÊé¢Á©∂(Deep convolutional models: case studies) 2.1 ‰∏∫‰ªÄ‰πàË¶ÅËøõË°åÂÆû‰æãÊé¢Á©∂ÔºüÔºàWhy look at case studies?Ôºâ 2.2 ÁªèÂÖ∏ÁΩëÁªúÔºàClassic networksÔºâ 2.3 ÊÆãÂ∑ÆÁΩëÁªúÔºàResidual Networks (ResNets)Ôºâ 2.4 ÊÆãÂ∑ÆÁΩëÁªú‰∏∫‰ªÄ‰πàÊúâÁî®ÔºüÔºàWhy ResNets work?Ôºâ 2.5 ÁΩëÁªú‰∏≠ÁöÑÁΩëÁªú‰ª•Âèä 1√ó1 Âç∑ÁßØÔºàNetwork in Network and 1√ó1 convolutionsÔºâ 2.6 Ë∞∑Ê≠å Inception ÁΩëÁªúÁÆÄ‰ªãÔºàInception network motivationÔºâ 2.7 Inception ÁΩëÁªúÔºàInception networkÔºâ 2.8 ‰ΩøÁî®ÂºÄÊ∫êÁöÑÂÆûÁé∞ÊñπÊ°àÔºàUsing open-source implementationsÔºâ 2.9 ËøÅÁßªÂ≠¶‰π†ÔºàTransfer LearningÔºâ 2.10 Êï∞ÊçÆÊâ©ÂÖÖÔºàData augmentationÔºâ 2.11 ËÆ°ÁÆóÊú∫ËßÜËßâÁé∞Áä∂ÔºàThe state of computer visionÔºâ Á¨¨‰∏âÂë® ÁõÆÊ†áÊ£ÄÊµãÔºàObject detectionÔºâ 3.1 ÁõÆÊ†áÂÆö‰ΩçÔºàObject localizationÔºâ 3.2 ÁâπÂæÅÁÇπÊ£ÄÊµãÔºàLandmark detectionÔºâ 3.3 ÁõÆÊ†áÊ£ÄÊµãÔºàObject detectionÔºâ 3.4 Âç∑ÁßØÁöÑÊªëÂä®Á™óÂè£ÂÆûÁé∞ÔºàConvolutional implementation of sliding windowsÔºâ 3.5 Bounding BoxÈ¢ÑÊµãÔºàBounding box predictionsÔºâ 3.6 ‰∫§Âπ∂ÊØîÔºàIntersection over unionÔºâ 3.7 ÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÔºàNon-max suppressionÔºâ 3.8 Anchor Boxes 3.9 YOLO ÁÆóÊ≥ïÔºàPutting it together: YOLO algorithmÔºâ 3.10 ÂÄôÈÄâÂå∫ÂüüÔºàÈÄâ‰øÆÔºâÔºàRegion proposals (Optional)Ôºâ Á¨¨ÂõõÂë® ÁâπÊÆäÂ∫îÁî®Ôºö‰∫∫ËÑ∏ËØÜÂà´ÂíåÁ•ûÁªèÈ£éÊ†ºËΩ¨Êç¢ÔºàSpecial applications: Face recognition &amp;Neural style transferÔºâ 4.1 ‰ªÄ‰πàÊòØ‰∫∫ËÑ∏ËØÜÂà´Ôºü(What is face recognition?) 4.2 One-ShotÂ≠¶‰π†ÔºàOne-shot learningÔºâ 4.3 Siamese ÁΩëÁªúÔºàSiamese networkÔºâ 4.4 Triplet ÊçüÂ§±ÔºàTriplet ÊçüÂ§±Ôºâ 4.5 Èù¢ÈÉ®È™åËØÅ‰∏é‰∫åÂàÜÁ±ªÔºàFace verification and binary classificationÔºâ 4.6 ‰ªÄ‰πàÊòØÁ•ûÁªèÈ£éÊ†ºËΩ¨Êç¢ÔºüÔºàWhat is neural style transfer?Ôºâ 4.7 ‰ªÄ‰πàÊòØÊ∑±Â∫¶Âç∑ÁßØÁΩëÁªúÔºüÔºàWhat are deep ConvNets learning?Ôºâ 4.8 ‰ª£‰ª∑ÂáΩÊï∞ÔºàCost functionÔºâ 4.9 ÂÜÖÂÆπ‰ª£‰ª∑ÂáΩÊï∞ÔºàContent cost functionÔºâ 4.10 È£éÊ†º‰ª£‰ª∑ÂáΩÊï∞ÔºàStyle cost functionÔºâ 4.11 ‰∏ÄÁª¥Âà∞‰∏âÁª¥Êé®ÂπøÔºà1D and 3D generalizations of modelsÔºâ Á¨¨‰∫îÈó®ËØæ Â∫èÂàóÊ®°Âûã(Sequence Models)Á¨¨‰∏ÄÂë® Âæ™ÁéØÂ∫èÂàóÊ®°ÂûãÔºàRecurrent Neural NetworksÔºâ 1.1 ‰∏∫‰ªÄ‰πàÈÄâÊã©Â∫èÂàóÊ®°ÂûãÔºüÔºàWhy Sequence Models?Ôºâ 1.2 Êï∞Â≠¶Á¨¶Âè∑ÔºàNotationÔºâ 1.3 Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÔºàRecurrent Neural Network ModelÔºâ 1.4 ÈÄöËøáÊó∂Èó¥ÁöÑÂèçÂêë‰º†Êí≠ÔºàBackpropagation through timeÔºâ 1.5 ‰∏çÂêåÁ±ªÂûãÁöÑÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºàDifferent types of RNNsÔºâ 1.6 ËØ≠Ë®ÄÊ®°ÂûãÂíåÂ∫èÂàóÁîüÊàêÔºàLanguage model and sequence generationÔºâ 1.7 ÂØπÊñ∞Â∫èÂàóÈááÊ†∑ÔºàSampling novel sequencesÔºâ 1.8 Âæ™ÁéØÁ•ûÁªèÁΩëÁªúÁöÑÊ¢ØÂ∫¶Ê∂àÂ§±ÔºàVanishing gradients with RNNsÔºâ 1.9 GRUÂçïÂÖÉÔºàGated Recurrent UnitÔºàGRUÔºâÔºâ 1.10 ÈïøÁü≠ÊúüËÆ∞ÂøÜÔºàLSTMÔºàlong short term memoryÔºâunitÔºâ 1.11 ÂèåÂêëÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºàBidirectional RNNÔºâ 1.12 Ê∑±Â±ÇÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºàDeep RNNsÔºâ Á¨¨‰∫åÂë® Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏éËØçÂµåÂÖ•ÔºàNatural Language Processing and Word EmbeddingsÔºâ 2.1 ËØçÊ±áË°®ÂæÅÔºàWord RepresentationÔºâ 2.2 ‰ΩøÁî®ËØçÂµåÂÖ•ÔºàUsing Word EmbeddingsÔºâ 2.3 ËØçÂµåÂÖ•ÁöÑÁâπÊÄßÔºàProperties of Word EmbeddingsÔºâ 2.4 ÂµåÂÖ•Áü©ÈòµÔºàEmbedding MatrixÔºâ 2.5 Â≠¶‰π†ËØçÂµåÂÖ•ÔºàLearning Word EmbeddingsÔºâ 2.6 Word2Vec 2.7 Ë¥üÈááÊ†∑ÔºàNegative SamplingÔºâ 2.8 GloVe ËØçÂêëÈáèÔºàGloVe Word VectorsÔºâ 2.9 ÊÉÖÁª™ÂàÜÁ±ªÔºàSentiment ClassificationÔºâ 2.10 ËØçÂµåÂÖ•Èô§ÂÅèÔºàDebiasing Word EmbeddingsÔºâ Á¨¨‰∏âÂë® Â∫èÂàóÊ®°ÂûãÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºàSequence models &amp; Attention mechanismÔºâ 3.1 Âü∫Á°ÄÊ®°ÂûãÔºàBasic ModelsÔºâ 3.2 ÈÄâÊã©ÊúÄÂèØËÉΩÁöÑÂè•Â≠êÔºàPicking the most likely sentenceÔºâ 3.3 ÈõÜÊùüÊêúÁ¥¢ÔºàBeam SearchÔºâ 3.4 ÊîπËøõÈõÜÊùüÊêúÁ¥¢ÔºàRefinements to Beam SearchÔºâ 3.5 ÈõÜÊùüÊêúÁ¥¢ÁöÑËØØÂ∑ÆÂàÜÊûêÔºàError analysis in beam searchÔºâ 3.6 Bleu ÂæóÂàÜÔºàÈÄâ‰øÆÔºâÔºàBleu Score (optional)Ôºâ 3.7 Ê≥®ÊÑèÂäõÊ®°ÂûãÁõ¥ËßÇÁêÜËß£ÔºàAttention Model IntuitionÔºâ 3.8Ê≥®ÊÑèÂäõÊ®°ÂûãÔºàAttention ModelÔºâ 3.9ËØ≠Èü≥ËØÜÂà´ÔºàSpeech recognitionÔºâ 3.10Ëß¶ÂèëÂ≠óÊ£ÄÊµãÔºàTrigger Word DetectionÔºâ 3.11ÁªìËÆ∫ÂíåËá¥Ë∞¢ÔºàConclusion and thank youÔºâ ‰∫∫Â∑•Êô∫ËÉΩÂ§ßÂ∏àËÆøË∞à Âê¥ÊÅ©ËææÈááËÆø Geoffery Hinton Âê¥ÊÅ©ËææÈááËÆø Ian Goodfellow Âê¥ÊÅ©ËææÈááËÆø Ruslan Salakhutdinov Âê¥ÊÅ©ËææÈááËÆø Yoshua Bengio Âê¥ÊÅ©ËææÈááËÆø ÊûóÂÖÉÂ∫Ü Âê¥ÊÅ©ËææÈááËÆø Pieter Abbeel Âê¥ÊÅ©ËææÈááËÆø Andrej Karpathy ÈôÑ‰ª∂ Ê∑±Â∫¶Â≠¶‰π†Á¨¶Âè∑ÊåáÂçóÔºàÂéüËØæÁ®ãÁøªËØëÔºâ]]></content>
      <categories>
        <category>ËßÜÈ¢ëÂ≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Ê∑±Â∫¶Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ë¥ùÂè∂ÊñØÂàÜÁ±ªÂô®]]></title>
    <url>%2F2019%2F03%2F28%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[[TOC] Ê¶ÇÁéáËÆ∫ÁöÑÁü•ËØÜ Êù°‰ª∂Ê¶ÇÁéá P(A|B)=P(A\cap B)/P(B)Â∑≤Áü•BÂèëÁîüÁöÑÊ¶ÇÁéáÔºåÊ±ÇAÂèëÁîüÁöÑÊ¶ÇÁéá ÂÖ®Ê¶ÇÁéá P(B) = \sum_{i=1}^{N}P(B \cap A_i)P(A_i)Ë¥ùÂè∂ÊñØÊé®Êñ≠ P(A|B)=P(A)\frac{P(B|A)}{P(B)} P(A_i|B)=P(A_i)\frac{P(B|A_i)}{\sum P(A_i)P(B|A_i)}$P(A)$ÔºöPrior probability ÂÖàÈ™åÊ¶ÇÁéáÔºåÂú®B‰∫ã‰ª∂ÂèëÁîü‰πãÂâçÔºåÂØπA‰∫ã‰ª∂ÂÅö‰∏Ä‰∏™Âà§Êñ≠ $P(A|B)$:Posterior probability ÂêéÈ™åÊ¶ÇÁéáÔºåÂú®B‰∫ã‰ª∂ÂèëÁîü‰πãÂêéÔºåÂØπA‰∫ã‰ª∂ÁöÑÊ¶ÇÁéáÈáçÊñ∞ËØÑ‰º∞ $P(B|A)/P(B)$:Áß∞‰∏∫ÂèØËÉΩÊÄßÂáΩÊï∞Ôºå‰∏Ä‰∏™Ë∞ÉÊï¥Âõ†Â≠ê ÂêéÈ™åÊ¶ÇÁéá=ÂÖàÈ™åÊ¶ÇÁéá*Ë∞ÉÊï¥Âõ†Â≠ê ÔºàÂèØÁü•ÔºåË∞ÉÊï¥Âõ†Ê≠§&gt;1,ÂèëÁîüÊ¶ÇÁéáÂ¢ûÂ§ß‰∫ÜÔºå Ë¥ùÂè∂ÊñØÂÜ≥Á≠ñËÆ∫Ëã±ÊñáÔºöBayesian decision theory ËÆæÊúâ$N$ÁßçÂèØËÉΩÁöÑÁ±ªÂà´, Âç≥Œ≥=${c_1,c_2,‚Ä¶,c_N}$. $Œª_ij$ÊòØÂ∞Ü‰∏Ä‰∏™ÁúüÂÆûÁ±ªÂà´‰∏∫$c_j$ÁöÑÊ†∑Êú¨Âà§‰∏∫$c_x$ÁöÑÊçüÂ§±„ÄÇ Âü∫‰∫éÂêéÈ™åÊ¶ÇÁéáÂèØÂæóÂ∞ÜÊ†∑Êú¨ÂàÜÁ±ªÊâÄ‰∫ßÁîüÁöÑÊúüÊúõÊçüÂ§±, ÊàñËÄÖÊàê‰∏∫Êù°‰ª∂È£éÈô©(Conditional Risk) R(C_i|x)=‚àë_{j=1}^NŒª_{ij}P(c_j|x)‰∫éÊòØÔºå Êàë‰ª¨ÁöÑ‰ªªÂä°Â∞±ÊòØÂØªÊâæÂà§ÂÆöÂáÜÂàôhÔºå ‰ª§$œá‚ÜíŒ≥$ ‰ΩøÂæóÊúÄÂ∞èÂåñÊÄª‰ΩìÈ£éÈô©Ôºå$R(h)=E_x[R(h(x)|x]$ÊúÄÂ∞è. ÂØπ‰∫éÊØè‰∏Ä‰∏™$x$ÔºåËã•$h$ÈÉΩËÉΩÊúÄÂ∞èÂåñÊù°‰ª∂È£éÈô©ÔºåÈÇ£‰πàÊÄª‰Ωì‰πüË¢´ÊúÄÂ∞èÂåñ‰∫Ü„ÄÇ ÂèØ‰ª•ÁÆÄÂåñ‰∏∫ÂØπÊØè‰∏™Ê†∑Êú¨ÈÄâÊã©ÂÖ∂Êù°‰ª∂È£éÈô©ÊúÄÂ∞èÁöÑÂàÜÁ±ª, Âç≥: h(x)=arg \min_{c‚äÇŒª}R(c|x)Ê≠§$h(x)$Â∞±ÊòØË¥ùÂè∂ÊñØÊúÄ‰ºòÂàÜÁ±ªÂô®„ÄÇ $R(h)$‰∏∫Ë¥ùÂè∂ÊñØÈ£éÈô©(Bayes Risk), $1‚àíR(h)$ÂèçÊò†‰∫ÜÂàÜÁ±ªÂô®ÁöÑÊúÄ‰ºòÊÄßËÉΩ. ÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂ¶ÇÊûúÁõÆÊ†áÊòØÊúÄÂ∞èÂåñÂàÜÁ±ªÈîôËØØÁéáÔºå \lambda_{ij}=\begin{cases} 0\ \ i==j\\1 \ \ \ i!=j \end{cases}Âàô$R(c|x)=1-p(c|x)$ÔºåÂõ†Ê≠§ÂèØÁü•Ôºå$h(x)=\max_{c\in C} p(c|x)$ ÂØπ‰∫éÊ†∑Êú¨$x$,ÈÄâÊã©ÂêéÈ™åÊ¶ÇÁéá$P(c|X)$ÊúÄÂ§ßÁöÑÁ±ªÂà´‰∏∫Ê†áËÆ∞„ÄÇ ÈóÆÈ¢òËΩ¨Êç¢‰∏∫ P(c_i|x)=\frac{P(c_i)P(x|c_i)}{\sum P(x)}Ê±ÇÂÖàÈ™åÊ¶ÇÁéáÂíå‰ººÁÑ∂($P(x|c)$) ÂÖ∂‰∏≠ $P(c)$Ë°®Ëææ‰∫ÜÊ†∑Êú¨Á©∫Èó¥ÁßçÂêÑÁ±ªÊ†∑Êú¨ÊâÄÂç†ÁöÑÊØîÂàóÔºåÊ†πÊçÆÂ§ßÊï∞ÂÆöÂæãÔºåÂΩìÊ†∑Êú¨Ë∂≥Â§üÂÖÖÂàÜÁöÑÁã¨Á´ãÂêåÂàÜÂ∏ÉÊ†∑Êú¨ÊòØÔºåÂèØ‰ª•È¢ëÁéá‰º∞ËÆ° $P(x|c)$,Ê∂âÂèäÂÖ≥‰∫éxÊâÄ‰ª•Â±ûÊÄßÁöÑËÅîÂêàÊ¶ÇÁéáÔºåÁî®È¢ëÁéá‰º∞ËÆ°Ê¶ÇÁéáÂèØËÉΩ‰∏çÂ§™Â•ΩÔºåÂØπ‰∫é‰º∞ËÆ°Á±ªÊù°‰ª∂Ê¶ÇÁéáÁöÑ‰∏ÄÁßçÂÆ†Áî®Á≠ñÁï•ÊòØÂÖàÂÅáËÆæÂÖ∑ÊúâÊüêÁßçÁ°ÆÂÆöÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÂΩ¢ÂºèÔºåÂÜçÂü∫‰∫éËÆ≠ÁªÉÊ†∑Êú¨ÂØπÊ¶ÇÁéáÂàÜÂ∏ÉÁöÑÂèÇÊï∞ËøõË°å‰º∞ËÆ°„ÄÇ $P(x|c)$ÊòØÁ±ªÊù°‰ª∂Ê¶ÇÁéáÔºåÁî±Êüê‰∏™ÂàÜÂ∏ÉÂÜ≥ÂÆöÔºå$P(x|\theta_c)$Êù•Ë°®Á§∫‰∫Ü È¢ëÁéáÊ≥®ÊÑèÊ¥æËÆ§‰∏∫ÂèØ‰ª•ÈÄöËøá‰ºòÂåñ‰ººÁÑ∂ÂáΩÊï∞‰º∞ËÆ°ÂèÇÊï∞„ÄÇ$D_c$Á±ªÂà´cÁöÑÊ†∑Êú¨ÈõÜÂêàÔºåÁã¨Á´ãÂêåÂàÜÂ∏É P(D_c|\theta_c)=\Pi_{x \in D_c}P(x|\theta_c) LL(\theta_c)=log P(D_c|\theta_c)Êú¥Á¥†Ë¥ùÂè∂ÊñØÂàÜÁ±ªÂô®Ëã±ÊñáÔºönaive Bayes classifier ÂÅáËÆæÔºöÂ±ûÊÄßÊù°‰ª∂Áã¨Á´ãÊÄßÂÅáËÆæÔºåÊØè‰∏™Â±ûÊÄßÁã¨Á´ãÊÄßÂØπÂàÜÁ±ªÁªìÊûúÂèëÁîüÂΩ±Âìç P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)\Pi_{i=1}^{d}P(x_i|c)}{P(x)}ÂØπ‰∫é‰∏Ä‰∏™$x$Ôºå$P(x)$ÈÉΩÊòØÁõ∏ÂêåÁöÑÔºåÂõ†Ê≠§Ë¥ùÂè∂ÊñØÊ®°ÂûãÂèØÂÜô‰∏∫ h_{nb}(x)=arg max_{c\in y}P(c)\Pi_{i=1}^{d}P(x_i|c)ËÆ°ÁÆóËøáÁ®ãÂÅáËÆæ$D_{c_i}$Ë°®Á§∫Á¨¨iÁ±ªÁöÑÊ†∑Êú¨ÈõÜÂêàÔºå $P(c_i)=\frac{|D_{c_i}|}{|D|}$ Â¶ÇÊûúÊòØÁ¶ªÊï£Â±ûÊÄß P(x_i|c_i)=\frac{|D_{c,x_i}|}{|D_{c_i}|}Â¶ÇÊûúÊòØËøûÁª≠Â±ûÊÄßÔºå$P(x_i|c_i)$Êúç‰ªé$N(u_{c,i},\theta_{c,i}^2)$ÁöÑÂàÜÂ∏É P(x_i|c)=\frac{1}{\sqrt{2\pi}\theta_{c,i}}exp(-\frac{(x_i-u_i)^2}{2\theta_{c,i}^2}) $P(c_i)\Pi_{i=1}^{N}P(x_i|c_i)$ Ê≥®ÊÑè‰∏∫‰∫ÜÈÅøÂÖçÂÖ∂‰ªñÂ±ûÊÄßÊê∫Â∏¶ÁöÑ‰ø°ÊÅØË¢´ËÆ≠ÁªÉÈõÜ‰∏≠Êú™Âá∫Áé∞ÁöÑÂ±ûÊÄßÂÄºÊäπÂéªÔºåÂõ†Ê≠§Áî®ÊãâÊôÆÊãâÊñØ‰øÆÊ≠£ÔºàLaplacian correction) P(c)=\frac{|D_{c_i}|+1}{|D|+N}\\ P(x_i|c)=\frac{|D_{x_i,c}|+1}{|D_c|+N_i}$N$:ËÆ≠ÁªÉÈõÜÂèØËÉΩÂá∫Áé∞ÁöÑÁ±ªÂà´Êï∞ $N_i$:Á¨¨i‰∏™Â±ûÊÄßÂèØËÉΩÁöÑÂèñÂÄºÊï∞ ÊòæÁÑ∂ÔºåÊãâÊôÆÊãâÊñØ‰øÆÊ≠£ÈÅøÂÖçÂõ†ËÆ≠ÁªÉÈõÜ‰∏çÂÖÖÂàÜÂØºÂá∫ÁöÑÊ¶ÇÁéá‰º∞ÂÄº‰∏∫0ÁöÑÊÉÖÂÜµ Êú¥Á¥†Ë¥ùÂè∂ÊñØÁöÑÁßçÁ±ªÂÜçscikit-learn‰∏≠Ôºå‰∏ÄÂÖ±Êúâ‰∏â‰∏™Êú¥Á¥†Ë¥ùÂè∂ÊñØÔºåÂàÜÂà´ÊòØ GaussianNB P(x_i|C_i)=\frac{1}{\sqrt{2\pi}\theta_{c,i}}exp(-\frac{(x_i-u_i)^2}{2\theta_{c,i}^2})12345678910111213141516171819202122#ÂØºÂÖ•ÂåÖimport pandas as pdfrom sklearn.naive_bayes import GaussianNBfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score#ÂØºÂÖ•Êï∞ÊçÆÈõÜfrom sklearn import datasetsiris=datasets.load_iris()#ÂàáÂàÜÊï∞ÊçÆÈõÜXtrain, Xtest, ytrain, ytest = train_test_split(iris.data, iris.target, random_state=42)#Âª∫Ê®°clf = GaussianNB()clf.fit(Xtrain, ytrain)#Âú®ÊµãËØïÈõÜ‰∏äÊâßË°åÈ¢ÑÊµãÔºåprobaÂØºÂá∫ÁöÑÊòØÊØè‰∏™Ê†∑Êú¨Â±û‰∫éÊüêÁ±ªÁöÑÊ¶ÇÁéáclf.predict(Xtest)clf.predict_proba(Xtest) #ÊØè‰∏ÄÁ±ªËÆ°ÁÆóÁªìÊûúÈÉΩËæìÂá∫#ÊµãËØïÂáÜÁ°ÆÁéáaccuracy_score(ytest, clf.predict(Xtest)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import numpy as npimport pandas as pdimport randomdataSet =pd.read_csv('iris.txt',header = None)dataSet.head()def randSplit(dataSet, rate): l = list(dataSet.index) #ÊèêÂèñÂá∫Á¥¢Âºï random.shuffle(l) #ÈöèÊú∫Êâì‰π±Á¥¢Âºï dataSet.index = l #Â∞ÜÊâì‰π±ÂêéÁöÑÁ¥¢ÂºïÈáçÊñ∞ËµãÂÄºÁªôÂéüÊï∞ÊçÆÈõÜ n = dataSet.shape[0] #ÊÄªË°åÊï∞ m = int(n * rate) #ËÆ≠ÁªÉÈõÜÁöÑÊï∞Èáè train = dataSet.loc[range(m), :] #ÊèêÂèñÂâçm‰∏™ËÆ∞ÂΩï‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜ test = dataSet.loc[range(m, n), :] #Ââ©‰∏ãÁöÑ‰Ωú‰∏∫ÊµãËØïÈõÜ dataSet.index = range(dataSet.shape[0]) #Êõ¥Êñ∞ÂéüÊï∞ÊçÆÈõÜÁöÑÁ¥¢Âºï test.index = range(test.shape[0]) #Êõ¥Êñ∞ÊµãËØïÈõÜÁöÑÁ¥¢Âºïtrain,test=randSplit(dataSet, 0.8)def gnb_classify(train,test): labels = train.iloc[:,-1].value_counts().index #ÊèêÂèñËÆ≠ÁªÉÈõÜÁöÑÊ†áÁ≠æÁßçÁ±ª mean =[] #Â≠òÊîæÊØè‰∏™Á±ªÂà´ÁöÑÂùáÂÄº std =[] #Â≠òÊîæÊØè‰∏™Á±ªÂà´ÁöÑÊñπÂ∑Æ result = [] #Â≠òÊîæÊµãËØïÈõÜÁöÑÈ¢ÑÊµãÁªìÊûú for i in labels: item = train.loc[train.iloc[:,-1]==i,:] #ÂàÜÂà´ÊèêÂèñÂá∫ÊØè‰∏ÄÁßçÁ±ªÂà´ m = item.iloc[:,:-1].mean() #ÂΩìÂâçÁ±ªÂà´ÁöÑÂπ≥ÂùáÂÄº s = np.sum((item.iloc[:,:-1]-m)**2)/(item.shape[0]) #ÂΩìÂâçÁ±ªÂà´ÁöÑÊñπÂ∑Æ mean.append(m) #Â∞ÜÂΩìÂâçÁ±ªÂà´ÁöÑÂπ≥ÂùáÂÄºËøΩÂä†Ëá≥ÂàóË°® std.append(s) #Â∞ÜÂΩìÂâçÁ±ªÂà´ÁöÑÊñπÂ∑ÆËøΩÂä†Ëá≥ÂàóË°® means = pd.DataFrame(mean,index=labels) #ÂèòÊàêDFÊ†ºÂºèÔºåÁ¥¢Âºï‰∏∫Á±ªÊ†áÁ≠æ stds = pd.DataFrame(std,index=labels) #ÂèòÊàêDFÊ†ºÂºèÔºåÁ¥¢Âºï‰∏∫Á±ªÊ†áÁ≠æ for j in range(test.shape[0]): iset = test.iloc[j,:-1].tolist() #ÂΩìÂâçÊµãËØïÂÆû‰æã iprob = np.exp(-1*(iset-means)**2/(stds*2))/(np.sqrt(2*np.pi*stds)) #Ê≠£ÊÄÅÂàÜÂ∏ÉÂÖ¨Âºè prob = train.iloc[:,-1].value_counts()/len(train.iloc[:,-1]) #ÂàùÂßãÂåñÂΩìÂâçÂÆû‰æãÊÄªÊ¶ÇÁéá for k in range(test.shape[1]-1): #ÈÅçÂéÜÊØè‰∏™ÁâπÂæÅ prob *= iprob[k] #ÁâπÂæÅÊ¶ÇÁéá‰πãÁßØÂç≥‰∏∫ÂΩìÂâçÂÆû‰æãÊ¶ÇÁéá cla = prob.index[np.argmax(prob.values)] #ËøîÂõûÊúÄÂ§ßÊ¶ÇÁéáÁöÑÁ±ªÂà´ result.append(cla) test['predict']=result acc = (test.iloc[:,-1]==test.iloc[:,-2]).mean() #ËÆ°ÁÆóÈ¢ÑÊµãÂáÜÁ°ÆÁéá print(f'Ê®°ÂûãÈ¢ÑÊµãÂáÜÁ°ÆÁéá‰∏∫&#123;acc&#125;') return testgnb_classify(train,test)for i in range(20): train,test= randSplit(dataSet, 0.8) gnb_classify(train,test) MultinomialNBÂÖàÈ™åÊ¶ÇÁéáÂ§öÈ°πÂºèÂàÜÂ∏ÉÁöÑÊú¥Á¥†Ë¥ùÂè∂ÊñØÔºåÂÅáËÆæÁâπÂæÅÊòØÁî±‰∏ÄÂÖ±ÁÆÄÂçïÂ§öÈ°πÂºèÂàÜÂ∏ÉÁîüÊàêÔºåÂ§öÈ°πÂàÜÂ∏ÉÂèØ‰ª•ÊèèËø∞ÂêÑÁßçÁ±ªÂûãÊ†∑Êú¨Âá∫Áé∞ÁöÑÈ¢ëÁéáÔºåËØ•Ê®°ÂûãÂ∏∏Áî®‰∫éÊñáÊú¨ÂàÜÁ±ªÔºåÁâπÂà´Ë°®Á§∫Ê¨°Êï∞„ÄÇ$\lambda$Â∏∏ÂèñÂÄº1 P(x_{il}|c)=\frac{x_{il}+\lambda}{m_k+n\lambda}12345678910def loadDataSet(): dataSet=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] #ÂàáÂàÜÂ•ΩÁöÑËØçÊù° classVec = [0,1,0,1,0,1] #Á±ªÂà´Ê†áÁ≠æÂêëÈáèÔºå1‰ª£Ë°®‰æÆËæ±ÊÄßËØçÊ±áÔºå0‰ª£Ë°®Èùû‰æÆËæ±ÊÄßËØçÊ±á return dataSet,classVecdataSet,classVec = loadDataSet() 12345678def createVocabList(dataSet): vocabSet = set() #ÂàõÂª∫‰∏Ä‰∏™Á©∫ÁöÑÈõÜÂêà for doc in dataSet: #ÈÅçÂéÜdataSet‰∏≠ÁöÑÊØè‰∏ÄÊù°Ë®ÄËÆ∫ vocabSet = vocabSet | set(doc) #ÂèñÂπ∂ÈõÜ vocabList = list(vocabSet) return vocabListvocabList = createVocabList(dataSet) 12345678def setOfWords2Vec(vocabList, inputSet): returnVec = [0] * len(vocabList) #ÂàõÂª∫‰∏Ä‰∏™ÂÖ∂‰∏≠ÊâÄÂê´ÂÖÉÁ¥†ÈÉΩ‰∏∫0ÁöÑÂêëÈáè for word in inputSet: #ÈÅçÂéÜÊØè‰∏™ËØçÊù° if word in vocabList: #Â¶ÇÊûúËØçÊù°Â≠òÂú®‰∫éËØçÊ±áË°®‰∏≠ÔºåÂàôÂèò‰∏∫1 returnVec[vocabList.index(word)] = 1 else: print(f" &#123;word&#125; is not in my Vocabulary!" ) return returnVec #ËøîÂõûÊñáÊ°£ÂêëÈáè 12345678def get_trainMat(dataSet): trainMat = [] #ÂàùÂßãÂåñÂêëÈáèÂàóË°® vocabList = createVocabList(dataSet) #ÁîüÊàêËØçÊ±áË°® for inputSet in dataSet: #ÈÅçÂéÜÊ†∑Êú¨ËØçÊù°‰∏≠ÁöÑÊØè‰∏ÄÊù°Ê†∑Êú¨ returnVec=setOfWords2Vec(vocabList, inputSet) #Â∞ÜÂΩìÂâçËØçÊù°ÂêëÈáèÂåñ trainMat.append(returnVec) #ËøΩÂä†Âà∞ÂêëÈáèÂàóË°®‰∏≠ return trainMattrainMat = get_trainMat(dataSet) 1234567891011121314151617181920def trainNB(trainMat,classVec): n = len(trainMat) #ËÆ°ÁÆóËÆ≠ÁªÉÁöÑÊñáÊ°£Êï∞ÁõÆ m = len(trainMat[0]) #ËÆ°ÁÆóÊØèÁØáÊñáÊ°£ÁöÑËØçÊù°Êï∞ pAb = sum(classVec)/n #ÊñáÊ°£Â±û‰∫é‰æÆËæ±Á±ªÁöÑÊ¶ÇÁéá p0Num = np.zeros(m) #ËØçÊù°Âá∫Áé∞Êï∞ÂàùÂßãÂåñ‰∏∫0 p1Num = np.zeros(m) #ËØçÊù°Âá∫Áé∞Êï∞ÂàùÂßãÂåñ‰∏∫0 p0Denom = 0 #ÂàÜÊØçÂàùÂßãÂåñ‰∏∫0 p1Denom = 0 #ÂàÜÊØçÂàùÂßãÂåñ‰∏∫0 for i in range(n): #ÈÅçÂéÜÊØè‰∏Ä‰∏™ÊñáÊ°£ if classVec[i] == 1: #ÁªüËÆ°Â±û‰∫é‰æÆËæ±Á±ªÁöÑÊù°‰ª∂Ê¶ÇÁéáÊâÄÈúÄÁöÑÊï∞ÊçÆ p1Num += trainMat[i] p1Denom += sum(trainMat[i]) else: #ÁªüËÆ°Â±û‰∫éÈùû‰æÆËæ±Á±ªÁöÑÊù°‰ª∂Ê¶ÇÁéáÊâÄÈúÄÁöÑÊï∞ÊçÆ p0Num += trainMat[i] p0Denom += sum(trainMat[i]) p1V = p1Num/p1Denom p0V = p0Num/p0Denom return p0V,p1V,pAb #ËøîÂõûÂ±û‰∫éÈùû‰æÆËæ±Á±ª,‰æÆËæ±Á±ªÂíåÊñáÊ°£Â±û‰∫é‰æÆËæ±Á±ªÁöÑÊ¶ÇÁéáp0V,p1V,pAb=trainNB(trainMat,classVec) 1234567891011121314151617181920212223from functools import reducedef classifyNB(vec2Classify, p0V, p1V, pAb): p1 = reduce(lambda x,y:x*y, vec2Classify * p1V) * pAb #ÂØπÂ∫îÂÖÉÁ¥†Áõ∏‰πò p0 = reduce(lambda x,y:x*y, vec2Classify * p0V) * (1 - pAb) print('p0:',p0) print('p1:',p1) if p1 &gt; p0: return 1 else: return 0def testingNB(testVec): dataSet,classVec = loadDataSet() #ÂàõÂª∫ÂÆûÈ™åÊ†∑Êú¨ vocabList = createVocabList(dataSet) #ÂàõÂª∫ËØçÊ±áË°® trainMat= get_trainMat(dataSet) #Â∞ÜÂÆûÈ™åÊ†∑Êú¨ÂêëÈáèÂåñ p0V,p1V,pAb = trainNB(trainMat,classVec) #ËÆ≠ÁªÉÊú¥Á¥†Ë¥ùÂè∂ÊñØÂàÜÁ±ªÂô® thisone = setOfWords2Vec(vocabList, testVec) #ÊµãËØïÊ†∑Êú¨ÂêëÈáèÂåñ if classifyNB(thisone,p0V,p1V,pAb): print(testVec,'Â±û‰∫é‰æÆËæ±Á±ª') #ÊâßË°åÂàÜÁ±ªÂπ∂ÊâìÂç∞ÂàÜÁ±ªÁªìÊûú else: print(testVec,'Â±û‰∫éÈùû‰æÆËæ±Á±ª') #ÊâßË°åÂàÜÁ±ªÂπ∂ÊâìÂç∞ÂàÜÁ±ªÁªìÊûú testVec1 = ['love', 'my', 'dalmation']testingNB(testVec1) BernoulliNB‰ºØÂä™Âà©ÂàÜÂ∏ÉÔºåÂ¶ÇÊûúÊòØ‰∫åÂÖÉ‰ºØÂä™Âà©ÂàÜÂ∏É P(x_{il}|C_i)=P(i|Y=C_i)x_{il}+(1-P(i|Y=C_i))(1-x_{il})Â¶ÇÊûúÊ†∑Êú¨Â±ûÊÄßÂ§ßÂ§öÊï∞Â±û‰∫éËøûÁª≠ÔºåGaussionNB Â¶ÇÊûúÊòØÁ¶ªÊï£ÂÄºÔºå‰ΩøÁî®MultinomialNB Â¶ÇÊûúÊ†∑Êú¨ÁâπÂæÅÊòØ‰∫åÂÖÉÁ¶ªÊï£ÂÄºÊàñËÄÖÁ®ÄÁñèÁ¶ªÊï£ÂÄºÔºåBernoulliNB ÂçäÊú¥Á¥†Ë¥ùÂè∂ÊñØ‰ø°ÊÅØÈáè„ÄÅÁÜµ„ÄÅËÅîÂêàÁÜµ„ÄÅÊù°‰ª∂ÁÜµ„ÄÅ‰∫í‰ø°ÊÅØ‰ø°ÊÅØÈáèÂèçÂ∫î‰∫ÜÈöèÊú∫ÂèòÈáèÂèñÊüê‰∏™ÂÄºÂê´ÁöÑÂèØËÉΩÊÄßÂ§ßÂ∞èÔºåÊàñËÄÖÊòØÂê´ÊúâÁöÑ‰ø°ÊÅØÂ§öÂ∞ë I(X=x)=-log_2^{p(xÔºâ}ÁÜµ(entropy)ÂèçÂ∫î‰∫Ü‰ø°Ê∫êÂπ≥ÂùáÊØè‰∏™Á¨¶Âè∑ÁöÑ‰ø°ÊÅØÈáè,ÊàñËÄÖÊòØÈöèÊú∫ÂèòÈáè‰∏çÁ°ÆÂÆöÊÄßÁöÑË°°Èáè H(X)=E(I(X))=\sum p(X=x)(-log_2^{p(x)})ËÅîÂêàÁÜµÂèçÂ∫î‰∫ÜÂ§ö‰∏™ÈöèÊú∫ÂèòÈáèÁöÑÂπ≥Âùá‰ø°ÊÅØÈáè H(X,Y)=\sum p(x,y)(-log_2^{p(x,y)})Êù°‰ª∂ÁÜµÔºàConditional entropyÔºâÂèçÂ∫î‰∫ÜÂ∑≤Áü•‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáè‰∏ãÔºåÂè¶‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáèÁöÑ‰∏çÁ°ÆÂÆöÊÄß H(X|Y)=-\sum p(y)H(X|Y=y)=-\sum p(x,y)log_2^{p(x|y)}‰∫í‰ø°ÊÅØ(mutual information)ÂèçÂ∫î‰∫ÜÂ∑≤Áü•‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºåÂè¶Â§ñ‰∏Ä‰∏™ÈöèÊú∫ÂèòÈáè‰∏çÁ°ÆÂÆöÊÄßÂáèÂ∞ë‰∫ÜÂ§öÂ∞ë,ÂèØ‰ª•Êää‰∫í‰ø°ÊÅØÁúãÊàêÁî±‰∫éÁü•ÈÅì y ÂÄºËÄåÈÄ†ÊàêÁöÑ x ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÁöÑÂáèÂ∞è I(X;Y)=\sum \sum p(x,y)log(\frac{p(x,y)}{p(x)p(y)})\\ =H(X)-H(X|Y)=H(Y)-H(Y|X)Â¶ÇÊûú‰∏§‰∏™ÈöèÊú∫ÂèòÈáèÁã¨Á´ãÔºåÂàô‰∫í‰ø°ÊÅØ‰∏∫0,Âõ†Ê≠§Ôºå‰∫í‰ø°ÊÅØÂèØ‰ª•Ë°°Èáè‰∏§‰∏™ÈöèÊú∫ÂèòÈáèÁöÑÁõ∏ÂÖ≥Á®ãÂ∫¶ Êù°‰ª∂‰∫í‰ø°ÊÅØÂú®Êù°‰ª∂zÂèëÁîüÊó∂ÁöÑÊù°‰ª∂‰∫í‰ø°ÊÅØ I(X;Y|Z) = \sum\sum p(x,y|z)log_2^{\frac{p(x,y|z)}{p(x|z)p(y|z)}} ÂçäÊú¥Á¥†Ë¥ùÂè∂ÊñØÈÄÇÂΩìÁöÑËÄÉËôë‰∏ÄÈÉ®ÂàÜÂ±ûÊÄßÈó¥ÁöÑÁõ∏‰∫í‰æùËµñÂÖ≥Á≥ªÔºåËøô‰∏™ÂÖ≥Á≥ªÂèØ‰ª•Áî®‰∫í‰ø°ÊÅØÊèèËø∞ Áã¨‰æùËµñÂÅáËÆæÊØè‰∏™Â±ûÊÄßÂè™Êúâ‰∏Ä‰∏™ÂÖ∂‰ªñ ÁöÑÂ±ûÊÄß.ÂàôËÆ°ÁÆóÂÖ¨ÂºèÊîπ‰∏ãÂ¶Ç‰∏ã p(C)\Pi_{i=1}^{d} P(x_i|C_i,pa_i)$pa_i$ÊòØÂ±ûÊÄß$x_i$ÊâÄ‰æùËµñÁöÑÂ±ûÊÄßÔºåË¢´Áß∞‰∏∫$x_i$ÁöÑÁà∂Â±ûÊÄß 1) SPODE ÊúÄÁÆÄÂçïÁöÑÊñπÊ≥ïÊòØÔºöÈÉΩÈÄâ‰∏Ä‰∏™Â±ûÊÄß‰Ωú‰∏∫Áà∂Â±ûÊÄß ÂèØ‰ª•ÈÄöËøá‰∫§ÂèâÈ™åËØÅÁöÑÊñπÊ≥ï 2) TAN :ÊúÄÂ§ßÂ∏¶ÊùÉÁîüÊàêÊ†ë ÊùÉÈáçÔºöÂΩìyÂàíÂàÜ‰∏∫$c_k$Á±ªÊó∂Êù°‰ª∂ÁÜµ I(x_i;y_i|y)=\sum_{x_i,y_i,c_k}p(x_i,y_j|c_k)log^{\frac{p(x_i;y_j|c_k)}{p(x_i|c_k)p(y_i|c_k)}}step 1: ËÆ°ÁÆó‰ªªÊÑè‰∏§‰∏™Â±ûÊÄß‰πãÈó¥Êù°‰ª∂‰∫í‰ø°ÊÅØ I(X;Y|Y)=\sum_{i}I(X;Y|c_i)step 2: ‰ª•Â±ûÊÄß‰∏∫ÁªìÁÇπÊûÑÂª∫ÂÆåÂÖ®Âõæ step 3: ÊúÄÂ§ßÂ∏¶ÊùÉÁîüÊàêÊ†ëÔºåÊåëÈÄâÊ†πÂèòÈáè step 4: Âä†ÂÖ•Á±ªÂà´ÁªìÁÇπy,Â¢ûÂä†Âà∞ÊØè‰∏™Â±ûÊÄßÁöÑÊúâÂêëËæπ Êù°‰ª∂‰∫í‰ø°ÊÅØÂèçÂ∫î‰∫ÜÂ±ûÊÄßÂú®Â∑≤Áü•Á±ªÂà´‰∏ãÁöÑÁõ∏ÂÖ≥ÊÄßÂ§ßÂ∞è ÈõÜÊàêÂ≠¶‰π†AODEÈÄâÊã©Ê®°ÂûãÂ∞ùËØïÂ∞ÜÊØè‰∏™Â±ûÊÄß‰Ωú‰∏∫Ë∂ÖÁà∂ÊûÑÂª∫SPODE P(c_i|X)Ê≠£ÊØî‰∫é \sum_{i=1,|D_{x_i}>=m}p(c,x_i)\Pi_{j=1}^{d}p(x_j|c_i,x_i)$m$ÈÄöÂ∏∏Âèñ30, P(c,x_i)=\frac{|D_{c,x_i}|+1}{|{D}|+N*N_i}\\ P(x_j|c,x_i)=\frac{|D_{c,x_i,x_j}+1|}{|D_{c,xi}|+N_j}Ë¥ùÂè∂ÊñØÁΩë(Bayesian network)ÂÄüÂä©ÊúâÂêëÊó†ÁéØÂõæÊù•ÂàªÁîªÂ±ûÊÄß‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ªÔºåÊù°‰ª∂Ê¶ÇÁéáË°®Êù•ÊèèËø∞Â±ûÊÄßÁöÑËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É„ÄÇ ‰∏Ä‰∏™Ë¥ùÂè∂ÊñØÁΩëÁªú$B$,ÂåÖÊã¨ÁªìÊûÑ$G$ÂíåÂèÇÊï∞$\Theta$ ,$B(G,\Theta)$,Â¶ÇÊûú‰∏§‰∏™Â±ûÊÄßÊúâÁõ¥Êé•‰æùËµñÂÖ≥Á≥ªÔºåÁî®ËæπËøûÊé•ÔºåÂØπ‰∫éÂ±ûÊÄß$x_i$,ÂÖ∂Áà∂ËäÇÁÇπÈõÜÂêà$G_i$,Âàô$\Theta$ÂåÖÊã¨ÊØè‰∏™Â±ûÊÄßÊù°‰ª∂Ê¶ÇÁéá$\Theta_{x_i|\pi_i}=P_B(x_i|\pi_i)$ ÁªìÊûÑ p(x_1,x_2,...,x_n)=\Pi_{i=1}^{n}p_{B}(x_i|\pi_i)=\Pi_{i=1}^{d}\Theta_{xi|\pi_i}\\ =\Pi_{i=1}^{d}P(x_i|Parents(x_i))Êé®Êñ≠‰∏ÄÊó¶ËÆ≠ÁªÉÂ•ΩË¥ùÂè∂ÊñØÁΩëÂêéÔºåÂ∞±ËÉΩÂõûÁ≠îquery,ÈÄöËøá‰∏Ä‰∫õÂ±ûÊÄßÁöÑËßÇÊµãËÄÖÊù•Êé®Êñ≠ÂÖ∂‰ªñÂ±ûÊÄßÂèòÈáèÁöÑÂèñÂÄºÔºåÂÖ∂‰∏≠ÔºåÂ∑≤Áü•ÂèòÈáèÁöÑÂÄºËßÇÊµãÊé®ÊµãÂæÖÊü•ËØ¢ÁöÑËøáÁ®ã‚ÄúÊé®Êñ≠‚Äù,Â∑≤Áü•ÂèòÈáèÁöÑËßÇÊµãËÄÖ‚ÄùËØÅÊçÆ‚Äú]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Ë¥ùÂè∂ÊñØÂàÜÁ±ªÂô®</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰∫åÊ¨°ËßÑÂàí]]></title>
    <url>%2F2019%2F03%2F25%2F%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[[TOC] KKT(Karush-Kuhn-Tucher)Êù°‰ª∂ ÁªôÂÆö‰ºòÂåñÈóÆÈ¢ò \min f(x)\\ subject\ to \begin{cases} g_i(x) = 0 (i=1,,,,m\\ h_i(x) =0 (i=m+1,...,n)\\ \lambda_i h_i(x)=0(i=m+1,..,n)‰∫åÊ¨°ËßÑÂàíÈóÆÈ¢òÈóÆÈ¢òÁöÑÊï∞Â≠¶Ë°®Ëææ \min Q(x) = \frac{1}{2}x^THx+g^Tx\\ s.t. a_i^Tx = b_i (i=1,..,m)\\ \ \ \ \ \ \ \ a_i^Tx =x^{*T}H(x-x^{*})+g^T(x-x^{*})=\lambda^TA(x-x^{*})http://www.hankcs.com/ml/lagrange-duality.html#h3-7 SMO ÔºöSequential minimal optimizationÊîØÊåÅÂêëÈáèÊú∫ÁöÑÂØπÂÅ∂ÈóÆÈ¢ò \min \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^{m}\alpha_i\\ s.t. \sum_{i=1}^{m}\alpha_iy_i=0\\ 0]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êï∞Â≠¶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle]]></title>
    <url>%2F2019%2F03%2F24%2Fkaggle%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn]]></title>
    <url>%2F2019%2F03%2F23%2Fscikit-learn%2F</url>
    <content type="text"><![CDATA[Cross-validation: evaluating estimator performance¬∂ 12345import numpy as npfrom sklearn.model_selection import train_test_split# Ë∞ÉÁî®train_test_splitÂáΩÊï∞ Ëá™Âä®ÂàíÂàÜÊï∞ÊçÆÈõÜ 40%for testingX_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target, test_size=0.4, random_state=0) corss validation 1234567from sklearn.model_selection import cross_validatefrom sklearn.metrics import recall_scorescoring = [&apos;precision_macro&apos;, &apos;recall_macro&apos;]clf = svm.SVC(kernel=&apos;linear&apos;, C=1, random_state=0)scores = cross_validate(clf, iris.data, iris.target, scoring=scoring, cv=5, return_train_score=False)sorted(scores.keys()) cross-validation metrics12from sklearn.model_selection import cross_val_scoreclf = svm.SVC(kernel = &apos;linear&apos;, C = 1) Cross validation of time series data Tuning the hyper-parameters of an estimatorA search consists of: an estimator (regressor or classifier such as sklearn.svm.SVC()); a parameter space; a method for searching or sampling candidates; a cross-validation scheme; and a score function. Grid Search1234param_grid = [ &#123;&apos;C&apos;: [1, 10, 100, 1000], &apos;kernel&apos;: [&apos;linear&apos;]&#125;, &#123;&apos;C&apos;: [1, 10, 100, 1000], &apos;gamma&apos;: [0.001, 0.0001], &apos;kernel&apos;: [&apos;rbf&apos;]&#125;, ] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from __future__ import print_functionfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import classification_reportfrom sklearn.svm import SVCprint(__doc__)# Loading the Digits datasetdigits = datasets.load_digits()# To apply an classifier on this data, we need to flatten the image, to# turn the data in a (samples, feature) matrix:n_samples = len(digits.images)X = digits.images.reshape((n_samples, -1))y = digits.target# Split the dataset in two equal partsX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=0)# Set the parameters by cross-validationtuned_parameters = [&#123;'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]&#125;, &#123;'kernel': ['linear'], 'C': [1, 10, 100, 1000]&#125;]scores = ['precision', 'recall']for score in scores: print("# Tuning hyper-parameters for %s" % score) print() clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='%s_macro' % score) clf.fit(X_train, y_train) print("Best parameters set found on development set:") print() print(clf.best_params_) print() print("Grid scores on development set:") print() means = clf.cv_results_['mean_test_score'] stds = clf.cv_results_['std_test_score'] for mean, std, params in zip(means, stds, clf.cv_results_['params']): print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params)) print() print("Detailed classification report:") print() print("The model is trained on the full development set.") print("The scores are computed on the full evaluation set.") print() y_true, y_pred = y_test, clf.predict(X_test) print(classification_report(y_true, y_pred)) print()# Note the problem is too easy: the hyperparameter plateau is too flat and the# output model is the same for precision and recall with ties in quality. Randomized Parameter Optimization123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566print(__doc__)import numpy as npfrom time import timefrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCVfrom sklearn.datasets import load_digitsfrom sklearn.ensemble import RandomForestClassifier# get some datadigits = load_digits()X, y = digits.data, digits.target# build a classifierclf = RandomForestClassifier(n_estimators=20)# Utility function to report best scoresdef report(results, n_top=3): for i in range(1, n_top + 1): candidates = np.flatnonzero(results['rank_test_score'] == i) for candidate in candidates: print("Model with rank: &#123;0&#125;".format(i)) print("Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)".format( results['mean_test_score'][candidate], results['std_test_score'][candidate])) print("Parameters: &#123;0&#125;".format(results['params'][candidate])) print("")# specify parameters and distributions to sample fromparam_dist = &#123;"max_depth": [3, None], "max_features": sp_randint(1, 11), "min_samples_split": sp_randint(2, 11), "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run randomized searchn_iter_search = 20random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5)start = time()random_search.fit(X, y)print("RandomizedSearchCV took %.2f seconds for %d candidates" " parameter settings." % ((time() - start), n_iter_search))report(random_search.cv_results_)# use a full grid over all parametersparam_grid = &#123;"max_depth": [3, None], "max_features": [1, 3, 10], "min_samples_split": [2, 3, 10], "bootstrap": [True, False], "criterion": ["gini", "entropy"]&#125;# run grid searchgrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)start = time()grid_search.fit(X, y)print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % (time() - start, len(grid_search.cv_results_['params'])))report(grid_search.cv_results_) step1Ôºö ‰∫§ÂèâÈ™åËØÅÔºàËØÑ‰ª∑Ê®°ÂûãÔºâ step2: Ë∂ÖÂèÇÊï∞ÈÄâÊã©ÔºåÊØè‰∏ÄÁªÑÂèÇÊï∞ÔºöÂØπÂ∫î‰∏ÄÊ¨°‰∫§ÂèâÈ™åËØÅ step 3: ÈõÜÊàêÂ≠¶‰π† ‰πüÂèØËøõË°åÂèÇÊï∞ÁöÑË∞ÉËß£ 12345678from sklearn.model_selection import cross_val_scorefrom sklearn.datasets import load_irisfrom sklearn.ensemble import AdaBoostClassifieriris = load_iris()clf = AdaBoostClassifier(n_estimators=100)scores = cross_val_score(clf, iris.data, iris.target, cv=5)scores.mean() 1234567891011121314151617181920212223from sklearn import datasetsfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom itertools import productfrom sklearn.ensemble import VotingClassifier# Loading some example datairis = datasets.load_iris()X = iris.data[:, [0, 2]]y = iris.target# Training classifiersclf1 = DecisionTreeClassifier(max_depth=4)clf2 = KNeighborsClassifier(n_neighbors=7)clf3 = SVC(gamma=&apos;scale&apos;, kernel=&apos;rbf&apos;, probability=True)eclf = VotingClassifier(estimators=[(&apos;dt&apos;, clf1), (&apos;knn&apos;, clf2), (&apos;svc&apos;, clf3)], voting=&apos;soft&apos;, weights=[2, 1, 2])clf1 = clf1.fit(X, y)clf2 = clf2.fit(X, y)clf3 = clf3.fit(X, y)eclf = eclf.fit(X, y) sklearn.model_selectionGridSearchCVclass sklearn.model_selection.``GridSearchCV(estimator, param_grid, **, scoring=None, n_jobs=None, iid=‚Äôdeprecated‚Äô, refit=True, cv=None, verbose=0, pre_dispatch=‚Äô2*n_jobs‚Äô, error_score=nan, return_train_score=False*)‚Äô estimator**estimator object.** param_grid**dict or list of dictionaries** scoring**str, callable, list/tuple or dict, default=None** cv**int, cross-validation generator or an iterable, default=None** 12345678910from sklearn import svm, datasetsfrom sklearn.model_selection import GridSearchCViris = datasets.load_iris()parameters = &#123;'kernel':('linear', 'rbf'), 'C':[1, 10]&#125;svc = svm.SVC()clf = GridSearchCV(svc, parameters)clf.fit(iris.data, iris.target)sorted(clf.cv_results_.keys())]]></content>
      <categories>
        <category>ÁºñÁ®ãËØ≠Ë®Ä</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Boosting]]></title>
    <url>%2F2019%2F03%2F22%2FBoosting%2F</url>
    <content type="text"><![CDATA[[TOC] BoostingÂéüÁêÜBoostingÁÆóÊ≥ïÊòØÂ∞Ü‚ÄúÂº±Â≠¶‰π†ÁÆóÊ≥ï‚ÄúÊèêÂçá‰∏∫‚ÄúÂº∫Â≠¶‰π†ÁÆóÊ≥ï‚ÄùÁöÑËøáÁ®ã„ÄÇ Âä†Ê≥ïÊ®°Âûã F_n(x;P) = \sum_{t=1}^{n}\alpha_th_t(x;a_t) ÂâçÂêëÂàÜÊ≠• F_m(x) = F_{m-1}(x)+\alpha_mh_m(x,a_m)Â¶ÇÊûúÈÄâÂèñ‰∏çÂêåÊçüÂ§±ÂáΩÊï∞ÔºåÂàô‰∫ßÁîü‰∏çÂêåÁöÑÁ±ªÂûã AdaBoostAdaBoostÂ∞±ÊòØÊçüÂ§±ÂáΩÊï∞‰∏∫ÊåáÊï∞ÊçüÂ§±ÁöÑBoostingÁÆóÊ≥ï„ÄÇ ÊØè‰∏ÄÊ¨°Ëø≠‰ª£ÁöÑÂº±Â≠¶‰π†$h(x;a_m)$Êúâ‰Ωï‰∏ç‰∏ÄÊ†∑ÔºåÂ¶Ç‰ΩïÂ≠¶‰π†Ôºü AdaBoostÊîπÂèò‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊùÉÂÄºÔºå‰πüÂ∞±ÊòØÊ†∑Êú¨ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÂÖ∂ÊÄùÊÉ≥ÊòØÂ∞ÜÂÖ≥Ê≥®ÁÇπÊîæÂú®Ë¢´ÈîôËØØÂàÜÁ±ªÁöÑÊ†∑Êú¨‰∏äÔºåÂáèÂ∞è‰∏ä‰∏ÄËΩÆË¢´Ê≠£Á°ÆÂàÜÁ±ªÁöÑÊ†∑Êú¨ÊùÉÂÄºÔºåÊèêÈ´òÈÇ£‰∫õË¢´ÈîôËØØÂàÜÁ±ªÁöÑÊ†∑Êú¨ÊùÉÂÄº„ÄÇ Âº±ÂàÜÁ±ªÂô®ÊùÉÂÄº$Œ≤_m$Â¶Ç‰ΩïÁ°ÆÂÆöÔºü AdaBoostÈááÁî®Âä†ÊùÉÂ§öÊï∞Ë°®ÂÜ≥ÁöÑÊñπÊ≥ïÔºåÂä†Â§ßÂàÜÁ±ªËØØÂ∑ÆÁéáÂ∞èÁöÑÂº±ÂàÜÁ±ªÂô®ÁöÑÊùÉÈáçÔºåÂáèÂ∞èÂàÜÁ±ªËØØÂ∑ÆÁéáÂ§ßÁöÑÂº±ÂàÜÁ±ªÂô®ÁöÑÊùÉÈáç„ÄÇËøô‰∏™ÂæàÂ•ΩÁêÜËß£ÔºåÊ≠£Á°ÆÁéáÈ´òÂàÜÂæóÂ•ΩÁöÑÂº±ÂàÜÁ±ªÂô®Âú®Âº∫ÂàÜÁ±ªÂô®‰∏≠ÂΩìÁÑ∂Â∫îËØ•ÊúâËæÉÂ§ßÁöÑÂèëË®ÄÊùÉ„ÄÇ ÂéüÁêÜÁêÜËß£Âü∫‰∫éBoostingÁöÑÁêÜËß£ÔºåÂØπ‰∫éAdaBoostÔºåÊàë‰ª¨Ë¶ÅÊêûÊ∏ÖÊ•ö‰∏§ÁÇπÔºö ÊØè‰∏ÄÊ¨°Ëø≠‰ª£ÁöÑÂº±Â≠¶‰π†h(x;am)Êúâ‰Ωï‰∏ç‰∏ÄÊ†∑ÔºåÂ¶Ç‰ΩïÂ≠¶‰π†ÔºüÂº±ÂàÜÁ±ªÂô®ÊùÉÂÄºŒ≤mÂ¶Ç‰ΩïÁ°ÆÂÆöÔºüÂØπ‰∫éÁ¨¨‰∏Ä‰∏™ÈóÆÈ¢òÔºåAdaBoostÊîπÂèò‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊùÉÂÄºÔºå‰πüÂ∞±ÊòØÊ†∑Êú¨ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºåÂÖ∂ÊÄùÊÉ≥ÊòØÂ∞ÜÂÖ≥Ê≥®ÁÇπÊîæÂú®Ë¢´ÈîôËØØÂàÜÁ±ªÁöÑÊ†∑Êú¨‰∏äÔºåÂáèÂ∞è‰∏ä‰∏ÄËΩÆË¢´Ê≠£Á°ÆÂàÜÁ±ªÁöÑÊ†∑Êú¨ÊùÉÂÄºÔºåÊèêÈ´òÈÇ£‰∫õË¢´ÈîôËØØÂàÜÁ±ªÁöÑÊ†∑Êú¨ÊùÉÂÄº„ÄÇÁÑ∂ÂêéÔºåÂÜçÊ†πÊçÆÊâÄÈááÁî®ÁöÑ‰∏Ä‰∫õÂü∫Êú¨Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïËøõË°åÂ≠¶‰π†ÔºåÊØîÂ¶ÇÈÄªËæëÂõûÂΩí„ÄÇ ÂØπ‰∫éÁ¨¨‰∫å‰∏™ÈóÆÈ¢òÔºåAdaBoostÈááÁî®Âä†ÊùÉÂ§öÊï∞Ë°®ÂÜ≥ÁöÑÊñπÊ≥ïÔºåÂä†Â§ßÂàÜÁ±ªËØØÂ∑ÆÁéáÂ∞èÁöÑÂº±ÂàÜÁ±ªÂô®ÁöÑÊùÉÈáçÔºåÂáèÂ∞èÂàÜÁ±ªËØØÂ∑ÆÁéáÂ§ßÁöÑÂº±ÂàÜÁ±ªÂô®ÁöÑÊùÉÈáç„ÄÇËøô‰∏™ÂæàÂ•ΩÁêÜËß£ÔºåÊ≠£Á°ÆÁéáÈ´òÂàÜÂæóÂ•ΩÁöÑÂº±ÂàÜÁ±ªÂô®Âú®Âº∫ÂàÜÁ±ªÂô®‰∏≠ÂΩìÁÑ∂Â∫îËØ•ÊúâËæÉÂ§ßÁöÑÂèëË®ÄÊùÉ„ÄÇ ÂÖ¨ÂºèÊé®ÂØºÊåáÊï∞ÊçüÂ§±ÂáΩÊï∞ L(Y,f(x))=exp(-Yf(x))ÊùÉÈáçÊõ¥Êñ∞ÂÖ¨Âºè: ÈááÁî®ÁöÑÊåáÊï∞ËØØÂ∑ÆÂáΩÊï∞ l_{exp}(a_th_t|D_t)=E(exp(-f(x)a_th_t(x)))\\ =p(f(x)=h_t(x))e^{-at}+p(f(x)!=h_t(x))e^{at}\\ =e^{-at}(1-\xi)+e^{at}\xi a_t=\frac{1}{2}ln \frac{1-\xi}{\xi}ÂàÜÂ∏ÉÊõ¥Êñ∞ÂÖ¨Âºè \begin{aligned} l\left(H_{t-1}(x)+\alpha h_{t}(x) | D\right) &=E_{X \sim D}\left(\exp \left(-y(x)\left(H_{t-1}(x)+\alpha h_{t}(x)\right)\right)\right) \\ &=E_{x \sim D}\left(\exp \left(-y(x) H_{t-1}(x)\right) \exp \left(-y(x) \alpha h_{t}(x)\right)\right) \end{aligned}Âú®Ê≥∞ÂãíÂ±ïÂºÄ$exp(-y(x)h_t(x))$ \begin{aligned} l\left(H_{t-1}(x)+h_{t}(x) | D\right) & \approx E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-\alpha y(x) h_{t}(x)+\frac{\alpha^{2} y^{2}(x) h_{t}^{2}(x)}{2}\right)\right] \\ &=E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\left(1-y(x) h_{t}(x)+0.5 \alpha^{2}\right)\right] \end{aligned} \begin{aligned} h(x) &=\arg \min _{h} l\left(H_{t-1}(x)+\alpha h_{t} | D\right) \\ &=\arg \max _{h} E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right) \alpha y(x) h_{t}(x)\right] \\ &=\arg \max _{h}\left[\frac{\exp \left(-y(x) H_{t-1}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]} y(x) h(x)\right] \end{aligned} ‰ª§‰∏Ä‰∏™Êñ∞ÂàÜÂ∏É,Ê≥®ÊÑèÂàÜÂ≠êÊòØÂ∏∏Êï∞ D_{t}(x)=\frac{D(x) \exp \left(-y(x) H_{t-1}(x)\right)^{L}}{E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right]} \begin{aligned} h(x) &=\arg \max _{h} E_{x \sim D,}(y(x) h(x)) \\ &=\arg \max _{h} E_{x \sim D_{t}}(1-2 \mathcal{I}(y(x) \neq h(x))) \\ &=\arg \min _{h} E_{x \sim D_{i}}(\mathcal{I}(y(x) \neq h(x))) \end{aligned}ÂêåÁêÜÂèØÂæó \begin{aligned} D_{t+1} &=\frac{D(x) \exp \left(-y(x) H_{t}(x)\right)}{E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \\ &=\frac{D_{t}(x) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t-1}(x)\right)\right] \cdot \exp \left(-y(x) H_{t}(x)\right)}{\exp \left(-y(x) H_{t-1}(x)\right) \cdot E_{x \sim D}\left[\exp \left(-y(x) H_{t}(x)\right)\right]} \\ &=D_{t}(x) \exp \left(-y(x) \alpha h_{t}(x)\right) \cdot C . \quad(C i s a \text {constant}) \end{aligned} Z_{t}=\sum_{i}^{m} D_{t}(x) \exp \left(-y(x) \alpha_{t} h_{y}(x)\right)ÊåáÊï∞ËØØÂ∑ÆÂáΩÊï∞ \begin{aligned} l(H(x) | D) &=\frac{1}{m} \sum_{i}^{m} \exp \left(-y_{i} H\left(x_{i}\right)\right) \\ &=\frac{1}{m} \sum_{i}^{m} \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ &=\sum_{i}^{m} D_{1}\left(x_{i}\right) \exp \left(-\sum_{j}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ &=Z_{1} Z_{2}\left(x_{i}\right) \exp \left(-\sum_{j=2}^{T} \alpha_{j} y_{i} h_{j}\left(x_{i}\right)\right) \\ & \vdots \\ &=\prod_{i=1}^{T} Z_{i} \end{aligned}ÁÆóÊ≥ïÊèèËø∞ÊÄªÁªì‰∏Ä‰∏ãÔºåÂæóÂà∞AdaBoostÁöÑÁÆóÊ≥ïÊµÅÁ®ãÔºö ËæìÂÖ•ÔºöËÆ≠ÁªÉÊï∞ÊçÆÈõÜ$T={(x1,y1),(x2,y2),(xN,yN)}T={(x1,y1),(x2,y2),(xN,yN)}$ÔºåÂÖ∂‰∏≠Ôºå$xi‚ààX‚äÜRnxi‚ààX‚äÜRnÔºåyi‚ààY=‚àí1,1yi‚ààY=‚àí1,1Ôºå$Ëø≠‰ª£Ê¨°Êï∞M ÂàùÂßãÂåñËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊùÉÂÄºÂàÜÂ∏ÉÔºö$D1=(w1,1,w1,2,‚Ä¶,w1,i),w,i=1,2,‚Ä¶,N$„ÄÇ ÂØπ‰∫é$m=1,2,‚Ä¶,M$ (a) ‰ΩøÁî®ÂÖ∑ÊúâÊùÉÂÄºÂàÜÂ∏É$D_m$ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜËøõË°åÂ≠¶‰π†ÔºåÂæóÂà∞Âº±ÂàÜÁ±ªÂô®$h_m(x)$ (b) ËÆ°ÁÆó$h_m(x)$Âú®ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏äÁöÑÂàÜÁ±ªËØØÂ∑ÆÁéáÔºö $e_m=‚àë_{i=1}^{N}w_m,iI(h_m(xi)‚â†y_i)$ (c) ËÆ°ÁÆó$h_m(x)$Âú®Âº∫ÂàÜÁ±ªÂô®‰∏≠ÊâÄÂç†ÁöÑÊùÉÈáçÔºö $\alpha_m=\frac{1}{2}log(\frac{1‚àíe_m}{e_m})$ (d) Êõ¥Êñ∞ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÊùÉÂÄºÂàÜÂ∏ÉÔºàËøôÈáåÔºå$z_mÊòØÂΩí‰∏ÄÂåñÂõ†Â≠êÔºå‰∏∫‰∫Ü‰ΩøÊ†∑Êú¨ÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÂíå‰∏∫1ÔºâÔºö w_{m+1,i}=\frac{w_{m,i}}exp(‚àíŒ±_my_ih_m(xi))Ôºåi=1,2,‚Ä¶,10z_m=‚àë_{i=1}^{N}w_{m,i}exp(‚àíŒ±_my_ih_m(xi)) ÂæóÂà∞ÊúÄÁªàÂàÜÁ±ªÂô®Ôºö F(x)=sign(‚àë_{i=1}^{N}Œ±_mh_m(x))Èù¢Áªè‰ªäÂπ¥8ÊúàÂºÄÂßãÊâæÂ∑•‰ΩúÔºåÂèÇÂä†Â§ßÂéÇÈù¢ËØïÈóÆÂà∞ÁöÑÁõ∏ÂÖ≥ÈóÆÈ¢òÊúâÂ¶Ç‰∏ãÂá†ÁÇπÔºö ÊâãÊé®AdaBoost ‰∏éGBDTÊØîËæÉ AdaBoostÂá†ÁßçÂü∫Êú¨Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÂì™‰∏™ÊäóÂô™ËÉΩÂäõÊúÄÂº∫ÔºåÂì™‰∏™ÂØπÈáçÈááÊ†∑‰∏çÊïèÊÑüÔºü ÁÆóÊ≥ïÊµÅÁ®ãÂÆû‰æãËÆ°ÁÆóPythonÂÆûÁé∞https://www.cnblogs.com/davidwang456/articles/8927029.html]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Boosting, AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÊîØÊåÅÂêëÈáèÂõûÂΩí]]></title>
    <url>%2F2019%2F03%2F19%2FSVR%2F</url>
    <content type="text"><![CDATA[[TOC] ÊîØÊåÅÂêëÈáèÊú∫Áî®‰∫éÂàÜÁ±ª:Á°¨Èó¥ÈöîÂíåËΩØ‰ª∂Èó¥ÈöîÊîØÊåÅÂêëÈáèÊú∫„ÄÇÂ∞ΩÂèØËÉΩÂàÜÂØπ ÊîØÊåÅÂêëÈáèÊú∫ÂõûÂΩíÔºö Â∏åÊúõ$f(x)$‰∏é$y$Â∞ΩÂèØËÉΩÁöÑÊé•Ëøë„ÄÇ ÊîØÊåÅÂêëÈáèÊú∫Âü∫Êú¨ÊÄùÊÉ≥Ëã±ÊñáÂêç:support vector regression ÁÆÄËÆ∞ÔºöSVR Ê†áÂáÜÁöÑÁ∫øÊÄßÊîØÊåÅÂêëÈáèÂõûÂΩíÊ®°ÂûãÂ≠¶‰π†ÁöÑÊ®°Âûã: f(x)=w^Tx+bÂÅáËÆæËÉΩÂÆπÂøç$f(x)$‰∏é$y$‰πãÈó¥Â∑ÆÂà´ÁªùÂØπÂÄº$\xi$,ËøôÂ∞±‰ª•$f(x)=w^Tx+b$ÂΩ¢Êàê‰∫Ü‰∏Ä‰∏™$2\xi$ÁöÑÈó¥ÈöîÂ∏¶ÔºåÂõ†Ê≠§Ê®°Âûã \min \frac{1}{2}w^Tw\\ s.t -\xi]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>ÊîØÊåÅÂêëÈáèÊú∫ÂõûÂΩí</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F17%2FSVMClassifiar%2F</url>
    <content type="text"><![CDATA[title: ÊîØÊåÅÂêëÈáèÊú∫(SVM) ‚Äî‚Äî- ÂàÜÁ±ªÂô®date: 2019-03-17 08:50:59tags: ÊîØÊåÅÂêëÈáèÊú∫categories: Êú∫Âô®Â≠¶‰π†mathjax: true [TOC] È¢ÑÂ§áÁöÑÊï∞Â≠¶Áü•ËØÜÁ∫¶Êùü‰ºòÂåñÈóÆÈ¢òÂéüÈóÆÈ¢ò,Â∏¶Á≠âÂºèÁ∫¶ÊùüÔºå‰πüÂ∏¶‰∏çÁ≠âÂºèÁ∫¶ÊùüÁöÑ‰∏ÄËà¨Á∫¶ÊùüÈóÆÈ¢ò \begin{cases} \min_{x}f(x)\\ s.t \begin{cases} m_i(x)>=0, i=1,..,m\\ n_j(x)=0Ôºåj=1,..,m\\ \end{cases} \end{cases}\tag{1}ÊûÑÈÄ†lagrange‰πòÂ≠êÊ≥ï L(x,\lambda_i,\eta_j)= f(x)-\sum_{i=1}^{m}\lambda_im_i(x)-\sum_{j=1}^{n}\eta_j \tag{2} \begin{cases} \min_{x} max_{\lambda_i,\eta_j} L(R^p)\\ s.t \lambda_i>=0 \end{cases}‰∏äËø∞‰∏§‰∏™ÈóÆÈ¢òÁöÑÁ≠â‰ª∑ÊÄßËØÅÊòé Â¶ÇÊûúx‰∏çÊª°Ë∂≥Á∫¶Êùü$m_i(x)$,Âàô$\lambda_i&gt;=0$,ÂêåÊó∂$m_i(x)&lt;$,Âàô$L(R^{p},\lambda,\eta)$Ë∂ãËøëÊó†Á©∑ÔºåÂèç‰πãÔºåÂàôÂ≠òÂú®ÊúÄÂ§ßÂÄº min_{x} max_{\lambda,\eta}=min_{x}(max fÊª°Ë∂≥Êù°‰ª∂,max f‰∏çÊª°Ë∂≥Á∫¶Êùü)\\=min_{x} max_{\lambda,\eta}{fÊª°Ë∂≥Êù°‰ª∂}ÂØπÂÅ∂ÈóÆÈ¢ò: ÂÖ≥‰∫é$\lambda,\eta‚Äã$ÁöÑÊúÄÂ§ßÂåñÈóÆÈ¢ò max min L(x,\lambda,\eta)\\ s.t \lambda_i>=0‚ÄãÂº±ÂØπÂÅ∂ÈóÆÈ¢òÔºöÂØπÂÅ∂ÈóÆÈ¢ò&lt;=ÂéüÈóÆÈ¢ò ËØÅÊòé: $max_{x} min(\lambda \eta ) L&lt;=min_{\eta,\lambda } max_{x} L$ \underbrace{\min_{x}L(x,\lambda,\eta)}_{A(\lambda,\eta)}0\end{cases}‚ÄãÊ≥®ÊÑèÔºå$y_i(w^Tx_i+b)&gt;0$,ÊâÄ‰ª•$\exists r&gt;0, min(y_i(w^Tx_i+b))=r$,ÂèØ‰ª§$r=1$,ËøôÊòØÂØπË∂ÖÂπ≥Èù¢ËåÉÊï∞ÁöÑÂõ∫ÂÆö‰ΩúÁî®ÔºåÂõ†‰∏∫$y=w^Tx+b$Âíå$y=2w^T+2b$ÊòØÂêå‰∏Ä‰∏™Ë∂ÖÂπ≥Èù¢ÔºåÊÄªËÉΩÊâæÂà∞Áº©Êîæ$w,b$‰ΩøÂæóÔºåÂèØ‰ª•Â∞Ü$r$Áº©ÊîæÂà∞1 \Longrightarrow\begin{cases} max \frac{1}{||w||}\\ st. y_i(w^Tx_i+b)>=1\end{cases}\Longrightarrow\begin{cases} \min \frac{1}{2}w^Tw\\ st. y_i(w^Tx_i+b)>=1\end{cases}ËøôÊòØ‰∏Ä‰∏™Âúü‰∫åÊ¨°ËßÑÂàíÈóÆÈ¢ò Á¨¨‰∫åÂÆù ÂØπÂÅ∂Âà©Áî®lagrange‰πòÂ≠êÊ≥ïÂæóÂá∫ÂØπÂÅ∂ÈóÆÈ¢ò Â∏¶Á∫¶Êùü \begin{cases} \min \frac{1}{2}w^Tw\\ st. y_i(w^Tx_i+b)-1>=0\end{cases}‚Äã\Longrightarrow L(w,b,\lambdaÔºâ=\frac{1}{2}w^Tw-\sum_{i=1}^{N}\lambda_i(1-y_i(w^Tx_i+b)Êó†Á∫¶Êùü \begin{cases}min_{w,b} max_{\lambda}L(w,b,\lambda) \\ s.t \lambda_i>=0\end{cases}‚ÄãÊ≠§Êó∂ÂÖ≥‰∫é$w,b‚Äã$Êó†Á∫¶ÊùüÁöÑ„ÄÇ ÂØπ$(L(w,b,\lambda))‚Äã$ ÂØπ$w‚Äã$,$b‚Äã$Ê±ÇÂÅèÂØº \frac{\partial L}{\partial w}=w+\sum_{i=1}^{N}y_ix_i\lambda_i=0 \Longrightarrow w=-\sum_{i=1}^{N}y_ix_i\lambda_i\\ \frac{\partial L}{\partial b}=-\sum_{i=1}^{N}\lambda_iy_i=0Â∏¶Âõû$L(w,b,\lambda)‚Äã$,ÂèØÂæóÂØπÂÅ∂ÈóÆÈ¢ò \begin{cases} max_{\lambda}L(w,b,\lambda ) =-\frac{1}{2}\sum_i^N\sum_j^N\lambda_i \lambda_jy_iy_jx_i^Tx_j +\sum_i^N\lambda_i \\ s .t. \sum_{i=1}^N\lambda_iy_i,\lambda_i>=0\end{cases} \Longrightarrow\\\begin{cases} min_{\lambda}L(w,b,\lambda ) =\frac{1}{2}\sum_i^N\sum_j^N\lambda_i \lambda_jy_iy_jx_i^Tx_j -\sum_i^N\lambda_i \\ s .t. \sum_{i=1}^N\lambda_iy_i,\lambda_i>=0\end{cases}ÂéüÈóÆÈ¢òÂíåÂØπÂÅ∂ÈóÆÈ¢òÊúâÁõ∏ÂêåËß£ÁöÑÂÖÖË¶ÅÊù°‰ª∂Êª°Ë∂≥ KKT \begin{cases} \frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0,\frac{\partial L}{\partial \lambda}=0\\ \lambda_i(y_i(w^Tx_i+b)-1)=0\\ \lambda_i>=0\\ y_i(w^Tx_i+b)-1>=0 \end{cases}Â¶ÇÊûúÂ≠òÂú®$(x_k,y_k)=+1or -1‚Äã$‰ΩøÂæó‚Äã$y_i(w^Tx_i+b)-1=0‚Äã$Âç≥ÂèØÊ±ÇËß£$b=y_k-\sum_{i=0}^{N}\lambda_ix_i^Tx_k‚Äã$ ‰ª£ÂÖ•Ê®°Âûã f(x)=sign(\sum_i^Na_iy_ix_i^Tx+y_k-\sum_{i=0}^{N}\lambda_ix_i^Tx_k)Ê≥®ÊÑèÔºåÂØπ‰∫é‰ªªÊÑèÁöÑËÆ≠ÁªÉÊ†∑Êú¨ÔºåÊÄªÊúâ$\lambda_i=0$ÊàñËÄÖ$y_if(x_i)=1$,Â¶ÇÊûú$\lambda_i&gt;0$,ËØ¥ÊòéÊ†∑Êú¨ÁÇπËêΩÂú®ÊúÄÂ§ßÈó¥ÈöîÁöÑËæπÁïå‰∏äÔºåËøô‰∫õÁÇπÂ∞±ÊòØÊîØÊåÅÂêëÈáèÔºåËøôÊù°ËæπÁïå$w^Tx+b=1or-1$ soft-marign ËΩØÈó¥Èöî ÊÉ≥Ê≥ïÔºöÂÖÅËÆ∏‰∏ÄÈÉ®ÂàÜÊ†∑Êú¨ÂèØ‰ª•‰∏çË¢´Ê≠£Á°ÆÂàÜÁ±ª ‰ºòÂåñÁõÆÊ†á \min_{w,b} \frac{1}{2}w^Tw+loss‰∏Ä‰∫õÊçüÂ§±ÂáΩÊï∞ 0-1ÊçüÂ§± ‰∏™Êï∞ loss=\sum_{i=1}^NI\{y_i(w^Tx+b)=0,\\ 1-y_i(w^tx_i+b), y_i(w^Tx_i+b)=1-\xi_i\\ \xi_i>=0 \end{cases} ÊåáÊï∞ÊçüÂ§±Ôºàexponential loss ) l_{exp}(z)=exp(-z) ÂØπÁéáÊçüÂ§±logistic loss l_{log}(z)=log(1+exp(-z)Ôºâ Ê†∏ÊñπÊ≥ïÊ†∏ÂáΩÊï∞ÁöÑÂÆö‰πâËÆæ $\chi$‰∏∫ËæìÂÖ•Á©∫Èó¥ÔºàInput SpaceÔºâÔºå $\mathrm{H}$‰∏∫ÁâπÂæÅÁ©∫Èó¥(Feature Space,‰∏ÄÂÆöÊòØÂ∏åÂ∞î‰ºØÁâπÁ©∫Èó¥ÔºâÔºåÂ≠òÂú®‰∏Ä‰∏™Êò†Â∞Ñ \varphi : \chi \rightarrow \mathrm{H}ÂØπ‰ªªÊÑèÁöÑ $x, y \in \mathrm{X}‚Äã$ÔºåÂáΩÊï∞ $K(x, y)‚Äã$ÔºåÊª°Ë∂≥ K(x, y)=ÂàôÁß∞ $K(x, y)$‰∏∫Ê†∏ÂáΩÊï∞„ÄÇÂèØ‰ª•ÁúãÂá∫ÔºåÊàë‰ª¨Âπ∂‰∏çÈúÄË¶ÅÁü•ÈÅìËæìÂÖ•Á©∫Èó¥ÂíåÁâπÂæÅÁ©∫Èó¥Êª°Ë∂≥ÁöÑÊò†Â∞ÑÂÖ≥Á≥ª ÔºåÂè™ÈúÄË¶ÅÁü•ÈÅìÊ†∏ÂáΩÊï∞Â∞±ÂèØ‰ª•ÁÆóÂá∫ÔºåËæìÂÖ•Á©∫Èó¥‰∏≠‰ªªÊÑè‰∏§ÁÇπÊò†Â∞ÑÂà∞ÁâπÂæÅÁ©∫Èó¥ÁöÑÂÜÖÁßØ„ÄÇ]]></content>
  </entry>
  <entry>
    <title><![CDATA[ÂõûÂΩíÊ†ë]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%9B%9E%E5%BD%92%E6%A0%91%2F</url>
    <content type="text"><![CDATA[[TOC] ÂàÜÁ±ªÊ†ë‰∏éÂõûÂΩíÊ†ëÂàÜÁ±ªÊ†ëÁî®‰∫éÂàÜÁ±ªÈóÆÈ¢ò„ÄÇÂàÜÁ±ªÂÜ≥Á≠ñÊ†ëÂú®ÈÄâÂèñÂàíÂàÜÁÇπÔºåÁî®‰ø°ÊÅØÁÜµ„ÄÅ‰ø°ÊÅØÂ¢ûÁõä„ÄÅÊàñËÄÖ‰ø°ÊÅØÂ¢ûÁõäÁéá„ÄÅÊàñËÄÖÂü∫Â∞ºÁ≥ªÊï∞‰∏∫Ê†áÂáÜ„ÄÇClassification tree analysis is when the predicted outcome is the class to which the data belongs. ÂõûÂΩíÂÜ≥Á≠ñÊ†ëÁî®‰∫éÂ§ÑÁêÜËæìÂá∫‰∏∫ËøûÁª≠ÂûãÁöÑÊï∞ÊçÆ„ÄÇÂõûÂΩíÂÜ≥Á≠ñÊ†ëÂú®ÈÄâÂèñÂàíÂàÜÁÇπÔºåÂ∞±Â∏åÊúõÂàíÂàÜÁöÑ‰∏§‰∏™ÂàÜÊîØÁöÑËØØÂ∑ÆË∂äÂ∞èË∂äÂ•Ω„ÄÇ Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient‚Äôs length of stay in a hospital)„ÄÇ ÂõûÂΩíÊ†ëËã±ÊñáÂêçÂ≠óÔºöRegression Tree ÂéüÁêÜ‰ªãÁªçÂÜ≥Á≠ñÊ†ëÊúÄÁõ¥ËßÇÁöÑÁêÜËß£ÂÖ∂ÂÆûÂ∞±ÊòØÔºåËæìÂÖ•ÁâπÂæÅÁ©∫Èó¥($R^n$)ÔºåÁÑ∂ÂêéÂØπÁâπÂæÅÁ©∫Èó¥ÂÅöÂàíÂàÜÔºåÊØè‰∏Ä‰∏™ÂàíÂàÜÂ±û‰∫éÂêå‰∏ÄÁ±ªÊàñËÄÖÂØπ‰∫é‰∏Ä‰∏™ËæìÂá∫ÁöÑÈ¢ÑÊµãÂÄº„ÄÇÈÇ£‰πàËøô‰∏™ÁÆóÊ≥ïÈúÄË¶ÅËß£ÂÜ≥ÁöÑÈóÆÈ¢òÊòØ1. Â¶Ç‰ΩïÂÜ≥Á≠ñËæπÁïå(ÂàíÂàÜÁÇπ)Ôºü2. Â∞ΩÂèØËÉΩÂ∞ëÁöÑÊØîËæÉÊ¨°Êï∞(ÂÜ≥Á≠ñÊ†ëÁöÑÂΩ¢Áä∂) Â¶Ç‰∏äÂõæÔºåÊØè‰∏Ä‰∏™ÈùûÂè∂Â≠êÂØπ‰∫éÊüê‰∏™ÁâπÂæÅÁöÑÂàíÂàÜ„ÄÇ ÊúÄÂ∞è‰∫å‰πòÂõûÂΩíÊ†ëÁîüÊàêÁÆóÊ≥ïQ1: ÈÄâÊã©ÂàíÂàÜÁÇπÔºüÈÅçÂéÜÊâÄÊúâÁöÑÁâπÂæÅ($n$),ÂØπ‰∫éÊØè‰∏Ä‰∏™ÁâπÂæÅÂØπÂ∫î$s_i$‰∏™ÂèñÂÄºÔºåÂ∞ùËØïÂÆåÊâÄÊúâÁâπÂæÅÔºå‰ª•ÂèäÁâπÂæÅÊâÄ‰ª•ÊúâÂàíÂàÜÔºåÈÄâÊã©‰ΩøÂæóÊçüÂ§±ÂáΩÊï∞ÊúÄÂ∞èÁöÑÈÇ£ÁªÑÁâπÂæÅ‰ª•ÂèäÁâπÂæÅÁöÑÂàíÂàÜÂèñÂÄº„ÄÇ Q2: Âè∂ËäÇÁÇπÁöÑËæìÂá∫ÔºüÂèñÊØè‰∏™Âå∫ÂüüÊâÄ‰ª•ÁªìÊûúÁöÑÂπ≥ÂùáÊï∞‰Ωú‰∏∫ËæìÂá∫ ËäÇÁÇπÁöÑÊçüÂ§±ÂáΩÊï∞ÁöÑÂΩ¢Âºè \min _{j, s}\left[\min _{c_{1}} Loss(y_i,c_1)+\min _{c_{2}} Loss(y_i,c_2)\right]ËäÇÁÇπÊúâ‰∏§Êù°ÂàÜÊîØÔºå$c1$ÊòØÂ∑¶ËäÇÁÇπÁöÑÂπ≥ÂùáÂÄºÔºå$c2$ÊòØÂè≥ËäÇÁÇπÁöÑÂπ≥ÂùáÂÄºÔºåÊç¢Âè•ËØùËØ¥ÔºåÂàÜ‰∏ÄÊ¨°ÂàíÂàÜÈÉΩÊòØ‰ΩøÂæóÂàíÂàÜÂá∫ÁöÑ‰∏§‰∏™ÂàÜÊîØÁöÑËØØÂ∑ÆÂíåÊúÄÂ∞è„ÄÇÊúÄÁªàÂæóÂà∞ÂáΩÊï∞ÊòØÂàÜÊÆµÂáΩÊï∞ CARTÁÆóÊ≥ïËæìÂÖ•Ôºö ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ ËæìÂá∫ÔºöÂõûÂΩíÊ†ë$f(x)$ ÈÄâÊã©ÊúÄ‰ºòÁöÑÁâπÂæÅ$j$ÂíåÂàÜÂàáÁÇπ$s$ \min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right] ÂØπ‰∫éÈÄâÂÆöÁöÑ$(j,s)$ÂàíÂàÜÂå∫ÂüüÔºåÂπ∂Á°ÆÂÆöËØ•Âå∫ÂüüÁöÑÈ¢ÑÊµãÂÄº ÂØπ‰∏§‰∏™Âå∫ÂüüÈÄíÂΩí1. 2. Áõ¥Âà∞Êª°Ë∂≥ÂÅúÊ≠¢Êù°‰ª∂ ËøîÂõûÁîüÊàêÊ†ë Ê≥®ÔºöÂàÜÂàáÁÇπÈÄâÊã©ÔºöÂÖàÊéíÂ∫èÔºå‰∫åÂàÜ„ÄÇ Python‰ª£Á†ÅËäÇÁÇπÁ±ªÂ±ûÊÄßÔºöÂ∑¶Âè≥ËäÇÁÇπ„ÄÅloss„ÄÅÁâπÂæÅÁºñÂè∑ÊàñËÄÖÁâπÂæÅ„ÄÅÂàÜÂâ≤ÁÇπ 12345678class Node(object): def __init__(self, score=None): # ÊûÑÈÄ†ÂáΩÊï∞ self.score = score self.left = None self.right = None self.feature = None self.split = None ÂõûÂΩíÊ†ëÁ±ªÊûÑÈÄ†ÊñπÊ≥ï 1234class RegressionTree(object): def __init__(self): self.root = Node() self.height = 0 ÁªôÂÆöÁâπÂæÅ„ÄÅÂàíÂàÜÁÇπÔºåËøîÂõûËÆ°ÁÆóMAPE 12345678910111213141516def _get_split_mse(self, X, y, idx, feature, split): ''' X:ËÆ≠ÁªÉÊ†∑Êú¨ËæìÂÖ• y:ËÆ≠ÁªÉÊ†∑Êú¨ËæìÂá∫ idx:ËØ•ÂàÜÊîØÂØπÂ∫îÁöÑÊ†∑Êú¨ÁºñÂè∑ feaure: ÁâπÂæÅ split: ÂàíÂàÜÁÇπ ''' split_x1=X[X[idex,feature]&lt;split] split_y1=y[X[idex,feature]&lt;split] split_x2=X[X[idex,feature]&gt;=split] split_y2=y[X[idex,feature]&gt;=split] split_avg = [np.mean(split_y1), np.mean(split_y2)] split_mape = [np.sum((split_y1-split_avg[0])**2),np.sum((split_y2-split_avg[1])**2)] return split_mse, split, split_avg ËÆ°ÁÆóÁªôÂÆöÁâπÂæÅÁöÑÊúÄ‰Ω≥ÂàÜÂâ≤ÁÇπ ÈÅçÂéÜÁâπÂæÅÊüê‰∏ÄÂàóÁöÑÊâÄÊúâÁöÑ‰∏çÈáçÂ§çÁöÑÁÇπÔºåÊâæÂá∫MAPEÊúÄÂ∞èÁöÑÁÇπ‰Ωú‰∏∫ÊúÄ‰Ω≥ÂàÜÂâ≤ÁÇπ„ÄÇÂ¶ÇÊûúÁâπÂæÅ‰∏≠Ê≤°Êúâ‰∏çÈáçÂ§çÁöÑÂÖÉÁ¥†ÂàôËøîÂõûNone„ÄÇ 12345678910def _choose_split_point(self, X, y, idx, feature): feature_x = X[idx,feature] uniques = np.unique(feature_x) if len(uniques)==1: return Noe mape, split, split_avg = min( (self._get_split_mse(X, y, idx, feature, split) for split in unique[1:]), key=lambda x: x[0]) return mape, feature, split, split_avg ÈÄâÊã©ÁâπÂæÅÈÅçÂéÜÂÖ®ÈÉ®ÁâπÂæÅÔºåËÆ°ÁÆómape,ÁÑ∂ÂêéÁ°ÆÂÆöÁâπÂæÅÂíåÂØπÂ∫îÁöÑÂàáÂâ≤ÁÇπÔºåÊ≥®ÊÑèÂ¶ÇÊûúÊüê‰∏™ÁâπÂæÅÁöÑÂÄºÊòØ‰∏ÄÊ†∑ÁöÑÔºåÂàôËøîÂõûNone12345678910111213141516171819def _choose_feature(self, X, y, idx): m = len(X[0]) split_rets = [x for x in map(lambda x: self._choose_split_point( X, y, idx, x), range(m)) if x is not None] if split_rets == []: return None _, feature, split, split_avg = min( split_rets, key=lambda x: x[0]) idx_split = [[], []] while idx: i = idx.pop() xi = X[i][feature] if xi &lt; split: idx_split[0].append(i) else: idx_split[1].append(i) return feature, split, split_avg, idx_split ÂØπÂ∫îÂè∂Â≠êËäÇÁÇπÔºåÊâìÂç∞Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØ1234def _expr2literal(self, expr): feature, op, split = expr op = "&gt;=" if op == 1 else "&lt;" return "Feature%d %s %.4f" % (feature, op, split) Âª∫Á´ãÂ•Ω‰∫åÂèâÊ†ë‰ª•ÂêéÔºåÈÅçÂéÜÊìç‰Ωú12345678910111213141516171819def _get_rules(self): que = [[self.root, []]] self.rules = [] while que: nd, exprs = que.pop(0) if not(nd.left or nd.right): literals = list(map(self._expr2literal, exprs)) self.rules.append([literals, nd.score]) if nd.left: rule_left = [] rule_left.append([nd.feature, -1, nd.split]) que.append([nd.left, rule_left]) if nd.right: rule_right =[] rule_right.append([nd.feature, 1, nd.split]) que.append([nd.right, rule_right]) Âª∫Á´ã‰∫åÂèâÊ†ëÁöÑËøáÁ®ãÔºå‰πüÂ∞±ÊòØËÆ≠ÁªÉÁöÑËøáÁ®ã ÊéßÂà∂Ê∑±Â∫¶ ÊéßÂà∂ËäÇÂè∂Â≠êËäÇÁÇπÁöÑÊúÄÂ∞ëÊ†∑Êú¨Êï∞Èáè Ëá≥Â∞ëÊúâ‰∏Ä‰∏™ÁâπÂæÅÊòØ‰∏çÈáçÂ§çÁöÑ12345678910111213141516171819202122232425def fit(self, X, y, max_depth=5, min_samples_split=2): self.root = Node() que = [[0, self.root, list(range(len(y)))]] while que: depth, nd, idx = que.pop(0) if depth == max_depth: break if len(idx) &lt; min_samples_split or set(map(lambda i: y[i,0], idx)) == 1: continue feature_rets = self._choose_feature(X, y, idx) if feature_rets is None: continue nd.feature, nd.split, split_avg, idx_split = feature_rets nd.left = Node(split_avg[0]) nd.right = Node(split_avg[1]) que.append([depth+1, nd.left, idx_split[0]]) que.append([depth+1, nd.right, idx_split[1]]) self.height = depth self._get_rules() ÊâìÂç∞Âè∂Â≠êËäÇÁÇπ12345def print_rules(self): for i, rule in enumerate(self.rules): literals, score = rule print("Rule %d: " % i, ' | '.join( literals) + ' =&gt; split_hat %.4f' % score) È¢ÑÊµãÂçïÊ†∑Êú¨ 123456789101112def _predict(self, row): nd = self.root while nd.left and nd.right: if row[nd.feature] &lt; nd.split: nd = nd.left else: nd = nd.right return nd.score # È¢ÑÊµãÂ§öÊù°Ê†∑Êú¨def predict(self, X): return [self._predict(Xi) for Xi in X] 1234567891011121314 def main(): print("Tesing the accuracy of RegressionTree...") X_train=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]]) y_train=np.array([[5.56 ],[5.7],[5.91],[6.4 ],[6.8],[7.05],[8.9],[8.7 ],[9 ],[9.05]]) reg = RegressionTree() print(reg) reg.fit(X=X_train, y=y_train, max_depth=3) reg.print_rules()main() ÁÆÄÂçïÁöÑ‰æãÂ≠êËÆ≠ÁªÉÊï∞ÊçÆ x 1 2 3 4 5 6 7 8 9 10 y 5.56 5.7 5.91 6.4 6.8 7.05 8.9 8.7 9 9.05 Ê†πÊçÆ‰∏äË°®ÔºåÂè™Êúâ‰∏Ä‰∏™ÁâπÂæÅ$x$. ÈÄâÊã©ÊúÄ‰ºòÁöÑÁâπÂæÅ$j$ÂíåÂàÜÂàáÁÇπ$s$ | ÂàÜÂàáÁÇπ(s) | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 6.5 | 7.5 | 8.5 | 9.5 || ‚Äî‚Äî‚Äî‚Äî- | ‚Äî‚Äî- | ‚Äî‚Äî- | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî- | ‚Äî‚Äî- || $c_1$ | 5.56 | 5.63 | 5.72 | 5.89 | 6.07 | 6.24 | 6.62 | 6.88 | 7.11 || $c_2$ | 7.5 | 7.73 | 7.99 | 8.25 | 8.54 | 8.91 | 8.92 | 9.03 | 9.05 || loss | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 | ÂΩìÂàÜÂàáÁÇπÂèñ$s=6.5$,ÊçüÂ§±ÊúÄÂ∞è$l(s=6.5)=1.93$,Ê≠§Êó∂ÂàíÂàÜÂá∫‰∏§‰∏™ÂàÜÊîØÔºåÂàÜÂà´ÊòØ$R_1=\{1,2,3,4,5,6\}$,$c_1=6.42$,$R_2=\{7,8,9,10\}$,$c_2=8.91$ a) ÂØπR1ÁªßÁª≠ÂàíÂàÜ | x | 1 | 2 | 3 | 4 | 5 | 6 || ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî || y | 5.56 | 5.7 | 5.91 | 6.4 | 6.8 | 7.05 | | ÂàÜÂàáÁÇπ(s) | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 || ‚Äî‚Äî‚Äî‚Äî- | ‚Äî‚Äî‚Äî | ‚Äî‚Äî- | ‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî || $c_1$ | 5.56 | 5.63 | 5.72 | 5.89 | 6.07 || $c_2$ | 6.37 | 6.54 | 6.75 | 6.93 | 7.05 || loss | 1.3087 | 0.754 | 0.2771 | 0.4368 | 1.0644 | ÂΩìÂàÜÂàáÁÇπÂèñ$s=3.5$,ÊçüÂ§±ÂáΩÊï∞$l(s=3.6)=0.2771$(ÂÅáËÆæÊ≠§Êó∂Êª°Ë∂≥ÂÅúÊ≠¢Êù°‰ª∂Ôºâ,Ê≠§Êó∂ÂæóÂà∞‰∏§‰∏™ÂàÜÊîØÔºåÂàÜÂà´ÊòØ$R_1=\{1,2,3\}$Ôºå$c_1=5.72$,$R_2={4,,5,6}$,$c_2=6.75$ b) ÂØπR2ÁªßÁª≠ÂàíÂàÜ | x | 7 | 8 | 9 | 10 || ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî | ‚Äî‚Äî || y | 8.9 | 8.7 | 9 | 9.05 | | ÂàÜÂàáÁÇπ(s) | 7.5 | 8.5 | 9.5 || ‚Äî‚Äî‚Äî‚Äî- | ‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî | ‚Äî‚Äî‚Äî || $c_1$ | 8.9 | 8.8 | 8.87 || $c_2$ | 8.92 | 9.03 | 9.05 || loss | 0.0717 | 0.0213 | 0.0467 | ÂΩìÂàÜÂàáÁÇπÂèñ$s=8.5$,ÊçüÂ§±ÂáΩÊï∞$l(s=8,5)=0.0213$(ÂÅáËÆæÊ≠§Êó∂Êª°Ë∂≥ÂÅúÊ≠¢Êù°‰ª∂Ôºâ,Ê≠§Êó∂ÂæóÂà∞‰∏§‰∏™ÂàÜÊîØÔºåÂàÜÂà´ÊòØ$R_1=\{7,8\}$Ôºå$c_1=8.8$,$R_2=\{9,10\}$,$c_2=9.03$ ÂáΩÊï∞Ë°®ËææÂºè $$ \begin{equation} f(x)=\left\{ \begin{aligned} 5.72 &amp; &amp; x&lt;3.5\\ 6.7 5&amp; &amp;3.5&lt;=x&lt;6.5\\ 8.8&amp; &amp;6.5&lt;=x&lt;8.5\\ 9.03&amp; &amp;8.5&lt;=x&lt;10\\ \end{aligned} \right. \end{equation} $$ PythonÂ∫ì1class sklearn.tree.DecisionTreeClassifier(criterion=‚Äôgini‚Äô, splitter=‚Äôbest‚Äô, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False) 12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-"""Created on Wed Mar 13 19:59:53 2019@author: 23230"""import numpy as npfrom sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as pltX=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]])y=np.array([[5.56 ],[5.7],[5.91],[6.4],[6.8],[7.05],[8.9],[8.7],[9 ],[9.05]])# Fit regression modelregr_1 = DecisionTreeRegressor(max_depth=2)regr_2 = DecisionTreeRegressor(max_depth=3)regr_3 = DecisionTreeRegressor(max_depth=4)regr_1.fit(X, y)regr_2.fit(X, y)regr_3.fit(X, y)X_test = np.copy(X)y_1 = regr_1.predict(X_test)y_2 = regr_2.predict(X_test)y_3 = regr_3.predict(X_test) # Plot the resultsplt.figure()plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data")plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2)plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=4", linewidth=2)plt.plot(X_test, y_3, color="r", label="max_depth=8", linewidth=2)plt.xlabel("data")plt.ylabel("target")plt.title("Decision Tree Regression")plt.legend()]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>ÂõûÂΩíÊ†ë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BPÁÆóÊ≥ï]]></title>
    <url>%2F2019%2F03%2F05%2FBP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[[TOC] 1. ÈúÄË¶ÅÁöÑÂæÆÁßØÂàÜÁü•ËØÜ1.1 ÂØºÊï∞ÂØπ‰∫é‰∏ÄÂÖÉÂáΩÊï∞ÔºåÂú®ÂØºÊï∞Â≠òÂú®ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂú®Êüê‰∏ÄÁÇπÁöÑÂØºÊï∞Ôºå‰πüÂ∞±ÊòØËØ•ÁÇπÁöÑÊñúÁéá„ÄÇÂØπ‰∫éÂ§öÂÖÉÂáΩÊï∞ÔºåÂØπ‰∫éÊüê‰∏ÄÁÇπÊ±ÇÂØºÔºåÂàôÈúÄË¶ÅÊåáÊòéÊñπÂêëÔºå‰∏§‰∏™ÁâπÊÆäÁöÑÊñπÂêëÔºå1. ÂÅèÂØºÔºöÂú®ÂùêÊ†áËΩ¥ÊñπÂêëÁöÑÂØºÊï∞ 2. Ê¢ØÂ∫¶ÁöÑÊñπÂêë:ÊÄªÊúâ‰∏Ä‰∏™ÊñπÂêëÊòØÂèòÂåñÊúÄÂø´ÁöÑ„ÄÇ 1.2 Ê±ÇÂØºÁöÑÈìæÂºèÊ≥ïÂàô $x \in R$, $z=g(f(x))$, $y=f(x)$ \frac{\partial z}{\partial x}=\frac{\partial z}{\partial y} \frac{\partial y}{\partial x} $ x \in R^m $, $f(x)$ÊòØ$R^M$Âà∞$R^n$ÁöÑÊò†Â∞ÑÔºå$g(f)$ÊòØ$R^n$Âà∞RÁöÑÊò†Â∞Ñ \frac{\partial g}{\partial x_i}=\sum_j^n \frac{\partial g}{\partial f_i} \frac{\partial f_i}{\partial x_i} Â¶ÇÊûú‰ΩøÁî®ÂêëÈáèË°®Á§∫ \nabla_x^z=(\frac{\partial f}{\partial x})^T \nabla_y^z2. Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï2.1 Ê¢ØÂ∫¶Ê¢ØÂ∫¶ÂÖ∂ÂÆûÊú¨Ë¥®‰πüÊòØ‰∏Ä‰∏™ÂêëÈáèÔºåÂØπ‰∫éÂáΩÊï∞$f(X,y)$Âú®$(W,y)$Ëøô‰∏ÄÁÇπÁöÑÊ¢ØÂ∫¶ $(\frac{\partial f}{\partial X},\frac{\partial f}{\partial y})$Ê¢ØÂ∫¶ÁöÑÂá†‰ΩïÊÑè‰πâÔºöÂú®ËØ•Â∫óÂèòÂåñÂ¢ûÂä†ÊúÄÂø´ÁöÑÂú∞Êñπ 2.2 Ê¢ØÂ∫¶ÁÆóÊ≥ïÁöÑËß£ÈáäÂõæÊù•Ëá™Âê¥ÊÅ©ËææÁöÑÊú∫Âô®Â≠¶‰π†ËØæÁ®ãÈ¢úËâ≤ÂÅèÁ∫¢(A)ÁöÑÂú∞ÊñπÂºÄÂßãÔºåÊ†πÊçÆÊ¢ØÂ∫¶ÁöÑË¥üÊñπÂêëÈÄöËøá9Ê¨°Êõ¥Êñ∞ÔºåËææÂà∞‰∫ÜÊúÄÂ∞èÂÄº(B)„ÄÇÁé∞Âú®ÁªôÂÆö‰∏Ä‰∏™ÁÇπ$A(\theta_0,\theta_1)$,Âπ≤ÂòõÂë¢ÔºåÊàë‰ª¨ÊÉ≥‰ªéAÂà∞BÁÇπÔºàÊúÄÂ∞èÂÄºÁÇπ),Á±ª‰ºº‰∫∫Á±ª‰∏ãÂ±±ÔºåÈúÄË¶ÅÁü•ÈÅìÂæÄÈÇ£‰∏™ÊñπÂêëÂêß„ÄÅËµ∞Â§ßÂ§ö‰∏ÄÊ≠•Âë¢ÔºüÊñπÂêëÔºöÊ¢ØÂ∫¶ÁöÑË¥üÊñπÂêë $ \delta=(\frac{\partial L}{\partial \theta_0},\frac{\partial L}{\partial \theta_1})$)Ê≠•ÈïøÔºöÂ≠¶‰π†ÁéáÔºà$\alpha$)Âõ†Ê≠§ÔºåËÆ°ÁÆó‰∏ÄÊ¨°ÈáåÁõÆÊ†áÊõ¥Ëøë‰∫Ü $(\theta_0,\theta_1)=(\theta_0,\theta_1)-\alpha \dot (\delta)$Âú®ÈáçÂ§ç‰∏ä‰∏§Ê≠•ÔºåÁõ¥Âà∞Êª°ÊÑè‰∏∫Ê≠¢„ÄÇ 3.ËØØÂ∑ÆÂèçÂêë‰º†Êí≠ÁÆóÊ≥ï3.1 ÁêÜËÆ∫Êé®ÂØº 3.1.1 Á¨¶Âè∑ËØ¥Êòé‰∏äÂõæÊòØ‰∏Ä‰∏™LÂ±ÇÁöÑÁ•ûÁªèÁΩëÁªúÔºåËæìÂÖ•Â±Ç‰∏∫Á¨¨‰∏ÄÂ±ÇÔºåÈöêËóèÂ±ÇÔºö2Ëá≥$L-1$Â±ÇÔºåËæìÂá∫Â±ÇL ‰ª§ ËæìÂÖ•ÂêëÈáè $\vec{X}$ \vec{X} = (x_1,x_2,...,x_{m-1},x_m)ËæìÂá∫ÂêëÈáè $\vec{Y}$ \vec{Y}=(y_1,y_2,...,y_{n-1},y_n)$$a Á¨¨jÂ±ÇÈöêËóèÂ±ÇÁöÑËæìÂá∫ÂêëÈáè $\vec{h^{(j)}}$ $$\vec{h^{(j)}}=(h_1^{(j)},h_2^,...,h_{t-1}^{(j)},h_tj^{(j)})ÂÖ∂‰∏≠Ôºå$tj$:Ë°®Á§∫Á¨¨jÁöÑÈöêËóèÂ±Ç‰∏™Êï∞Á¨¨$(l-1)$Â±ÇÁöÑÁ¨¨i‰∏™Á•ûÁªèÂÖÉÂà∞Á¨¨$l$Â±ÇÁöÑÁ¨¨j‰∏™Á•ûÁªèÂÖÉÁöÑËøûÊé•ÊùÉÈáçÔºö$w_{ij}^{(l)}$ÔºåÂàôÁ¨¨$(l-1)$Â±ÇÁ•ûÁªèÂÖÉÂà∞Á¨¨$l$Â±ÇÁ•ûÁªèÂÖÉÁöÑËøûÊé•ÊùÉÈáçÁü©Èòµ W^{(l)}=\left( \begin{matrix}w_{11}^{(l)}& \cdots & w_{1(tj)}\\ & \dots &\\ w_{s(l-1)}^{l}&\cdots&w_{s(l-1)s(l)}^{l} \end{matrix}\right)3.1.2 Êé®ÂØºËøáÁ®ã3.1.2.1 ËØØÂ∑ÆÂÆö‰πâÁöÑËØØÂ∑ÆÂáΩÊï∞,Â∏∏ËßÅÁöÑË°°ÈáèÊÄßÊåáÊ†áËßÅ Êà≥Êàë,ËøôÈáåÈÄâÊã©ÁöÑËØØÂ∑ÆÂπ≥ÊñπÂíåÊúÄÂ∞èÁ¨¨$i$‰∏™ËæìÂá∫ÁöÑËØØÂ∑Æ,ÂÅáËÆæÂÆûÈôÖËæìÂá∫$(d(1),d(2),‚Ä¶,d(n))$Ôºö,‰∏Ä‰∏™ËæìÂÖ•Ê†∑Êú¨ÂØπÂ∫îÁöÑËØØÂ∑Æ E(i)=\frac{1}{2}\sum_{k=1}^n(y(i)-d(i))^2=\frac{1}{2}||y-d||^2ÊâÄÊúâËÆ≠ÁªÉÊ†∑Êú¨($N$)ÁöÑËØØÂ∑ÆÔºö E(i)=\frac{1}{2}\sum_{j=1}^{N}(\sum_{k=1}^n(y(i)-d(i))^2)=\frac{1}{2N}\sum_{j=1}^{N}(||y(i)-d(i)||^2)Âõ†Ê≠§Ôºå E = \frac{1}{2N}\sum_{i=1}^N(||y(i)-d(i)||^2)ÂÖ∂ÂÆûÔºåÁ•ûÁªèÁΩëÁªúÁöÑËæìÂá∫ÊòØÂÖ≥‰∫éËäÇÁÇπÁöÑÂ§çÂêàÂáΩÊï∞„ÄÇ‰ª£‰ª∑ÂáΩÊï∞ÊòØÂÖ≥‰∫é$W$Âíå$b$ÁöÑÂáΩÊï∞„ÄÇ 3.1.2.2 Ê≠£Âêë‰º†Êí≠ËæìÂÖ•Â±Ç$\hat{X}$Ôºö X =(x_1,x_2,x_3,...,x_m)ÂΩìÊúâ$N$‰∏™ËÆ≠ÁªÉÊ†∑Êú¨Êó∂ÔºåÂèØÁî®Áü©ÈòµË°®Á§∫ X=\left( \begin{matrix} x_{11} &x_{12}&...&x_{1m}\\ x_{21} & x_{22}&...&x_{2m}\\ \vdots & \vdots&\dots&\vdots\\ x_{N1} & \vdots&\vdots&x_{Nm}\\ \end{matrix} \right)Á¨¨‰∫åÂ±Ç $h^{(2)}$,‰∏ÄÂÖ±$s2$‰∏™ËäÇÁÇπ:Á¨¨i‰∏™ËäÇÁÇπÁöÑËÆ°ÁÆó h^{(2)}(i)=f(\sum_{j=1}^{s2}x(j)*w_{ji}^{(l)}+b_i)=f(x*w(:,i)+b_i)Áü©ÈòµË°®Á§∫ h^{(2)}=f(x*W^{(l)}+b^{(2)})Á¨¨iÂ±Ç Áü©ÈòµÂΩ¢Âºè h^{(l)}=f(h^{(l-1)}*W^{(l)}+b)3.1.2.3 ÂèçÂêë‰º†Êí≠Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÊõ¥Êñ∞ÊùÉÈáçÔºå‰∏çÊñ≠Ëø≠‰ª£Âà∞ÊúÄ‰ºòËß£„ÄÇÂØπ$w_{ij}$Ê±ÇÂØºÊï∞ÂèØÂæó,ÂèØÊõ¥Êñ∞$w_{ij}$Êõ¥Êñ∞ÂÖ¨ÂºèÔºö w_{ij}=w_{ij}-\alpha \frac{\partial E}{\partial w_{ij}}ÂΩìÁÑ∂ÁÆÄÂçïÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØÁõ¥Êé•ÂÜôÂá∫ÂÖ¨ÂºèÔºåÂΩìÂ§™Â§çÊùÇÁöÑÊó∂ÂÄôÔºåÂºïÂÖ•BPÁÆÄÂåñÊ±ÇÂØº Êñπ‰æø‰π¶ÂÜôÂÖ¨ÂºèÔºåÂØπ‰∫éÁ¨¨iÁöÑËæìÂÖ•$h^{(i-1)}*W^{(i)}+b^{(i)}$ËÆ∞‰Ωú$net^{(i)}$,ÂÖ∂‰∏≠ÔºåÁ¨¨$i$ÁöÑËæìÂÖ•ÂíåËæìÂá∫ÁöÑÂÖ≥Á≥ªÔºå$ËæìÂÖ•=f(ËæìÂá∫)$‰∏ãÈù¢ÂºÄÂßãÊé®ÂØº È¶ñÂÖàÔºåÂØπ‰∫é$L$Â±ÇÔºå ÂØπ‰∫é$W^{(L)}$ÔºåÂÖàÁúãÂØπ$W_{ij}^{(L)}$Ê±ÇÂØºÔºå \frac{\partial E}{\partial W_{ij}^{(L)}} =\frac{\partial E}{\partial y(j)} * \frac{\partial y(i)}{\partial net_{j}^{L}} * \frac{\partial net_{j}^{L}}{\partial W_{ij}^{(L)}}\\ =(y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}h_i^{(L-1)}‰ª§$\delta_i^{(L)}=y(i)-d(i)$ ‰∏äËø∞ÁªôÂá∫‰∫ÜÂçï‰∏™ÂàÜÈáèÁöÑÊ±ÇÂÅèÂØºÁöÑÁªìÊûúÔºåÂØπ‰∫é$W^{(L)}$ \frac{\partial E}{\partial W^{(L)}} =\left[\begin{matrix} \frac{\partial E}{\partial W_{11}^{(L)}} & \frac{\partial E}{\partial W_{12}^{(L)}}&\dots & \frac{\partial E}{\partial W_{1n}^{(L)}}\\ \frac{\partial E}{\partial W_{21}^{(L)}} & \frac{\partial E}{\partial W_{22}^{(L)}}&\dots& \frac{\partial E}{\partial W_{2n}^{(L)}}\\ \vdots& \dots& \dots& \dots\\ \frac{\partial E}{\partial W_{sL,1}^{(L)}} & \frac{\partial E}{\partial W_{sL,2}^{(L)}}&\dots& \frac{\partial E}{\partial W_{sL,n}^{(L)}} \end{matrix}\right] \\= \left[ \begin{matrix} h^{(L-1)}_1\\h^{(L-1)}_2\\ \dots\\h^{(L-1)}_n \end{matrix} \right] *\left[\begin{matrix} \delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\ \delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\ \dots\\ \delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}} \end{matrix}\right] ^T =h^{(L-1)}S^{(L)}ÂÖ∂‰∏≠Ôºå S^{(L)}=\left[\begin{matrix} \delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\ \delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\ \dots\\ \delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}} \end{matrix}\right]^TÂêåÁêÜÂèØÂæóÔºå \frac{\partial E}{\partial b_k^{(L)}}=(y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}ÂÖ∂Ê¨°ÔºåÂØπ‰∫éÈöêÂê´Â±Ç$L-1$Â±ÇÔºåÂØπ$W_{ij}^{(L)}$Ê±ÇÂØº \frac{\partial E}{\partial W_{ij}^{(L-1)}} =\sum_{k=1}^{n}\frac{\partial E}{\partial y(k)} * \frac{\partial y(k)}{\partial net_{k}^{L}} * \frac{\partial net_{k}^{L}}{\partial f(net_j^{(L-1)})}*\frac{\partial f(net_j^{(L-1)})}{\partial net_j^{(L-1)}}*\frac{\partial net_j^{(L-1)}}{\partial W_{ij}^{(L-1)}}\\ =\sum_{k=1}^{n} (y(j)-d(j))*f(x)^{'}|_{x=net_j^{(L)}}W_{kj}^{(L)}f(x)^{'}|_{x=net_j^{L-1}}h_i^{L-2}\\ =\sum_{k=1}^{n}S_i^{(L)}W_{kj}^{(L)}f(x)^{'}|_{x=net_j^{L-1}}h_i^{L-2}\\ÂÜôÂá∫Áü©ÈòµÂΩ¢Âºè,ÂØπ$W^{(L-1)}$ \frac{\partial E}{\partial W^{(L-1)}}=\left[\begin{matrix} h^{(L-2)}_1\\h^{(L-2)}_2\\\vdots\\h^{(L-2)}_{s(L-2)}\end{matrix}\right] \left[\begin{matrix} \delta_1^{(L)}f(x)^{'}|_{x=net_1^{(L)}}\\ \delta_2^{(L)}f(x)^{'}|_{x=net_2^{(L)}}\\ \dots\\ \delta_n^{(L)}f(x)^{'}|_{x=net_n^{(L)}} \end{matrix}\right]^T \left[\begin{matrix} W_{11}^{(L)} & W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\ W_{21}^{(L)} & W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\ \vdots& \dots& \dots& \dots\\ W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)} \end{matrix}\right]^T \\ \left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\ 0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]\\ =h^{(L-2)}S^{(L-1)} S^{(L-1)}=\left(\left[\begin{matrix} f(x)^{'(L)}|_{x=net_1^{(L)}}&0& \dots& 0\\ 0&f(x)^{'}|_{x=net_2^{(L)}}0& \dots& 0\\ 0&\dots&\dots&0\\ 0&0&0&f(x)^{'(L)}|_{x=net_n^{(L)}} \end{matrix}\right]\left[\begin{matrix} \delta_1^{(L)}\\\delta_2^{(L)}\\\vdots\\\delta_n^{(L)}\end{matrix}\right] \right)^T\\ \left[\begin{matrix} W_{11}^{(L)} & W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\ W_{21}^{(L)} & W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\ \vdots& \dots& \dots& \dots\\ W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)}* \end{matrix}\right]^T \left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\ 0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]\\ =S^{(L)}\left[\begin{matrix} W_{11}^{(L)} & W_{12}^{(L)}&\dots & W_{1n}^{(L)}\\ W_{21}^{(L)} & W_{22}^{(L)}&\dots& W_{2n}^{(L)}\\ \vdots& \dots& \dots& \dots\\ W_{s(L-1),1}^{(L)} & W_{s(L-1),2}^{(L)}&\dots& W_{s(L-1),n}^{(L)}* \end{matrix}\right]^T\left[ \begin{array}{ccc}{f^{'(L-1)}\left(net^{(L-1)}_{(1)}\right)} & {0} & {0}&{0} \\ {0} & {f^{'(L-1)}\left(net^{(L-1)}_{(2)}\right)} & {0} &{0}\\ 0 & \dots & \vdots & 0\\{0} & {0} & {0}&{f^{(L-1)}\left(ne t_{s(L-1)}^{(L-1)}\right)}\end{array}\right]*\\ÂØπ$1&lt;l&lt;L$,Ê±Ç$W^{(l)}$ÁöÑÂÅèÂØº, ÊúÄÂêéÔºåÊ†πÊçÆ‰∏äËø∞ÁöÑÊé®ÂØºÂñîÔºåÂæàÂÆπÊòìÂæóÂá∫$S^{(l)}$Âíå$S^{(l+1)}$, S^{(l)}=S^{(l+1)}W^{(l+1)^T}F^{'(l)}(net^{(l)})\\ S^{(L)}=(Y-\hat{Y})F^{'(L)}(net^{(L)}) \frac{\partial E}{\part W^{(l)}}=\left[\begin{matrix}h^{(l-1)}_1\\h^{(l-1)}_2 \\\dots \\h^{(l-1)}_{sl}\end{matrix}\right]S^{(l+1)} \left[\begin{matrix}W_{11}^{(l+1)}&W_{12}^{(l+1)} &\dots& W_{2(sl+1)}^{(l+1)}\\ W_{21}^{(l+1)}&W_{22}^{(l+1)} &\dots& W_{2(sl+1)}^{(l+1)}\\ \dots&\dots&\dots&\dots\\ W_{sl1}^{(l+1)}&W_{sl2}^{(l+1)} &\dots& W_{sl(sl+1)}^{(l+1)}\\ \end{matrix} \right]^T\left[\begin{matrix} \part f^{'(l)}(net_1^{l})&0&\dots & 0\\ 0\\0 &\part f^{'(l)}(net_2^{l})&\dots&0\\ 0 & 0&\dots&0\\ 0&0&\dots&\part f^{'(l)}(net_l^{l})\end{matrix}\right]3.2 BPÁÆóÊ≥ïÁöÑÂ∞èÁªìÁÆóÊ≥ïÂàÜ‰∏∫‰∏§‰∏™Èò∂ÊÆµÔºöÂâçÂêëÈò∂ÊÆµÂíåÂêéÂêë‰º†Êí≠Èò∂ÊÆµ ÂêéÂêëÈò∂ÊÆµÁÆóÊ≥ïÔºö Step 1: ËÆ°ÁÆó$\hat{y}^{(L)}$ Step 2: for l =L:2 ‚Äã ËÆ°ÁÆó$S^{(l)}=S^{(l+1)}W^{(l+1)}F‚Äô(net^{(l)})$ ‚Äã ËÆ°ÁÆó $\Delta W^{(l)}=h^{(l-1)}S^{(l)} $ ‚Äã ËÆ°ÁÆó$W^{(l)}=W^{(l)}-\delta \Delta W^{(l)}$ 3.3 PythonÂÆûÁé∞3.3.1 ÊúÄÁÆÄÂçï‰∏âÂ±ÇÁΩëÁªú1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071'''‰∏çÁî®‰ªª‰ΩïÊ°ÜÊû∂ÔºåËá™Â∑±ÂÜô‰∏Ä‰∏™‰∏âÂ±ÇÁöÑÁ•ûÁªèÁΩëÁªú# input-3,hidden-4 output-1'''import numpy as npnp.random.seed(1)# Input MatrixX = np.array([[0, 0, 1], [0, 1, 1], [1, 0 ,1], [1, 1, 1],])# Output Matrixy = np.array([[0], [1], [1], [0]])# Nonlinear functiondef sigmoid(X,derive=False): if not derive: return 1 / (1 + np.exp(-X)) else: return X*(1-X)# reludef relu(X,derive = False): if not derive: return np.maximum(0,X) else: return (X&gt;0).astype(float) # Weight biasW1 = 2 * np.random.random((3, 4))-1b1 = 0.1 * np.ones((4,)) W2 = 2 * np.random.random((4,1))-1b2 = 0.1 * np.ones((1,)) rate = 0.1noline = relu# Trainingtrain_times = 200 for time in range(train_times): # Layer one A1 = np.dot(X,W1)+b1 Z1 = noline(A1) # Layer two A2 = np.dot(Z1, W2)+b2 Z2 = noline(A2) cost = -y+Z2 # Calc deltas S2= cost*noline(A2,True) delta_W2 = np.dot(Z1.T,S2) bias2 = S2.sum(axis=0) S1 = np.dot(S2, W2.T)*noline(A1,True) delta_W1= np.dot(X.T, S1) bias1 = S1.sum(axis=0) # update W1 = W1-rate*delta_W1 b1 = b1-rate*bias1 W2 = W2-rate*delta_W2 b2 = b2-rate*bias2 print('error',np.mean(((y-Z2)*(y-Z2))**2))print("prediction",Z2) 3.4 ÈôÑÂΩïÔºö Name Abbreviation Mean absolute percentage error MAPE Root mean squares percentage error RMSPE Mean absolute percentage error MAE Mean squares error MSE Index of agreement IA Theil U statistic 1 U1 Theil U statistic 2 U2 Correlation coefficient R MAPE = $\frac{1}{n} \sum_{k=1}^{n}\left|\frac{x^{(0)}(k)-\hat{x}^{(0)}(k)}{x^{(0)}(k)}\right| \times 100$RMSPE = $\sqrt{\frac{1}{n} \sum_{k=1}^{n}\left(\frac{\hat{x}^{(0)}(k)-x^{(0)}(k)}{x^{(0)}(k)}\right)^{2}} \times 100$MAE = $\frac{1}{n} \sum_{k=1}^{n}\left|\hat{x}^{(0)}(k)-x^{(0)}(k)\right|$MSE = $\frac{1}{n} \sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}$IA = $1-\frac{\sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}}{\sum_{k=1}^{n} \left( \left| \hat{x}^{(0)}(k)-\overline{x} \right|+\left| x^{(0)}(k)-\overline{x}\right| \right)^{2}}$U1 = $\frac{\sqrt{\frac{1}{n} \sum_{k=1}^{n}\left(x^{(0)}(k)-x^{(0)}(k)\right)^{2}}}{\sqrt{\frac{1}{n} \sum_{k=1}^{n} x^{(0)}(k)^{2}}+\sqrt{\frac{1}{n} \sum_{k=1}^{n} x^{(0)}(k)^{2}}}$U2 = $\frac{\left[\sum_{k=1}^{n}\left(\hat{x}^{(0)}(k)-x^{(0)}(k)\right)^{2}\right]^{1 / 2}}{\left[\sum_{k=1}^{n} x^{(0)}(k)^{2}\right]^{1 / 2}}$R = $\frac{\operatorname{Cov}(\hat{x}^{(0)}, x^{(0)})}{\sqrt{\operatorname{Var}[\hat{x}^{(0)}] \operatorname{Var}[x^{(0)}]}}$]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÂÜ≥Á≠ñÊ†ë]]></title>
    <url>%2F2019%2F03%2F03%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[‰∏ªË¶ÅÊòØÂàÜ‰∫´ÂÜ≥Á≠ñÁöÑÂü∫Êú¨Áü•ËØÜÁÇπÔºåÈáçÁÇπÂú®ÂàÜÁ±ªÂÜ≥Á≠ñÊ†ë‰∏äÔºåÂØπ‰∫éÂõûÂΩíÁöÑÂÜ≥Á≠ñÊ†ëÂêéÈù¢Âú®ÁªôÂá∫„ÄÇÂ∏åÊúõÂ§ßÂÆ∂ÂíåÊàë‰∏ÄËµ∑ÂÅöÁü•ËØÜÁöÑ‰º†Êí≠ËÄÖÂï¶ÔºÅ:smile: :smiley: :grin: :open_mouth: [TOC] ÂÜ≥Á≠ñÊ†ëËã±ÊñáÂêçÂ≠óÔºöDescision Tree ‰ªÄ‰πàÊòØÂÜ≥Á≠ñÊ†ë‰∏æ‰∏™Ê†°Âõ≠Áõ∏‰∫≤ÁöÑ‰æãÂ≠êÔºå‰ªäÂ§©Ê†°Âõ≠ÁöÑÂ∞èÁå´(Â•≥)ÂíåÂ∞èÁãó(Áî∑)ÂáÜÂ§áÈÖçÂØπÔºåÂ∞èÁå´Â¶Ç‰ΩïÊâçËÉΩÂú®‰ºóÂ§öÁöÑ‰ºòË¥®üê∂ÁöÑÂøÉ‰ª™ÁöÑÁãóÂë¢Ôºü‰∫éÊòØÂë¢ÔºüÊúâ‰∏ÄÂè™Áâπ‰πñÂ∑ßÁöÑÂ∞èÁå´ÊâæÂà∞‰∫Ü‰Ω†Ôºå‰Ω†Ê≠£Âú®Â≠¶‰π†Êú∫Âô®Â≠¶‰π†ÔºåÂàöÂ•ΩÂ≠¶‰π†‰∫ÜÂÜ≥Á≠ñÊ†ëÔºåÂáÜÂ§áÁªôËøôÂè™Áå´Áå´ÊåëÈÄâ‰ºòË¥®ÁãóÔºåÂΩìÁÑ∂Ôºå‰Ω†‰∏ç‰ªÖ‰ªÖÊòØÁõ¥Êé•ÂëäËØâÁå´Âì™‰∫õÁãóÊòØÂêàÈÄÇ‰Ω†ÁöÑÔºü‰Ω†Êõ¥Â∫îËØ•ËØ¶ÁªÜÁöÑÁªôÁå´ËÆ≤Ëß£ÂÜ≥Á≠ñÊ†ëÊòØÂ¶Ç‰ΩïÊ†πÊçÆÂÆÉÊèêÂá∫ÁöÑÊ†áÂáÜÈÄâÂá∫ÁöÑÁ¨¶ÂêàË¶ÅÊ±ÇÁöÑÁãóÂë¢ÔºüÁå´ÁªôÂá∫Â¶Ç‰∏ã‰ø°ÊÅØÔºöÂπ¥ÈæÑ=0.5 6.5&lt;=‰ΩìÈáç&lt;=8.5;ÂøÉ‰ª™; Âπ¥ÈæÑ&gt;=0.5 ‰ΩìÈáç&gt;8.5 ÈïøÁõ∏Â•Ω ÂøÉ‰ª™;ÂÖ∂‰ΩôÊÉÖÂÜµ‰∏çÂøÉ‰ª™; Ê†πÊçÆ‰∏äËø∞Êù°‰ª∂ÂèØ‰ª•ÊûÑÈÄ†‰∏ÄÈ¢óÊ†ëÔºö‰∏äÈù¢ÁöÑÂõæÂ∞±ÊòØÂÜ≥Á≠ñÊ†ëÔºåÊúÄÁªàÁöÑÁªìÊûúÊòØÂøÉ‰ª™ÊàñËÄÖ‰∏çÂøÉ‰ª™„ÄÇÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ï‰ª•Ê†ëÂΩ¢ÁªìÊûÑË°®Á§∫Êï∞ÊçÆÂàÜÁ±ªÁöÑÁªìÊûú Âü∫Êú¨Ê¶ÇÂøµÂÜ≥Á≠ñÊ†ëÂ±û‰∫é‰πüÂè™ËÉΩÈùûÂèÇÊï∞Â≠¶‰π†ÁÆóÊ≥ï„ÄÅÂèØ‰ª•Áî®‰∫éËß£ÂÜ≥(Â§ö)ÂàÜÁ±ªÈóÆÈ¢òÔºåÂõûÂΩíÈóÆÈ¢ò„ÄÇ ÂõûÂΩíÈóÆÈ¢òÁöÑÁªìÊûúÔºåÂè∂Â≠êÁªìÁÇπÁöÑÂπ≥ÂùáÂÄºÊòØÂõûÂΩíÈóÆÈ¢òÁöÑËß£„ÄÇÊ†πËäÇÁÇπÔºöÂÜ≥Á≠ñÊ†ëÂÖ∑ÊúâÊï∞ÊçÆÁªìÊûÑÈáåÈù¢ÁöÑ‰∫åÂèâÊ†ë„ÄÅÊ†ëÁöÑÂÖ®ÈÉ®Â±ûÊÄßÈùûÂè∂Â≠êËäÇÁÇπ ÔºöÔºàÂÜ≥Á≠ñÁÇπÔºâ ‰ª£Ë°®ÊµãËØïÁöÑÊù°‰ª∂ÔºåÊï∞ÊçÆÁöÑÂ±ûÊÄßÁöÑÊµãËØïÂè∂Â≠êËäÇÁÇπ ÔºöÂàÜÁ±ªÂêéËé∑ÂæóÂàÜÁ±ªÊ†áËÆ∞ÂàÜÊîØÔºö ÊµãËØïÁöÑÁªìÊûú Êï∞Â≠¶ÈóÆÈ¢ò-ÁÜµ-GiniÁ≥ªÊï∞‰ªÄ‰πàÊòØÁÜµÔºöÁÜµÁöÑÊ¶ÇÂøµÊ∫ê‰∫éÁâ©ÁêÜÂ≠¶ÔºåÁî®‰∫éÂ∫¶Èáè‰∏Ä‰∏™ÁÉ≠ÂäõÂ≠¶Á≥ªÁªüÁöÑÊó†Â∫èÁ®ãÂ∫¶„ÄÇ‰ø°ÊÅØÁÜµÔºö‰∏çÂæó‰∏çÊèêÈ¶ôÂÜúËøô‰∏™Â§ßÂÜôÁöÑ‰∫∫Âï¶ÔºÅ‰ø°ÊÅØËÆ∫ÈáåÈù¢ÁöÑÁü•ËØÜ„ÄÇÂú®‰ø°ÊÅØËÆ∫ÈáåÈù¢Ôºå‰ø°ÊÅØÁÜµË°°Èáè‰ø°ÊÅØÈáèÁöÑÂ§ßÂ∞èÔºå‰πüÂ∞±ÊòØÂØπÈöèÊú∫ÂèòÈáè‰∏çÁ°ÆÂÆöÂ∫¶ÁöÑ‰∏Ä‰∏™Ë°°Èáè„ÄÇÁÜµË∂äÂ§ßÔºå‰∏çÁ°ÆÂÆöÊÄßË∂äÂ§ßÔºõÂØπ‰∫éÊüê‰∏™ÂçïÁ¨¶Âè∑Êó†ËÆ∞ÂøÜ‰ø°Ê∫êÔºåÂèëÂá∫Á¨¶Âè∑($x_i$)ÁöÑÊ¶ÇÁéáÊòØ$p_i$,Ê¶ÇÁéáË∂äÂ§ßÔºåÁ¨¶Âè∑ÁöÑ‰ø°ÊÅØÈáèÂ∞±Ë∂äÂ∞èÔºåÈ¶ôÂÜúÂÖ¨Âºè $I(x_i)=-log_{p_i}$„ÄÇ‰ø°Ê∫êÊâÄÂê´ÁöÑ‰ø°ÊÅØÁÜµÂ∞±ÊòØ‰ø°ÊÅØÈáèÁöÑÊúüÊúõ]$H(x)=-\sum p_i*log_{p_i}$GiniÁ≥ªÊï∞Ôºö $Gimi(p) = 1-\sum_{k=1}^{K}p_k^2$ ÂÜ≥Á≠ñÊ†ëÂ¶Ç‰ΩïÊûÑÂª∫ÁöÑÈóÆÈ¢òËá™ÊàëÊèêÈóÆÈò∂ÊÆµÔºö ÊØè‰∏™ËäÇÁÇπÁöÑ‰ΩçÁΩÆÂ¶Ç‰ΩïÁ°ÆÂÆöÔºüÁâπÂæÅÁöÑÈÄâÊã©ÔºöÊØèÊ¨°ÈÄâÂÖ•ÁöÑÁâπÂæÅ‰Ωú‰∏∫ÂàÜË£ÇÁöÑÊ†áÂáÜÔºåÈÉΩÊòØ‰ΩøÂæóÂÜ≥Á≠ñÊ†ëÂú®Ëøô‰∏™ËäÇÁÇπÁöÑÊ†πÊçÆ‰Ω†Ëá™Â∑±ÈÄâÊã©ÁöÑÊ†áÂáÜÔºà‰ø°ÊÅØÁÜµÊúÄÂ∞è„ÄÅ‰ø°ÊÅØÂ¢ûÁõäÊúÄÂ§ß„ÄÅginiÁ≥ªÊï∞ÊúÄÂ∞èÔºâ. ÊØè‰∏™ËäÇÁÇπÂú®Âì™‰∏™ÂÄº‰∏äÂÅöÂàíÂàÜÔºåÁ°ÆÂÆöÂàÜÊîØÁªìÊûÑÂë¢ÔºüÈÅçÂéÜÂàíÂàÜÁöÑËäÇÁÇπÁöÑÂàÜÁïåÂÄºÊìç‰ΩúÊù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢ò ÂèØ‰ª•ÊÉ≥Ë±°ÔºåÊàë‰ª¨ÊûÑÈÄ†ÁöÑÂÜ≥Á≠ñÊ†ëË∂≥Â§üÂ∫ûÂ§ßÔºåÂÜ≥Á≠ñÊ†ëÂèØ‰ª•ÊääÊØè‰∏Ä‰∏™Ê†∑Êú¨ÈÉΩÂàÜÂØπÔºåÈÇ£‰πàÂÜ≥Á≠ñÊ†ëÁöÑÊ≥õÂåñËÉΩÂäõÂ∞±ÂèØ‰ª•ÂæàÂ∑Æ‰∫Ü‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÂ∞±ÈúÄË¶ÅÂâ™ÊûùÊìç‰Ωú‰∫Ü ËÆ≠ÁªÉÁÆóÊ≥ïÂü∫‰∫é‰ø°ÊÅØÁÜµÁöÑÊûÑÈÄ†ÂΩìÈÄâÊã©Êüê‰∏™ÁâπÂæÅ‰Ωú‰∏∫ËäÇÁÇπÊó∂ÔºåÊàë‰ª¨Â∞±Â∏åÊúõËøô‰∏™ÁâπÂæÅÁöÑ‰ø°ÊÅØÁÜµË∂äÂ∞èË∂äÂ•ΩÔºåÈÇ£‰πà‰∏çÁ°ÆÂÆöÊÄßË∂äÂ∞è„ÄÇËÆ°ÁÆóÁâπÂæÅÁöÑ‰ø°ÊÅØÁÜµÂÖ¨ÂºèÂ¶Ç‰∏ãÔºö H(x) = -p_i(x)log^{p_i(x)} = -\frac{n_j}{S}log^{\frac{n_j}{S}}$n_j$: Á¨¨j‰∏™Á±ªÂà´ÔºåÂú®Ê†∑Êú¨‰∏≠Âá∫Áé∞ÁöÑÈ¢ëÊï∞$S$: Ê†∑Êú¨‰∏™Êï∞ÂØπ‰∫éÁ¶ªÊï£Â±ûÊÄßÔºåÁõ¥Êé•ËÆ°ÁÆó‰ø°ÊÅØÁÜµÔºåËøûÁª≠Â±ûÊÄßÔºåÂ∞±ÈúÄË¶ÅÂàíÂàÜÂå∫Èó¥ÔºåÊåâÂå∫Èó¥ËÆ°ÁÆó‰ø°ÊÅØÁÜµ„ÄÇ Âü∫‰∫éÊüê‰∏ÄÂ±ÇÁöÑÊï∞ÊçÆÈõÜ a. ÈÅçÂéÜËÆ°ÁÆóÊâÄÊúâÂ±ûÊÄßÔºåÈÅçÂéÜÁõ∏Â∫îÂ±ûÊÄß‰ª•‰∏çÂêåÂÄº‰∏∫ÂàÜÊà™ÁÇπÁöÑ‰ø°ÊÅØÁÜµ b. ÈÄâÊã©‰ø°ÊÅØÁÜµÊúÄÂ∞èÁöÑ‰Ωú‰∏∫ËäÇÁÇπ Â¶ÇÊûúÂà∞ËææÁªàÊ≠¢Êù°‰ª∂ÔºåËøîÂõûÁõ∏Â∫î‰ø°ÊÅØÔºåÂê¶ÂàôÔºåÊåâÁÖßÂàÜÊîØÈáçÂ§çÊ≠•È™§1ID3ÁÆóÊ≥ïÔºö ‰ø°ÊÅØÂ¢ûÁõäÊúÄÂ§ßÂåñC:Á±ªÂà´H(C)=-\sum_{i=1}^{m}p_i log _2^{p_i}ÊåâÁÖßDÁªÑÂàíÂàÜCH(C/D)=\sum_{i=1}^{v}\frac{|C_i|}{|C|}H(C_i)‰ø°ÊÅØÂ¢ûÁõägain(D) = gain(C)-H(C/D)ËøôÈáåÊàëÂ∞±‰ª•ÁΩë‰∏äÁªôÂá∫ÁöÑÊï∞ÊçÆ‰∏∫‰æãÔºåÁªôÂá∫Ê†πÊçÆ‰ø°ÊÅØÁÜµÊûÑÊàêÂÜ≥Á≠ñÊ†ëÁöÑËÆ°ÁÆóËøáÁ®ã„ÄÇ Á°ÆÂÆöÁâπÂæÅÔºåÁªüËÆ°Â±ûÊÄßÂÄºÂíåÂàÜËß£ÁªìÊûúÔºåÊÄªÂÖ±Âõõ‰∏™ÁâπÂæÅÔºåÂõõÁßçÁâπÂæÅÁöÑÁªüËÆ°ÁªìÊûúÂ¶Ç‰∏ãÂõæÔºö Ê†πÊçÆÂéÜÂè≤Êï∞ÊçÆÔºåÂú®‰∏çÁü•Âà∞‰ªª‰ΩïÊÉÖÂÜµ‰∏ãÔºåËÆ°ÁÆóÊï∞ÊçÆÊú¨Ë∫´ÁöÑÁÜµ‰∏∫ - \frac{9}{14}log_2 \frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940 ËÆ°ÁÆóÊØè‰∏™ÁâπÂæÅÂÅö‰∏∫ËäÇÁÇπÁöÑ‰ø°ÊÅØÁÜµ‰ª•Â§©Ê∞î‰∏∫‰æãÔºåÂ§©Ê∞î‰∏âÁßçÂ±ûÊÄßÔºåÂΩìOutlook = sunnyÊó∂ÔºåH(x) = $-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5}$; ÂΩìOutlook= overcast,$H(x)=0$,ÂΩìOutlook = rainy ,$H(x) = 0.971$ÊâÄ‰ª•ÔºåÂΩìÈÄâÂ§©Ê∞î‰Ωú‰∏∫ËäÇÁÇπÊó∂ÔºåÊ≠§Êó∂$H(x)=\frac{5}{14}0.971+\frac{4}{14}0+\frac{5}{14}*0.971 = 0.693$,gain(Â§©Ê∞î) = 0.247ÂêåÁêÜÔºåÂèØÂæógain(Ê∏©Â∫¶) =0.029 gain(ÊπøÂ∫¶)=0.152Ôºågain(È£é)=0.048Âõ†Ê≠§ÈÄâÊã©Â§©Ê∞îËäÇÁÇπÔºåÂú®ÈÄíÂΩíÂÆûÁé∞ÂÖ∂‰ªñËäÇÁÇπÁöÑÈÄâÊã©„ÄÇ‰ø°ÊÅØÂ¢ûÁõäÁöÑÊñπÊ≥ïÂÅèÂêëÈÄâÊã©ÂÖ∑ÊúâÂ§ßÈáèÂÄºÁöÑÂ±ûÊÄßÔºå‰πüÂ∞±ÊòØËØ¥Êüê‰∏™Â±ûÊÄßÁâπÂæÅÁ¥¢ÂèñÁöÑ‰∏çÂêåÂÄºË∂äÂ§öÔºåÈÇ£‰πàË∂äÊúâÂèØËÉΩ‰Ωú‰∏∫ÂàÜË£ÇÂ±ûÊÄßÔºåËøôÊ†∑ÊòØ‰∏çÂêàÁêÜÁöÑÔºõ C4.5: ‰ø°ÊÅØÂ¢ûÁõäÁéáÂ¶ÇÊûúËøôÈáåËÄÉËôë‰∫Ü‰∏ÄÂàóID,ÊØè‰∏™IDÂá∫Áé∞‰∏ÄÊ¨°ÔºåÊâÄ‰ª•ÁÆóÂá∫ÁöÑ‰ø°ÊÅØÂ¢ûÁõäÂ§ß„ÄÇ$ H(x) = 0$,‰ø°ÊÅØÂ¢ûÁõäÊúÄÂ§ßÂåñ‰∫ÜÔºåÂèØ‰ª•ÂºïÂÖ•‰ø°ÊÅØÂ¢ûÁõäÁéá C(T) = \frac{‰ø°ÊÅØÂ¢ûÁõä}{H(T)} =\frac{H(C)-H(C/T)}{H(T)}CART:Âü∫Â∞º(Gini)Á≥ªÊï∞G = 1-\sum_{i=l_k}^{k}p_i^2$$,‰πüÊòØÂØπÈöèÊú∫ÂèòÈáè‰∏çÁ°ÆÂÆöÊÄßÁöÑ‰∏Ä‰∏™Ë°°ÈáèÔºåginiË∂äÂ§ßÔºå‰∏çÁ°ÆÂÆöÊÄßË∂äÂ§ß ### ËøûÁª≠Â±ûÊÄßÁöÑÂ§ÑÁêÜÊñπÊ≥ï ÈÄâÂèñÂàÜËß£ÁÇπÁöÑÈóÆÈ¢òÔºö ÂàÜÊàê‰∏çÂêåÁöÑÂå∫Èó¥Ôºà‰∫åÂàÜ„ÄÅ‰∏âÂàÜ....)ÔºåÂàÜÂà´ËÆ°ÁÆóÂ¢ûÁõäÂÄºÔºåÁÑ∂ÂêéÊØîËæÉÈÄâÊã©„ÄÇ Â∞ÜÈúÄË¶ÅÂ§ÑÁêÜÁöÑÊ†∑Êú¨ÔºàÂØπÂ∫îÊ†πËäÇÁÇπÔºâÊàñÊ†∑Êú¨Â≠êÈõÜÔºàÂØπÂ∫îÂ≠êÊ†ëÔºâÊåâÁÖßËøûÁª≠ÂèòÈáèÁöÑÂ§ßÂ∞è‰ªéÂ∞èÂà∞Â§ßËøõË°åÊéíÂ∫è ÂÅáËÆæËØ•Â±ûÊÄßÂØπÂ∫î‰∏çÂêåÁöÑÂ±ûÊÄßÂÄºÂÖ±N‰∏™ÔºåÈÇ£‰πàÊÄªÂÖ±ÊúâN-1‰∏™ÂèØËÉΩÁöÑÂÄôÈÄâÂàÜÂâ≤ÂÄºÁÇπÔºåÊØè‰∏™ÂÄôÈÄâÁöÑÂàÜÂâ≤ÈòàÂÄºÁÇπÁöÑÂÄº‰∏∫‰∏äËø∞ÊéíÂ∫èÂêéÁöÑÂ±ûÊÄßÂÄº‰∏≠‰∏§‰∏§ÂâçÂêéËøûÁª≠ÂÖÉÁ¥†ÁöÑ‰∏≠ÁÇπ ## ËØÑ‰ª∑ ËØÑ‰ª∑ÂáΩÊï∞Ôºö $$C(T) = \sum_{releaf} N_t*H(T)$ N_t$ÔºöÊØè‰∏™Âè∂Â≠êËäÇÁÇπÈáåÈù¢Âê´ÊúâÁöÑÊ†∑Êú¨‰∏™Êï∞$H(T)$:Âè∂Â≠êËäÇÁÇπÂê´ÊúâÁöÑ‰ø°ÊÅØÁÜµ ËøáÊãüÂêàÂ¶ÇÊûúÂÜ≥Á≠ñÊ†ëËøá‰∫éÂ∫ûÂ§ßÔºåÂàÜÊîØÂ§™Â§öÔºåÂèØËÉΩÈÄ†ÊàêËøáÊãüÂêà„ÄÇÂØπÂ∫îËÆ≠ÁªÉÊ†∑Êú¨ÈÉΩÂ∞ΩÂèØËÉΩÁöÑÂàÜÂØπÔºå‰πüËÆ∏Ê†∑Êú¨Êú¨Ë∫´Â∞±Â≠òÂú®ÂºÇÂ∏∏ÁÇπÂë¢ÔºüI. È¢ÑÂâ™ÊûùÔºöËæπÊûÑÂª∫ÔºåËæπÂâ™Êûù ÊåáÂÆöÊ∑±Â∫¶d ËäÇÁÇπÁöÑmin_sample ËäÇÁÇπÁÜµÂÄºÊàñËÄÖginiÂÄºÂ∞è‰∫éÈòôÂÄºÁÜµÂíåÂü∫Â∞ºÂÄºÁöÑÂ§ßÂ∞èË°®Á§∫Êï∞ÊçÆÁöÑÂ§çÊùÇÁ®ãÂ∫¶ÔºåÂΩìÁÜµÊàñËÄÖÂü∫Â∞ºÂÄºËøáÂ∞èÊó∂ÔºåË°®Á§∫Êï∞ÊçÆÁöÑÁ∫ØÂ∫¶ÊØîËæÉÂ§ßÔºåÂ¶ÇÊûúÁÜµÊàñËÄÖÂü∫Â∞ºÂÄºÂ∞è‰∫é‰∏ÄÂÆöÁ®ãÂ∫¶Êï∞ÔºåËäÇÁÇπÂÅúÊ≠¢ÂàÜË£Ç„ÄÇ ÂΩìÊâÄ‰ª•ÁâπÂæÅÈÉΩÁî®ÂÆå‰∫Ü ÊåáÂÆöËäÇÁÇπ‰∏™Êï∞ÂΩìËäÇÁÇπÁöÑÊï∞ÊçÆÈáèÂ∞è‰∫é‰∏Ä‰∏™ÊåáÂÆöÁöÑÊï∞ÈáèÊó∂Ôºå‰∏çÁªßÁª≠ÂàÜË£Ç„ÄÇ‰∏§‰∏™ÂéüÂõ†Ôºö‰∏ÄÊòØÊï∞ÊçÆÈáèËæÉÂ∞ëÊó∂ÔºåÂÜçÂÅöÂàÜË£ÇÂÆπÊòìÂº∫ÂåñÂô™Â£∞Êï∞ÊçÆÁöÑ‰ΩúÁî®Ôºõ‰∫åÊòØÈôç‰ΩéÊ†ëÁîüÈïøÁöÑÂ§çÊùÇÊÄß„ÄÇÊèêÂâçÁªìÊùüÂàÜË£Ç‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÊúâÂà©‰∫éÈôç‰ΩéËøáÊãüÂêàÁöÑÂΩ±Âìç„ÄÇ II. ÂêéÂâ™ÊûùÔºö ÊûÑÂª∫Â•ΩÂêéÔºåÁÑ∂ÂêéÊâçÂºÄÂßãË£ÅÂâ™ C_\alpha(T) = C(T)+\alpha|T_{leaf}|Âú®ÊûÑÈÄ†Âê´‰∏ÄÊ£µÊ†ëÂêéÔºåÈÄâ‰∏Ä‰∫õËäÇÁÇπÂÅöËÆ°ÁÆóÔºåÁúãÊòØÂê¶ÈúÄË¶ÅÂâ™Êûù ÂÜ≥Á≠ñÊ†ëÂçï‰∏™ËäÇÁÇπÈÄâÊã©ÁöÑ‰ª£Á†ÅÂÆûÁé∞ÁÆÄÂçïÂÆûÁé∞‰∫ÜÂçï‰∏™ËäÇÁÇπÂÜ≥Á≠ñÊûÑÈÄ†ËøáÁ®ã12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182def split(X,y,d,value):'''Âú®dÁ∫¨Â∫¶‰∏äÔºåÊåâÁÖßvalueËøõË°åÂàíÂàÜ''' index_a =(X[:,d]&lt;=value) index_b =(X[:,d]&gt;value) return X[index_a],X[index_b],y[index_a],y[index_b]from collections import Counterfrom math import log from numpy as npdef entropy(y): counter = Counter(y) # Â≠óÂÖ∏ res = 0.0 for num in counter.values(): p = num/len(y) res+=-p*log(p) return resdef gain(X,y,d,v): X_l,X_r,y_l,y_r = split(X,y,d,v) e = len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r) return (entropy(y)-e)def gainratio(X,y,d,v): X_l,X_r,y_l,y_r = split(X,y,d,v) gain =entropy(y) - len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r) return gain/(entropy(y_l)+entropy(y_r))def gini(y): counter = Counter(y) res = 1.0 for num in counter.values(): p = num / len(y) res += -p**2 return res #X_l,X_r,y_l,y_r = split(X,y,d,v) #return 1-(len(y_l)/len(y))**2-(len(y_r)/len(y))**2def try_split(X,y): best_entropy = float('inf') best_d,best_v=-1,-1 for d in range(X.shape[1]): sorted_index = np.argsort(X[:,d]) for i in range(1, len(X)): if (X[sorted_index[i],d] != X[sorted_index[i-1],d]): v = (X[sorted_index[i-1],d]+X[sorted_index[i],d])/2 X_l,X_r,y_l,y_r = split(X,y,d,v) # ‰ø°ÊÅØÁÜµ e = entropy(y_l)+entropy(y_r) #gini e = gini(y_l) + gini(y_r) # ‰ø°ÊÅØÂ¢ûÁõä e = -gain(X,y,d,v) if e &lt; best_entropy: best_entropy, best_d,best_v = e,d,v return best_entropy, best_d, best_v# ÊâãÂä®Êù•ÂàíÂàÜdata =np.array([[ 0.3 , 5 , 2 , 0 ],[ 0.4 , 6 , 0 , 0 ],[ 0.5 , 6.5 , 1 , 1 ],[ 0.6 , 6 , 0 , 0 ],[ 0.7 , 9 , 2 , 1 ],[ 0.5 , 7 , 1 , 0 ],[ 0.4 , 6 , 0 , 0 ],[ 0.6 , 8.5 , 0 , 1 ],[ 0.3 , 5.5 , 2 , 0 ],[ 0.9 , 10 , 0 , 1 ],[ 1 , 12 , 1 , 0 ],[ 0.6 , 9 , 1 , 0 ],])X =data[:,0:3]y = data[:,-1]# ÊâãÂä®Êù•ÂàíÂàÜbest_entropy, best_d, best_v = try_split(X, y)print(best_entropy, best_d, best_v)X1_l, X1_r, y1_l, y1_r = split(X,y,best_d,best_v)print(X1_l, X1_r, y1_l, y1_r)best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)X2_l, X2_r, y2_l, y2_r = split(X1_r,y1_r,best_d2,best_v2)entropy(y2_l) Python skleanÈáåÈù¢treeÊ®°ÂùóÈáåÈù¢ÁöÑDecisionTreeClassifier1234from sklearn import treeclf =tree.DecisionTreeClassifier(max_depth=1,criterion ='gini') # criterion='entropy|gini'clf = clf.fit(X,y) ËÆ≠ÁªÉÂ•Ω‰∏ÄÈ¢óÂÜ≥Á≠ñÊ†ë‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•‰ΩøÁî®export_graphvizÂØºÂá∫Âô®‰ª•GraphvizÊ†ºÂºèÂØºÂá∫Ê†ë„ÄÇ1234import graphviz dot_data = tree.export_graphviz(clf, out_file=None,) graph = graphviz.Source(dot_data) graph.render("data") Âú®ËøêË°åÊó∂ÂèØ‰ª•Âá∫ÈîôÔºöExecutableNotFound: failed to execute [‚Äòdot‚Äô, ‚Äò-Tpdf‚Äô, ‚Äò-O‚Äô, ‚Äòdata‚Äô], make sure the Graphviz executables are on your systems‚Äô PATHÂéüÂõ†ÔºögraphvizÊú¨Ë∫´ÊòØ‰∏Ä‰∏™ËΩØ‰ª∂ÔºåÈúÄË¶ÅÈ¢ùÂ§ñ‰∏ãËΩΩÔºåÂπ∂Â∞ÜÂÖ∂binÂä†ÂÖ•ÁéØÂ¢ÉÂèòÈáè‰πã‰∏≠„ÄÇ‰∏ãËΩΩ]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>ÂÜ≥Á≠ñÊ†ë</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÊàëÁöÑËØª‰π¶Á¨îËÆ∞]]></title>
    <url>%2F2019%2F02%2F28%2FSVD%2F</url>
    <content type="text"><![CDATA[ÁõÆÂΩï :smile::one: ÁÆÄÂçïËØ¥‰∏Ä‰∏ãÁâπÂæÅÂÄº„ÄÅÁâπÂæÅÂêëÈáè‰∏éÁâπÂæÅÂàÜËß£&nbsp;&nbsp; I. ÁâπÂæÅÂÄº„ÄÅÁâπÂæÅÂêëÈáè‰∏éÁâπÂæÅÂàÜËß£&nbsp;&nbsp; II. Âá†‰ΩïÊÑè‰πâ&nbsp;&nbsp; III. Â¶Ç‰ΩïÂÆûÁé∞ÈÄöËøáMatlab„ÄÅPythonÂÆûÁé∞:two:ËØ¶ÁªÜËß£ËØ¥SVD&nbsp;&nbsp; I. Âá†‰ΩïÊÑè‰πâ&nbsp;&nbsp; I. Â•áÂºÇÂÄºÂàÜËß£ÁöÑÊé®ÂØºËøáÁ®ã&nbsp;&nbsp; I. SVDÁÆó‰æã&nbsp;&nbsp; I. Â¶Ç‰ΩïÈÄöËøáMatlabÂíåPython:three:Â∫îÁî®‰∏æ‰æã&nbsp;&nbsp; I. ÁâπÂæÅÂÄº„ÄÅÁâπÂæÅÂêëÈáè‰∏éÁâπÂæÅÂàÜËß£:four:ÁâπÂæÅÂàÜËß£„ÄÅÂ•áÂºÇÂÄºÂàÜËß£ÁöÑÂå∫Âà´&nbsp;&nbsp; I. ÁâπÂæÅÂàÜËß£„ÄÅÂ•áÂºÇÂÄºÂàÜËß£ÁöÑÂå∫Âà´ ÁÆÄÂçïËØ¥‰∏Ä‰∏ãÁâπÂæÅÂÄº„ÄÅÁâπÂæÅÂêëÈáè‰∏éÁâπÂæÅÂàÜËß£ ÁâπÂæÅÂÄº„ÄÅÁâπÂæÅÂêëÈáè‰∏éÁâπÂæÅÂàÜËß£Theory:ÂØπ‰∫é‰∏Ä‰∏™Ê≠£Èòµ$M$ÔºåÊª°Ë∂≥Â¶Ç‰∏ãÔºö Mx=\lambda xÂÖ∂‰∏≠$\lambda$Ë¢´Êàê‰∏∫ÁâπÂæÅÂÄºÔºåÊª°Ë∂≥$||M-\lambda E||=0$ÂÜçÊúâ$(M-\lambda E)x=0$ÔºåÂèØËÆ°ÁÆóÂÖ∂ÁâπÂæÅÂêëÈáè„ÄÇÂ¶ÇÊûúÊúâ‰∫ÜÁâπÂæÅÂÄºÂíåÁâπÂæÅÂêëÈáèÂêéÂë¢ÔºåÂàôÂèØ‰ª•Â∞ÜÁü©Èòµ$M$Áî®ÁâπÂæÅÂàÜËß£Ôºö M=W\sum W^{-1}$W={w_1,w_2,‚Ä¶,w_n}$ÂàÜÂà´ÊòØÁâπÂæÅÂÄº$\lambda_1,\lambda_2,‚Ä¶,\lambda_n$ÂØπÂ∫îÁöÑÁâπÂæÅÂêëÈáèÊûÑÊàêÁöÑÊñπÈòµ Âá†‰ΩïÊÑè‰πâ ÂØπÂ∫îÁü©ÈòµM,ÂÖ∂ÂØπÂ∫îÁöÑÁ∫øÊÄßÂèòÂåñ Mx = x'‰∏äÈù¢Ëøô‰∏™ÂºèÂ≠êÔºå$MxÔºåx‚Äô$ÊòØ‰∏Ä‰∏™ÂêëÈáèÔºå$x,x‚Äô$ÂèØËÉΩÊòØ‰∏çÂÖ±Á∫øÁöÑ(Â¶ÇÂõæ(b))ÔºåÂ¶ÇÊûúÂêëÈáè$Mx,x‚Äô$Êª°Ë∂≥$Mx=x‚Äô=\lambda x$,ÂàôÂ¶ÇÂõæ(b)ÔºåËøôËØ¥Êòé‰∫ÜËøô‰∏™ÂèòÊç¢Â∞±ÊòØÂØπÂêëÈáèxÂÅö‰∏Ä‰∏™Êãâ‰º∏ÊàñËÄÖÂéãÁº©„ÄÇ Â¶Ç‰ΩïÂÆûÁé∞ÈÄöËøáMatlab„ÄÅPythonÂÆûÁé∞Êï∞Â≠¶Êé®ÂØºÔºö Mx = \lambda xMx-\lambda x=(M-\lambda E)x=0ÈΩêÊ¨°Á∫øÊÄßÊñπÁ®ãÁªÑÊúâÈùûÈõ∂Ëß£ÔºåÂàô$||M-\lambda E||=0$ÂèØÊ±ÇÂæóÁâπÂæÅÂêëÈáèÂÜçÂ∏¶ÂõûÔºåÂèØÂæóÁâπÂæÅÂêëÈáè„ÄÇMatlab:123d = eig(M) % Ê±ÇÂèñÁü©ÈòµMÁöÑÁâπÂæÅÂÄºÔºåÂêëÈáèÂΩ¢ÂºèÂ≠òÂÇ®[V,D] = eig(M) % ËÆ°ÁÆóMÁöÑÁâπÂæÅÂÄºÂØπËßíÈòµDÂíåÁâπÂæÅÂêëÈáèVÔºå‰ΩøÂæóMV = VDÊàêÁ´ã[V,D] = eig(M,'nobalance') %ÂΩìÁü©ÈòµM‰∏≠Êúâ‰∏éÊà™Êñ≠ËØØÂ∑ÆÊï∞ÈáèÁ∫ßÁõ∏Â∑Æ‰∏çËøúÁöÑÂÄºÊó∂ÔºåËØ•Êåá‰ª§ÂèØËÉΩÊõ¥Á≤æÁ°Æ„ÄÇ'nobalance'Ëµ∑ËØØÂ∑ÆË∞ÉËäÇ‰ΩúÁî® PythonnumpyÁßëÂ≠¶ËÆ°ÁÆóÂ∫ìÊèê‰æõÁõ∏Â∫îÁöÑÊñπÊ≥ï1234import numpy as npx = np.diag((1,2,3)) # ËøôÊòØ‰Ω†ÊÉ≥Ë¶ÅÊ±ÇÂèñÁâπÂæÅÂÄºÁöÑÊï∞ÁªÑa,b = numpy.linalg.elg(x) # ÁâπÂæÅÂÄºËµãÂÄºÁªôa,ÂØπÂ∫îÁöÑÁâπÂæÅÂêëÈáèËµãÂÄºÁªôb ËØ¶ÁªÜËß£ËØ¥SVDSVDÁöÑËã±ÊñáÂÖ®Áß∞Ôºö Singular Value DecompositionÔºå‰∏≠ÊñáÂêçÂ≠óÔºöÂ•áÂºÇÂÄºÂàÜËß£ Âá†‰ΩïÊÑè‰πâÂõæÊù•Ê∫ê‰ª•‰∫åÁª¥Á©∫Èó¥‰∏∫‰æãÂá†‰ΩïÊÑè‰πâÂ∞±ÊòØÊää‰∏Ä‰∏™Âçï‰ΩçÊ≠£‰∫§ÁöÑÁΩëÊ†ºÔºåËΩ¨Êç¢‰∏∫Âè¶Â§ñ‰∏Ä‰∏™Âçï‰ΩçÊ≠£‰∫§ÁöÑÁΩëÊ†º ÂÅáÂ¶ÇÈÄâÂèñ‰∫Ü‰∏ÄÁªÑÂçï‰ΩçÊ≠£‰∫§Âü∫{$\vec{v}_1$,$\vec{v}_2$},ÂàöÂ•ΩÁü©Èòµ$M$ÁöÑÁ∫øÊÄßÂèòÂåñ$M\vec{v}_1 $,$M\vec{v}_2 $ ‰πüÊ≠£‰∫§ÔºåÁî®$\vec{u}_1,\vec{u}_2 $ÂàÜÂà´Ë°®Á§∫$M\vec{v}_1 $,$M\vec{v}_2 $ ÁöÑÂçï‰ΩçÂêëÈáèÔºåÁî®$\lambda_1,\lambda_2 $Ë°®Á§∫$M\vec{v}_1 $,$M\vec{v}_2$ÁöÑÈïøÂ∫¶ÔºåÊèèËø∞ÁΩëÊ†ºÂú®Ëøô‰∫õÁâπÂÆöÊñπÂêë‰∏äÁöÑÊãâ‰º∏ÈáèÔºå‰πüË¢´Áß∞‰ΩúÁü©ÈòµMÁöÑÂ•áÂºÇÂÄº„ÄÇ$M\vec{v}_1 =\lambda_1\vec{u}_1 $$M\vec{v}_2 =\lambda_2\vec{u}_2 $ÂØπ‰ªªÊÑèÁªôÂÆöÁöÑÂêëÈáè $\vec{x}$ ,ÂàôÊúâ \mathbf{x}=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) \mathbf{v}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) \mathbf{v}_{2} ÂÜçÂ∞ÜMÁöÑÁ∫øÊÄßÂèòÊç¢ \begin{aligned} M \mathbf{x} &=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) M \mathbf{N}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) M \mathbf{v}_{2} \\ M \mathbf{x} &=\left(\mathbf{v}_{1} \cdot \mathbf{x}\right) \sigma_{1} \mathbf{u}_{1}+\left(\mathbf{v}_{2} \cdot \mathbf{x}\right) \sigma_{2} \mathbf{u}_{2} \end{aligned} \begin{array}{c}{M \mathbf{x}=\mathbf{u}_{1} \sigma_{1} \mathbf{v}_{1}^{\top} \mathbf{x}+\mathbf{u}_{2} \sigma_{2} \mathbf{v}_{2}^{\top} \mathbf{x}} \\ {M=\mathbf{u}_{1} \sigma_{1} \mathbf{v}_{1}^{\top}+\mathbf{u}_{2} \sigma_{2} \mathbf{v}_{2}^{\top}}\end{array} so M=U \Sigma V^{T}Â•áÂºÇÂÄºÂàÜËß£ÁöÑÊé®ÂØºËøáÁ®ã$u=(u_1,u_2,‚Ä¶,u_m)$$v=(v_1,v_2,‚Ä¶,v_n)$$u,v$ÈÉΩÊòØÁ©∫Èó¥ÁöÑÂü∫,ÊòØÊ≠£‰∫§Áü©Èòµ $u^Tu=E,v^Tv = E$‰ªª‰Ωï‰∏Ä‰∏™Áü©Èòµ$M_{m*n}$Ôºå$rank(M)=k$Ôºå‰∏ÄÂÆöÂ≠òÂú®Ôº≥Ôº∂Ôº§,Êç¢Âè•ËØùËØ¥ÔºåMÂèØ‰ª•Â∞Ü‰∏ÄÁªÑÂçï‰ΩçÊ≠£‰∫§Âü∫Êò†Â∞ÑÂà∞Âè¶‰∏ÄÁªÑÂçï‰ΩçÊ≠£‰∫§Âü∫„ÄÇÁ≠îÊ°àÊòØËÇØÂÆöÁöÑËØÅÊòéÂ¶Ç‰∏ãÔºöÂú®n‰∏∫Á©∫Èó¥‰∏≠ÔºåÊúâ‰∏ÄÁªÑÂçï‰ΩçÊ≠£‰∫§Âü∫{$\vec{v}_1,\vec{v}_2,‚Ä¶,\vec{v}_n$},Á∫øÊÄßÂèòÂåñ‰ΩúÁî®‰ª•Âêé {M\vec{v}_1,M\vec{v}_2,...,M\vec{v}_n}‰πüÊòØÊ≠£‰∫§ÁöÑÔºåÂàôÊúâ (M\vec{v}_i,M\vec{v}_j) = (M\vec{x}_i)^TM\vec{v}_j=\vec{v}_i^TM^TM\vec{v}_j=0Ê≥®ÊÑèÂñîÔºå$M^TM$ÊòØÁü©ÈòµÂñîÔºåÂàô‰ºöÊúâ$M^TM\vec{v}_j=\lambda \vec{v}_j$Êé•‰∏ãÂéªÔºå \begin{aligned} v_{i}^{T} M^{T} \mathrm{M} v_{j}=& v_{i}^{T} \lambda_{j} v_{j} \\ &=\lambda_{j} v_{i}^{T} v_{j} \\ &=\lambda_{j} v_{i}\dot v_{j}=0 \end{aligned} ‰∏äËø∞Â∞±ËØÅÊòé‰∫ÜÊòØÊúâÁöÑÔºö‰ªª‰Ωï‰∏Ä‰∏™Áü©ÈòµÔºåÈÉΩÂèØ‰ª•Â∞Ü‰∏ÄÁªÑÂçï‰ΩçÊ≠£‰∫§Âü∫ËΩ¨Êç¢ÊàêÂè¶Â§ñ‰∏ÄÁªÑÊ≠£‰∫§Âü∫„ÄÇ ÂΩì$i=j$,$=\lambda_i \vec{v}_i \vec{v}_i=\lambda_i$ ËøõË°å‰∏Ä‰∫õÂçï‰ΩçÂåñÔºåËÆ∞$u_i=\frac{A\vec{v}_i}{|M\vec{v}_i|}=\frac{1}{\sqrt{\lambda_i}}M\vec{v}_i$Âàô A v_{i}=\sigma_{i} u_{i}, \sigma_{i}(\operatorname{Â•áÂºÇÂÄº})=\sqrt{\lambda_{i}}, 0 \leq i \leq \mathrm{k}, \mathrm{k}=\operatorname{Rank}(\mathrm{A}) ÂΩì$k &lt; i &lt;= m$Êó∂ÔºåÂØπ$u1Ôºåu2Ôºå‚Ä¶Ôºåuk$ËøõË°åÊâ©Â±ï$u(k+1),‚Ä¶,um$Ôºå‰ΩøÂæó$u1Ôºåu2Ôºå‚Ä¶Ôºåum$‰∏∫$m$Áª¥Á©∫Èó¥‰∏≠ÁöÑ‰∏ÄÁªÑÊ≠£‰∫§Âü∫.‰πüÂèØÂØπ$\vec{v}_1,\vec{v}_2,‚Ä¶,\vec{v}_k$ËøõË°åÊâ©Â±ïÔºåÊâ©Â±ïÁöÑ$\vec{v}_{k+1},‚Ä¶,\vec{v}_{n}$Â≠òÂú®Èõ∂Â≠êÁ©∫Èó¥ÈáåÈù¢„ÄÇ M\left[ \begin{array}{lll}{\vec{v}_{1}} & {\cdots} & {\vec{v}_{k}}\end{array}\right| \vec{v}_{k+1} \quad \cdots \quad \vec{v}_{m} ]= \left[ \begin{array}{c}{\vec{u}_{1}^{T}} \\ {\vdots} \\ {\frac{\vec{u}_{k}^{T}}{\vec{u}_{k+1}}} \\ {\vdots} \\ {\vec{u}_{n}^{T}}\end{array}\right] \left[ \begin{array}{ccc|c}\sigma_{1} & & 0 & 0\\ & {\ddots} & \sigma_{k} & 0 \\ \hline 0 & & 0 &0\end{array}\right] M=\left[ \begin{array}{lll}{\vec{u}_{1}} & {\cdots} & {\vec{u}_{k}}\end{array}\right] \left [ \begin{array}{ccc}\sigma_{1} & & \\ & {\ddots} & \\ & & {\sigma_{k}}\end{array}\right] \left[ \begin{array}{c}{\vec{v}_{1}^{T}} \\ {\vdots} \\ {\vec{v}_{k}^{T}}\end{array}\right]+ \left[ \begin{array}{ccc}{\vec{u}_{k+1}} & {\cdots} & {\vec{u}_{m}}\end{array}\right] \left[\begin{array}{c} 0 \end{array} \right] \left[ \begin{array}{c}{\vec{v}_{k+1}^{T}} \\ {\vdots} \\ {\vec{v}_{n}^{T}}\end{array}\right]SVDÁÆó‰æãUÔºö$AA^T$ÁöÑÁâπÂæÅÂÄºÂíåÁâπÂæÅÂêëÈáèÔºåÁî®Âçï‰ΩçÂåñÁöÑÁâπÂæÅÂêëÈáèÊûÑÊàê UV: $A^TA$ ÁöÑÁâπÂæÅÂÄºÂíåÁâπÂæÅÂêëÈáèÔºåÁî®Âçï‰ΩçÂåñÁöÑÁâπÂæÅÂêëÈáèÊûÑÊàê V$\sum_{mn} $ :Â∞Ü$ AA^{T} $ÊàñËÄÖ A^{T}A ÁöÑÁâπÂæÅÂÄºÊ±ÇÂπ≥ÊñπÊ†πÔºåÁÑ∂ÂêéÊûÑÊàê Œ£‰ª•Áü©Èòµ$A = \left[\begin{matrix} 1 &amp; 1\\1 &amp;1\\ 0 &amp;0\\\end{matrix} \right]$Á¨¨‰∏ÄÊ≠• U Ôºå‰∏ãÈù¢ÊòØ‰∏ÄÁßçËÆ°ÁÆóÊñπÊ≥ïÂØπÁü©Èòµ A A^{T}=\left[ \begin{array}{lll}{2} & {2} & {0} \\ {2} & {2} & {0} \\ {0} & {0} & {0}\end{array}\right] ÁâπÂæÅÂàÜËß£Ôºå ÁâπÂæÅÊòØ4Ôºå0Ôºå0 ÁâπÂæÅÂêëÈáèÊòØ $\left[\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0\right]^{T},\left[-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0\right]^{T},[0,0,1]^{T}$,ÂèØÂæóÂà∞ U=\left[ \begin{array}{ccc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} & {0} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}} & {0} \\ {0} & {0} & {1}\end{array}\right] Á¨¨‰∫åÊ≠• ËÆ°ÁÆóÁü©Èòµ$A^TA$ÁöÑÁâπÂæÅÂàÜËß£ÔºåÂèØÂæó ÁâπÂæÅÂÄº4Ôºå0Ôºå V=\left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}}\end{array}\right]Á¨¨‰∏âÊ≠•ËÆ°ÁÆó$\sum_{mn}$ \Sigma=\left[ \begin{array}{ll}{2} & {0} \\ {0} & {0} \\ {0} & {0}\end{array}\right] ÊúÄÂêéÔºå A=U \Sigma V^{T}=\left[ \begin{array}{ccc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} & {0} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}} & {0} \\ {0} & {0} & {1}\end{array}\right] \left[ \begin{array}{ll}{2} & {0} \\ {0} & {0} \\ {0} & {0}\end{array}\right] \left[ \begin{array}{cc}{\frac{1}{\sqrt{2}}} & {-\frac{1}{\sqrt{2}}} \\ {\frac{1}{\sqrt{2}}} & {\frac{1}{\sqrt{2}}}\end{array}\right]^{T}=\left[ \begin{array}{cc}{1} & {1} \\ {1} & {1} \\ {0} & {0}\end{array}\right]Â¶Ç‰ΩïÈÄöËøáMatlabÂíåPythonMatlabÔºö1234567891011s = svd(A)[U,S,V] = svd(A)[U,S,V] = svd(A,'econ')[U,S,V] = svd(A,0)input: A Áü©Èòµoutput: s:Â•áÂºÇÂÄºÔºå‰ª•ÂàóÂêëÈáèÂΩ¢ÂºèËøîÂõû„ÄÇÂ•áÂºÇÂÄºÊòØ‰ª•ÈôçÂ∫èÈ°∫Â∫èÂàóÂá∫ÁöÑÈùûË¥üÂÆûÊï∞ SÔºö U:Â∑¶Â•áÂºÇÂêëÈáèÔºå‰ª•Áü©ÈòµÁöÑÂàóÂΩ¢ÂºèËøîÂõû„ÄÇ V:Â•áÂºÇÂÄºÔºå‰ª•ÂØπËßíÁü©ÈòµÂΩ¢ÂºèËøîÂõû„ÄÇS ÁöÑÂØπËßíÂÖÉÁ¥†ÊòØ‰ª•ÈôçÂ∫èÊéíÂàóÁöÑÈùûË¥üÂ•áÂºÇÂÄº„ÄÇ Âè≥Â•áÂºÇÂêëÈáèÔºå‰ª•Áü©ÈòµÁöÑÂàóÂΩ¢ÂºèËøîÂõû„ÄÇ Python123import numpy as npM = np.array([ [1,1,2],[0,0,1]])U,S,V = np.linalg.svd(M) Â∫îÁî®‰∏æ‰æãÂ∫îÁî® 2.1 ‰ø°ÊÅØÊ£ÄÁ¥¢ 2.2 Êé®ËçêÁ≥ªÁªü 2.3 Âü∫‰∫éÂçèÂêåËøáÊª§ÁöÑÊé®ËçêÁ≥ªÁªü 2.4 ÂõæÂÉèÂéãÁº© ÁâπÂæÅÂÄºÂàÜËß£ÂíåÂ•áÂºÇÂÄºÂàÜËß£ÁöÑÂå∫Âà´ ÁâπÂæÅÂÄºÂàÜËß£Âè™ËÉΩÊòØÊñπÈòµÔºåËÄåÂ•áÂºÇÂÄºÂàÜËß£ÊòØÁü©ÈòµÂ∞±ÂèØ‰ª• ÁâπÂæÅÂÄºÂàÜËß£Âè™ËÄÉËôë‰∫ÜÂØπÁü©ÈòµÁº©ÊîæÊïàÊûúÔºåÂ•áÂºÇÂÄºÂàÜËß£ÂØπÁü©ÈòµÊúâÈÄâÊã©„ÄÅÊî∂Áº©„ÄÅÊäïÂΩ±ÁöÑÊïàÊûú]]></content>
      <categories>
        <category>Êï∞Â≠¶</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonÂ∫ì]]></title>
    <url>%2F2019%2F02%2F24%2Fpython%E5%BA%93%2F</url>
    <content type="text"><![CDATA[ÂºÄÂßãÊé•Ëß¶PythonÊòØÂ§ß‰∫åÁªìÊùüÁöÑÊó∂ÂÄôÔºåÂà∞Áé∞Âú®ÈÉΩÂø´‰∏§Âπ¥‰∫ÜÔºåÂÖ∂ÂÆû‰∏ÄÁõ¥Âπ∂‰∏çÊòØÂæàÁªÜËäÇÁöÑÂ≠¶‰π†ÔºåÂè™ÊòØÂ∏åÊúõËÉΩÂ§üË∑ë‰∏™ÁªìÊûú„ÄÇ‰∏çËøáÂë¢ÔºüÔºå‰ª•ÂêéËÇØÂÆöÊòØ‰ºöÁªèÂ∏∏Áî®PythonÔºåÊâÄ‰ª•Âë¢ÔºüÊàëÊé•‰∏ãÊù•‰ºöËÆ§ÁúüÂ≠¶‰π†Python Python È´òÁ∫ßÁî®Ê≥ïÊÄªÁªìÂü∫Êú¨Êï∞ÊçÆÁ±ªÂûãÔºöÊï¥Âûã„ÄÅÊµÆÁÇπÂûã„ÄÅÂ∏ÉÂ∞îÁ±ªÂûã ÂÆπÂô®Ôºö ContainersÂÆπÂô®ÊòØ‰∏ÄÁßçÊääÂ§ö‰∏™ÂÖÉÁ¥†ÁªÑÁªáÂú®‰∏ÄËµ∑ÁöÑÊï∞ÊçÆÁªìÊûÑÔºåÂÆπÂô®‰∏≠ÁöÑÂÖÉÁ¥†ÂèØ‰ª•ÈÄê‰∏™Âú∞Ëø≠‰ª£Ëé∑ÂèñÔºåÂèØ‰ª•Áî®in, not inÂÖ≥ÈîÆÂ≠óÂà§Êñ≠ÂÖÉÁ¥†ÊòØÂê¶ÂåÖÂê´Âú®ÂÆπÂô®‰∏≠„ÄÇÈÄöÂ∏∏ËøôÁ±ªÊï∞ÊçÆÁªìÊûÑÊääÊâÄÊúâÁöÑÂÖÉÁ¥†Â≠òÂÇ®Âú®ÂÜÖÂ≠ò‰∏≠Ôºà‰πüÊúâ‰∏Ä‰∫õÁâπ‰æãÔºåÂπ∂‰∏çÊòØÊâÄÊúâÁöÑÂÖÉÁ¥†ÈÉΩÊîæÂú®ÂÜÖÂ≠òÔºåÊØîÂ¶ÇËø≠‰ª£Âô®ÂíåÁîüÊàêÂô®ÂØπË±°ÔºâÂú®Python‰∏≠ÔºåÂ∏∏ËßÅÁöÑÂÆπÂô®ÂØπË±°ÊúâÔºölist, dequeset, frozensetsdict, defaultdict, OrderedDict, Countertuple, namedtuplestr listÊé®ÂØºÔºàlist comprehensions)ÂÆòÊñπËß£ÈáäÔºöÂàóË°®Ëß£ÊûêÂºèÊòØPythonÂÜÖÁΩÆÁöÑÈùûÂ∏∏ÁÆÄÂçïÂç¥Âº∫Â§ßÁöÑÂèØ‰ª•Áî®Êù•ÂàõÂª∫listÁöÑÁîüÊàêÂºè„ÄÇ 1ÂØπ‰∫é‰∏Ä‰∏™ÂàóË°®ÔºåÊó¢Ë¶ÅÈÅçÂéÜÁ¥¢ÂºïÂèàË¶ÅÈÅçÂéÜÂÖÉÁ¥†„ÄÇ 123array = ['I', 'love', 'Python']for i, element in enumerate(array): array[i] = '%d: %s' % (i, seq[i]) 12345def getitem(index, element): return '%d: %s' % (index, element)array = ['I', 'love', 'Python']arrayIndex = [getitem(index, element) for index, element in enumerate(array)] Ëø≠‰ª£Âô®ÂíåÁîüÊàêÂô®ÂèØËø≠‰ª£ÂØπË±°ÔºöÂá°ÊòØÂèØ‰ª•ËøîÂõû‰∏Ä‰∏™Ëø≠‰ª£Âô®ÁöÑÂØπË±°ÈÉΩÂèØÁß∞‰πã‰∏∫ÂèØËø≠‰ª£ÂØπË±°‰æãÂ¶ÇÔºölist dic str set tuple range() enumerate(Êûö‰∏æ) f=open()ÔºàÊñá‰ª∂Âè•ÊüÑÔºâ123456789### Ëø≠‰ª£Âô®(iterator)ÊòØ‰∏Ä‰∏™Â∏¶Áä∂ÊÄÅÁöÑÂØπË±°Ôºå‰ªñËÉΩÂú®‰Ω†Ë∞ÉÁî®next()ÊñπÊ≥ïÁöÑÊó∂ÂÄôËøîÂõûÂÆπÂô®‰∏≠ÁöÑ‰∏ã‰∏Ä‰∏™ÂÄºÔºå‰ªª‰ΩïÂÆûÁé∞‰∫Ü__iter__Âíå__next__()Ôºàpython2‰∏≠ÂÆûÁé∞next()ÔºâÊñπÊ≥ïÁöÑÂØπË±°ÈÉΩÊòØËø≠‰ª£Âô®Ôºå__iter__ËøîÂõûËø≠‰ª£Âô®Ëá™Ë∫´Ôºå__next__ËøîÂõûÂÆπÂô®‰∏≠ÁöÑ‰∏ã‰∏Ä‰∏™ÂÄºÔºåÂ¶ÇÊûúÂÆπÂô®‰∏≠Ê≤°ÊúâÊõ¥Â§öÂÖÉÁ¥†‰∫ÜÔºåÂàôÊäõÂá∫StopIterationÂºÇÂ∏∏### ÁîüÊàêÂô®(generator)ÁîüÊàêÂô®ÂÖ∂ÂÆûÊòØ‰∏ÄÁßçÁâπÊÆäÁöÑËø≠‰ª£Âô®Ôºå‰∏çËøáËøôÁßçËø≠‰ª£Âô®Êõ¥Âä†‰ºòÈõÖ„ÄÇÂÆÉ‰∏çÈúÄË¶ÅÂÜçÂÉè‰∏äÈù¢ÁöÑÁ±ª‰∏ÄÊ†∑ÂÜô__iter__()Âíå__next__()ÊñπÊ≥ï‰∫ÜÔºåÂè™ÈúÄË¶Å‰∏Ä‰∏™yiledÂÖ≥ÈîÆÂ≠ó„ÄÇ ÁîüÊàêÂô®‰∏ÄÂÆöÊòØËø≠‰ª£Âô®ÔºàÂèç‰πã‰∏çÊàêÁ´ãÔºâ#ÂàóË°®ÁîüÊàêÂºèlis = [x*x for x in range(10)]# ÂèóÂà∞ÂÜÖÂ≠òÈôêÂà∂ÔºåÂàóË°®ÂÆπÈáèËÇØÂÆöÊòØÊúâÈôêÁöÑ#ÁîüÊàêÂô®Ë°®ËææÂºègenerator_ex = (x*x for x in range(10)) ÁîüÊàêÂô®Ôºö ‰∏çÁî®ÂàõÂª∫ÂÆåÊï¥ÁöÑlistÔºå‰∏∫ËäÇÁúÅÂ§ßÈáèÁöÑÁ©∫Èó¥ÔºåÂú®Python‰∏≠ÔºåËøôÁßç‰∏ÄËæπÂæ™ÁéØ‰∏ÄËæπËÆ°ÁÆóÁöÑÊú∫Âà∂ÔºåÁß∞‰∏∫ÁîüÊàêÂô®ÔºögeneratorTuples:() Â≠óÂÖ∏Ôºö{ÔºöÔºå} Sets: {,}ÂáΩÊï∞Á±ª PythonÂ∫ì‚Äî‚ÄînumpyWhatNumPy=Numerical+Python‰∏ªË¶ÅÊòØÊèê‰æõ‰∫ÜÈ´òÊÄßËÉΩÂ§öÁª¥Êï∞ÁªÑËøô‰∏™ÂØπË±°Ôºå‰ª•ÂèäÂ§ÑÁêÜÁõ∏ÂÖ≥ÁöÑÊñπÊ≥ï How Ëá™ÂÆö‰πâ‰∏Ä‰∏™Ôºà1D or MD)Êï∞ÁªÑÊàñËÄÖÁâπÊÆäÁöÑÊï∞ÁªÑ,‰∏ÄÁª¥Ôºå‰∫åÁª¥ Êï∞ÁªÑÂàáÁâáÔºà‰πüÂ∞±ÊòØÊèêÂèñÊï∞ÁªÑÂÖÉÁ¥†ÔºâÔºåÊ≥®ÊÑè a[:,0]Âíåa[:,0:1]ÊòØ‰∏çÂêåÁöÑÂñî ÂÖ≥‰∫éÊï∞ÁªÑÂ±ûÊÄßÁöÑÊñπÊ≥ï Êï∞ÁªÑËøêÁÆó Á¥¢Âºï where ÂáΩÊï∞ Á¥¢ÂºïÁöÑÂ∏ÉÂ∞îÊï∞ÁªÑ ÂπøÊí≠ÔºàBroadcastingÔºâÁî®‰∫éÂ§ÑÁêÜ‰∏çÂêåÊÄßÁä∂ÁöÑ Êï∞ÁªÑ„ÄÇ BroadcastingÊèê‰æõ‰∫Ü‰∏ÄÁßçÁü¢ÈáèÂåñÊï∞ÁªÑÊìç‰ΩúÁöÑÊñπÊ≥ïÔºå‰ΩøÂæóÂæ™ÁéØÂèëÁîüÂú®CËÄå‰∏çÊòØPython„ÄÇÊ†áÈáè‰πò‰ª•‰∏Ä‰∏™Áü¢ÈáèÁöÑÊó∂ÂÄôÔºåÁî®BoradcastingÊõ¥Âø´ÔºåÂõ†‰∏∫ broadcastingÂú®‰πòÊ≥ïÊúüÈó¥ÁßªÂä®ËæÉÂ∞ëÁöÑÂÜÖÂ≠ò array Âíå matrix ÈÄâÊã©Âì™‰∏™? Êà≥Êàë Áü¢ÈáèÂåñÂíåÂπøÊí≠„ÄÅÁ¥¢ÂºïÂú®Python‰∏≠Âæ™ÁéØÊï∞ÁªÑÊàñ‰ªª‰ΩïÊï∞ÊçÆÁªìÊûÑÊó∂Ôºå‰ºöÊ∂âÂèäÂæàÂ§öÂºÄÈîÄ„ÄÇ NumPy‰∏≠ÁöÑÂêëÈáèÂåñÊìç‰ΩúÂ∞ÜÂÜÖÈÉ®Âæ™ÁéØÂßîÊâòÁªôÈ´òÂ∫¶‰ºòÂåñÁöÑCÂíåFortranÂáΩÊï∞Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥Ê∏ÖÊô∞ÔºåÊõ¥Âø´ÈÄüÁöÑPython‰ª£Á†Å„ÄÇ PythonÂ∫ì‚Äî‚ÄîpandasËÆ∞ÂæóÂ≠¶‰π†pandasÊòØÂú®Â§ß‰∏âÊó∂ÂÄôÁöÑÁæéËµõÔºåËä±‰∫Ü‰∏ÄÂ§©Â§öÊó∂Èó¥Â≠¶‰π†pandasÔºåÁÑ∂ÂêéÈ¢ÑÂ§ÑÁêÜÊï∞ÊçÆÔºåÂΩìÊó∂‰∏â‰∏™ÈòüÂèãÈÉΩÊòØÂêÑËá™ÁöÑÂÆ∂ÔºåÊòØÈùûÂ∏∏ÊÑâÂø´ÁöÑÔºÅÔºÅÔºÅ whatPython Data Analysis Library ‰∏âÁßçÊï∞ÊçÆÁªìÊûÑÂ∫èÂàóÔºö Series 1DÊï∞ÊçÆÂ∏ßÔºö DataFrame 2DÈù¢ÊùøÔºö Panel &gt;2D Ëá™ÂÆö‰πâÂàõÂª∫ ÂèØ‰ª•ÈÄöËøáÂ≠óÊÆµ„ÄÅÊï∞ÊçÆ„ÄÅseries„ÄÅÂàóË°® ÂàóË°®‰º†ÂÖ•ÁöÑÊó∂ÂÄôÔºå‰∏ªË¶ÅË°åÂàóÔºåÂ¶ÇÊûúÂçï‰∏™ÂàóË°®ÔºöÂàóÔºõÂ¶ÇÊûúÊòØ[[],[]]ÊòØÊåâË°å[] Â¶ÇÊûú‰ΩçÁΩÆ‰∏çÂØπÂèØËΩ¨ÁΩÆ ÂàõÂª∫Á©∫ pd.DataFrame() ÈÄâÊã©Âå∫Âùó a) Series [] b) DataFrame ÂàóÈÄâÊã© [‚ÄòcolumsÁöÑÂêçÂ≠ó‚Äô] Ë°åÂàóÈÄâÊã©Ôºö.loc[ÂàóÂêç,Ë°åÂêç]ÂêçÁß∞ .iloc[ÂàóÁ¥¢Âºï,Ë°åÁ¥¢Âºï]Êï¥Êï∞ array.value ÁªüËÆ°ÊèèËø∞ .descibe(include = ‚Äòall‚Äô) .head() .tail() .select_dtype(include=[]) .columns .dtype Áº∫Â∞ëÊï∞ÊçÆ Êü•ÁúãÁº∫Â§±ÂÄºisnull() notnull() ‰πüÂèØ‰ª• ÂÅö‰∏Ä‰∫õÁªüËÆ°Ôºåsum, any,all Ê∏ÖÁêÜÁº∫Â§±ÂÄº dropna(axis=0)Ôºöaxis = 0:index axis=1,columns Â°´ÂÖÖÁº∫Â∞ëÊåá fillna() Ê†áÈáèÊõøÊç¢ ÊõøÊç¢ ÁªüËÆ°ÂáΩÊï∞ Pandas ÂáΩÊï∞Â∫îÁî®Ë°®ÂêàÁêÜÂáΩÊï∞Â∫îÁî®Ôºöpipe()Ë°åÊàñÂàóÂáΩÊï∞Â∫îÁî®Ôºöapply()ÂÖÉÁ¥†ÂáΩÊï∞Â∫îÁî®Ôºöapplymap()egÔºö pd.pipe(lambda x: x*100) Á±ªÂà´ÂèòÈáèÂêëÈáèÂåñÈùûÊï∞ÂÄºÁ±ªÂûãÁöÑÂ§ÑÁêÜÊñπÊ≥ï Êó∂Èó¥Â∫èÂàóÁîüÊàê data_range pandas.date_range(‚Äú11:00‚Äù, ‚Äú21:30‚Äù, freq=‚Äù30min‚Äù) ÂèÇÊï∞1Return a fixed frequency DatetimeIndex. Parametersstartstr or datetime-like, optionalLeft bound for generating dates. endstr or datetime-like, optionalRight bound for generating dates. periodsint, optionalNumber of periods to generate. freqstr or DateOffset, default ‚ÄòD‚ÄôFrequency strings can have multiples, e.g. ‚Äò5H‚Äô. See here for a list of frequency aliases. tzstr or tzinfo, optionalTime zone name for returning localized DatetimeIndex, for example ‚ÄòAsia/Hong_Kong‚Äô. By default, the resulting DatetimeIndex is timezone-naive. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. namestr, default NoneName of the resulting DatetimeIndex. closed{None, ‚Äòleft‚Äô, ‚Äòright‚Äô}, optionalMake the interval closed with respect to the given frequency to the ‚Äòleft‚Äô, ‚Äòright‚Äô, or both sides (None, the default). **kwargsFor compatibility. Has no effect on the result. ReturnsrngDatetimeIndex12345678910111213141516171819202111. DataFrame.stackParameterslevelint, str, list, default -1Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels.dropnabool, default TrueWhether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section.ReturnsDataFrame or SeriesStacked dataframe or series.‚Äã```pythondf_single_level_cols weight heightcat 0 1dog 2 3df_single_level_cols.stack()cat weight 0 height 1dog weight 2 height DataFrame.value_connts()ËøîÂõûÂ∫èÂàóÔºåindex=ÁªüËÆ°ÂÄºÔºåÂÄºÔºöÁªüËÆ°‰∏™Êï∞ Matplotlibmatplotlib.pyplot as plt Á™óÂè£Ôºöfigure: ‰∏Ä‰∏™Á™óÂè£Ôºåplt.figure(num=,figsize=(h,w))‰∏ãÈù¢Êï∞ÊçÆÈÉΩÂ±û‰∫éÂΩìÂâçÁöÑfigure,Êúâ‰∏ÄÂÆöÁöÑÈ°∫Â∫èÂñî ÁîªÂõæÔºöplt.plot(x,y,color=,linewidth=,linestyle,label=) Ê†áÊ≥®‰ø°ÊÅØÔºö plt.xlim((,)), plt.yxlim((,)),plt.xlabel(),plt.ylabel(),ticks:ÂõæÂÉèÁöÑÂ∞èÊ†áÔºåplt.xticks(),plt.yticks([ÂÄº1ÔºåÂÄº2],[r‚Äô$ÂÄº1\ ÂØπÂ∫îÁöÑÊñáÂ≠ó$‚Äô,r‚ÄôÂÄº2ÁöÑÊñáÂ≠ó \alpha]) ÂùêÊ†áËΩ¥Ôºöaxis gac=‚Äôget current axis‚Äôax = plt.gca() # ËΩ¥# Ëé∑ÂèñÂõõ‰∏™ËΩ¥ax.spines[‚Äòright|left|top|‚Äô].set_color(‚Äònone‚Äô)ax.xaxis.set_ticks_position(‚Äòbottom‚Äô)ax.spines[‚Äòbottom‚Äô].set_position((‚Äòdata‚Äô,-1)) Âõæ‰æãÔºölegend: a. plt.plot(,label=), plt.legend() b. l1, = plt.plot() plt.legend(handles=[l1,],labels=[,],loc=‚Äôbest|upper right|‚Äô) Ê≥®Ëß£ annotationa. ÁÇπÁöÑ‰ΩçÁΩÆ(x0Ôºåy0) plt.scatter(). plt.plot([x0,y0],[y0,0],‚Äôk‚Äî‚Äò,lw=)b . method 1:plt.annotate(r‚Äôname‚Äô,xy=(,)Ëµ∑ÂßãÁÇπÔºåxycoords=‚Äôdata‚Äô//Âü∫‰∫éxy,xytext=(+30,30),textcoords=‚Äôoffseet points‚Äô//ÊñáÊú¨Âü∫‰∫éxy,arrowprops=dict(arrowstyle=‚Äô-&gt;‚ÄôÁÆ≠Â§¥,connectionstyle=‚Äôarc3,rad=.2‚Äô)ÂºßÂ∫¶) Bar Êü±Áä∂Âõæplt.bar(x,+|-y,facecolor=‚Äù‚Äù,edgecolor,)|# ha horizontal alignment ÂØπÈΩêÊñπÂºèfor x,y in zip(x,y): plt.text(x+0,4,y+0.05,‚Äô%.2f‚Äô%y,ha=‚Äôcenter‚Äô,va=‚Äôbottom‚Äô) ÂæàÂ§öËá™Âä® subplot(ÊÄªË°åÔºåÂΩìÂâçË°åÁöÑÂàóÔºåÊÄªÁöÑÊåâÊúÄÂ∞èÂàÜÁöÑÁ¨¨Âá†‰∏™)subplot(,,)scikit-learnÂÆòÊñπÊïôÁ®ãÁªùÂØπÊòØÊúÄÂ•ΩÊúÄÊ£íÁöÑÈÄâÊã©ÔºåÊúâÁÆÄÂçïÊï∞Â≠¶ÊåáÂØº„ÄÅÁõ¥ËßÇÁ´ãÈ©¨Â∞±ËÉΩ‰∏äÊâãÁöÑÊ°à‰æãÔºåËøòËÉΩÊèêÈòÖËØªËã±ÊñáÁöÑËÉΩÂäõÂñîÔºåÂÆûÂú®ÊòØ‰∏Ä‰∏æÂ§öÂæóÂïäÔºÅÔºÅÔºÅÔºÅ scikit-learn.org ÊúÄÂ∞è‰∫å‰πòÊ≥ïÔºöOrdinary Least SquaresËØ¶ÁªÜÁöÑÊ°à‰æã123456789101112131415161718192021import numpy as npfrom sklearn import linear_modelfrom sklearn.metrics import mean_squared_error, r2_score# Create some train datatrain_x = np.array([[0,0],[1,1],[2,2],[3,3]])train_y = np.array([[0],[1],[2],[4]])# Create linear regression objectregr = linear_model.LinearRegression()# Create the model using training setsregr.fit(train_x,train_y)# Make predictions using the testing sety_pred = regr.predict(train_x)# Get same parm# The coefficients wprint('Coefficients:', regr.coef_)# The intercept bprint('intercept', regr.intercept_)# The mean squared errorprint("Mean squared error: %.2f", mean_squared_error(train_y ,y_pred))# Explained variance score: 1 is perfect predictionprint('Variance score: %.2f',r2_score(train_y, y_pred))]]></content>
      <categories>
        <category>ÁºñÁ®ãËØ≠Ë®Ä</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ÊàëÁöÑËØª‰π¶Á¨îËÆ∞]]></title>
    <url>%2F2019%2F02%2F22%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ÊÑüÊÇü „ÄäÊÄ¶ÁÑ∂ÂøÉÂä®„Äã2020.6.13Âè∑ÔºåÁ¨¨‰∏âÊ¨°Áúã„ÄäÊÄ¶ÁÑ∂ÂøÉÂä®„Äã ‰∏ÄÈ¢óÁªøËâ≤ÁöÑÊ†ë ‰∏§‰∏™ÊáµÊáÇÁöÑÂ∞èÂ≠© ËÆ∏Â§öÊöñÂøÉÁöÑÂè∞ËØç ‚Äã Êúâ‰∫∫‰ΩèÈ´òÊ•ºÔºåÊúâ‰∫∫Âú®Ê∑±Ê≤üÔºåÊúâ‰∫∫ÂÖâËäí‰∏á‰∏àÔºåÊúâ‰∫∫‰∏ÄË∫´ÈîàÔºå‰∏ñ‰∫∫‰∏áÂçÉÁßçÔºåÊµÆ‰∫ëËé´ÂéªÊ±ÇÔºåÊñØ‰∫∫Ëã•ÂΩ©ËôπÔºåÈÅá‰∏äÊñπÁü•Êúâ„ÄÇ ‚Äã ÊúâÁöÑ‰∫∫ÈªØÊ∑°ÊµÖËñÑÔºåÊúâÁöÑ‰∫∫ÂÖâÂΩ©‰∏á‰∏àÔºåÊúâÁöÑ‰∫∫Ë¥•ÁµÆËóèÂøÉÔºåËÄåÂΩì‰Ω†Êó†ÊÑèÈó¥ÈÇÇÈÄÖÔºå‰∏Ä‰∏™ÁÅøËã•ËôπÈúìÁöÑ‰∫∫ÔºåËá™Ê≠§‰πãÂêéÔºå‰∏ñÈó¥Âè™Âæó‰∏Ä‰∏™Â•π„ÄÇ ‚Äã Ê¥õÊñØÂ•áÂÖàÁîüÂ§ñË°®ÁúãËµ∑Êù•ÂÖâÈ≤ú‰ΩìÈù¢Ôºå‰ΩÜ‰ºº‰πéÁúãËµ∑Êù•ÂøÉÈáåË£ÖÁùÄÔºå‰ªÄ‰πàËÖêÁÉÇÊéâÁöÑ‰∏úË•ø„ÄÇ ‚Äã ÊàëËø∑‰∏ä‰∫ÜÂ•πÔºåÂΩªÂ∫ïËø∑‰∏ä‰∫ÜÂ•π„ÄÇ ‚Äã Êúâ‰∏ÄÂ§©‰Ω†‰ºöÈÅáÂà∞Âà∞‰∏Ä‰∏™ÂΩ©ËôπËà¨Áªö‰∏ΩÁöÑ‰∫∫Ôºå‰ªéÊ≠§‰ª•ÂêéÔºåÂÖ∂‰ªñ‰∫∫‰∏çËøáÂ∞±ÊòØÂåÜÂåÜÊµÆ‰∫ë„ÄÇ 2019 Á¨¨ÂçÅ‰∫îÂë®‰∏âÊúà‰ªΩËá≥2019.4.9ËøôÊÆµÊó∂Èó¥ÔºåÊâçÂèëÁé∞ÊàëÊòØÂ¶ÇÊ≠§Ê≤°ÊúâËá™ÂæãÁöÑ‰∫∫ÔºåÂÖÖÂàÜ‰ΩìÁé∞‰∫ÜÊàëÊòØ‰∫∫ÁöÑÁâπÊÄßÔºåÈÇ£Â∞±ÊòØÊàëÊòØÁæ§‰ΩìÂä®Áâ©ÔºåËã¶Á¨ë.jpg,Ëã¶Á¨ë.jpg, en, ÊúÄËøëÁ™ÅÁÑ∂ÊÉ≥ÁªôËá™Â∑±Êâì‰∏äÂé®Â®òÁöÑË∫´‰ªΩÔºåÂ¶ÇÊûúÂèØ‰ª•ÊØèÂ§©Ëä±‰∏§‰∏™Â∞èÊó∂ÂÅöÈ•≠Â∞±Â•Ω‰∫Ü ÊÑø‰Ω†Ë¢´‰∏ñÁïåÊ∏©ÊüîÁöÑÁõ∏ÂæÖ Êé•Ëß¶ÁöÑ‰∏úË•øË∂äÂ§öÔºåË∂äÊ∑±ÂÖ•ÔºåÂ∞±‰ºöÂèëÁé∞ÊàëÊòØÂ¶ÇÊ≠§ÁöÑËèúÔºåÂºÄÂßãÊúâ‰∫õÁü•ËØÜÁÑ¶Ëôë‰∫ÜÔºåÁü•ËØÜÈÇ£‰πàÂ§ö~ÔºåÂèØÊòØÊàëÂè™Êúâ‰∏Ä‰∏™Â§¥ËÑëÂïä~ ÂºÄÂßã‰∏çÊÉ≥ÂÜô‰∏Ä‰∫õÁâπÂà´‰Ωé‰øóÁöÑÂçöÂÆ¢‰∫ÜÔºå‰∏ÄÊòØËßâÂæóÊµ™Ë¥πÊó∂Èó¥Ôºå‰∫åÊòØËæìÂá∫ÊïàÊûúÂ§™Â∑ÆÔºåÂºï‰∏çËµ∑ÁâπÂà´Â§ßÁöÑÂÖ≥Ê≥®ÔºåËôΩÁÑ∂ÊàëÂÜôÂçöÂÆ¢ÔºåÂÆåÂÖ®ÊòØÁ´ôÂú®Ëá™Â∑±ÁöÑËßíÂ∫¶ÔºåÊ≤°ÊúâËÄÉËôëËØªËÄÖÁöÑÊÑèÊÑøÔºåÔºàÊªëÁ®Ω.jpg)„ÄÇ Áé∞Âú®ÁöÑËá™Â∑±Ôºå‰∏çÊòØÂÅúÁïôÂú®Âü∫Êú¨ÁöÑÈóÆÈ¢ò‰∏äÔºåÊõ¥Â∫îËØ•ÂéªÊé¢Á¥¢Êú™Áü• ÁöÑÁü•ËØÜ‰∏ñÁïåÔºåËôΩÁÑ∂Á¶ªËøô‰∏™flagÂèØËÉΩËøòÊúâÂá†Âπ¥ÁöÑÊó∂Èó¥ÔºåËÉΩÂ§üÁªô‰∏ñÁïåÁöÑÁü•ËØÜÂàõÈÄ†‰∏ÄÁÇπÁÇπ‰ª∑ÂÄºÔºåÂì™ÊÄïÂè™ÊòØ‰∏ÄÂ∞èÁÇπÁÇπ„ÄÇÁ¶ªËøô‰∏™ÁõÆÊ†áËøòÈúÄË¶ÅÂä™ÂäõÂïäÔºÅÔºÅÔºÅÔºÅÔºÅ ÊàëÊÉ≥ÊàëÂ∫îËØ•ÂéªËÆ∞ÂΩïÂ≠¶‰π†Áü•ËØÜÁöÑËøáÁ®ãÔºåÁ™ÅÁ†¥Êõ¥Â§ßÁöÑÊõ¥Âõ∞ÈöæÁöÑÈóÆÈ¢ò„ÄÇ 2019-Á¨¨ÂõõÂë®ËØª‰π¶Á¨îËÆ∞ ËøôÂë®ËØª‰∫Ü‰∏ÄÊú¨Â∞èËØ¥ÔºåÊòØÂº†Áà±Áé≤ÁöÑ„ÄäÂÄæÂüé‰πãÊÅã„ÄãÔºåÂéüÊù•ÂíåÁîµËßÜÂâßÁöÑ‰ΩïÊôüÈì≠‰∏ªÊºî„ÄäÂÄæÂüé‰πãÊÅã„Äã‰∏çÊòØÂêå‰∏Ä‰∏™‰∫ãÊÉÖÂïäÔºÅ Áúã‰∫Ü„ÄäÈòøÁîòÊ≠£‰º†„ÄãÔºå‚ÄúÁîüÊ¥ªÂ∞±ÂÉè‰∏ÄÁõíÂ∑ßÂÖãÂäõÔºå‰Ω†Ê∞∏Ëøú‰∏çÁü•ÈÅì‰∏ã‰∏ÄÈ¢óÊòØ‰ªÄ‰πàÂë≥ÈÅì„ÄÇ‚ÄúËøôÊòØÈòøÁîòÂØπÁîüÊ¥ªÊúÄÂ•ΩÁöÑËØ†Èáä„ÄÇÂ∞èÊó∂ÂÄôÔºåÊúâ‰∫∫È™ëÁùÄËá™Ë°åËΩ¶ÁæûËæ±‰ªñÔºå‰ªñÂè™‰ºöË∑ëÔºåÊãºÂëΩÁöÑË∑ëÔºåÂè™‰ºöÂÜçÂÖ¨Ë∑Ø‰∏äË∑ë„ÄÇÈïøÂ§ßÂêéÔºåÂà´‰∫∫È™ëÁùÄËΩ¶ÊÉ≥Êâì‰ªñÔºåÈòøÁîòËøòÊòØË∑ëÔºå‰ΩÜÊòØËøôÊ¨°ÈòøÁîòÂ≠¶‰ºö‰∫ÜÁΩëËçâÂù™‰∏äË∑ëÔºÅÂ∞±Ë¢´Â§ßÂ≠¶Áúã‰∏äÔºåËøõÂÖ•ËøêÂä®Â§ßÂ≠¶ÔºåËøòÈÄöËøáÂèÇÂä†ÊØîËµõËµ¢Âæó‰∫ÜÂÜ†ÂÜõÔºåÁÑ∂ÂêéÔºåÈòøÁîòÂΩìÂÖµ‰∫ÜÔºåÂÜçÂêéÊù•ÔºåÊâì‰πí‰πìÁêÉÂæàÂá∫Ëâ≤„ÄÇÈòøÁîò‰ºº‰πéÂÅö‰ªÄ‰πàÈÉΩËÉΩÊàêÂäüÔºå‰πüËÆ∏ÂøÉÊó†ÊóÅÈ™õÔºåÊúÄÁ¨®ÁöÑÊñπÊ≥ï+Êó∂Èó¥=Êî∂Ëé∑„ÄÇ ÊàëËßâÂæóÂæàÂøÉÈÖ∏ÁöÑÊòØÔºåÂΩìÁèçÂ¶ÆÂëäËØâ‰ªñÊúâÂÑøÂ≠êÊó∂ÂÄôÔºåÈòøÁîòÈóÆÔºå‚Äù‰ªñËÅ™ÊòéÂêó‚ÄúÔºü 2019Á¨¨ÂõõÂë®ÂÆâÊéí ÊîπËÆ∫ÊñáÔºåÊîπÂèòËá™Â∑±ÁöÑÂäû‰∫ãÊïàÁéáÂñîÔºåÊãíÁªùÈáçÂ§çÂ∑•‰Ωú ÁºñÁ®ãËÉΩÂäõ ÊÖ¢ÊÖ¢ÁöÑÂÅö‰∫ãÊÉÖÔºåÂÖàÊÖ¢ÂêéÂø´Ôºå ÁîüÊ¥ª„ÄÅÂ≠¶‰π†„ÄÅ‰∫§Âèã„ÄÅÊñáÈáá2019-Á¨¨‰∏âÂë®ËØª‰π¶Á¨îËÆ∞ËøôÊ¨°ËØª‰∫Ü„ÄäÊûÅÁÆÄÊÄùÁª¥ÔºöÈ¢†Ë¶Ü‰º†ÁªüÊÄùÁª¥Ê®°ÂºèÁöÑÊûÅÁÆÄÊ≥ïÂàô„Äã‰ΩúËÄÖÔºöS.JÊñØÁßëÁâπ Â∑¥Èáå.ËææÊñáÊ≥¢Áâπ Êàë‰ª¨ÁîüÊ¥ªÂÖÖÊª°‰∫ÜÂêÑÁßçËØ±ÊÉë„ÄÅÊùÇ‰π±‰ø°ÊÅØ„ÄÅÂØºËá¥‰∫ÜÁîüÊ¥ªÁöÑÊ∑∑‰π±Ôºå‰∫ßÁîüÁü•ËØÜÁÑ¶Ëôë„ÄÅÂπ¥ÈæÑÂç±Êú∫„ÄÅ‰∫∫ÈôÖÂÖ≥Á≥ªÁöÑÊ∑°Âåñ„ÄÇ‰ΩúËÄÖÁªôÊàë‰ª¨‰ªãÁªç‰∫ÜËÆ∏Â§öÈóÆÈ¢ò„ÄÅËÆ∏Â§öÁöÑËß£ÂÜ≥ÊñπÊ≥ïÔºåËÆ©Êàë‰ª¨Ëøô‰∏™‰ø°ÊÅØÁàÜÁÇ∏ÁöÑÊó∂‰ª£ÂèØ‰ª•ËøáÁöÑÂÖÖÂÆû‰∫õ„ÄÇ ÊØèÂ§©Áù°8‰∏™Â∞èÊó∂„ÄÅËøòÂâ©‰∏ã16‰∏™Â∞èÊó∂ÔºåÂú®ÂáèÂéª2‰∏™Â∞èÊó∂Ëß£ÂÜ≥‰∏™‰∫∫Âç´ÁîüÂíåÈ•ÆÈ£üÔºåÈÇ£‰πàËøòÊúâ14‰∏™Â∞èÊó∂Ôºå‰∏Ä‰∏™ÊòüÊúü98‰∏™Â∞èÊó∂„ÄÇÈÇ£‰πà98‰∏™Â∞èÊó∂Ôºå‰Ω†ÊäïÂÖ•Âú®Âì™ÈáåÂë¢Ôºü ÊÄªÁöÑÊù•ËØ¥ÔºåËøôÊú¨‰π¶‰º†ËææÁöÑ‰∏úË•øÔºåÊàëËøòÊòØÂæàÂñúÊ¨¢ÁöÑÔºåÊûÅÁÆÄ‰∏ª‰πâËÄÖÔºåÂ∞ë‰∏çÂæó‰πüÂ§ö‰∏çÂæóÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅ ËØª„ÄäÊãÜÊéâÊÄùÁª¥ÈáåÁöÑÂ¢ô„ÄãÊëòÂΩï ÔºöÊàë‰ª¨ÁöÑÁîüÊ¥ª‰πüÁî±‰∏â‰∏™ÊîØÊû∂ÁªÑÊàêÔºöËá™Êàë„ÄÅÂÆ∂Â∫≠‰∏éÂõ¢‰ΩìÂíåËÅå‰∏ö„ÄÇËøôÊ†∑ÁöÑÊîØÊû∂ÊîØÊíëÁùÄÊàë‰ª¨ÁöÑÁÅµÈ≠ÇÔºåÂÆÉÂú®ËÆ∞ÂΩïÊàë‰ª¨ÁöÑÁîüÂëΩ„ÄÇÊàë‰ª¨‰∏ÄÁõ¥ÈÉΩÂú®Ë∞ÉÊï¥ÁùÄ‰∏â‰∏™‰ΩçÁΩÆÁöÑÂπ≥Á®≥Ôºå‰Ωø‰πãÊàê‰∏∫ÊúÄÁ®≥Âõ∫ÁöÑËÅîÂä®‰∏âËÑöÊû∂„ÄÇ ËøôÂè•Â§ßÊ¶ÇÊòØÁªìÂêàÊàëÁöÑÁªèÂéÜÔºåÊúÄÂÖ∑ÊúâÊÑüÊÇüÁöÑ„ÄÇÂõ†‰∏∫‰∏ÄÊó¶Ëµ∞Âá∫Â§ßÂ≠¶ÔºåËøô‰∏âËÄÖÊâçÂºÄÂßãÁúüÊ≠£ÁöÑÁªÑÊàêÊàë‰ª¨ÁöÑÁîüÊ¥ª„ÄÇ Âè§ÂÖ∏ËÄÅÂ∏àÔºå‰ªéËÅå‰∏ö„ÄÅÊàêÂäüÂ≠¶„ÄÅÁà±ÊÉÖ„ÄÅÂÆ∂Â∫≠Á≠âÁ≠â‰∏çÂêåÁöÑÊ°à‰æãÔºåÁªôÊàëÂàÜÊûê‰∫ÜÂ§ßÂ§öÊï∞‰∫∫‰ºöÈù¢‰∏¥ÁöÑÊó†ÂΩ¢ÁöÑ‚ÄùÂ¢ô‚ÄúÔºåÁªô‰∫ÜÊàë‰ª¨Â¶Ç‰ΩïÊãÜÊéâËøô‰∫õÂ¢ôÁöÑÊñπÊ≥ï„ÄÇ‰ΩÜÊòØÂë¢ÔºåÂØπ‰∫éÂè§ÂÖ∏ËÄÅÂ∏àÁöÑÁà±ÊÉÖËßÇÁÇπÔºåÊàëÂπ∂‰∏çÊòØÂæàËµûÂêåÔºåÂõ†‰∏∫Âë¢ÔºåÈÇ£‰∫õÊÑøÊÑèÈô™‰Ω†Â∫¶Ëøá‰ΩôÁîüÁöÑ‰∫∫‰ªòÂá∫ÁöÑÊÑüÊÉÖÔºåÊòØÂ¶ÇÊ≠§ÁöÑÂªâ‰ª∑ÂêóÔºüÊúâÁöÑ‰∫∫Êó¢ÂèØ‰ª•ÊòØÁôΩÁé´Áë∞Ôºå‰πüÂèØ‰ª•Á∫¢Áé´Áë∞ÂïäÔºÅ 2019Âπ¥Á¨¨‰∫åÂë®ÂÆâTÊéíÊØèÂ§©‰∏§‰∏™Â∞èÊó∂ÈòÖËØªËÆ∫ÊñáÊàñËÄÖ‰∏ì‰∏ö‰π¶Á±çÁöÑÈòÖËØªÂºÄÈ¢òÊä•Âëä‰øÆÊîπÂíåPPTÂà∂‰ΩúÔºà3h)„ÄäÊãÜÊéâÊÄùÁª¥ÈáåÈù¢ÁöÑÂ¢ô„ÄãÔºà3h)ÁúãÂìàÂà©Ê≥¢ÁâπÔºà‰∏ÄÈõÜÔºâ ‚Äî-2018Âπ¥ÁöÑÊÄªÁªì Â∞èÂ∞èÁöÑÊÇîÊÅ®‰∏éÈÅóÊÜæ Â§ß‰∏â‰∏ãÔºåÂú®ËØæÂ†Ç‰∏äÔºåÊâì‰∫ÜÂçäÂ≠¶ÊúüÁöÑÊ∏∏Êàè ÁîüÊ¥ªËøòÊòØ‰∏çËßÑÂæãÔºåË∂ÖÂñúÊ¨¢Ê∑±Â§úÈÄõÁü•‰πé„ÄÅÂà∑BÁ´ô È¢ùÂ§¥‰∏äÔºå‰∏çÂÅúÁöÑÂÜíÁùÄÁóòÁóòÂïä Ëã±ËØ≠ÂçïËØçÈáèÂú®‰∏ãÈôçing ËøêÂä®ÈáèÂú®Èôç‰ΩéÂñî ÂæàËÆ®ÂéåÊ¥óË°£Êúç Áõ∏ÊØî‰∫é‰∏ä‰∏ÄÂπ¥ËøõÊ≠•ÁöÑÊñπÈù¢ ÊÑøÊÑèÂéªÊâøÊãÖÊõ¥Â§öÁöÑË¥£‰ªª Êõ¥‰πêÊÑèÂéª‰∫§ÊµÅ Ë∂äÊù•Ë∂äÈáçËßÜÂÅ•Â∫∑style ‰∏ç‰ºöÈöèÊÑèÂèëÊ≥ÑËá™Â∑±ÁöÑÊÉÖÁª™‰∫Ü Êõ¥Âä†ËÆ§ËØÜÂà∞Ëá™Ë∫´ÁöÑ‰ºòÂäø‰∏éÂä£Âäø‰∫ÜÊÑüÂà∞ÊÑâÂø´ÁöÑ‰∫ãÊÉÖ Áü•ÈÅìËá™Â∑±ÊÉ≥Ë¶Å‰ªÄ‰πàÔºåÁü•ÈÅìËá™Â∑±Âú®ÂÅö‰ªÄ‰πà Êï¥ÁêÜÂÆå‰∫ÜÂ§ßÂ≠¶ÊúüÈó¥ÊâÄÊúâÁöÑ‰∏úË•øÔºåÂæÄ‰∫ã‰∏çÂ†™ÂõûÈ¶ñÔºå ‰ΩÜ‰πüÂè™ËÉΩÊòØÊü≥ÊöóËä±ÊòéÂèà‰∏ÄÊùë„ÄÇ ËÉΩËØªÁ†îÁ©∂Áîü‰∫Ü ËÅäËÅä2019Âπ¥ÁöÑÁÇπÁÇπÊúüËÆ∏Â≠¶‰π†‰∏ä Â§öÁúã19Âú∫Áü•‰πélive ÈòÖËØª10Êú¨‰π¶Á±çÔºå‰π¶Âçï‰πüÊúâ‰∫Ü Âú®‰∏ì‰∏öÂ≠¶‰π†‰∏äÔºåÂ∏åÊúõÊúâÊâÄÊèêÂçáÂíØÁîüÊ¥ª‰∏≠ Êó©Áù°Êó©Ëµ∑Ë∫´‰ΩìÂ•Ω ÁúãÂçÅÈÉ®ÁæéÂâßÔºåÂ∞ΩÁÆ°ÊàëÊúÄÂ§ßÁöÑÂÖ¥Ë∂£ÊòØÁù°Ëßâ Êó∂Â∏∏Êõ¥Êñ∞Ê≠åÂçïÔºå‰∏çÊÉ≥Âú®‰∏ÄÂπ¥ÈáåÈù¢ÈÉΩÊòØÁõ∏ÂêåÁöÑÊóãÂæã ÈùôÈùôÈùôÈùôÈùôÈùôÈùô ÂêàÁêÜÂÆâÊéí ÊäòÊòüÊòü Áï™ËåÑÈóπÈíü ÂÅ∂Â∞îÂê¨Âê¨ TEDÊäÄÊúØ Ê∏ÖÁêÜ‰∏ã‰∫Ügithub ‰ªìÂ∫ì ÈáçÊñ∞Êõ¥Êñ∞‰∫Ü github page Â§öËØª„ÄÅÂ§öÂÜô„ÄÅÂ§öÊÉ≥]]></content>
      <categories>
        <category>ËØª‰π¶Êó•Â∏∏</category>
      </categories>
      <tags>
        <tag>ËØª‰π¶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MayMay]]></title>
    <url>%2F2019%2F02%2F22%2FMayMay%2F</url>
    <content type="text"><![CDATA[https://www.kaggle.com/dgawlik/house-prices-eda/datahttps://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python]]></content>
      <tags>
        <tag>wan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Â≠¶‰π†„ÅÆÂéÜÁ®ã]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%AD%A6%E4%B9%A0Daily%2F</url>
    <content type="text"><![CDATA[Ë¥¥Âº†‰ºèÈ≠îÂíí 2020 29Âë® Input Â∞ùËØïË∞ÉÁ†îÂ∞èÈ¢ÜÂüüÔºàÊÑüËßâ‰∏ãËΩΩÁöÑÊñáÁåÆË¥®ÈáèÈùûÂ∏∏È´òÔºâ Â≠¶‰π†ÂÜô‰Ωú Êó∂Èó¥Â∫èÂàóÔºàÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºâ Output PPT Â§öËÆ∞ÁÇπ‰∏ì‰∏öËØçÊ±áÂíåÂè•ÂºèÔºàÊàëÂ∞±ÊòØÁÖßÁùÄÂà´‰∫∫ÂÜôÁöÑÔºå‰ΩÜÊòØËØªËµ∑Êù•‰∏çÂØπÂä≤Ôºå‰∏çÈÄöÁïÖÔºâ 2020-7-12 ÂΩì‰Ω†Áü•ÈÅìËá™Â∑±ÁöÑÈúÄÊ±ÇÔºå‰πüÁü•ÊôìÂà´‰∫∫ÁöÑÈúÄÊ±ÇÔºåÊâÄÊúâÁöÑ‰∫ãÊÉÖÈÉΩÂçÅÂàÜÂ•ΩÂπ≤‰∫Ü„ÄÇÂ¶ÇÊûúÂá∫Áé∞‰∫ÜÈóÆÈ¢òÔºå‰∏ÄÂÆöÊòØÊàëÂÅöÁöÑ‰∏çÂ•Ω„ÄÇ ‰ªäÂ§©Êâæ‰∫ÜÂá†ÁØáÈùûÂ∏∏Â•ΩÁöÑÊñáÁåÆÔºåÂèØ‰ª•ÂÅöËøôÊñπÈù¢ÁöÑÁªÜË∞ÉÁ†îÔºåÂ§ßÊ¶ÇÂ∞±10ÁØáÂ∑¶Âè≥„ÄÇ ‰ªäÂ§©Â§ÑÁêÜ‰∫ÜÊï∞ÊçÆÔºåÂÆûÂú®ÊòØÈ∫ªÁÉ¶„ÄÇ Á™ÅÁÑ∂ÂèëÁé∞ÊñáÁ´†ÊúÄÈöæÁöÑÊòØÊñáÁåÆÁªºËø∞„ÄÇ Input ‚úîÔ∏è‚ùå Output ‚ùå‚ùå‚úîÔ∏è 2020-7-11 ÁúãÂâßÔºåË∑ëÁ®ãÂ∫è ‰ªäÂ§©Êó©‰∏äÁù°ËßâÔºåÁúãËßÜÈ¢ë ‰ªäÂ§©‰∏ãÂçàÂíåÊôö‰∏äÔºåÂ§ÑÁêÜÊï∞ÊçÆÔºåÂïäÈ∫ªÁÉ¶ Á≤æËã±Á≤æÁ•ûÔºö 1. „ÄäË∞ÅËØ¥ÊàëÁªì‰∏ç‰∫ÜÂ©ö„Äã Âà©ÁõäÔºöË¶Å‰∏çË¶ÅÂÅöÔºõÈ£éÈô©ÔºöËØ•‰∏çËØ•ÂçöÔºõ ËÉΩÂäõÔºöËØ•‰∏çËØ•Âπ≤ÔºõÁªìÊûúÔºöÂàí‰∏çÂàíÁÆóÔºõËÄå‰∏çÊòØÂà´‰∫∫ÂëäËØâÊàëÔºöÂØπ‰∏çÂØπ„ÄÇ 2. ÊòéÁ°ÆÂêÑËá™ÈúÄÊ±Ç 2020-7-10 Ë∑ëÁ®ãÂ∫è ‰ªäÂ§©Êó©‰∏äË∑ëÁ®ãÂ∫èÔºàÂ∞ÅË£ÖÊé•Âè£Ôºâ ‰ªäÂ§©‰∏ãÂçàË∑ëÁ®ãÂ∫èÔºåÂ≠¶‰π†ÂÜô‰ΩúÁöÑÊÄùÁª¥ ‰ªäÂ§©Êôö‰∏äË∑ëÁ®ãÂ∫èÔºåÁúãÁ∫™ÊôìÂ≤öÂíåÂíåÁèÖ„ÄÇ 2020-7-9 ÁßëÁ†îÂÜô‰ΩúÂ•ΩÁóõËã¶ ‰ªäÂ§©Êó©‰∏äÂê¨‰∫ÜÂê¨ÂäõÔºåÂà∑BÁ´ô ‰ªäÂ§©‰∏ãÂçàÂ≠¶‰π†‰∫ÜÂ¶Ç‰ΩïËøõË°åÁßëÁ†îÂÜô‰Ωú„ÄÇÂ§™‰∏çÂÆπÊòì‰∫Ü„ÄÇ„ÄäSome Tips on Writing„Äã„ÄÇÂéüÂõ†Â¶Ç‰∏ãÔºöËØçÊ±áÁº∫‰πèÔºõÁõ¥ËØëÔºõÊØ´Êó†ËÆ≤ÊïÖ‰∫ãÁöÑÊÄùË∑ØÔºõ‰∫§‰ª£‰∏çÊ∏ÖÊô∞ÔºåÂê´Á≥ä„ÄÇÊõ¥Âà´ÊèêÈÄªËæë‰∫ÜÔºåÊàëËßâÂæó‰∏≠ÊñáËÆ∫ÊñáÁöÑÈÄªËæëÂÖ≥Á≥ªÈÉΩÊääÊè°‰∏çÂ•Ω„ÄÇ 2020-7-8 ‰Ω†Ë¶ÅËá™Â∑±Êë∏Áà¨ÊâìÊªöÁöÑÂ∫¶ËøáËâ∞ÈöæÊó∂ÊúüÔºå‰Ω†ÂèØ‰ª•Êâæ‰∫∫Ôºå‰ΩÜÁªù‰∏çËÉΩ‰æùÈù†‰ªª‰Ωï‰∏Ä‰∏™‰∫∫„ÄÇÊàë‰ª¨ÊòØÂ≠§Áã¨ÁöÑ‰∏™‰ΩìÔºåÊõ¥ÊòØÊó†Êï∞ÂÖ≥Á≥ªËøûËæπ‰∏≠ÁöÑ‰∏ÄÊù°„ÄÇË¶ÅÂ≠¶‰ºöÂíåËøô‰∏™‰∏ñÁïå‰∫ßÁîüÂÖ±È∏£„ÄÇ ‰ªäÂ§©Êó©‰∏äÂïÉ‰∫Ü‰∏Ä‰∏™üçé„ÄÇ Âà∑Âà∑Â∞èÁ∫¢‰π¶ÔºåBÁ´ô„ÄÇ ‰∏ãÂçàÂÜô‰∫Ü‰ª£Á†ÅÊ≥®Èáä„ÄÇÂ∞ÅË£ÖÊàêÊé•Âè£„ÄÇËßÑÂàí‰∫Ü‰∏ã‰∏ÄÊ≠•ÊñπÊ°à„ÄÇ Êôö‰∏äËØª‰∫Ü‰∏ÄÁØáData ScienceÁöÑÊñáÁ´†ÔºåÊï∞ÊçÆÂ•ΩÊâçËÉΩËÆ≤Â•ΩÊïÖ‰∫ãÂïä„ÄÇÂºïÂäõÊ®°Âûã„ÄÅÁ∫øÊÄßÂõûÂΩíÊ®°Âûã„ÄÇÂèòÈáèÁöÑÊûÑÈÄ†ÊòØ‰∫ÆÁÇπ„ÄÇÊØèÂ§©ÂùöÊåÅÂÜô‰ΩúÔºàËã±ÊñáÂÜô‰ΩúÂ•ΩÈöæÔºâ 2020-7-7 ÂΩìÂºÄÂêØ**ÔºåÂ∞±ÂÉèÂºÄÂêØ‰∫ÜÈóØÂÖ≥Ê∏∏ÊàèÔºåË¶Å‰∏ÄÂÖ≥‰∏ÄÂÖ≥ÁöÑÊâìÊÄ™ÂçáÁ∫ßÔºåÊç¢ÂÖàËøõË£ÖÂ§á„ÄÇ‰∏çÊàêÂäü‰æøÊàê‰ªÅ„ÄÇ‰∏çÊàêÂäü‰æøÊàê‰ªÅ„ÄÇ‰∏çÊàêÂäü‰æøÊàê‰ªÅ„ÄÇ ‰ªäÂ§©Êó©‰∏äÂèàË¢´Âà∫ÊøÄÂà∞‰∫Ü„ÄÇ ‰∏ãÂçàÂÆûÁé∞‰∫Üxgboost, Êï∞ÊçÆ-&gt;Ê®°Âûã-&gt;Ë∞ÉË∂ÖÂèÇÊï∞ Êôö‰∏äÊî∂Êãæ‰∫ÜËÆ∫ÊñáÔºåÊÑüËßâÂ∞±ÂÉè‰øÑÁΩóÊñØÊñπÂùóÔºåÂ∫ïÂ±ÇÊ≤°ÊúâÂÅöÂ•ΩÔºåË∂äÂæÄ‰∏äÂ†ÜÔºåÂ∞±Ë¶ÅÂõûÂΩíÂ∫ïÂ±Ç„ÄÇÂÜô‰∫Ü1000Â§öÁÇπÁöÑÂ∞èËÆ∫ÊñáÔºå‰πü‰∏çÁü•ÈÅìÊØè‰∏Ä‰∏™‰ºòÁßÄÁöÑÂçöÂ£´Ë¶ÅÁªèÂéÜ‰ªÄ‰πàÊâçËÉΩÂàõÈÄ†Âá∫Ëøô‰πàÂ§öÁöÑÊñáÂ≠ó„ÄÇÂÄºÂæóÂ≠¶‰π†„ÄÇ ÈáäÊîæÂÖâËäíÔºÅ 2020-7-6 ÁªßÁª≠Âä™ÂäõÁöÑ‰∏ÄÂ§© ‰ªäÂ§©Êó©Âê¨‰∫ÜËã±ËØ≠ËÆøË∞à ‰ªäÂ§©‰∏ãÂçàÂ§ç‰π†‰∫ÜÂØÜÁ†ÅÂ≠¶Á¨¨1-2Á´†Ôºà1h); ÊääÈ´òÈìÅÁöÑÊèèËø∞ÊÄßÁªüËÆ°ÂÅö‰∫ÜÔºåÊÑüËßâÁªìÊûúËøòÊòØ‰∏çÈîôÔºÅ(Áî®Êó∂ËæÉÈïø) 2020 28Âë® 2020-7-5 Ë¶ÅÂÅöÁöÑ‰∫ãÊÉÖËøòÂæàÂ§öÔºåÊàëÂä™ÂäõÂÅöÊàëÂñúÊ¨¢ÁöÑ‰∏ÄÂàáÔºåÂπ∂‰∏çÊòØ‰∏∫‰∫ÜËµ¢Âà´‰∫∫ÔºåËÄåÊòØË¶ÅËá™Â∑±Êª°Ë∂≥„ÄÇ ‰ªäÂ§©ÂèàÊòØÁù°ÂæóÊôöËµ∑ÁöÑÊó©ÁöÑ‰∏ÄÂ§©„ÄÇa. ÊàëÊäätrfershÂÆåÂÖ®ÊêûÊáÇ‰∫Ü„ÄÇb. Ë∞àÊï¥ÂêàËµÑÊñôÁöÑÈáçË¶ÅÊÄßÔºå‰∏∫‰ªÄ‰πàÊàëÊÄªÊòØ‰∏Ä‰∏™‰∏ìÈ¢òÁöÑËµÑÊñôÔºåÊØèÊ¨°ÈÉΩË¶ÅÈáçÂ§çÂºÑÂë¢„ÄÇ‰∏çÂ•Ω‰∏çÂ•Ω‰∏çÂ•Ω„ÄÇ Ëøô‰∏ÄÂë®ËøòÊòØ‰∏çÈîôÁöÑÔºåÂú®Á≤æÁ•û‰∏äÔºåÊØèÂ§©Áù°Ë∂≥‰∫Ü10hÔºåÂïäÂìàÂìàÂìàÂìàÂìà„ÄÇÊó∂Èó¥Â∫èÂàóÂü∫Êú¨‰∏äÊêûÈÄö‰∫Ü„ÄÇËØª‰∫ÜÂá†ÁØá‰∏çÈîôÁöÑËÆ∫ÊñáÔºåËøòÊäästataËΩØ‰ª∂Â≠¶‰ºö‰∫ÜÔºåÂÖ∂ÂÆûËÆ°ÈáèÁªèÊµéÂ≠¶Êó†ÈùûÂ∞±ÊòØË¶ÅËß£ÂÜ≥Âõ†ÊûúÂÖ≥Á≥ªÔºåÈÅóÊºèÂèòÈáèÔºåÂ∫èÂàóÁõ∏ÂÖ≥ÊÄßÔºåÂºÇÊñπÂ∑ÆÊÄßÁ≠âÁ≠âÔºåÂá†‰∏™ÈóÆÈ¢òÔºåË¶ÅËß£ÂÜ≥ÊòØ‰∏çÂÆπÊòìÁöÑ„ÄÇsklearnÁî®ÁöÑ‰∏çÁÜüÁªÉ Input ‚úîÔ∏è Output ‚ùå‚úîÔ∏è ËøôÂë®‰∏ªË¶ÅÊääÁªüËÆ°Â≠¶ÈáåÈù¢ÁöÑÂü∫Á°ÄÂ§ç‰π†‰∫ÜÔºåÊÄªÊÑüËßâÊ≤°ÊâæÂà∞ÊàëÊÉ≥Ë¶ÅÁöÑÈÇ£ÁßçÊ∑±Â∫¶„ÄÇÊàë‰πü‰∏çÁü•ÈÅìÊàëÂà∞Â∫ïÈúÄË¶ÅÊÄé‰πàÊ†∑ÁöÑÊ∑±Â∫¶„ÄÇÊÑüËßâËá™Â∑±ÂèàËÉñ‰∫ÜÔºÅ Input ÂØÜÁ†ÅÂ≠¶Â§ç‰π†‰∏§Á´†ÔºàPPT+‰π†È¢ò+ÁôæÂ∫¶Ôºâ ÁΩëÁªúÂÆâÂÖ®Â§ç‰π†‰∏§Á´†ÔºàËßÜÈ¢ë+Á¨îËÆ∞+Ê¶ÇÂøµÔºâ ÂÜô1000Â≠óÂ∑¶Âè≥ÁöÑÁªìËØæËÆ∫ÊñáÔºàÂÖ≥‰∫éÂ§ßÊï∞ÊçÆ‰∏ãÂ≠ïËÇ≤ËÄåÁîüÁöÑËÆ°ÁÆóÁ§æ‰ºöÁªèÊµéÂ≠¶ÔºåÊäÑË¢≠ÊäÑË¢≠ÔºåÂÄüÈâ¥ÂíØÔºâ Â∞ÅË£ÖÁâπÂæÅÊèêÂèñ ËÆ∫ÊñáÊï∞ÊçÆÂàÜÊûê(Áé∞Âú®Â∞±Â∑ÆÂàÜÊûêÁªìÊûú)Ôºà‰∏çÁü•ÈÅìÊàëÊÄé‰πà‰ºöÁªôÈÄ†Êàê‰∏ÄÁßçÔºåÊàëÂæà‰ºöÂÜôËÆ∫ÊñáÁöÑÊ†∑Â≠êÔºåÊÉ≥Â§™Â§ö‰∫ÜÂêß) Áé∞Âú®ËøòÊòØÊï∞ÊçÆÈ©±Âä®Á†îÁ©∂ÔºåÂπ∂ÈùûÁ†îÁ©∂ÈóÆÈ¢òÈ©±Âä®Êï∞ÊçÆÁöÑÈò∂ÊÆµ ÂáÜÂ§áÂÖ≠Á∫ß reading record Output Âü∫Êú¨ÁöÑÊï∞ÊçÆÂàÜÊûêÁªìÊûú ËÆ∫ÊñáÈòÖËØªÔºö‰∏ÄÂÆöË¶Å‰ª•PPTÁöÑÂΩ¢ÂºèÁªôÂá∫ÔºàÊØèÂë®ÁªôËá™Â∑±ÂÅö‰∏™1-3È°µÁöÑPPT) Áúã‰∏ÄÈÅìÂª∫Ê®°È¢ò Â≠¶‰π†Êú∫Âô®Â≠¶‰π†ÈõÜÊàêÁÆóÊ≥ï 2020-7-4 ÊáíÂæóÁöÑÊàë ‰ªäÂ§©Êó©‰∏äÂèàÁù°‰∫ÜÂæà‰πÖÔºÅÂèØ‰ª•ÊòØÊò®Â§©Êôö‰∏äÁúãÂà´‰∫∫ÁöÑvlogÂ§™‰πÖ‰∫ÜÔºåÁæ°ÊÖïÈÇ£ÁßçÁã¨Â±ÖÁîüÊ¥ª„ÄÇÊàë‰ªÄ‰πàÊó∂ÂÄôÂèØ‰ª•Ëøá‰∏äÁã¨Â±ÖÁöÑÊó•Â≠êÂïäÔºÅ ‰ªäÂ§©‰∏ãÂçàÂ≠¶‰∫ÜstataËøô‰∏™Â∑•ÂÖ∑ÔºåÂü∫Êú¨‰∏äÂ≠¶‰ºö‰∫ÜÔºåÊàëÂèëÁé∞Êú∫Âô®Â≠¶‰π†ÂºÄÊ∫êÂ∑•ÂÖ∑ÔºåÊúâ‰∫õ‰∏çËâØÂøÉÂïäÔºÅ ‰ªäÂ§©Êôö‰∏äÁúã‰∫Ü‰ºöÁîµËßÜÂâß„ÄÇË∑üË∏™ÂÖ¨‰ºóÂè∑„ÄÇÂ≠¶‰π†ÁªüËÆ°Â≠¶„ÄÇ 2020-7-3 Áù°È•±ÂñùË∂≥ÔºåËØª‰ºöËÆ∫ÊñáÔºå&amp; Êï≤‰ª£Á†Å ‰ªäÂ§©‰∏ãÂçàÁé©‰∫Üxgboost,ÈõÜÊàêÂ≠¶‰π†ÊúâÁÇπÈöæÔºå‰ΩÜ‰πüË¶ÅÊâãÂä®Êé®ÂØºÔºåËøôÂèØ‰ª•Âä†Ê∑±ÂØπÁÆóÊ≥ïÂÜÖÊ∂µÁöÑÁêÜËß£„ÄÇ Êôö‰∏äÁªüËÆ°Â≠¶‰π†ÔºåstataÂíåËÆ°ÈáèÁªèÊµéÂ≠¶ÔºàÂ≠¶ÁöÑÂæàÂü∫Á°ÄÔºå‰ΩÜÂÆûÈôÖÂ∫îÁî®‰∏çÊòØÈÇ£‰πàÂõû‰∫ã‰∫ÜÔºâ„ÄÇÂÅöÊï∞ÊçÆÁßëÂ≠¶ÔºåËÇØÂÆöË¶ÅÂ≠¶‰π†RËØ≠Ë®Ä Ê≤âÊÄùÔºöüòîüòîüòî„ÄÇ 2020-7-2 Èô∑ÂÖ•Ê∑±Ê∏äÁöÑÊàë ‰ªäÂ§©ÂèàÊòØ‰∏äËØæÔºå‰∏çÁü•ÈÅìËÄÅÂ∏àÊâØ‰∫Ü‰∫õ‰ªÄ‰πàÔºåÂê¨ÁùÄÂê¨ÁùÄÂ∞±ÂÖ≥ÊàêÈùôÈü≥‰∫Ü„ÄÇ(Èò¥Èô©.jpg) ÂÆûÁé∞‰∫ÜÊó∂Èó¥Â∫èÂàóÁöÑÁâπÂæÅÂ∑•Á®ãÔºåËØª‰∫Ü‰∏ÄÁØáÂ•ΩÊñáÁ´†ÔºàÂÖ≥‰∫éÂ•≥ÊÄßÊîøÊ≤ªÂú∞‰ΩçÁöÑÊèêÈ´òÔºâÔºåËøòÊâæÂà∞‰∫Ü‰∏ÄÁØáEPJ data science‰∏äÁöÑÂ•ΩÊñáÁ´†ÔºàËøΩË∏™ÊñáÁåÆÁöÑÈáçË¶ÅÊÄßÔºâ ÊòéÂ§©Ôºö1ÔºâÂÆûÁé∞xgboost 2) Ê®°ÂûãËÆæÁΩÆÔºà‰∏≠ÊñáÔºâÁªºËø∞ÂíåÂèòÈáè 2020-7-1 Êó†ËÅäÁöÑÊàë ‰ªäÂ§©Áé©‰∫ÜtsfreshÂ∫ìÔºåÊÄé‰πàÈÇ£‰πàÈöæÁêÜËß£ÂïäÔºÅÂÆûÂú®ÊòØ‰∏çÁü•ÈÅìÂà´‰∫∫ÊÄé‰πàÁºñÁ®ãÁöÑÔºÅÂÖ≥ÈîÆÊòØËøîÂõûÁöÑÊï∞ÊçÆÁªìÊûÑÔºåÂíåÂ∫ïÂ±ÇÂÆûÁé∞ÊúâÂÖ≥Á≥ªÔºÅÂÜçÊ¨°Â≠¶‰π†‰∫ÜÂèÇÊï∞‰º∞ËÆ°ÔºàÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºåÊúÄÂ§ßÂêéÈ™å‰º∞ËÆ°Ôºâ„ÄÇÊàëÂèØËÉΩË¶ÅÊâãÊääÊâãÊïôÈÇ£Áßç„ÄÇ 2020-6-30 ÂøÉÁ¥ØÁöÑ‰∏ÄÂ§© ‰ªäÂ§©Êó©‰∏äÔºå‰∏ãÂçà‰∏äËØæÔºåÊòØÂº†ËÄÅÂ∏àÁöÑËØæÔºåËØæÁ®ãÈöæÂ∫¶ÂæàÂ§ß„ÄÇÊàë‰πüÊòØÂçäÂê¨ÂçäÁé©ËÄç„ÄÇÂèØËÉΩÊòØÊï∞Â≠¶Â≠¶Â§ö‰∫ÜÔºåÂØπËøô‰∏™Á§æ‰ºöÈóÆÈ¢òÊÑüÊÇüËÉΩÂäõË∑ü‰∏ç‰∏ä„ÄÇÂ∞±‰∏ÄÂè•ËØùÂ≠òÂú®Âç≥ÂêàÁêÜÔºÅÈ°∫ÈÅìÁúã‰∫ÜtsfreshÁöÑdoc Êôö‰∏äÔºåËØª‰∫Ü‰∏ÄÁØáscienceÔºåÂºÄÂàõÊÄßÂ∑•‰ΩúÂ∞±ÊòØÂéâÂÆ≥ÂïäÔºÅ‰ªé0Âà∞1ÊòØÈ£ûË∑ÉÔºå1Âà∞2Ôºå3‚Ä¶ÊòØÂèëÂ±ï„ÄÇËøòÊúâË∑®ÁïåÁöÑÁ≤æËã±„ÄÇÊ∏©‰π†‰∫ÜÊú∫Âô®Â≠¶‰π†ÈáåÈù¢ÁöÑÂü∫Êú¨Ê¶ÇÂøµÔºÅ 2020-6-29 ‰ªäÂ§©Êó©‰∏äÔºå‰∏ãÂçà‰∏äËØæÂéª‰∫Ü„ÄÇËøòÂéªÁúã‰∫ÜÂõûÂΩíÂàÜÊûêsklearnÁöÑÂÆûÁé∞„ÄÇ ÂÖ≥ÈîÆÁÇπÁõ∏ÂÖ≥ÊÄßÂàÜÊûê-&gt;Âõ†ÊûúÂàÜÊûêÔºå correlation Ôºç causality - prediction - control„ÄÇËß£ÈáäÂûãÁ†îÁ©∂ÔºàÊèèËø∞ÔºåÁªüËÆ°ÊñπÊ≥ï)‚Äî‚Äî&gt;ÂÖÖÂàÜËß£ÈáäÔºàÂõ†ÊûúÂÖ≥Á≥ªÔºâÁßëÂ≠¶Á†îÁ©∂Ôºö Ëß£ÈáäÔºåÈ¢ÑÊµãÔºåÊéßÂà∂Èó≠ÁéØÔºàÂπ≤È¢ÑÔºâÔºåÊâæ‰∏çÂà∞Âõ†ÊûúÂÖ≥Á≥ª„ÄÇ Âõ†ÊûúÂÖ≥Á≥ªÂàÜÊûêÊñπÊ≥ïÔºöCausality: Models, Reasoning, and Inference Êôö‰∏äÂ§ç‰π†ÁªüËÆ°Â≠¶day Ox 01ÔºàÊú¨ÁßëÊàëÊÄé‰πàÊ≤°Êúâ‰ΩúÁîµÂ≠êÁ®øÁ¨îËÆ∞ÂïäÔºåÂì≠Ê≠ª‰∫ÜÔºåËøòÂéªÂèçÁøªÁúã‰∫ÜËøáÂéªÁöÑÁ¨îËÆ∞ÂíåÂçöÂÆ¢ÔºåËèúÊ≠ª‰∫ÜÔºâ„ÄÇÂä†‰∏™Â≠¶‰π†Â∞èÁªÑÔºåÊÑüËßâÂà´‰∫∫ÂÅöÁöÑËµÑÊñôÔºåÂíåÊàëÊÉ≥Ë¶ÅÁöÑÊ∑±Â∫¶Â∑ÆÂ§™Ëøú‰∫Ü„ÄÇÊï∞ÊçÆÊåñÊéò‰∏äÊ¨°ÁúãÈÇ£‰∏™Dr.yuanÁöÑËØæÁ®ãÔºåÁ¨îËÆ∞‰∏çÊòØÁâπÂà´Â•ΩÔºÅËøòË¶ÅÊääÊó∂Èó¥Â∫èÂàóÁâπÂæÅÂ∑•Á®ãÂÅö‰∫ÜÔºÅ ÊàëÂèëÁé∞Áî≥ËØ∑ÂÆûË∑µÂ≠¶ÂàÜÔºåÂÅöÂä©ÊïôÔºåÊÄé‰πàÈÇ£‰πàÂ§ö‰∫∫Áî≥ËØ∑ÂïäÔºÅ ÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ïÈÇ£‰∏™ÔºåÊàëÂèëÁé∞ÊàëÂÅöÁöÑÁ¨îËÆ∞ÔºåÂ•ΩÂ∑ÆÂä≤ÂïäÔºÅ 2020-6-28 ÂÖÉÊ∞îÂ∞ëÂ•≥ÁöÑÁã¨ÁôΩ‚Äî ‰∏™‰∫∫‰πãÊóÖ 2020 27Âë®ÔºöË∫¨Ë°åÂÆûË∑µ INPUT a. Linear Models &amp; Python Â≠¶‰ºö https://scikit-learn.org/stable/supervised_learning.html#supervised-learning b. Time series &amp; Feature select tfresh c. English: youtube &amp; shanbei e. reading record f. Ê¶ÇÁéáËÆ∫ÔºàÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°ÔºåÂÅáËÆæÊ£ÄÈ™åÔºåÊòæËëóÊÄßÊ£ÄÊµãÔºåÂèÇÊï∞‰º∞ËÆ°Ôºâ OUTPUT a. kaggle project b. ÊéåÊè°‰∏Ä‰∏™‰∏≠È´òÈöæÂ∫¶ÁöÑÊï∞Â≠¶Ê®°Âûã c. Reading Record ‰ªäÂ§©Êó©‰∏äÁù°ËßâÂï¶ÔºÅ ‰∏ãÂçàÂíåÁîµËÑë‰∫∫‰∏ÄËµ∑Áé©È∫ªÂ∞ÜÔºåÊî∂Êãæ‰∫ÜÊú¨Âë®ÁöÑËµÑÊñôÊ±áÊÄª Êôö‰∏äÁúã‰∫ÜËßÜÈ¢ëÂ≠¶‰π†ÂÖ≥‰∫éxgboost,ÂíåÂÖ≥‰∫é‰∏Ä‰∫õÂÖ¨‰ºóÂè∑ÁöÑÊñáÁ´† F: ‚Äã ËæìÂá∫Ôºà‚úîÔ∏èÔºâÔºõ ËÆ∫ÊñáÈòÖËØªÔºà‚úîÔ∏èÔºâÔºõËØª‰π¶Ôºà‚ùåÔºå‰∏ªË¶ÅÊòØÊ≤°ÊúâÈÇ£ÁßçÂøÉÂ¢ÉÔºå‰∏ç‰∏äÁòæÔºâÔºõËã±ËØ≠Ôºà‚úîÔ∏èÔºâ S: ËÆ∫ËµÑÊñôÁöÑÊî∂ÈõÜÁöÑÈáçË¶ÅÊÄß„ÄÇÁü•ËØÜÂú®ËÑëÊµ∑Èáå‰ºöÂøòËÆ∞Ôºå‰ΩÜÁ°¨Áõò‰∏ç‰ºö„ÄÇ‰∏ÄÂÆöË¶ÅÊääËá™Â∑±ÈÅáÂà∞ÁöÑÈ°∂Á∫ßËµÑÊñôÊî∂ÈõÜÂ•ΩÔºåÂéòÊàêdoc„ÄÇ‰∏ÄÂÆöÊòØÈ´òË¥®ÈáèÂíåÈ´òÂØÜÂ∫¶ÁöÑËµÑÊñôÊâçËÆ∞ÂΩï„ÄÇÂéüÊù•‰∏ÄÁõ¥Ê≥®ÈáçËµÑÊñôÁöÑÂ≠¶‰π†ÔºåÂøΩÁï•‰∫ÜËµÑÊñôÊï¥ÂêàÁöÑÈáçË¶ÅÊÄß„ÄÇÂπ∂‰∏îÊúÄÂ•ΩÊØèÊÆµÊó∂Èó¥ÔºåÂÜô‰∏ÄÁØáÂ§ßÊ±áÊÄª„ÄÇ ÂàáËÆ∞‰ªéÂ§¥ÂºÄÂßã„ÄÇÂ≠¶‰π†ÂõûË∑ØÔºöËµÑÊñôÊî∂ÈõÜÔºàÂàÜÁ≠âÁ∫ßÔºöÂÖ®Â±ÄËßÇ‚Äî&gt;ÂÖ•Èó®-ËøõÈò∂-È´òÁ∫ßÔºåÁßëÊôÆ)‚Äî‚Äî&gt; ÊääËá™Â∑±ÁöÑËæìÂá∫ÊèêÂçáÂà∞Âà´‰∫∫‰πüÂèØ‰ª•Áõ¥Êé•Áî®ÁöÑÊ∞¥Âπ≥„ÄÇ‰∏çÈîôÔºåÊòØËøô‰πàÂõû‰∫ãÔºÅËøô‰πüÊòØÂØπËá™Â∑±ÁöÑ‰∏ÄÁßçË¶ÅÊ±Ç„ÄÇ Âø´‰πêÂ§πÊùÇÁùÄÊÇ≤‰º§ÔºåËá≥Â∞ëÁªìÂ∞æ‰∏çËÉΩËÆ©Ëá™Â∑±Â§™ÈöæÂ†™ÔºÅËøôÊù°Ë∑ØÂè™‰ºö‰∏éËá™Â∑±Áõ∏ÂÖ≥„ÄÇ 2020-6-27 Â§ñÂÖ¨ÂÖ´ÂçÅÂ≤ÅÁîüÊó• ÁôΩÂ§©Âú®Â§ñÂ©ÜÂÆ∂ÔºåÁªôÊïô‰π¶ÂÖàÁîüÂ§ñÂÖ¨ËøáÁîüÊó•„ÄÇ Êôö‰∏äÔºåÂê¨‰∫Ü‰∏ÄÂú∫ËÆ≤Â∫ßÔºåÂÖ≥‰∫é‚ÄúÂΩì‰∫§ÈÄöÈÅá‰∏äÊú∫Âô®Â≠¶‰π†‚Äù,ÊòØÂåó‰∫§ÂâØÊïôÊéà‰∏áÊÄÄÂÆáÂçöÂ£´ËÆ≤ÊéàÁöÑ‰∏ìÈ¢ò„ÄÇ‰∏ªÈ°µÔºöhttp://faculty.bjtu.edu.cn/8793/„ÄÇÂÖ®ÂΩìÊòØÁßëÊôÆ‰∫ÜÔºå‰∏çËøáÔºåÂÜÖÂÆπÊå∫Ê∑±Â••ÁöÑ„ÄÇ ‰Ω†Ê∞∏ËøúÊó†Ê≥ïÂè´ÈÜí‰∏Ä‰∏™Ë£ÖÁù°ÁöÑ‰∫∫ ÂΩì‰Ω†ÁúüÂøÉÊ∏¥ÊúõÊüêÊ†∑‰∏úË•øÊó∂ÔºåÊï¥‰∏™ÂÆáÂÆôÈÉΩ‰ºöËÅîÂêàËµ∑Êù•Â∏ÆÂä©‰Ω†ÂÆåÊàêÔºÅ Â≠¶Âà∞‰∫Ü‰∏§Âè•ËØùÔºåÊàë‰∏çÁü•ÈÅìÈÄâÂì™‰∏™ÔºÅ ÂÖ≥ÈîÆËØç: Á°¨Ê†∏ 2020-6-26 ‰ªäÂ§©Êó©‰∏äÔºåÁù°Âà∞‰∫Ü11Ôºö00 ‰∏ãÂçàÔºåÊü•ÈáçÔºåÂøÉÁ¥ØÔºÅÊó∂Èó¥Â∫èÂàóÔºÅ Êôö‰∏äÔºåËØª‰∫Ü‰∏ÄÁØáÊó∂Èó¥Â∫èÂàóÂú®È¢ÑÊµãËÉΩÊ∫êÊ∂àË¥πÈáåÈù¢ÁöÑÂ§ßÁªºËø∞„ÄÇÂíåÊú¨ÁßëÁöÑÁªèÂéÜÂ•ëÂêà„ÄÇÈáçÁÇπÁúã‰∫ÜÊúÄÂ∞è‰∫å‰πòÊîØÊåÅÂêëÈáèÊú∫ÁöÑÂ∫îÁî®ÔºåÁ†îÁ©∂ÁªìÊûúÊãüÂêàÁ≤æÂ∫¶ÂíåÈ¢ÑÊµãÁ≤æÂ∫¶ËøòË°å„ÄÇËøô‰∫õÊ®°ÂûãÔºåÊàëËøòÊå∫ÁÜüÊÇâÁöÑ„ÄÇ 2020-6-25 Á´ØÂçàÂø´‰πê,biubiubiu Êó©‰∏äÔºåÁù°Âà∞‰∫Ü‰∏≠ÂçàÔºåÊÑüËßâÂ§™ÁÉ≠‰∫ÜÔºå‰∏çÊÉ≥Êó©Ëµ∑„ÄÇ ‰∏≠ÂçàÔºåÊàëÂíåÂºüÂºüÂéªË°ó‰∏äÂêÉÈ•≠‰∫ÜÔºöË±ÜËÖê„ÄÅÊ∞¥È•∫„ÄÅÊ∞¥ÁÖÆËÇâÁâá„ÄÇÊÑüËßâÊàëË¶ÅÊòØÊÑøÊÑèÂÅö‰∏ÄÊ¨°È•≠ÔºåÈÇ£ÁÆÄÁõ¥ÊòØÂ§™Èò≥‰ªéË•øÊñπÂá∫Êù•‰∫Ü„ÄÇÂ¶àÂ¶àÊûúÁÑ∂ÊòØ‰∏ñÁïå‰∏äÊúÄËæõËã¶ÁöÑ‰∫∫‰∫Ü„ÄÇ ‰∏ãÂçàÔºå‰∫§Êé•‰∏Ä‰∏ãÊó∂Èó¥Â∫èÂàóÈ°πÁõÆÔºåËØùËØ¥Ëøô‰∏™‰∏úË•ø‰∏çÊòØÂæàÁÅ´ÂêóÔºå‰∏∫‰ªÄ‰πàËµÑÊñôËøô‰πàÂ∞ëÂïäÔºÅÔºÅÔºÅË∑ë‰∫Ü‰∏ÄÂ§©‰∏ÄÂ§úARIMA‰∫Ü„ÄÇÂ∏ÆÊàëË°®ÂºüÂÜô‰∏ÄÁØá‰∏≠Â∞è‰ºÅ‰∏öÂèëÂ±ïÁé∞Áä∂ÁöÑËÆ∫ÊñáÔºåÊâæ‰∫ÜÂá†ÁØáÊñáÁ´†ÊäÑË¢≠ÔºåÁúã‰∫ÜÂà´‰∫∫ÁöÑÁ°ïÂ£´Â≠¶‰ΩçËÆ∫ÊñáÔºåÊàëÊâçÊÑèËØÜÂà∞ÊàëËøòÊòØÊú¨ÁßëËÆ∫ÊñáÁöÑÊ∞¥Âπ≥„ÄÇ Êôö‰∏äÔºåÁ†îËØªËÆ∫Êñá„ÄÇ 2020-6-24 7hÁù°Áú†‰∏çË∂≥ÁöÑÊàë.jpg ‰ªäÂ§©Êó©‰∏äËø∑Ëø∑Á≥äÁ≥äËµ∑Â∫äÔºåÂíåÂêåÂ≠¶Âî†ÂóëÔºåÊò®Â§©Áù°ÁöÑÂ•ΩÁé©Ôºå1Ôºö30ÔºåÂê¨ËØ¥Â¢®Ë•øÂì•Âú∞Èúá‰∫ÜÔºåÁÑ∂ÂêéÊàëÂèë‰∫ÜÊ∂àÊÅØÔºåÂ±ÖÁÑ∂ÂøÉÂÆâÁêÜÂæóÁöÑÁù°ÁùÄ‰∫ÜÔºÅ ‰∏ãÂçàÔºåÊùâÊùâËØ¥ÂèØ‰ª•ÂÅ∑ÂÅ∑Êë∏Êë∏ÂõûÂéªÔºåÊàëÂøÉÈáåÊöóËá™‰∏ÄÊÉ≥ÔºåÂú®ÂÆ∂Èáå‰πüÂæàÂ•ΩÂïäÔºåÁù°-ÂêÉ-ÁîµËÑë-ÂêÉ-Áù°-ÈÜíÁöÑÊó†Á∫øÊ≠ªÂæ™ÁéØ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇÁúã‰∫ÜÊú¨ÁßëÁöÑÂ≠¶ÂºüÂ≠¶Â¶πÊØï‰∏ö‰∫ÜÔºåÊÑüÂèπÊó∂ÂÖâÂïäÔºåÊàëËøô‰∏ÄÂπ¥Â≠¶‰∫Ü‰ªÄ‰πàÔºåÊó∂Èó¥ÈÉΩÂéªÂì™‰∫Ü‰∫ÜÔºüÊó∂Èó¥ÈÉΩÂéªÈÇ£ÂÑø‰∫ÜÔºüÊàëËßâÂæóËá™Â∑±‰∏ÄÂπ¥‰ªÄ‰πàÊàêÂäüÈÉΩÊ≤°ÊúâÔºåËÆ∫ÊñáË¢´ÊãíÁ®øÔºåË¢´Âõ∞ÂÆ∂‰∏≠ÂçäÂπ¥‰∫ÜÂêßÔºåÊÑüËßâÊú¨ÁßëÂõõÂπ¥ÈÉΩÊ≤°ÊúâÂú®ÂÆ∂ÂëÜÈÇ£‰πàÈïøÁöÑÊó∂Èó¥„ÄÇËøô‰∏çÊòØÊú¨ÁßëÁöÑÊâÄÊúâÊó∂Èó¥ÔºåÂÖ®ÈÉ®ÂõûÂéª‰∫ÜÂêóÔºü Êôö‰∏äÔºå‰∫ÜËß£Êó∂Èó¥Â∫èÂàóÁöÑÁâπÂæÅÊèêÂèñÔºå‰ªÄ‰πàÈ¨ºÔºåÊÄé‰πàÈÇ£‰πàÈöæÂïäÔºÅÊôö‰∏äÁúã‰∫ÜÂ§ßÊï∞ÊçÆÊú∫Âô®Â≠¶‰π†Êó∂‰ª£ÔºåÁßëÊôÆ„ÄÇ Á™ÅÁÑ∂ÊÉ≥Âà∞ËøòÊúâÂ•ΩÂ§öÂ§ç‰π†ÁöÑËØæÁ®ãË¶ÅËÄÉËØïÂïäÔºåÁ™ÅÁÑ∂ÊÉ≥Âà∞‰∫Ü„ÄÇ Áúã‰∫ÜÂæà‰πÖÂæà‰πÖÁöÑËØ¥ËØ¥‰∫ÜÔºåÊàëÂèëÁé∞Â§ßÂ≠¶ÊàëÈîôËøá‰∫ÜÂæàÂ§öÁöÑÁæéÂ•ΩÔºå ÊÄÄÂøµÂ§ßÂ≠¶ÁöÑÊó∂ÂÖâÔºåÊó†ÊãòÊó†ÊùüÔºåÂèØ‰ª•Ëµ∑ÂæàÊó©ÂéªÂÆûÈ™åÂÆ§ÔºåÂèØ‰ª•ËÆ§ËÆ§ÁúüÁúüÁöÑ‰∏äËØæÔºåÂèØ‰ª•ÂíåÂêåÂ≠¶‰∏ÄËµ∑Âª∫Ê®°ÂíåÁÜ¨Â§úÈÄöÂÆµÂÅö‰Ωú‰∏öÔºåÂ∞§ÂÖ∂ÊòØÂíåÁÅØÂ§ú‰∏ÄËµ∑Â§ßÊôö‰∏äÂæÖÂú®ÂÆûÈ™åÂÆ§ÁöÑÊó•Â≠ê„ÄÇ Probit vs logit https://www.econometrics-with-r.org/11-2-palr.html 2020-6-23 ÊàëË¶ÅÊîæÈïøÂÅá.mp4 ‰ªäÂ§©Áù°‰∫Ü10hÂ§ö„ÄÇÊôö‰∏äÂçÅ‰∏ÄÁÇπÁù°Âà∞Êó©‰∏ä11.00Ôºå‰∏≠ÈÄîËøòËø∑Ëø∑Á≥äÁ≥äÁöÑÂú®Á†îÁ©∂ÁîüÁ≥ªÁªüÊâìÂç°ÔºÅÔºÅÔºÅÔºÅÂ∑≤ÁªèÊòØËá™ÁÑ∂Áä∂ÊÄÅ‰∫ÜÂïäÔºÅ ‰∏ãÂçàÔºöÁªßÁª≠Êó∂Èó¥Â∫èÂàóÔºåÁêÜËÆ∫ÂÆåÂÖ®‰∏çÊáÇ„ÄÇËøôÊòØÊÄé‰πàÂõû‰∫ã„ÄÇ Êôö‰∏äÔºöÂºÄÂºÄÂøÉÂøÉÂ§ç‰π†ÂõõÂ∑ùÂ§ßÂ≠¶ÁöÑÊ¶ÇÁéáËÆ∫„ÄÇÂ•Ω‰πÖÊ≤°ÊúâÊé•Ëß¶Êú∫Âô®Â≠¶‰π†‰∫ÜÔºåÂ∑Æ‰∏çÂ§öËøòÁªô‰π¶Êú¨‰∫Ü„ÄÇ Êú∫Âô®Â≠¶‰π†ÁÆóÊ≥ï+Ê¶ÇÁéáËÆ∫ÁªüËÆ°Â≠¶ 2020-6-22 Â¶ÇÊûúÂèØ‰ª•‰∏ÄÁõ¥‰∏ÄÁõ¥Áù°‰∏ãÂéªÂ§öÂ•ΩÂïä.eps ‰ªäÂ§©Áù°‰∫Ü11Â§ö‰∏™Â∞èÊó∂ÔºåÂ§™Âπ∏Á¶è‰∫ÜÂïäÔºÅ ‰∏ãÂçàÁé©‰∫Ü‰ºöÊó∂Èó¥Â∫èÂàó„ÄÇÂíåÁÅØÂ§úËÅä‰∫ÜÂæà‰πÖÁöÑÂ§©ÔºåÂ∞±ÊòØÂÖ≥‰∫éÂÖ¥Ë∂£Âíå‰∫∫ÁîüËøΩÊ±Ç„ÄÇ 2020-6-21 ‰ªäÂ§©‰∏äË°óÂéª‰∫ÜÔºå‰π∞‰∫Ü‰∏Ä‰∏™Â§ßÂ§ßÁöÑÂÜ∞Ê∑áÊ∑ã„ÄÇ ËæìÂá∫ ÂÆåÊàê‰∏ÄÁØáÂçöÂÆ¢ Êé®ËøõÊó∂Èó¥Â∫èÂàóÈ°πÁõÆÔºàËØªÁõ∏ÂÖ≥ËÆ∫ÊñáÔºâ ËÆ∫ÊñáÈòÖËØª ‰ªäÂ§©Êó©‰∏äÂíå‰∏ãÂçàÁù°‰∫ÜÂ•Ω‰πÖÂïäÔºÅ ‰∏ãÂë®ËßÑÂàíÔºö Êó∂Èó¥Â∫èÂàóÈ°πÁõÆÔºà‰∏ªÁ∫ø1ÔºâÔºöÂÖ∑‰ΩìÊääÁõ∏ÂÖ≥ÁöÑÊñπÊ≥ïÊï¥ÁêÜÊàêÁ¨îËÆ∞ ÁªüËÆ°Â≠¶Ôºà‰∏ªÁ∫ø2ÔºâÔºöÁªüËÆ°Â≠¶ËØæÁ®ãÊØèÂ§©Â≠¶‰∏ÄÂ≠¶ÔºåÂÅöÁªÉ‰π†„ÄÇ mooc: https://www.icourse163.org/learn/NJUE-1001752031?tid=1206103246&amp;from=study#/learn/content ÁºñÁ®ãÔºöSQL„ÄÅExcel„ÄÅPythonÁöÑÁ≤æÈ´ìÁî®Ê≥ïÔºåÊú¨ÁùÄÊèêÈ´òÊïàÁéáÂéªÁöÑ„ÄÇ ÁúãÂâß „ÄäÁôΩÁÆ±„Äã Áúã‰∏§ÁØáÊú∫Âô®Â≠¶‰π†ÂõûÂΩíÂàÜÊûêÁöÑËÆ∫Êñá ÁªüËÆ°Â≠¶‰π†Ë°•‰∏ä ‰π¶Á±çÈòÖËØªÔºö ‚Äã „ÄäÂ§ßÁß¶Â∏ùÂõΩ„Äã ‚Äã „Ää‰∫∫ÊÄßÁöÑÂº±ÁÇπ„Äã ÊØèÂ§©2hÁöÑËã±ËØ≠Â≠¶‰π†ÔºåËøáÂÖ≠Á∫ßÂïä ÂèçÊÄùÔºö ‚Äã Êó•Âá∫ËÄå‰ΩúÔºåÊó•ËêΩËÄåÊÅØ„ÄÇ 2020-6-20 ÂøÉÂ°û ‰ªäÂ§©ËÇöÂ≠ê‰∏çËàíÊúçÔºå‰ªÄ‰πàÈÉΩ‰∏çÊÉ≥Âπ≤‰∫Ü„ÄÇÂ•ΩÁóõÔºåÂ•ΩÁóõÔºåÂ•ΩÁóõÔºÅÔºÅÔºÅÔºÅ ‰∏ãÂçàÂ≠¶‰∫ÜpythonÊäÄÂ∑ßÁªòÂà∂ÂêÑÁ±ªbar,barh 2020-06-19 Êàë‰∏éÊò•È£éÁöÜËøáÂÆ¢Ôºå‰Ω†Êê∫ÁßãÊ∞¥ÊèΩÊòüÊ≤≥ÔºõÊÆäÈÄîÂêåÂΩíÊòØÂÅ∂ÁÑ∂ÔºåËÉåÈÅìËÄåÈ©∞ÊòØÂ∏∏ÊÄÅ„ÄÇ ‰∏ãÂë®ËßÑÂàíÔºö Êó∂Èó¥Â∫èÂàóÈ°πÁõÆÔºà‰∏ªÁ∫ø1ÔºâÔºöÂÖ∑‰ΩìÊääÁõ∏ÂÖ≥ÁöÑÊñπÊ≥ïÊï¥ÁêÜÊàêÁ¨îËÆ∞ ÁªüËÆ°Â≠¶Ôºà‰∏ªÁ∫ø2ÔºâÔºöÁªüËÆ°Â≠¶ËØæÁ®ãÊØèÂ§©Â≠¶‰∏ÄÂ≠¶ÔºåÂÅöÁªÉ‰π†„ÄÇ ÁºñÁ®ãÔºöSQL„ÄÅExcel„ÄÅPythonÁöÑÁ≤æÈ´ìÁî®Ê≥ïÔºåÊú¨ÁùÄÊèêÈ´òÊïàÁéáÂéªÁöÑ„ÄÇ ‰π¶Á±çÈòÖËØªÔºö ‚Äã „ÄäÂ§ßÁß¶Â∏ùÂõΩ„Äã ‚Äã „Ää‰∫∫ÊÄßÁöÑÂº±ÁÇπ„Äã Êó©‰∏äËµ∑ÁöÑÊôöÔºåËøòË∑üÂØºÂ∏àÊâØ‰∫ÜÁöÆÔºå‰Ωú‰∏∫‰∏Ä‰∏™‰ªéÂ∞èÂ∞±ÈÄÉÈÅøËÄÅÂ∏àÁöÑ‰∫∫ÔºåÂ±ÖÁÑ∂ËøòÂéªÊâìÊâ∞Â§ßÂøô‰∫∫ÁöÑÊó∂Èó¥ÔºåÁΩ™ËøáÂïä„ÄÇ ‰∏ãÂçàÔºöÊêû‰∫Ü‰ºöÁßëÁ†îÔºåÊï∞ÊçÆ‰∏çÁªôÂäõÂïä„ÄÇÂìéÔºåÂÜÖÂøÉÂ•îÊ∫É‰∫Ü„ÄÇ Êôö‰∏äÔºöÊÅ∂Ë°•pythonicÂíåÁªüËÆ°Â≠¶ https://mp.weixin.qq.com/s?__biz=MzI1MzAwODMyMQ==&amp;mid=2650338461&amp;idx=1&amp;sn=be67a2565cf5f0922e84e5076fe1c0d2&amp;chksm=f1d75433c6a0dd25fa6e25fae624cb21738b0fc4b28f833db0844f8bbb6e02c1dc9dfddac03a&amp;scene=0&amp;xtrack=1#rd 2020-06-18 üò¥ ÂøÉ‰∏çÂú®ÁÑâ „ÄÇÂÖ®Ê†à„ÄÇ ‰ªäÂ§©ÂèëÁé∞Ëá™Â∑±Á¶ªÂÖ®Ê†àÂú®Á®ãÂ∫¶Âú®Èôç‰Ωé„ÄÇ ‚Äã 1. ‰ΩúÂõæ„ÄÅÂÜô‰Ωú„ÄÅÁúãÊñáÁåÆ„ÄÅÊÉ≥ÁßëÁ†îÊÉ≥Ê≥ï„ÄÅÁºñÁ®ãÂá†‰πéÈÉΩÂèØ‰ª•ÂçäÁã¨Á´ã„ÄÇ‰ΩÜÊòØÊàëÁ™ÅÁ†¥‰∏ç‰∫ÜÔºåÂ∞±ÊòØÂæÄÈ´òË¥®ÈáèÂú®ÊúüÂàäÂèëÔºåÊÑüËßâÂ∑≤ÁªèËææÂà∞‰∫ÜËá™Â∑±ËÉΩÂäõÁöÑÈ°∂Â≥∞‰∫Ü„ÄÇÂú®ÁªßÁª≠ÔºåÂèØËÉΩÂ∞±ÊòØÊï∞Â≠¶Â±ÇÈù¢„ÄÇË¶ÅÊèêÂçáÂïä Ëá™Â∑±Â≠¶‰∫ÜÂæàÂ§ö‰∏úË•øÔºå‰ΩÜÊòØÂπ∂Ê≤°ÊúâÁã¨ÁâπÁöÑ‰ºòÂäøÂú®ÔºåÈóÆÈ¢òÂ∞±ÊòØ‰∏ç‰∏ì‰∏öÔºå‰∏çÈÄèÂΩª„ÄÇË¶ÅÊääËá™Â∑±ÂñúÊ¨¢Âú®ÁöÑÊ∂âÂèäÁöÑÁü•ËØÜ„ÄÅÁºñÁ®ãÈÉΩË¶ÅÂ≠¶Âà∞‰ΩçÔºåËøôÂæàÂÖ≥ÈîÆ„ÄÇ ‰ªäÂ§©ÂíåËêåËêåÁªÑ‰∏™Â≠¶‰π†ÈòüÔºåÂä†Ê≤πÔºåËÄÉÁ†îÂä†Ê≤πÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅ Â∞±ÊòØÂèÇËÄÉÊñáÁåÆÁöÑÊèíÂÖ•„ÄÇ 2020-06-17 Áé©ÁöÑfun ÊòéÂ§©Ôºö Â¶ÇÊûú‰∏çÂá∫ÊÑèÂ§ñÔºåÂ∞ΩÈáèÁù°Âà∞‰πùÁÇπÔºÅÁúã‰ºöÁîµËßÜÂâßÔºÅ ÁªßÁª≠Ë∑ëÁ®ãÂ∫è Â≠¶‰π†ÁªüËÆ°Â≠¶ ‰ªäÂ§©Êó©‰∏äËµ∑ÁöÑÂæàÊôöÔºåÁù°‰∫ÜÂæà‰πÖÔºÅÊò®Â§©Áúã‰∫ÜÂ•Ω‰πÖÁöÑÁîµËßÜÂâßÔºåÂà∑bÁ´ô„ÄÇÁúã‰∫ÜÂÖ≥‰∫é‰∫§ÈÄöÊ≤ªÁêÜ‰∏éÁñ´ÊÉÖÈò≤ÊéßÁöÑÊñáÁ´†„ÄÇÁâπÂà´Â≠¶Âà∞‰∫ÜÊñ∞‰∏úË•øÔºåÂ∞±ÊòØÁü©ÈòµÁöÑÂ•áÂºÇÂÄºÂàÜËß£ÔºåÊâÄ‰∫ßÁîüÁöÑÁé∞ÂÆûÊÑè‰πâÔºåÂèØÈÄöËøáÈôçÁª¥ÔºåËé∑ÂèñÂÖ≥ÈîÆÊÄß‰ø°ÊÅØÔºåÂèØÂæóÁªìÊûÑ‰ø°ÊÅØ„ÄÇhttps://mp.weixin.qq.com/s?__biz=MjM5MTM5NDAzNA==&amp;mid=2651320319&amp;idx=1&amp;sn=6a0e27ff1e5b9d0f3cc1d4ac5fff2ef0&amp;scene=19#wechat_redirect ‰∏ãÂçàÊêû‰∫Ü‰ºöÊï∞ÊçÆ Êôö‰∏äÔºö ËßÑÂàí‰∏Ä‰ºöÁªüËÆ°Â≠¶Â≠¶‰π† Áúã‰∫Ü‰∏ãËá™Â∑±ÁöÑÂ°ëË∫´ËÆ°ÂàíÔºåÁ∫†Ê≠£‰ΩìÂûãÔºåÊÑüËßâËøòÂ∑ÆÂ•ΩÂ§öÂïä Áúã‰∏ãËá™Â∑±ÁöÑÊäÄËÉΩÊ†ë https://github.com/xiemaycherry/picture/blob/master/%E6%88%91%E7%9A%84%E6%8A%80%E8%83%BD%E6%A0%91.png 2020-06-16 ‰ªäÂ§©Êó©‰∏äÊ≤âÊÄù‰∫ÜËá™Â∑±ÊúÄËøëÁöÑË°å‰∏∫„ÄÇÈáçÂª∫‰∫ÜËá™Â∑±ÁöÑÂêÑÁßçË°å‰∏∫ÔºåÊàëÂèØËÉΩËÑëÂ≠êÈáåÈù¢Â∞ë‰∫Ü‰∏ÄÊ†πÁ≠ã„ÄÇÂìàÂìàÂìàÂìàÂìàÂìàÂìàÂìàÂìà ‰∏ãÂçàÁªßÁª≠Êê¨Á†ñ„ÄÇÊó©ÁÇπÂäûÂÆåÂêß„ÄÇÂ∏åÊúõÊØè‰∏ÄÊÆµÊ≤âÊΩúÁöÑÊó∂ÂÖâÈÉΩÂèØ‰ª•Èó™Èó™ÂèëÂÖâ„ÄÇËôΩÁÑ∂Âú®Âà´‰∫∫ÁúãÊù•Ê≤°Êúâ‰ªÄ‰πàÊÑèÊÄùÔºå‰ΩÜÊòØÊàë‰æùÁÑ∂Ë¶ÅÂä™Âäõ!ÂèëÁé∞Ëá™Â∑±ÁâπÂà´Ê≤°ÊúâËÑëÂ≠êÂïäÔºÅÂèëËßâËá™Â∑±Â§™Á¨®‰∫ÜÂêßÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅ ‰∏ãÂçàÂíåÁà∑Áà∑ËÅäÂ§©‰∫ÜÔºå Êôö‰∏äÂ≠¶‰π†‰∫ÜÊñ∞Áü•ËØÜ ÁªßÁª≠Â≠¶‰π†ÁªüËÆ°Â≠¶ ÈáéÂ§ñ‰∏ÄÊ∏∏ Âè§ËØóËØç ‚Äã ÊÑøÊàëÂ¶ÇÊòüÂêõÂ¶ÇÊúàÔºåÂ§úÂ§úÊµÅÂÖâÁõ∏ÁöéÊ¥Å„ÄÇ ‚Äã ‰∫ëÊÉ≥Ë°£Ë£≥Ëä±ÊÉ≥ÂÆπÔºåÊò•È£éÊãÇÊßõÈú≤ÂçéÊµì„ÄÇ ‚Äã ÊòîÊàëÂæÄÁü£ÔºåÊù®Êü≥‰æù‰æù„ÄÇ‰ªäÊàëÊù•ÊÄùÔºåÈõ®Èõ™ÈúèÈúè„ÄÇ ‚Äã ÊàëÊñ≠‰∏çÊÄùÈáèÔºå‰Ω†Ëé´ÊÄùÈáèÊàë„ÄÇ ‚Äã ËêΩÁ∫¢‰∏çÊòØÊó†ÊÉÖÁâ©ÔºåÂåñ‰ΩúÊò•Ê≥•Êõ¥Êä§Ëä±„ÄÇ ‚Äã ÊõæÁªèÊ≤ßÊµ∑Èöæ‰∏∫Ê∞¥,Èô§Âç¥Â∑´Â±±‰∏çÊòØ‰∫ë„ÄÇ ‚Äã ÁßãÈ£éÁîüÊ∏≠Ê∞¥ÔºåËêΩÂè∂Êª°ÈïøÂÆâ„ÄÇ 2020-6-15 ‰∫åÊ¨°ÂÖÉÂ∞ëÂ•≥ÁöÑÂæÆÁ¨ë.jpg ÂìéÔºåÊàëËøôÊòØÊÄé‰πà‰∫Ü„ÄÇËÄÅÊòØËØç‰∏çËææÊÑèÂïäÔºÅ Â∏åÊúõÂ§ßÂÆ∂ÁêÜËß£Êàë„ÄÇÊàëÊòØ‰∏Ä‰∏™Âæà‰∏çÂêàÁæ§ÁöÑ‰∫∫ÔºåËøòÊúâÁÇπÂÅèÊøÄÁöÑ‰∫∫„ÄÇ‰ΩÜ‰∏çÂàÜÂú∫ÂêàÁöÑÂ≠¶‰π†ÔºåÊ≤°ÂÖ≥Á≥ªÁöÑÂï¶Ôºå‰∏çË¶ÅÁÆ°ÊàëÔºåÊâÄÊúâ‰ª•ÂêéÊàëÊ≥®ÊÑè‰∏ãËá™Â∑±ÁöÑÊñπÂºèÊñπÊ≥ïÂáèÂ∞ëËØØ‰ºöÂìàÔºÅÔºÅÔºÅÔºÅÔºÅÂØπ‰∏çËµ∑ÔºåÂØπ‰∏çËµ∑ÔºåÂØπ‰∏çËµ∑ÔºåÂØπ‰∏çËµ∑ÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅ 2020-6-14 ÂºÄÂøÉÁù°Ëßâing ‰ªäÂ§©ÂÆ∂ÈáåÊ≤°ÊúâÁΩëÔºåÊñ≠ÁΩëÂïäÔºÅ Â§çÁõòËøôÂë®Â∑•‰ΩúÔºåÊÑüËßâË¶ÅÊääÊüê‰∏™Áü•ËØÜ„ÄÅÁêÜËÆ∫Â≠¶Âà∞ËÑëÂ≠êÈáåÔºå‰∏çÂøòËÆ∞ÔºåÈöèÊú∫Â∫îÂèòÔºåÂ§™Èöæ‰∫ÜÔºÅ ‚Äã Á¨¨‰∏ÄÔºöË∑ëÁ®ãÂ∫è„ÄÇÊàëËßâÂæóË¶ÅË∑ëÂ§ßÁ®ãÂ∫èÔºåÊâçÊòØÊääÁ®ãÂ∫èÁ≤æÈ´ìÂ≠¶Âà∞ÂÆ∂ÔºåÊó∂Èó¥ÊïàÁéá„ÄÇ ‚Äã Á¨¨‰∫åÔºöÈòÖËØª„ÄÇÊòæËÄåÊòìËßÅÁöÑÁé∞Ë±°ÈóÆÈ¢òÂïä„ÄÇ ‚Äã Á¨¨‰∏âÔºöËØæÁ®ãÁªìËØæ„ÄÇ Âá†Èó®ËØæÁ®ãÔºåÊÑüËßâ‰∏çÊòØËá™Â∑±ÂΩìÂàùÊÉ≥ÈÄâÁöÑÔºåÂ≠¶Ëµ∑Êù•ÊÄé‰πàÈÉΩ‰∏çÈ°∫ÂøÉ„ÄÇÊàëÂ∞±Â≠¶‰∏çÊù•‰∫ÜÔºåÈÖç‰∏ç‰∏äÔºÅ ÊúÄ‰∏ªË¶ÅÁöÑÊî∂Ëé∑ ‚Äã pandas‰∏çÊñ≠Êñ∞Â¢ûË°åÁöÑÊñπÊ≥ï„ÄÇ ‚Äã pandas if ÁöÑÁî®Ê≥ï 2020-6-15‚Äî2020-6-21ËÆ°Âàí ÊÑüËßâËá™Â∑±ÊòØÂê¨Ë†¢Á¨®ÁöÑÂïä„ÄÇ 2020-6-13 ‰∏çÊÇ≤‰∏çÂñú.png ‰ªäÂ§©a .Âá∫Èó®ÊãúËÆø‰∫≤ÊàöÔºåË∫´ÂøÉÁñ≤ÊÉ´ b. Â•ΩÊÉ≥ÂõûÂ≠¶Ê†°ÂïäÔºåÊÉ≥ÂøµÈ£üÂ†ÇÔºåÊÉ≥ÂøµÊùâÊùâÂÆùË¥ù c. ‰ªÄ‰πàËÉΩÂäõÈÉΩÂú®‰∏ãÈôç„ÄÇ„ÄÇ„ÄÇ„ÄÇ‰∏çÂ•ΩÁöÑÈ¢ÑÊÑüÂïäÔºÅËøáÂéªÂ≠¶ÁöÑ‰∏úË•øÔºåÈïøÊó∂Èó¥ÊêÅÁΩÆÔºåÂ∑≤ÁªèÂèëÈúâ‰∫ÜÔºåÂïäÔºåÂïäÔºåÂïäÔºåÂïäÔºåÂïäÔºå Âïä‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî d. ËøòÊòØË¶ÅÂ§öÂÖÉÂåñÂèëÂ±ïÔºåÊÑüËßâËá™Â∑±Â∑≤ÁªèÊòØÂçï‰∏ÄÁª¥Â∫¶‰∫ÜÔºå‰∏çÁü•ÈÅìÊÄé‰πàÂõû‰∫ã„ÄÇ e. ÊàëÂèëÁé∞ÊàëËÆ∞ÂøÜÂäõË°∞ÈÄÄÁöÑÂ§™ÂéâÂÆ≥‰∫ÜÔºåËÑëÂ≠ê‰∏çÂ§üÁÅµÊ¥ªÔºå‰∏∫‰ªÄ‰πàÂë¢„ÄÇË¶ÅÂèäÊó∂ËÆ∞ÂΩïÂïä„ÄÇ Â•ΩÊúâÊïàÁéáÈóÆÈ¢òÂïäÔºåÊïàÁéáÂïäÔºåÊïàÁéáÂïäÔºåÊïàÁéáÂïäÔºÅ f. ÁúãÊ∏ÖËá™Â∑±ÔºåÁªàËÉΩÁúãÁ†¥Á∫¢Â∞ò„ÄÇ ‰ªäÂ§©ÔºöÈÅáÂà∞ÁöÑ‰∏Ä‰∫õÈóÆÈ¢òÔºö ‚Äã 1. ÂØπÂ∫îDataFrameÊï∞ÊçÆÁ±ªÂûãÁöÑË°åÁ¥¢Âºï„ÄÇÂú®ÊèíÂÖ•Êñ∞Ë°åÁöÑÈóÆÈ¢òÔºå‰ªéÂÖ∂‰ªñDataFrameÊàñËÄÖËá™ÂÆö‰πâÊèíÂÖ•„ÄÇÂèØ‰ª•Áî®append()Âú®Êú´Â∞æÊñ∞Â¢ûË°åÔºå‰ΩÜ‰º†ÂèÇÊï∞ÂàáËÆ∞ÊòØÂàóË°®ÔºåÊúÄÂ•ΩÁî®DataFrame 2. ËøòÊúâÂàóË°®ÂíåÂ≠óÁ¨¶‰∏≤ÁöÑÁõ∏‰∫íËΩ¨Êç¢„ÄÇÂ¶ÇÊûúË¶ÅÊää[]‰º†ÂÖ•DataFrame 2020-6-12 ‰∏çÁ∫†Áªì‰∫Ü.svg ÊòéÂ§©Âë®Êú´‰∫Ü„ÄÇ‰ªäÊôöË¶ÅËøΩÂâßÔºõËΩ¶Ê∞¥È©¨ÈæôÔºõÂú®ÊúàÂÖâ‰∏ãÂ•îË∑ëÔºåÊàë‰ªÄ‰πàÈÉΩ‰∏çÊÉ≥Ë¶ÅÔºå‰Ω†Áà±ÊàëÂ∞±Â•Ω ‰ªäÂ§©ÊòØÊ≤°ÊúâËØæÁöÑ‰∏ÄÂ§©„ÄÇËøòÊúâÊùâÊùâÂú®Â≠¶Ê†°‰∫Ü„ÄÇ 2020-6-11 ÂçäÁù°ÂçäÈÜí : ÊòéÂ§© ‚Äã ‰ªäÂ§©ÁöÑ‰ªªÂä°Ê≤°ÊúâÂÅöÂÆåÔºåÊòéÂ§©ÁªßÁª≠ ‰ªäÂ§© Ë∑ëÁ®ãÂ∫èÁöÑËøáÁ®ã‰∏≠ÔºåÊääÊñá‰ª∂Â§πÊ∏ÖÁêÜ‰∫Ü ÈÇ£‰πàÊúâË∂£ÔºåÈÇ£‰πàÊúâ‰ª∑ÂÄº-‚Äî‚ÄîËØÑÂà§ÂÖ¨Âπ≥ÊÄßÊ†áÂáÜ ÂìÅËØª‰∫Ü„ÄäÂñúÊ¨¢‰Ω†ÊòØÂØÇÈùôÁöÑ„Äã Âê¨‰∫ÜÈôàÂ•ïËøÖ„ÄäÊàëË¶ÅÁ®≥Á®≥ÁöÑÂπ∏Á¶è„Äã 2020-6-10 ÂºÄÂøÉ.eps ÊòéÂ§©Ôºö Ë∞ÉÁî®Êó∂Èó¥Â∫èÂàóÊåáÊ†á 2. Á∫øÊÄßÂõûÂΩíÊ®°Âûã 3. Ë∑ëÁ®ãÂ∫èÔºåËÆ°ÁÆó‰∫∫ÊâçÊµÅÂä®ÊåáÊ†á ‰ªäÂ§© ÊÄùËÄÉ‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢òÔºöËÅå‰∏öËßÑÂàí„ÄÇÊàëÂ∫îËØ•ÊÄé‰πàÁªôËá™Â∑±‰∏Ä‰∏™ÂÆö‰ΩçÔºüÊàëÂñúÊ¨¢‰ªÄ‰πàÔºüÊàëÂèó‰ªÄ‰πàÈ©±Âä®Ôºü ‰∫§Êé•‰∫Ü‰ªªÂä°ÔºåË¢´ÈóÆÊÉ®‰∫Ü„ÄÇ 2020-6-9 ÊÑâÊÇ¶.jpg ‰ªäÂ§©Ôºö Á≤æËØª‰∫Ü‰∏ÄÁØáËÆ∫ÊñáÔºåÊî∂Ëé∑Êª°Êª°ÔºåÂõ†‰∏∫ÈÉΩÊòØËá™Â∑±Â≠¶ËøáÔºåÁúãËøáÁöÑÊñπÊ≥ïÔºåÁúãÂà´‰∫∫ËÆ∫ÊñáÈáåÈù¢ÁöÑÂ∫îÁî®ÔºåÊúâÊâÄÂêØÂèëÔºÅ‰ª•ÂêéË¶ÅÁúãÈ°∂Á∫ßÁöÑËµÑÊñô ÂñùÁùÄÂÆâÊÖïÂ∏åÔºåÁúãÂà´‰∫∫ÁöÑÊï∞ÊçÆÂàÜÊûêÊä•Âëä„ÄÇ ÊúÄËøëÁü•ËØÜËæìÂÖ•Â§™Ëøá‰∫ÜÔºåËôΩÁÑ∂ÈÉΩÊòØÊú¨Áßë‰∫ÜËß£ËøáÁöÑÔºàÊï∞Â≠¶ÊñπÈù¢Ôºâ ÈáçË¶ÅË¶ÅÊ∏ÖÊ¥óÂá∫Êï∞ÊçÆÔºå‰∏çÁü•ÈÅì‰∏∫‰ªÄ‰πàÔºåÊàëÂèëÁé∞ÊàëÁöÑÊï∞ÊçÆÔºåÂ±ÖÁÑ∂Âè™ËØª‰∫ÜÂçÅÂá†‰∏áÊù°ÔºåÁÑ∂ÂêéÈáçÊñ∞Ë∑ë„ÄÇ Á≠âÂøôËøá‰∫ÜËøôÊÆµÊó∂Èó¥Ôºå‰∏ÄÂÆöË¶ÅÂ•ΩÂ•ΩÂéòÊ∏ÖÊúÄËøëÁöÑÊÄùË∑Ø ÂèØËßÜÂåñ Êï∞ÊçÆÊ∏ÖÊ¥óÔºöpython Á∫øÊÄßÈ¢ÑÊµãÊ®°Âûã ËÆ°ÈáèÁªèÊµéÂ≠¶ 2020-6-8 ÂÖÉÊ∞î ÂºÄÂøÉ ÂñúÂá∫ÊúõÂ§ñ ÊòéÂ§©1. Âü∫Êú¨Êï∞ÊçÆÁªüËÆ°ÁªìÊûú 2. È°πÁõÆ‰∫§Êé•ÔºÅ3. good night ‰ªäÂ§©Êó©‰∏äÔºåÂèëÁé∞Â±ÖÁÑ∂ÈîôËøáÊñ∞ÂûãÂÜ†Áä∂ÁóÖÊØíÁöÑËØæÁ®ãÔºåÂ∑≤ÁªèÁΩë‰∏äÁªìËØæÔºå‰∏çÂæó‰ª•ËØ¢ÈóÆËæÖÂØºÂëò„ÄÅÁ†îÁÆ°ÁßëÔºåÊâì‰∫ÜÂ•ΩÂ§öÁîµËØù„ÄÇÁ†îÁ©∂ÁîüÁöÑËÄÅÂ∏àÂ±ÖÁÑ∂ÁªôÊàë‰ª¨ÈáçÊñ∞ÁªìËØæÁöÑÊó∂Èó¥ÔºåÂ§™Áà±‰∫ÜÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÂ§™ÊÑüË∞¢ÔºåÊÑüÂä®‰∫ÜÔºÅ ‰ªäÂ§©ÂÅö‰∫ÜÁöÑÊï∞ÊçÆÂ∫èÂàóÈ°πÁõÆ‰∏äÂë®ÁöÑÊä•ÂëäÔºÅ‰∏çÂæó‰∏çËØ¥Ëá™Â∑±ÊéíÁâàËÉΩÂäõË∂äÊù•Ë∂äÂéâÂÆ≥‰∫ÜÔºÅ ‰ΩÜÊòØË∑ëÂ§ÑÁêÜÁöÑÊ®°ÂûãARIMAÊïàÊûú‰∏çË°åÔºÅËøòÊúâÊÑüË∞¢Âº†ËÄÅÂ∏àÊèê‰æõÁöÑÊúçÂä°Âô®ÔºÅÊâæÂà∞Ë∑ë‰ª£Á†ÅÁöÑÊÑüËßâ‰∫Ü ‰ªäÂ§©Áªà‰∫éÈ¢ÑÂ§ÑÁêÜÂÆåËÆ∫ÊñáÊï∞ÊçÆ‰∫ÜÔºå‰ΩÜÊòØËÉΩ‰∏çËÉΩÁî®Â•ΩÔºåÊåñÊéòÊõ¥Â§ßÈöêËóè‰ø°ÊÅØÔºåËøòÂæóÂä†ÊääÂä≤ÔºÅ ËøòÊòØË¶ÅÂÜôÂáΩÊï∞ÔºåËÄå‰∏çÊòØÂ§çÂà∂Á≤òË¥¥ÁöÑ ‰ªäÂ§©ÈòÖËØª‰∫ÜÊú¨ÁßëÂÅöÁöÑÈ°πÁõÆÔºåÊâæ‰∫Ü‰∏§Âº†ÂõæÁâáÔºåËôΩÁÑ∂Â∑≤ÁªèË¢´ÊãíÁ®ø‰∫Ü‰∏§Ê¨°Ôºå‰ΩÜÊòØÊàëÂÅöÁöÑËøáÁ®ãËøòÊòØÊÑâÂø´ÔºåÊõ¥ÈáçË¶ÅÁöÑÊãøÂà∞Ëã±ËØ≠ËØæ‰∏äÂéªÂêπÁâõÔºÅÂ∏åÊúõÂ•ΩÂ•ΩÊêûÊé•‰∏ãÊù•ÁöÑÁ†îÁ©∂ÔºÅÔºÅÔºÅÔºÅ ‰∏Ä‰∫õËØªÊñáÁåÆÁöÑÁªèÈ™åÂÄºÔºõÊÄé‰πàÁî®ËØ≠Ë®ÄË°®Ëææ‰∏çÂêåÂú∫ÊôØÁöÑÊñáÁåÆÂÜÖÂÆπÔºåËøôÊòØÈúÄË¶ÅÂä†Âº∫ÁöÑ„ÄÇ ËøôÂë®Âü∫Êú¨‰∏äÁÜüÊÇâ‰∫Üpandas,numpyÁöÑÁõ∏ÂÖ≥Êìç‰ΩúÂíåÊ≥®ÊÑè‰∫ãÈ°πÔºåÊé•‰∏ãÊù•Ë¶ÅÊêûÂ•ΩËøô‰∏™È°πÁõÆ(‰∏ªÁ∫ø2) ÁªßÁª≠ÈòÖËØªÊñáÁåÆÔºåËßÅÂì•ÁöÑÊñáÁ´† 2020-6-6 ‰ªäÂ§©ÂèàÊòØÂÖÉÊ∞îÊª°Êª°ÁöÑ‰∏ÄÂ§©ÔºåÂíåÂ§ñÂ©Ü„ÄÅÂß®Â¶àÁ≠â‰∫≤‰∫∫Âà∞ÊàëÂÆ∂ÈáåÈù¢ÂÅöÂÆ¢ÔºåÂêÉ‰∫ÜüêüÔºõËøòÊúâÈÇªÂ±ÖËÄÅ‰∫∫Ëµ†ÈÄÅÁöÑüçë Ê∏©‰π†‰∫ÜnumpyÁöÑÁî®Ê≥ïÔºånumpy,pandas,listÁõ¥Êé•ÁöÑÁõ∏‰∫íËΩ¨Êç¢ÂÖ≥Á≥ªÔºå‰ª•Âèäsklearn metricÈáåÈù¢ÁöÑÂêÑÁ±ªÊåáÊ†áÔºå‰ªÄ‰πàÂè´ÊåáÊ†áÁöÑÈ≤ÅÊ£íÊÄßÔºåÊÄé‰πàÊåâÈúÄÊ±ÇÈÄâÊàñËÄÖÊûÑÈÄ†ÈÄÇÂêàËá™Â∑±ÁöÑËØÑ‰º∞ÊåáÊ†á„ÄÇ ËØª‰∫ÜÈ´òËßÅÂ∏àÂÖÑÁöÑÁªºËø∞„ÄÇ ÂÅö‰∫ÜÊåáÊ†á‰ΩìÁ≥ªÂõæÂíåÈ´òÈìÅÁ´ôÁÇπÔºàEdraw‰∏≠ÊñáÂùëÔºåÊÄé‰πà‰∏çÊòØÁü¢ÈáèÂõæÂïäÔºåVisioÁ†¥Ëß£‰∏çÂÆâÂÖ®ÔºåAIËøò‰∏çÁü•ÈÅìÊÄé‰πàÊìç‰ΩúÔºâ ÊòéÂ§©ÂΩïËßÜÈ¢ëÔºå 2020-6-5 ‰ªäÂ§©ÁÜüÊÇâ‰∫ÜÊó∂Èó¥Â∫èÂàóÊµÅÁ®ãÔºõ1. Âπ≥Á®≥ÊÄß„ÄÇÂ∫èÂàóÂπ≥Á®≥ÊÄßÊòØÂÅöÂàÜÊûêÁöÑÂü∫Á°Ä„ÄÇÂπ≥Á®≥ÊÄßÊ£ÄÊµã‚ÄîÂçï‰ΩçÊ†πÔºõÈùûÂπ≥Á®≥ÊÄßÂ§ÑÁêÜÔºöÂ∑ÆÂàÜ~log~ÂàÜËß£~Âπ≥ÊªëÂ§ÑÁêÜÔºõ2. ARIMAÊ®°Âûã Âü∫Êú¨ÊÄùË∑ØÔºõÁõ∏ÂÖ≥ÊÄßÂíåÂÅèÁõ∏ÂÖ≥ÊÄßÂÆöÈò∂Êï∞ 3. ÁΩëÊ†º+‰ø°ÊÅØÂáÜÂàô Ë∞ÉÂèÇÔºöÊÉ©ÁΩöÊÄß nbeatsÂ∑•ÂÖ∑Â∞ÅË£ÖÂ•Ω‰∫Ü 4. predictÊ†∑Êú¨ÂÜÖÔºåÂä®ÊÄÅÂíåÈùôÊÄÅÈ¢ÑÊµã forecaseÂ§ñÊé®Ôºåtimedelt ÁªòÂõæÔºåÊåáÊ†á ÂÅáËÆæÊ£ÄÈ™åÔºöÁªüËÆ°ÈáèÂíåÊòæËëóÊÄßÊ∞¥Âπ≥ÔºåÊÄªÊÑüËßâÁêÜËß£Ëµ∑Êù•Â§™Èöæ‰∫ÜÔºåÁ≠âÂøô‰∫Ü‰∏ÄÂÆöË¶ÅÂ•ΩÂ•ΩÂ§ç‰π†ÁªüËÆ°Â≠¶ÔºåÂèÇÊï∞‰º∞ËÆ°ÔºåÊòæËëóÊÄßÊ£ÄÊµãÔºåÂ•ΩÂ•ΩË°•„ÄÇ ËÆ∞‰∏ç‰ΩèÁöÑÂáΩÊï∞ pd.date_range(start = sub.index[-1],end = sub.index[-1]+timedelta(days = 2),freq = ‚Äò1h‚Äô) stat_rawdata = rawdata[rawdata[‚ÄòÁ´ôÁÇπÂêçÁß∞‚Äô]==stat] Â∏ÉÂ∞îÁ±ªÂûãÁöÑÂàáÁâáÔºå‰∏ç‰∏äÂæàÊòéÁôΩÂéüÁêÜ plt.xlim(sub.index[0],sub.index[-1]+timedelta(days = 2)) ÁªòÂà∂ÁÉ≠ÂäõÂõæ seabornÈáåÈù¢ÁöÑheamap()123456import numpy as npimport seaborn as snsx = np.random.rand(10, 10)sns.heatmap(x, vmin=0, vmax=1, center=0)plt.show() 2020-6-4‰ªäÂ§©ÊúÄÂ§ßÁöÑÂ§¥Áñº‰πãÂ§ÑÂíåÈ¢ÜÊÇüÂ∞±ÊòØÔºåË∑ëÂ§ßÂûãÁ®ãÂ∫èÔºå‰∏ÄÂÆöË¶ÅÂáÜÂ§á‰∏Ä‰ªΩ‰∏ãÁ®ãÂ∫èÔºåË∞ÉÂ•ΩÁªìÊûúÊâçÊîæÂú®ÊúçÂä°Âô®‰∏äÈù¢Ë∑ëÔºå‰∏çÁÑ∂ÁªìÊûúÈöæ‰ª•ÊÉ≥Ë±°„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ ÊÑüËßâË∞ÉÂåÖ‰æ†‰πü‰∏çÊòØÊÉ≥Ë±°‰∏≠ÈÇ£‰πàÂ•ΩÂΩìÁöÑÔºå‰∏ÄÊòØÁé∞ÂÆû‰∏≠ÁöÑÊï∞ÊçÆ‰∏ÄËà¨Ëà¨‰∏çËßÑÂàôÔºåÊàëÂèëË™ì‰ª•Âêé‰∏ÄÂÆöË¶ÅÂÅö‰∏™ÂêàÊ†ºÁöÑÂÆ¢Êà∑Ôºå‰∫åÊòØÂèÇÊï∞ÈªòËÆ§ÊòØÊúÄËÆ®ÂéåÁöÑ ‰ªäÂ§©ÂèàÊîπÁ®ãÂ∫èÔºåÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊûúÁÑ∂Â§¥ÁñºÂïäÔºÅÂ∏åÊúõÂë®Êú´ÂèØ‰ª•Âá∫‰∏™Â§ßÊ¶ÇÁªìÊûúÔºåÂÜçÈáçÊñ∞Ë∑ëÊï∞ÊçÆ ÁªùÁü•Ê≠§‰∫ãË¶ÅË∫¨Ë°åÔºåÂéüÊù•ËßâÂæóËá™Â∑±Â≠¶ÂêÑÁßçÂ∫ìËøò‰∏çÈîôÔºåÂÆûË∑µÁöÑÊó∂ÂÄôËøòÊòØÁî®‰∏ç‰∏äÁöÑÊó∂ÂÄôÔºåüòî ËøòÊúâÂ∞±ÊòØÂëΩÂêçÁöÑÈáçË¶ÅÊÄßÔºå ÂáÜÂ§áÂá∫‰∏ÄÁØáÂÆûË∑µÁöÑÂøÉÂæóÔºåÊúàÊú´Ôºå‰∏ªË¶ÅÊòØÁé∞Âú®ÂÜôÁ®ãÂ∫èÂ§™ÂÆâÈÄ∏‰∫ÜÔºå Áü•ËØÜ pandasÊñ∞Â¢ûÂàó Êõ¥ÊîπÁ¥¢Âºï range() Ë°åÊîøÂçï‰ΩçÔºåÂèòÊõ¥ÊÉÖÂÜµ ÂáΩÊï∞ÔºåÂÜôÂáΩÊï∞Ôºå‰∏çÁü•ÈÅì‰∏∫‰ªÄ‰πàÔºåÂÉèpythonËøôÁßçËß£ÈáäÂûãÁöÑËØ≠Ë®ÄÔºåËá™Â∑±ËÄÅÊòØÈáçÂ§çÔºå‰∏çÂ•ΩÔºå‰∏çÂ•ΩÔºå 2020-6-3‰ªäÂ§©ËøòÊòØÂ≠¶‰∫Ü‰∏çÂ∞ë‰∏úË•øÔºåÊïàÁéá‰∏çÈ´òÔºå‰∏ªË¶ÅÊòØËÆ∞‰∏ç‰ΩèÂáΩÊï∞‰º†ÂèÇÊï∞ÔºÅ ÊéåÊè°‰∫Ü‰∏Ä‰∫õpythonicÁöÑÁî®Ê≥ïÔºåÊÉ≥in for if ; if else ÁöÑ‰∏ÄË°å‰ª£Á†ÅÔºõlambdaÂçïË°åÂáΩÊï∞ÂäüËÉΩÁöÑÁÆÄÂåñÁ≠âÁ≠â ‰∏Ä‰∫õÂ∏∏Áî®ÁöÑÊï∞ÊçÆÂàÜÊûêÂáΩÊï∞Ôºåsort_values();concat; append; time_range();ÁªòÂõæÂäüËÉΩÔºõÂ∞±ÊòØÂ∞±‰∏çÁü•ÈÅìÔºåËÇØÂÆöÊòØÂà´‰∫∫Â∑≤ÁªèÂ∞ÅË£ÖÂ•Ω‰∫ÜÔºÅ 2020-6-2‰ªäÂ§©ÊÄª‰ΩìËøòÊòØÊª°‰∏çÈîôÁöÑÔºÅ Ë∑ë‰∫ÜÂ§ßÁ®ãÂ∫èÔºåËá™Â∑±ÁöÑÁîµËÑëÂíåÊúçÂä°Âô®ÁöÑÊïàÊûúËøòÁúüÊòØ‰∏çÁõ∏Âêå„ÄÇ‰∏çËøáÔºåËÉΩÂ§üÂÅöÂà∞200Ë°åÂÜÖ‰∏çË∞ÉËØïÂ∞±ËÉΩÂ•Ω‰∫ÜÔºåÁé∞Âú®ÊâæÂà∞‰∫ÜÂÜô‰ª£Á†ÅÁöÑÊÑüËßâÔºåËôΩÁÑ∂ËøòÊòØbug~bug~bug, ÂéüÂõ†Âú®‰∫éÂºÄÂßã‰∫§Êé•Â∑•‰ΩúÊ≤°ÊúâÂÅöÂ•ΩÔºåÂØºËá¥ÂèçÂèçÂ§çÂ§çÁöÑ‰øÆÊîπcodes. Âú®Â∏àÂÖÑÁöÑ‰ªãÁªç‰∏ãÔºåÊé•‰∫Ü‰∏Ä‰∏™Êó∂Èó¥Â∫èÂàóÁöÑÈ°πÁõÆ„ÄÇÂú®ÊØï‰∏öËÆæËÆ°ÈÇ£‰ºöÔºåË∑ëÂ§©ÁÑ∂Ê∞îÊï∞ÊçÆÔºåÂá∫Êù•‰∫ÜÁöÑÁªìÊûúÂ§™Â∑Æ‰∫ÜÔºå‰πü‰∏çÁü•ÈÅìwhy,ËøôÊ¨°Â∏åÊúõËÉΩÂ§üË∑üËøõËøô‰∏™È°πÁõÆ„ÄÇ ÊÑüËßâËá™Â∑±ËøòÁïôÊúâ‰ΩôÂú∞ÔºåËøòÊ≤°ÊúâÊÅ¢Â§çÂà∞ÊúÄ‰Ω≥Áä∂ÊÄÅÂëê~ 2020-6-1‰ªäÂ§©ÂàöÂ•ΩÊòØ2020Âπ¥ÁöÑ‰∏ÄÂçäÔºåÂøôÂÆåÁ¨¨‰∏ÄÂ≠¶ÊúüÔºàÂçäÊ¢¶ÂçäÈÜíÔºâ,ÊúÄÁªàÂ¶ÇÊ¢¶ÂàùÈÜíÔºÅ‰ªäÂ§©Êé•Ëß¶ËÆ∏Â§öÊñ∞È≤úÁöÑÁé©ÊÑèÔºö ArcMapÁªòÂà∂Âú∞Âõæ„ÄÅÊµÅÂêëÂõæÔºàÂÖ≥ÈîÆÊòØËµ∑ÁÇπÂíåÁªàÁÇπÂùêÊ†áÔºåÂ¶Ç‰ΩïÊ†πÊçÆÂêçÁß∞Ëé∑ÂèñÂùêÊ†áÔºåË¶ÅÂÅöËøûÊé•„ÄÇArc ToolBox‰∏≠Êèê‰æõ‰∫ÜËÆ∏Â§öÂ∑•ÂÖ∑ÔºåÊñπ‰æø‰∫ÜÁî®Êà∑ÂÆåÊàê‰∏Ä‰∫õÁÆÄÂçïÁöÑÊìç‰ΩúÔºåÂ¶ÇJoin„ÄÅExcel to TabelÂ∏∏Áî®ÁöÑÂ∑•ÂÖ∑ÁÆ±„ÄÇ‰ªäÂ§©Â≠¶‰π†ÁªòÂà∂Âú∞ÂõæÂíåÊµÅÂêëÂõæÊ∂âÂèäÁöÑÊìç‰ΩúÂåÖÊã¨ÔºöÊñá‰ª∂Â§πÈìæÊé•Âà∞Â∑•‰ΩúÁõÆÂΩïÔºõArcMapÂØºÂÖ•ExcelÂùêÊ†áÊï∞ÊçÆÂπ∂ÊòæÁ§∫ÔºõXY to LineÂ∑•ÂÖ∑ÊµÅÂêëÂõæÔºõÊèêÂèñÈù¢Ë¶ÅÁ¥†ÁöÑË¥®ÂøÉÁÇπÔºõÂ§öË°®ÈìæÊé•Êìç‰ΩúÔºõÂ±ûÊÄßËÆæÁΩÆÔºàbar)„ÄÇÊ∏≤ÊüìÁªìÊûúÁöÑÁ°ÆÊºÇ‰∫Æ linuxÁ≥ªÁªüÂêéÁ´ØËøêË°å nohup &amp;ÂëΩ‰ª§ÁöÑ‰ΩøÁî®ÔºåÂ¶Ç‰ΩïËÆ∞ÂΩïÊó•ÂøóÊñá‰ª∂ÔºåÂÆöÂêëËæìÂá∫ÔºõÂ≠¶‰∫ÜpsÂëΩ‰ª§ ps Êü•ÁúãËøõÁ®ãÔºõËøòÊúâ|ÈÄöÈÅìÔºågrepÊü•ÊâæÔºõ ps -ef| grep pyton matplotlibÁªòÂõæÁöÑÂéüÁêÜ„ÄÇ 123456789101112131415ÂàõÂª∫figureÂêéÔºåËøòÈúÄË¶ÅËΩ¥fig = plt.figure()ax1 = fig.add_subplot(221)ax2 = fig.add_subplot(222)ax3 = fig.add_subplot(224)fig, axes = plt.subplots(nrows=2, ncols=2)axes[0,0].set(title=&apos;Upper Left&apos;)axes[0,1].set(title=&apos;Upper Right&apos;)axes[1,0].set(title=&apos;Lower Left&apos;)axes[1,1].set(title=&apos;Lower Right&apos;)axes[0,0].plot()axes[0,0].set_xlim([-1,6])axes[0,0].legend() matplotlib.plotÁöÑÂü∫Á°ÄÁªòÂõæÊµÅÁ®ãÔºö ÂàõÂª∫ÁîªÂ∏ÉÔºàÈÄâÊã©ÊòØÂê¶ÁªòÂà∂Â≠êÂõæÔºåÊåáÂÆöÁîªÂ∏ÉÂ§ßÂ∞èÔºåÂÉèÁ¥†ÔºâÊ∑ªÂä†Ê†áÈ¢ò‚ÄîÊ∑ªÂä†xËΩ¥ÁöÑÂêçÁß∞ÔºåÂàªÂ∫¶‰∏éËåÉÂõ¥‚ÄîÊ∑ªÂä†yËΩ¥ÁöÑÂêçÁß∞ÔºåÂàªÂ∫¶‰∏éËåÉÂõ¥ÁªòÂà∂ÂõæÂΩ¢ÔºåËÆæÁΩÆÂõæÂΩ¢ÁöÑÊ†∑ÂºèÔºåÈ¢úËâ≤ÔºåËÉåÊôØÔºåÂπ∂Ê∑ªÂä†Âõæ‰æã‰øùÂ≠òÂõæÂΩ¢ÔºåÊòæÁ§∫ÂõæÂΩ¢ ÊÑüË∞¢Áà∑Áà∑Êèê‰æõÁöÑCSDNË¥¶Âè∑ ‰∏ãËΩΩ‰∏çÂ∞ëËµÑÊñôÔºå‰∏ÄÊ¨°ÊÄßËß£ÂÜ≥ÂÆå‰∫ÜÔºÅ 2020-5-31 ËøôÊòØ‰∏äÂÆåÁ†îÁ©∂ÁîüÁ¨¨‰∏ÄÂ≠¶Êúü‰∏äÔºåÁ¨¨‰∫åÂ≠¶Êúü‰∏ã‰∏≠Êó¨ÔºÅ‰πüÊòØÈöî‰∫ÜÂæà‰πÖÊâçÊõ¥Êñ∞‰∏™‰∫∫ËÆ∞ÂΩï„ÄÇÁ†î‰∏Ä‰∏äÔºåÊç¢‰∫Ü‰∏Ä‰∏™Êñ∞ÁéØÂ¢ÉÔºåÁªà‰∫éÊúâ‰∏™ÁâπÂà´ËàíÈÄÇÁöÑÁéØÂ¢É‰∫ÜÔºÅÂ§ßÂ±èÂπïÔºåÊüîËΩØÁöÑÂá≥Â≠ê,Â§™ËàíÊúç‰∫ÜÔºÅÊÄéÂèØËæúË¥üÂë¢„ÄÇÁîµÁßëÁöÑËØæÁ®ãÂÆûÂú®ÊòØÂ§™Â§ö‰∫ÜÔºåÈÉΩ‰∏çËÉΩÂÆâÂøÉÁé©ËÄç‰∫ÜÔºÅËøôÊÆµÊó∂Èó¥ÁÆó‰∏™ÁßØÊ∑ÄÂêßÔºÅ 2019.6.2 ‰ªäÂ§©ÊâçÊù•ÂÜôÔºåÁΩ™ËøáÂïäÔºÅËÄÅÂ∏à‰∏çÂú®ÔºåÂÆ§Âèã‰∏çÂú®ÔºåÊó†ËÅäËá≥ÊûÅÔºåÂè™ËÉΩ‰ª•ËßÜÈ¢ëÂîØÂèãÔºåËß£Èó∑‰πüÔºÅÁù°‰∫Ü‰∏Ä‰∏™ÊòüÊúüÔºå‰ΩÜÊòØÊàëÁò¶‰∫Ü2Êñ§‰∫Ü„ÄÇÂºÄÂßã‰∏çÂú®ÊáíÊÉ∞‰∫ÜÔºåÂú®ËøôÊ†∑‰∏ãÂéªÊàëÂ∞±ÂÆåËõã‰∫Ü„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ ‰∏çÁü•ÈÅìÊÄé‰πàÂõû‰∫ãÔºåVPNËÄÅÊòØ time out ,ÊòéÊòéËøòÊúâ$ÂïäÔºÅ ÈÄ†Êàê‰∫ÜÂè™ËÉΩ‰∏ÄÊÆµ‰∏ÄÊÆµÁøªËØëÔºåÊó†ËØ≠ÂáùÂôéing ËøòÊúâÁôæÂ∫¶‰∏äËØ¥ÊòØ‰∏§ÁØáÔºåÂèØÊòØË¶ÅÊ±ÇÊòØ‰∏ÄÁØáÔºå‰∫éÊòØÊàëÂ§öËä±‰∫Ü2h,Â§©ÊùÄÁöÑÔºÅÔºÅÔºÅ ‰∏ãÂçàË∑ë‰∫Ü‰∏™Ê≠•Ôºå Ë∂äÊù•Ë∂äÂèëÁé∞Ôºå‰∫ãÂÖàËÆ°Âàí‰∏Ä‰∏ãÔºåÂÜçÂéªÂÅöÔºåÊïàÁéáÊõ¥Â•Ω‰∫ÜÔºÅ 2019.5.12 ‰ªäÂ§©ÊâçÊù•Êõ¥Êñ∞Ëøô‰∏™Êó•Â∏∏ÔºåÁúüÊòØÁΩ™ËøáÔºå‰∏äÊÆµÊó∂Èó¥‰∏ÄÁõ¥ÂøôÂÖ∂‰ªñÁöÑ‰∫ãÊÉÖÔºåÊØï‰∏öËÆæËÆ°ÔºåÂõûÂÆ∂ÔºåÂ§ÑÁêÜÂÆ∂Âä°Âïä„ÄÇÁõ¥Âà∞‰ªäÂ§©Â≠¶ÂÆå‰∫ÜCourse 3,Â¶Ç‰ΩïÊîπÂñÑÁ•ûÁªèÁΩëÁªúÁöÑÊÄßËÉΩÔºåËøòÊúâËã±ËØ≠Â≠¶‰π†Ôºå‰∏ÄÂÆöË¶ÅÂä†Ê≤πÂïäÔºÅ 2019.4.18 ‰ªäÂ§©Â≠¶‰∫Ü‰∏ÄÈÉ®ÂàÜcourse two week twoÁöÑËØæÁ®ãÔºåÁªà‰∫éÂ≠¶Âà∞Á≤æÂΩ©ÁöÑÈÉ®ÂàÜ‰∫ÜÔºåÁî±‰∫é‰ªäÂ§©ÊÉ≥ÁªÉÂΩ©ÈìÖÂíåÂ∏ÆÂøôÊî∂ÊãæÊïôÂÆ§ÔºåÊâÄ‰ª•Â∞±Ê≤°ÊúâÂ≠¶ÂÆå‰∫ÜÔºåÂ≠¶‰∫ÜÂá†ÁßçÂèòÁßçÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÔºåÊèêÈ´òÈÄüÂ∫¶ÔºåÂ§™Ê£í‰∫ÜÔºåÂ§™Ê£í‰∫ÜÔºåÂ§™Ê£í‰∫ÜÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅ 2019.4.17 ‰ªäÂ§©Â≠¶ÂÆåÂõΩcourse twoÁöÑÁ¨¨‰∏ÄÂë®ËØæÁ®ãÔºåÂ§ßÊ¶ÇËÆ∞Êó∂4hÔºåËøòÊòØÈ¢áÂ§öÊî∂Ëé∑ÔºåÂ≠¶Âà∞‰∫Ü‰ª•ÂâçÂÆåÂÖ®Ê≤°ÊúâÊé•Ëß¶ÁöÑ‰∏úË•øÔºåÊ≠£ÂàôÂåñÊñπÊ≥ïÔºåÊ¢ØÂ∫¶Ê∂àÂ§±ÁöÑÂíåÁàÜÁÇ∏ÁöÑÈóÆÈ¢òÔºåÊÑüËßâÂæàÊ£íÔºå 2019.4.16 ‰ªäÂ§©Â≠¶‰∫ÜÂê¥ÊÅ©ËææËÄÅÂ∏àÁöÑÁ¨¨‰∏ÄÂë®ÂÖ≥‰∫éÁ•ûÁªèÁΩëÁªúÁöÑÂü∫Á°ÄÁöÑËØæÁ®ãÔºåÂ•ΩÂéâÂÆ≥ÁöÑ 2019.4.15 ‰ªäÂ§©Â≠¶‰π†‰∫Üdeep-layer neural networkÁöÑÊ≠£Âêë‰º†Êí≠ÂíåÂèçÂêë‰º†Êí≠ÁöÑËøáÁ®ãÔºåÁü©ÈòµÂåñËÆ°ÁÆóÁöÑÊñπÂºè„ÄÇ 2019.4.14 ‰ªäÂ§©Â≠¶‰π†‰∫ÜÁ¨¨‰∏âÂë®ËØæÁ®ãÔºåshallow (2Â±Ç)Á•ûÁªèÁΩëÁªúÁöÑÊ≠£Âêë‰º†Êí≠ÂíåÂèçÂêë‰º†Êí≠Ôºå‰ª•ÂèäÁü©ÈòµÂåñÁöÑËÆ°ÁÆóÊñπÂºèÔºå‰ª•ÂèäÂíålogistics regressionÁöÑË°®Á§∫‰∏äÁöÑ‰∏çÂêåÂú∞Êñπ 2019.4.13 ‰ªäÂ§©ÁªßÁª≠Â≠¶‰π†Á¨¨‰∫åÂë®ËØæÁ®ãÔºålogistics regression ÁöÑÊ®°ÂûãÔºåÁ≠ñÁï•ÔºåÁÆóÊ≥ïÁöÑÁõ∏ÂÖ≥ÔºåÂ¶Ç‰ΩïÊääÂ≠¶‰π†ÁöÑÂà∞Êï∞Â≠¶Áü•ËØÜÂ∫îÁî®‰∏äÂéª„ÄÇ 2019.4.12 ‰ªäÂ§©Ê≤°ÊúâÂéªË∑ëÊ≠•ÔºåÂ§ÑÁêÜ‰∏Ä‰∫õÊÉÖÊÑüÈóÆÈ¢òÂéª‰∫ÜÔºåÂ≠¶‰∫ÜAndrow NgÁöÑ Á¨¨‰∫åÂë®ÁöÑÈÉ®ÂàÜËØæÁ®ãÔºå‰∏ªË¶ÅÊòØÈìæÂºèÊ±ÇÂØºÊ≥ïÂàôÔºåÈÄöËøáËÆ°ÁÆóÂõæÔºå‰∏ÄÊ≠•‰∏ÄÊ≠•ÁöÑÂ§çÂêàÊ±ÇÂØºÔºåÂ§çÂêàÂáΩÊï∞Áî®ËÆ°ÁÆóÂõæË°®Á§∫ÔºåÂπ∂ÈìæÂºèÊ±ÇÂØº 2019.4.11 ‰ªäÂ§©ÊòØÂæàÊÑâÂø´ÁöÑ‰∏ÄÂ§©ÔºåËµ∑ÁöÑÂæàÊó©ÔºåÁ≤æÁ•ûÂæàË∂≥ÔºåÂ≠¶‰∫ÜÂ≠¶‰π†ÔºåÂÜôÂÆå‰∫ÜÂê¥ÊÅ©ËææËÄÅÂ∏àÁöÑÁ¨¨‰∏ÄÂë®Ê∑±Â∫¶Â≠¶‰π†„ÄÇ 2019.4.10 ‰ªäÂ§©Âà∑ÂÆå‰∫ÜÁ¨¨‰∏ÄÈÅçÔºåÂÆå‰∫ÜË•øÁìú‰π¶Á¨¨‰∏ÄÁ´†Âà∞Á¨¨ÂçÅ‰∏ÄÁ´†Ôºå‰∏çËøáÂë¢ÔºåËøòÊòØË¶ÅÁªßÁª≠Âú®Âà∑Âü∫Á°ÄËøòÁúãËÆ∫Êñá ËøôÊÆµÊó∂Èó¥ÔºåË∑ëÊ≠•ÈîªÁÇºÁ¨¨‰∏ÄÔºåÁù°ËßâÔºåË¥™Áù°Á¨¨‰∫åÔºåÁ¨¨‰∏âÔºåÂºÄÂßãÊï£Êº´‰∫ÜÔºåËá™ÂæãÁöÑÊàëÔºåÂú®Âì™ÈáåÂéª‰∫ÜÂïäÔºÅ ‰∫âÂèñÂú®‰∏äÁ†îÁ©∂ÁîüËøôÊÆµÊó∂Èó¥ÔºåÊääÊú∫Âô®Â≠¶‰π†„ÄÅPythonÁºñÁ®ãËÉΩÂäõÊèêÂçá‰∏äÂéª ‰ªäÂ§©Êôö‰∏äÂºÑ‰∫Ü‰∏Ä‰∏™ÔºåÂê¥ÊÅ©ËææÁöÑdeep Learning AiËØæÁ®ãÁ¨îËÆ∞ÔºåÂºÄÂßãÂ≠¶‰π†Ëøô‰∏™Á≥ªÂàóÁöÑËØæÁ®ã‰∫Ü Á¨îËÆ∞Ê®°Êùø Ëã±ÊñáÔºöhttp://dl-notes.imshuai.com/#/](http://dl-notes.imshuai.com/#/c1w1) ‰ªÄ‰πàÈÉΩÊúâÁöÑ https://redstonewill.com/category/deeplearning/ ‰∏≠ÊñáÁ¨îËÆ∞Ôºö ËØ¶ÁªÜÔºåÁõ∏ÂΩì‰∫éÁøªËØëÔºö http://www.ai-start.com/dl2017/html/lesson1-week1.html ÊÄùËÄÉÂíåÊÄªÁªìÔºö https://link.zhihu.com/?target=http%3A//kyonhuang.top/Andrew-Ng-Deep-Learning-notes/ https://zhuanlan.zhihu.com/p/35333489 http://imshuai.com/tag/deeplearning-ai-notes/ https://zhuanlan.zhihu.com/koalatree ‰∏äÁè≠ÊóèÁöÑÂ≠¶‰π†Êó∂ÈïøÔºö Êï¥ÁêÜÁ¨îËÆ∞Ë¶ÅÊòéÊòæÊ∂àËÄóÊõ¥Â§öÁöÑÊó∂Èó¥ÔºåÂü∫Êú¨‰∏ä10ÂàÜÈíüÁöÑËßÜÈ¢ëÔºåÂÅöÁ¨îËÆ∞Ëá≥Â∞ëË¶Å50ÂàÜÈíüÊâçËÉΩÁúãÂÆå„ÄÇÊØèÂë®ÁöÑËßÜÈ¢ëÂ§ßÊ¶ÇÂú®2-3‰∏™Â∞èÊó∂ÔºåËøôÂ∞±ÊÑèÂë≥ÁùÄÁúãÂ≠¶‰π†ËßÜÈ¢ëÂ∞±Ë¶Å10-15‰∏™Â∞èÊó∂„ÄÇÁºñÁ®ã‰Ωú‰∏ö3‰∏™Â∞èÊó∂Â∑¶Âè≥ÔºåÊØèÂë®Âπ≥ÂùáÂú®15‰∏™Â∞èÊó∂Ôºå16Âë®ÁöÑËØæÔºåÂáÄÊó∂Èó¥Â∞±Ëä±‰∫Ü240‰∏™Â∞èÊó∂ÔºÅ video https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/playlists Ë•øÁìú‰π¶ÁöÑÂÖ¨ÂºèÊé®ÂØºÔºö https://datawhalechina.github.io/pumpkin-book/#/chapter1/chapter1 ÁªüËÆ°Êú∫Âô®Â≠¶‰π†Ôºö https://github.com/SmirkCao/Lihang 2019.3.23-2019.3.24 ‰ªäÂ§©Ôºà‰ªäÂ§©ÔºâÊòØÂë®Êú´Ôºå‰∏§‰∏™Êó©‰∏äÈÉΩÁù°ËßâÂéª‰∫ÜÔºåÂ§ÑÁêÜ‰∏Ä‰∏ãÔºåÊàëÂºüÂºüÁöÑÈóÆÈ¢òÔºåÂíåÂ≠©Â≠ê‰∫§ÊµÅÁöÑÈáçË¶ÅÊÄßÔºåÊôö‰∏ä‰ªÄ‰πàÈÉΩÊ≤°ÊúâÂπ≤ÔºåÂë®Êó•‰∏ãÂçàË∑ë‰∫ÜÊ≠•ÔºåÂ≠¶‰π†‰∫Üpython sci-learnÈáåÈù¢ÁöÑ‰∫§ÂèâÈ™åËØÅ„ÄÅË∂ÖÂèÇÊï∞ÈÄâÊã©ÔºåÈõÜÊàêÂ≠¶‰π†Â∫ìÁöÑÁî®Ê≥ïÔºåÁªà‰∫é‰∏çÂÜçËø∑Ëå´‰∫ÜÔºåÊÖ¢ÊÖ¢Êù• ÂèäÊó∂ÁöÑËÆ∞ÂΩïÂ≠¶‰π†ËøáÁ®ã Â§öÂéªÁúãÂà´‰∫∫ÁöÑËøõÂ±ï ÂéªÂÅöÂà´‰∫∫Ê≤°ÊúâÂÅöËøáÁöÑ‰∫ãÊÉÖ Âè™Êúâ‰Ω†Âä™ÂäõÔºåÂä™ÂäõÁöÑÊñπÂêëÊòØÊ≠£Á°ÆÁöÑÔºåÂ∞±ÂèØ‰ª•ÂÅöÂá∫‰ºüÂ§ßÁöÑÊàêÂ∞± https://zhuanlan.zhihu.com/p/29704017 2019.3.21 ‰ªäÂ§©‰∫ÜËß£‰∏Ä‰∏ãÊìçÁõòÔºåÂ∞±ÊòØÈÇ£‰∏™ËÇ°Á•®ÔºåÊÑüËßâÂÅöÊï∞ÊçÆÁßëÂ≠¶ÂÆ∂Â•ΩÂêÉÈ¶ôÂïäÔºåÂä†Ê≤πÔºåÊé•‰∏ãÊù•ÊúâÂæóÂøô‰∫Ü 2019.3.20 ‰ªäÂ§©Áªà‰∫éÁúã‰∫ÜÊñáÁåÆÔºåÊÉ≥Âá∫‰∫Ü‰∏Ä‰∏™ÂàõÊñ∞ÁÇπÂ≠êÔºåÂèØ‰ª•ÂèëÊñáÁ´†‰∫ÜÔºåÂä†Ê≤πÂñî„ÄÇ ÁºñÂÜôÂÆå‰∫ÜÊµãËØïÁ®ãÂ∫èÔºåÂä†Ê≤π 2019.3.19 ‰ªäÂ§©Êó©‰∏äÁúã‰∫ÜÂÖ≥‰∫éËÆ∏Â§öÁÅ∞Ëâ≤Ê®°ÂûãÁöÑÁé∞Èò∂ÊÆµÊñáÁåÆÔºåÂæàÂèóÂêØÂèëÔºåÊÑüËßâËØªÊñáÁåÆÁúüÁöÑÂØπËÉΩÂäõ„ÄÅÊÄùÁª¥ÁöÑÊèêÈ´òÂæàÂ§ßÔºå‰πüÂèëÁé∞Ëá™Â∑±ÁöÑÊΩúÂäõÂæàÂ§ßÔºåÂä†Ê≤πÔºåÂä†Ê≤πÔºåÂä†Ê≤πÔºåÁªßÁª≠ËØªÔºå‰∫ßÁîü‰∏Ä‰∫õÂ∞èÁÇπÂ≠êÔºåËôΩÁÑ∂‰∏çÂ§üÂ•ΩÔºå‰ΩÜÊòØ‰πüÊòØÁ™ÅÁ†¥ 2019.3.18 ‰ªäÂ§©ÁúãÁóÖÂéª‰∫ÜÔºåÁù°‰∫ÜÂ•Ω‰πÖÔºåËçØÁâ©‰ΩúÁî®ÊûúÁÑ∂Âá∫‰πéÊÑèÊñôÔºå 2019.3.17 ‰ªäÂ§©‰∏ªË¶ÅÂ≠¶‰π†ÊîØÊåÅÂêëÈáèÊú∫ÁöÑÂâç‰∏ñ‰ªäÁîüÔºåÊÄé‰πàÁî±Êù•ÁöÑ 2019.3.16 ‰ªäÂ§©Âë®ÂÖ≠ÔºåÊîæÊùæ‰∫Ü‰∏Ä‰∏ãËá™Â∑±ÔºåÈáçÊñ∞ÂõûÂΩí‰∏Ä‰∏ãÁÅ∞Ëâ≤È¢ÑÊµãÊ®°ÂûãÔºåÂ∞§ÂÖ∂ÊòØÁ¶ªÊï£ÊñπÁ®ãÔºåÂÖ∂ÂÆûÂéªÊéâËÉåÊôØÔºåÊäì‰ΩèÊï∞Â≠¶Ê®°ÂûãÔºåÊâçÁü•ÈÅìÂÖ∂ÂÆûÂ∞±ÊòØËøôÊ†∑ÁöÑÔºåÂ∏åÊúõÊó©Êó•Á™ÅÁ†¥Ëá™Â∑±ÁöÑÁïåÈôê 2019.3.14 ‰ªäÂ§©Êîπ‰∫ÜËÆ∫ÊñáÔºåÊÑüËßâÂø´ÂÆå‰∫ÜÔºõÂ≠¶‰π†‰∫ÜBPÁü©ÈòµÊé®ÂØºÔºå‰∫∫ÁúüÁöÑÊòØË∂äÊù•Ë∂äËÅ™ÊòéÂíåÁÅµÊ¥ª 2019.3.13 ‰ªäÂ§©Êôö‰∏äÂ≠¶ÂÆå‰∫ÜÂõûÂΩíÊ†ëÔºåÂ•ΩÊ£íÔºåËôΩÁÑ∂ÂéüÁêÜÁªô‰∫∫ÁöÑÊÑüËßâÂæàÁõ¥Êé•Ôºå‰ΩÜÊòØ‰πüÊòØ‰∏ÄÁßç‰ΩìÁé∞ 2019.3.12 ‰ªäÂ§©Ë∑ë‰∫ÜÊ≠•ÔºåÊó©‰∏äÊîπ‰∫ÜÊØï‰∏öËÆæËÆ°Ôºå‰∏ãÂçàÈÖçÁΩÆ‰∫ÜÊñ∞ÊâãÊú∫ÔºåÊôö‰∏äÂÜô‰∫ÜÊó•ËÆ∞ 2019.3.11 ‰ªäÂ§©Êó©‰∏äÂê¨‰∫ÜÂê¨ÂäõÔºå‰øÆÊîπ‰∫ÜÊØï‰∏öËÆæËÆ°ÔºåÊ†∏ÂáΩÊï∞ËøòÊòØ‰∏çÊòØÂæàÊáÇÔºõ‰∏ãÂçàÁúã‰∫ÜÂÜ≥Á≠ñÂõûÂΩíÊ†ëÔºåÊÑüËßâÂæàÊ£íÔºåËøáÊãüÂêàÊâçÊòØËØ•Ëß£ÂÜ≥ÁöÑÂÖ≥ÈîÆÈóÆÈ¢òÔºÅË∑ë‰∫ÜÊ≠•ÔºåÊÑüËßâËá™Â∑±Ë∫´‰ΩìÁä∂ÂÜµÂæà‰∏çÂ•ΩÂïäÔºÅ Êôö‰∏äÂê¨ÂäõÁîµÂè∞ÔºåÂÜô‰∫ÜÁ®ãÂ∫èÔºÅ ÊÖ¢ÊÖ¢ÁöÑÂ≠¶‰π†ÔºåÊÖ¢ÊÖ¢ÁöÑÂä™ÂäõÔºåÊÖ¢ÊÖ¢ÁöÑÂä†Ê≤πÔºåÊÖ¢ÊÖ¢ÁöÑÈÅáËßÅËá™Â∑±ÁöÑÊÜßÊÜ¨Â§©Á©∫ 2019.3.10 ‰ªäÂ§©ÂÜô‰∫ÜÊó•ËÆ∞ÔºåËØ¥‰∏ç‰∏äËá™Â∑±Âì™ÈáåÈÉÅÈó∑ÔºåÂì™ÈáåÂºÄÂøÉÔºÅÂ∏åÊúõÊó©ÁÇπË∑ëÂÆåÁ®ãÂ∫èÔºåÊó©ÁÇπÂÆå‰∫ã 2019.3.9 ‰ªäÂ§©ÊâçÂèëÁé∞ÔºåÊúÄËøëÁä∂ÊÄÅÈùûÂ∏∏ÁöÑ‰∏çÂ•ΩÔºåÂèØËÉΩÊòØÊó†ÊâÄ‰∫ã‰∫ãÂêßÔºåÂèØÊòØÊàëÊúâÂæàÂ§ö‰∫ãÊÉÖË¶ÅÂÅöÔºåÂä†Ê≤πÔºåÂ∞ëÂ•≥ÔºåÂä†Ê≤πÔºåÂ∞ëÂ•≥Ôºå‰ª•Âêé‰∏ÄÂÆöË¶ÅËÆ∞ÂæóÂÜô‰∫Ü!ÊÄªËßâÂæóËá™Â∑±ÈÄÉ‰∏çÂá∫Ëá™Â∑±ÁöÑÁæÅÁªäÔºåË¢´ÊùüÁºö 2019.2.28‰ªäÂ§©Ë∞ÉÊï¥Ëá™Â∑±ÁöÑÂøÉÊÄÅÔºåÂõûÈ°æËá™Â∑±ÁöÑÁîüÊ¥ªÔºåËá™Â∑±Â§™ÊÄ•‰∫éÊ±ÇÊàê‰∫ÜÔºåÂøÉÊÄ•ÂêÉ‰∏ç‰∫ÜÁÉ≠Ë±ÜËÖêÂïäÔºÅÔºÅÔºÅÔºÅÔºÅ‰∏≠ÂõΩÊúâÂè•ËÄÅËØùËØ¥ÂæóÂØπÔºåÁßØÂ∞ëÊàêÂ§öÔºå‰∏çÁßØË∑¨Ê≠•Êó†‰ª•Ëá≥ÂçÉÈáåÔºå‰∏çÁßØÂ∞èÊµÅÊó†‰ª•ÊàêÊ±üÊ≤≥ÔºåÊÖ¢ÊÖ¢Êù•ÔºåÂºÑÈÄèÂºÑÊ∏ÖÊ•ö ÊúÄÂêéÔºåËã±ËØ≠+Êï∞Â≠¶+ÁºñÁ®ã+Ëß£ÂÜ≥ÈóÆÈ¢òÁöÑËÉΩÂäõ 2019.2.27‰ªäÂ§©Êó©‰∏äÔºåÊàëËØª‰∫ÜÂêåÊ†°ÂêåÂ≠¶ÂÜôÁöÑÁÆÄ‰π¶ÊñáÁ´†ÔºåÂÆûÂú®ÊòØÊÑüÂà∞Â•ΩÊÉ≥Á¨ëÔºåÊêúÁ¥¢‰∫ÜÂà´‰∫∫ÁöÑËß£ÂÜ≥ÊñπÊ≥ïÔºåËøòÊòØËá™Â∑±ÁöÑÁü•ËØÜÈáè‰∏çË∂≥ÂïäÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅÔºÅ‰∏ãÂçàÁúã‰∫Ü‰∏ãÂÅáËÆæÊ£ÄÈ™åÁöÑËßÜÈ¢ëÔºåÂÜçÂéªË∑ë‰∫Ü‰∏§‰∏™Â∞èÊó∂ÁöÑÊ≠• Êôö‰∏äÈÄõ‰∫Ü‰∏Ä‰∏ãÂçàÂêÑ‰ΩçÁΩëÁ´ô 2019.2.26‰ªäÂ§©Áúã‰∫Ü‰∏ãÊüê‰∏™ËÆ°ÁÆóÊú∫Â§ß‰Ω¨ÁöÑÂéÜÁ®ãÔºåÊ∑±Ê∑±Âú∞ÊÑüÂà∞‰Ω©Êúç„ÄÇ„ÄäÊ¢¶ÊÉ≥Â∞èÈïá„ÄãÂèàÂ§ö‰∫Ü‰∏ÄÂùóÂú∞Áõò‰∫ÜÔºå ÂèàÈáçÊñ∞Â≠¶‰π†‰∫Ümatplotlib,ÊâçÂèëÁé∞,ÁÑ∂Âêé‰∫ÜÁé©‰∫Ü‰∏Ä‰∏ãsklearnÈáåÈù¢ÁöÑÂ∏¶cross-validationÁöÑlassoÁöÑregression ÂÜçkaggle housr-prices ÈáåÈù¢Ôºårmse=$0.15386$,rank = 2903,‰∏çËøáÂÅöÂæó‰πüÁõ∏ÂΩì 2019.02.25 ‰ªäÂ§©Â•ΩÂÉè‰∏çÂú®Áä∂ÊÄÅÔºåÂèØËÉΩÊòØÁÑ¶Ëôë+Ëø∑Ëå´ÔºåÊúâÂä®ÂäõÔºåÂä®ÂäõÁöÑÊñπÂêëÂú®Âì™ÈáåÂïäÔºÅÔºåËøòÊòØÂ•ΩÂ•ΩÁöÑÂÅöÂ•ΩÂΩì‰∏ãÂêßÔºÅ ÈòøË•øÂêß 2019.02.24‰ªäÂ§©ÁâπÂà´‰∏çÊÉ≥Ëµ∑Â∫äÔºåÊúâÁÇπÂ∞èÊÑüÂÜíÔºåÊï¥ÁêÜÂë®ÂøóÂçéÁöÑÁ¨¨‰∏ÄÁ´†Á¨îËÆ∞‰∏ãÂçàÊÄªÁªìÊó•ÂâçÂ≠¶‰π†ÁöÑpythonÂ∫ìÈ°∫‰æøÂéªkaggleÂÅö‰∫ÜÂ∞èÁªÉ‰π†ÔºåÊï∞ÊçÆÁöÑÈ¢ÑÂ§ÑÁêÜÂ∑•‰Ωú„ÄÇ]]></content>
      <categories>
        <category>Â≠¶‰π†„ÅÆÂéÜÁ®ã(Journal of Studying)</category>
      </categories>
      <tags>
        <tag>Daily</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(Âë®ÂøóÂçé)]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%AD%A6%E4%B9%A0%E3%81%AE%E5%8E%86%E7%A8%8B-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%91%A8%E5%BF%97%E5%8D%8E%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[ÈòÖËØªÁõÆÂΩï[TOC] Á¨¨‰∏ÄÁ´† What is the machine learning?ÈùûÂ∏∏ÂÆòÊñπÁöÑÂÆö‰πâÔºö Tom mitchell(1998) Well-posed LearningProblem:A compute program is said to learn from experience E with respect to same task T and some performance measure P,if its performance on T,as measured by P, improves with experience E„ÄÇÔºàËøô‰∏™ÊàëËé´Ê≥ïÁøªËØëÂñîÔºâÂ§ßÊ¶ÇÊÑèÊÄùÊòØÂº∫Â§ßÁöÑËÆ°ÁÆóÊú∫ËÉΩÂ§ü‰∫ãÂÖàÂú∞ÂÆåÊàê‰∫∫‰∏∫ÈùûÊòæÁ§∫ÁºñÁ®ãÂ•ΩÁöÑ‰ªªÂä°ÔºåÊÄé‰πàÂÆåÊàêÂë¢ÔºüÂØπ‰∫éÊüê‰∏™‰ªªÂä°T,ÁªôÂÆö‰∏Ä‰∏™ÊÄßËÉΩÂ∫¶ÈáèÊñπÊ≥ïP,Âú®ÁªèÈ™åEÁöÑÂΩ±Âìç‰∏ãÔºåÂ¶ÇÊûúPÂØπTÁöÑÊµãÈáèÁªìÊûúÂæóÂà∞‰∫ÜÊîπËøõÔºåÂàôËØ¥ÊòéËØ•Á®ãÂ∫è‰ªéE‰∏≠Â≠¶‰π†‰∫ÜÊú∫Âô®Â≠¶‰π†ÁöÑËøáÁ®ãÂ§ßËá¥Â¶ÇÊ≠§ÔºöËÆ©ËÆ°ÁÆóÊú∫‰ªéÊï∞ÊçÆ‰∏≠‰∫ßÁîüÊ®°Âûã(model)ÔºåÈ¶ñÂÖàÊèê‰æõÁªèÈ™åÊï∞ÊçÆÔºåÁªôÂÆöÂ≠¶‰π†ÁÆóÊ≥ï(learning algorithm)ÂíåÊÄßËÉΩÊµãÈáèÊñπÊ≥ïÔºåÂÆÉÂ∞±ËÉΩÊ†πÊçÆÊï∞ÊçÆ‰∫ßÁîüÊ®°Âûã„ÄÇÊ®°ÂûãÔºö Ê≥õÊåá‰ªéÊï∞ÊçÆ‰∏≠Â≠¶ÂæóÁöÑÁªìÊûúÊ®°ÂºèÔºö Â±ÄÈÉ®ÊÄßÁöÑÁªìÊûú Âü∫Êú¨ÊúØËØ≠Êï∞ÊçÆÈõÜ: data setÊ†∑Êú¨Ôºö sampleÂ±ûÊÄßÔºàÁâπÂæÅÔºâÔºö attributeÔºàfeature)Â±ûÊÄßÂÄºÔºö attribute valueÂ±ûÊÄßÁ©∫Èó¥ÔºàÁâπÂæÅÁ©∫Èó¥ÔºâÔºö attribute space Ôºà sample spaceÔºâÁâπÂæÅÂêëÈáèÔºö feature vectorÂ≠¶‰π†ÔºàËÆ≠ÁªÉÔºâÔºölearningÔºàtrainingÔºâËÆ≠ÁªÉÊï∞ÊçÆÔºö training dataËÆ≠ÁªÉÈõÜÔºö training setÂÅáËÆæÔºöhypothesis Â≠¶ÂæóÊ®°ÂûãÂØπÂ∫î‰∫ÜÂÖ≥‰∫éÊï∞ÊçÆÁöÑÊüêÁßçÊΩúÂú®ËßÑÂæãÊ≥õÂáΩËÉΩÂäõ: generalization ÂÅáËÆæÁ©∫Èó¥ÂΩíÁ∫≥ÔºàinductionÔºâÔºö ‰ªéÁâπÊÆäÂà∞‰∏ÄËà¨ÁöÑ‚ÄúÊ≥õÂåñ‚Äù(generalization)ËøáÁ®ãÊºîÁªéÔºàdeduction)Ôºö ‰ªé‰∏ÄËà¨Âà∞ÁâπÊÆäÁöÑ‚ÄúÁâπÂåñ‚Äù(specialization)ËøáÁ®ãÊú∫Âô®Â≠¶‰π†ÊòæÁÑ∂ÊòØÂΩíÁ∫≥Â≠¶‰π†Ôºàinductive learning)ÂΩíÁ∫≥Â≠¶‰π†ÂàÜÁã≠‰πâ‰∏éÂπø‰πâÔºåÁã≠‰πâÊòØÊåáË¶ÅÊ±Ç‰ªétraining set ‰∏≠Â≠¶ÂæóÊ¶ÇÂøµÔºåÂπø‰πâÊòØÊåá‰ªésample‰∏≠Â≠¶‰π† Â≠¶‰π†ËøáÁ®ãÔºàËÆ≠ÁªÉËøáÁ®ãÔºâÁúã‰ΩúÊòØÂú®ÊâÄ‰ª•ÂÅáËÆæÁªÑÊàêÁöÑÁ©∫Èó¥‰∏≠ËøõË°åÊêúÁ¥¢ÁöÑËøáÁ®ãÔºåÊêúÁ¥¢ÁõÆÊ†áÊòØÊâæÂà∞‰∏étraining setÂåπÈÖçÁöÑÂÅáËÆæ„ÄÇÂ¶ÇÊûúÂÅáËÆæÁöÑË°®Á§∫‰∏ÄÊó¶Á°ÆÂÆöÔºåÂÅáËÆæÁ©∫Èó¥‰∏éÂÖ∂ËßÑÊ®°Â∞±Á°ÆÂÆö‰∫Ü„ÄÇÊÉ≥Êõ¥ËØ¶ÁªÜ‰∫ÜËß£ÂÅáËÆæÁ©∫Èó¥ÔºåÊà≥ÊàëÂï¶5.2Áé∞ÂÆûÈóÆÈ¢ò‰∏≠Â∏∏Èù¢‰∏¥ÂæàÂ§ßÁöÑÂÅáËÆæÁ©∫Èó¥ÔºåÊàë‰ª¨ÂèØ‰ª•ÂØªÊâæ‰∏Ä‰∏™‰∏éËÆ≠ÁªÉÈõÜ‰∏ÄËá¥ÁöÑÂÅáËÆæÈõÜÂêàÔºåÁß∞‰πã‰∏∫ÁâàÊú¨Á©∫Èó¥„ÄÇÁâàÊú¨Á©∫Èó¥‰ªéÂÅáËÆæÁ©∫Èó¥ÂâîÈô§‰∫Ü‰∏éÊ≠£‰æã‰∏ç‰∏ÄËá¥Âíå‰∏éÂèç‰æã‰∏ÄËá¥ÁöÑÂÅáËÆæÔºåÂÆÉÂèØ‰ª•ÁúãÊàêÊòØÂØπÊ≠£‰æãÁöÑÊúÄÂ§ßÊ≥õÂåñ„ÄÇÂΩíÁ∫≥ÂÅèÂ•ΩÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÂú®Â≠¶‰π†ËøáÁ®ã‰∏≠ÂØπÊüêÁßçÁ±ªÂûãÂÅáËÆæÁöÑÂÅèÂ•ΩÔºåÁß∞‰∏∫‚ÄúÂΩíÁ∫≥ÂÅèÂ•Ω‚ÄùÔºàinductive bias),‰πüÂ∞±ÊòØÂ≠¶‰π†ÁÆóÊ≥ïÂú®‰∏Ä‰∏™ÂèØËÉΩÂæàÂ∫ûÂ§ßÁöÑÂÅáËÆæÁ©∫Èó¥‰∏≠ÂØπÂÅáËÆæËøõË°åÈÄâÊã©ÁöÑÂêØÂèëÂºèÊàñËÄÖ‚Äú‰ª∑ÂÄºËßÇ‚ÄùÂ••Âç°ÂßÜÂâÉÂàÄÂÆöÂæãÔºö Ëã•ÊúâÂ§ö‰∏™ÂÅáËÆæ‰∏éËßÇÊµã‰∏ÄËá¥ÔºåÂàôÈÄâÊã©ÂÅöÁÆÄÂçïÁöÑÂì™‰∏™„ÄÇÊ≤°ÊúâÂÖçË¥πÁöÑÊó†È§êÂÆöÁêÜÔºàNo Free Lunch Theorem[NFL]) Âú®ÊâÄ‰ª•ÈóÆÈ¢òÂá∫Áé∞ÁöÑÊú∫‰ºöÁõ∏ÂêåÔºåÊàñËÄÖÊâÄ‰ª•ÈóÆÈ¢òÂêåÁ≠âÈáçË¶Å‰∏ãÔºåÊâÄÊúâÁÆóÊ≥ïÁöÑÊúüÊúõ‰∏ÄÊ†∑„ÄÇ‰ΩÜÂú®ÂÆûÈôÖÈóÆÈ¢ò‰∏≠ÔºåÈíàÂØπÂÖ∑‰ΩìÁöÑÈóÆÈ¢òÔºå‰∏çÂêåÁöÑÁÆóÊ≥ïÊâç‰ºöÂá∫Áé∞Áõ∏ÂØπ‰ºòÂä£„ÄÇ ÂèëÂ±ïÂéÜÁ®ãÊé®ÁêÜÊúüÔºö‰∫åÂçÅ‰∏ñÁ∫™‰∫îÂçÅÂπ¥‰ª£Âà∞‰∏ÉÂçÅÂπ¥‰ª£ÂàùÔºåAIÂ§Ñ‰∫éÊé®ÁêÜÂå∫Ôºå‰ª£Ë°®ÊÄßÂ∑•‰Ωú‰∏ªË¶ÅÊòØA.Newell ÂíåH.SimonÁöÑ‚ÄúÈÄªËæëÁêÜËÆ∫ÂÆ∂‚ÄùÁ®ãÂ∫èÂíåÊ≠§ÂêéÁöÑ‚ÄúÈÄöÁî®ÈóÆÈ¢òÊ±ÇËß£‚ÄùÁ®ãÂ∫èÁ≠â„ÄÇ‚ÄúÈÄªËæëÁêÜËÆ∫ÂÆ∂‚ÄùÁ®ãÂ∫èËØÅÊòé‰∫ÜÊï∞Â≠¶ÂÆ∂ÁΩóÁ¥†ÂíåÊÄÄÁâπÊµ∑ÁöÑ„ÄäÊï∞Â≠¶ÂéüÁêÜ„ÄãÈáåÈù¢ÁöÑÊüê‰∫õÂÆöÁêÜÔºåËé∑ÂæóÂõæÁÅµÂ•ñ„ÄÇÁü•ËØÜÊúüÔºö‰ªé‰∫åÂçÅ‰∏ñÁ∫™‰∏ÉÂçÅÂπ¥‰ª£‰∏≠ÊúüÂºÄÂßãÔºåAIÁöÑÁ†îÁ©∂ËøõÂÖ•‰∫Ü‚ÄúÁü•ËØÜÊúü‚ÄùÔºåÂ§ßÈáèÁöÑ‰∏ìÂÆ∂Á≥ªÁªüÂá∫Áé∞ÔºåE.A.FeigenbaumÔºàÁü•ËØÜÂ∑•Á®ã‰πãÁà∂ÔºâÂú®1994Ëé∑ÂæóÂõæÁÅµÂ•ñ„ÄÇ‰∫∫‰ª¨ÊÑèËØÜÂà∞Ôºå‰∏ìÂÆ∂Á≥ªÁªüÈù¢‰∏¥‚ÄúÁü•ËØÜÂ∑•Á®ãÁì∂È¢à‚Äù,Âú®ÈÇ£‰∏™Êó∂ÂÄôÔºåÊúâ‰∫∫ÊääÁü•ËØÜÊÄªÁªìÂá∫Êù•ÂÜçÊïôÁªôËÆ°ÁÆóÊú∫ÊòØÁõ∏ÂΩìÂõ∞ÈöæÁöÑ„ÄÇ1950Âπ¥ÔºåÂõæÁÅµÂÜçÂÖ≥‰∫éÂõæÁÅµÊµãËØïÁöÑÊñáÁ´†‰∏≠ÔºåÊõæÊèêÂà∞Êú∫Âô®Â≠¶‰π†ÁöÑÂèØËÉΩ‰∫åÂçÅ‰∏ñÁ∫™‰∫îÂçÅÂπ¥‰ª£ÂàùÔºåA.SamuelËëóÂêçË∑≥Ê£ãÁ®ãÂ∫è„ÄÇ‰∫îÂçÅÂπ¥‰ª£‰∏≠ÂêéÊúüÔºåÂü∫‰∫éÁ•ûÁªèÁΩëÁªúÁöÑ‚ÄùËøûÊé•‰∏ª‰πâ‚ÄúÂ≠¶‰π†ÔºåÂ¶ÇF.RosenblattÁöÑÊÑüÁü•Âô®ÔºàPerceptroÔºâÔºåB.WidremÁöÑAdaline,ÂÖ≠‰∏ÉÂçÅÂπ¥‰ª£ÔºåÂü∫‰∫éÈÄªËæëË°®Á§∫ÁöÑ‚ÄùÁ¨¶Âè∑‰∏ª‰πâÂ≠¶‰π†ÊäÄÊúØËì¨ÂãÉÂèëÂ±ïÂ≠¶‰π†ÊúüÔºö‰∫åÂçÅ‰∏ñÁ∫™ÂÖ´ÂçÅÂπ¥‰ª£ÊòØÊú∫Âô®Â≠¶‰π†ÁôæËä±ÂàùÊîæÁöÑÊó∂Êúü„ÄÇ‰∏ÄÂ§ß‰∏ªÊµÅÊòØÁ¨¶Âè∑‰∏ª‰πâÂ≠¶‰π†Ôºå‰ª£Ë°®ÂÜ≥Á≠ñÊ†ëÔºàdecision tree).‰∫åÂçÅ‰∏ñÁ∫™‰πùÂçÅÂπ¥‰ª£‰∏≠Êúü‰πãÂâçÔºåÂè¶Â§ñ‰∏ÄÂ§ß‰∏ªÊµÅÊäÄÊúØÊòØÂü∫‰∫éÁ•ûÁªèÁΩëÁªúÁöÑËøûÊé•‰∏ª‰πâÂ≠¶‰π†„ÄÇ‰∫åÂçÅ‰∏ñÁ∫™‰πùÂçÅÂπ¥‰ª£‰∏≠ÊúüÔºå‚ÄùÁªüËÆ°Â≠¶‰π†‚ÄúÂç†ÊçÆ‰∏ªÊµÅÔºå‰ª£Ë°®ÊîØÊåÅÂêëÈáèÊú∫„ÄÇ‰∫åÂçÅ‰∏Ä‰∏ñÁ∫™ÂàùÔºåËøûÊé•‰∏ª‰πâÂ≠¶‰π†ÊéÄËµ∑‰∫Ü‚ÄùÊ∑±Â∫¶Â≠¶‰π†‚Äú‰∏∫ÂêçÁöÑÁÉ≠ÊΩÆ„ÄÇ Á¨¨‰∫åÁ´† Ôºö Ê®°ÂûãËØÑ‰º∞‰∏éÈÄâÊã©ÁªèÈ™åËØØÂ∑Æ‰∏éËøáÊãüÂêà„ÄÅÊ¨†ÊãüÂêàËÆ≠ÁªÉËØØÂ∑ÆÔºàtraining error) or ÁªèÈ™åËØØÂ∑ÆÔºàempirical error): Â≠¶‰π†Âô®Âú®ËÆ≠ÁªÉÈõÜ‰∏äÁöÑËæìÂá∫‰∏éËÆ≠ÁªÉÈõÜ‰πãÈó¥ÁöÑÂ∑ÆÂºÇËøáÊãüÂêàÔºàover fittingÔºâÔºöÂú®ËÆ≠ÁªÉÈõÜ‰∏äË°®Áé∞ÈùûÂ∏∏Â•ΩÔºåÊ≥õÂåñËÉΩÂäõÂ§™Â∑ÆÔºåÊúÄÂ∏∏ËßÅÁöÑÊÉÖÂÜµÊòØÂ≠¶‰π†ËÉΩÂäõÂ§™Âº∫Â≠¶‰π†Âà∞‰∏çÂ§™‰∏ÄËà¨ÁöÑÁâπÊÄßÔºåÊó†Ê≥ïÂΩªÂ∫ïÈÅøÂÖçÔºåÂè™ËÉΩ‚ÄúÁºìËß£‚ÄùÊ¨†ÊãüÂêàÔºàunder fittingÔºâÔºöËøôÁßçÊÉÖÂÜµÂÆπÊòìÂÖãÊúçÊ®°ÂûãÈÄâÊã©(model selection): ‰∏çÂêåÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºå‰∫ßÁîü‰∏çÂêåÁöÑÊ®°Âûã„ÄÇÁêÜËÆ∫‰∏äÊúÄÂ•ΩÁöÑÊ®°ÂûãÊòØÂØπÊ≥õÂåñËÉΩÂäõËøõË°åËØÑ‰º∞ÔºåÊúÄÂ•ΩÁöÑÂ∞±ÊòØÊ≥õÂåñËØØÂ∑ÆÊúÄÂ∞èÁöÑÔºåÊ≥õÂåñËØØÂ∑ÆÊòØÊó†Ê≥ïÁõ¥Êé•Ëé∑ÂèñÁöÑ ËØÑ‰º∞ÊñπÊ≥ïËÆæÁΩÆ‰∏Ä‰∏™‚ÄùÊµãËØïÈõÜÔºàtesting set)‚ÄùÊù•ÊµãËØïÂ≠¶‰π†Âô®Âú®Êñ∞Ê†∑Êú¨ÁöÑÂà§Êñ≠ËÉΩÂäõÔºåÁî®ÊµãËØïËØØÂ∑ÆËøë‰ººÊ≥õÂåñËØØÂ∑ÆË¶ÅÊ±ÇÔºö ÊµãËØïÊ†∑Êú¨‰∏éËÆ≠ÁªÉÊ†∑Êú¨Áã¨Á´ãÂêåÂàÜÂ∏ÉÁöÑ ÊµãËØïÈõÜÂ∫îËØ•Â∞ΩÂèØËÉΩ‰∏éËÆ≠ÁªÉÈõÜ‰∫íÊñ•ÔºåÊµãËØïÊ†∑Êú¨Â∞ΩÈáè‰∏çÂá∫Áé∞Âú®ËÆ≠ÁªÉÈõÜ‰∏≠Â¶Ç‰Ωï‰∫ßÁîütraining set Âíå testing set ÁïôÂá∫Ê≥ïÔºàhold-out)Ë¶ÅÊ±ÇÔºöÊï∞ÊçÆÈõÜ($D$)ÂàíÂàÜÊàê‰∏§‰∏™‰∫íÊñ•ÁöÑÈõÜÂêàÔºàËÆ≠ÁªÉÈõÜ($S$,ÊµãËØïÈõÜ$T$),ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÂàíÂàÜÂêéÔºåÂ∞ΩÈáèÂèØËÉΩÁöÑ‰øùÊåÅÊï∞ÊçÆÂàÜÂ∏ÉÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ‰∏çÂêåÁöÑÂàíÂàÜÁªìÊûúÔºåÂæóÂà∞‰∏çÂêåÁöÑÊµãËØïËØØÂ∑Æ„ÄÇÂçïÊ¨°‰ΩøÁî®ÁïôÂá∫Ê≥ïÂæóÂà∞ÁöÑÁªìÊûúÊòØ‰∏çÂ§üÁ®≥ÂÆöÁöÑÔºåÊâÄ‰ª•‰∏ÄËà¨ÈááÁî®Ëã•Âπ≤Ê¨°ÁöÑÈöèÊú∫ÂàíÂàÜÔºåÈáçÂ§çËøõË°åÂÆûÈ™åËØÑ‰º∞ÂêéÂéªÂπ≥ÂùáÂÄº ‰∫§ÂèâÈ™åËØÅÊ≥ïÔºàcross validation)I. Â∞ÜÊï∞ÊçÆ($D$)ÂàíÂàÜÊàê$k$‰∏™Â§ßÂ∞èÁõ∏‰ººÁöÑ‰∫íÊñ•Â≠êÈõÜÔºåÊØè‰∏™Â≠êÈõÜ$D_i$ÈÉΩÂ∞ΩÂèØËÉΩ‰øùÊåÅÊï∞ÊçÆÂàÜÂ∏ÉÁöÑ‰∏ÄËá¥ÊÄßII. ÊØèÊ¨°ÈÉΩÁî®$k-1$‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜÔºå‰Ωô‰∏ãÁöÑÂì™‰∏™Â≠êÈõÜ‰Ωú‰∏∫ÊµãËØïÈõÜÔºå‰∫éÊòØ‰πéÈÉΩÂà∞‰∫Ük‰∏™ÊµãËØïÁªìÊûúÁöÑÂùáÂÄºÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºå$k$ÁöÑÂèñÂÄºÂØπÁªìÊûúÁöÑÁ®≥ÂÆöÊÄßÂíå‰øùÁúüÊÄßÊúâÂæàÂ§ßÁöÑÂΩ±ÂìçÔºåÂõ†Ê≠§‰πüÂè´kËÄÖ‰∫§ÂèâÈ™åËØÅÔºàk-flold cross validation) kÁöÑÈÄöÂ∏∏ÂèñÂÄºÊòØ10ÂêåÊ†∑ÁöÑÔºåÊï∞ÊçÆÈõÜ$D$ÂàíÂàÜ‰∏∫$k$‰∏™Â≠êÈõÜÊúâÂæàÂ§öÁöÑÂàíÂàÜÊñπÂºèÔºåÂèØÈáçÂ§ç$P$Ê¨°$k$Êäò‰∫§ÂèâÈ™åËØÅ„ÄÇ Ëá™Âä©Ê≥ï (bootstrapping)Ê≥®ÊÑèÁöÑÊòØÊàë‰ª¨Â∏åÊúõÈÄöËøáÊâÄ‰ª•ÁöÑËÆ≠ÁªÉÈõÜÔºà$D$)ËÆ≠ÁªÉÂá∫Ê®°ÂûãÔºå‰ΩÜÊòØÊµÅÂá∫Ê≥ïÂíå‰∫§ÂèâÈ™åËØÅÁöÑÊñπÊ≥ïÔºåÈÉΩ‰øùÁïô‰∏ÄÈÉ®ÂàÜ‰Ωú‰∏∫ÊµãËØïÈõÜÔºåÂõ†Ê≠§ÂÆûÈôÖËØÑ‰º∞ÁöÑÊ®°ÂûãÊâÄ‰ΩøÁî®ÁöÑËÆ≠ÁªÉÈõÜÊõ¥‰∏ãÔºåËøô‰πüËÆ∏‰ºöÂØºËá¥‰º∞ËÆ°ÂÅèÂ∑Æ„ÄÇËá™Âä©Ê≥ïÔºö ÂèØÈáçÂ§çÈááÊ†∑ÊàñËÄÖÊúâÊîæÂõûÈááÊ†∑ ËÆ∞ÈááÊ†∑‰∫ßÁîüÁöÑÊï∞ÊçÆÈõÜÔºà$D‚Äô$),ÊØèÊ¨°‰ªé$D$‰∏≠ÊåëÈÄâÂ∫îËØ•Ê†∑Êú¨ÔºåÂ∞ÜÂÖ∂Êã∑Ë¥ùËá≥($D‚Äô$),Âπ∂ÂÜçÂ∞ÜÈááÊ†∑ÁöÑÊ†∑Êú¨ÊîæÂõûÊï∞ÊçÆÈõÜ($D$),ÈáçÂ§ç($m$)Ê¨°‰ª•ÂêéÔºåÂæóÂà∞‰∫ÜÂåÖÂê´($m$)‰∏™Ê†∑Êú¨ÁöÑÊï∞ÊçÆÈõÜ($D‚Äô$) ÂØπ‰∫éÂèØÈáçÂ§çÈááÊ†∑ÔºåÊ†∑Êú¨ÂßãÁªà‰∏çÈááÂà∞ÁöÑÊ¶ÇÁéáÊòØ$(1-\frac{1}{m})^m$,ÂèñÊûÅÈôêÂæóÂà∞ÔºöÂàùÂºèÊï∞ÊçÆÈõÜ‰∏≠$36.8%$‰∏∫Âá∫Áé∞Âú®ÈááÊ†∑Êï∞ÊçÆÈõÜ‰∏≠ÔºåÂõ†Ê≠§ÂèØÂ∞Ü($D$)‰Ωú‰∏∫ËÆ≠ÁªÉÈõÜÔºå($D\D‚Äô$)‰Ωú‰∏∫ÊµãËØïÈõÜÔºåÂèàÁß∞Â§ñÂåÖ‰º∞ËÆ°(out-of-bag estimate)Ëá™Âä©Ê≥ïÈÄÇÁî®‰∫éÊï∞ÊçÆÈáèÂ∞ëÔºåÈöæÂå∫Âà´ÊµãËØïÈõÜÂíåËÆ≠ÁªÉÈõÜÊó∂ÔºåËá™Âä©Ê≥ï‰ºöÊîπÂèòÂàùÂßãÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÔºåÂú®ÂàùÂßãÊï∞ÊçÆË∂≥Â§üÁöÑÊÉÖÂÜµ‰∏ãÔºåÊµÅÂá∫Ê≥ïÂíå‰∫§ÂèâÈ™åËØÅÊõ¥Â∏∏Áî®‰∏Ä‰∫õ Ë∞ÉÂèÇÂíåÊúÄÁªàÁöÑÊ®°ÂûãÂ≠¶‰π†ÁÆóÊ≥ïÈÉΩÊúâÂèÇÊï∞(parameter),‰∏çÂêåÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºåÂ≠¶ÂæóÊ®°ÂûãÁöÑÊÄßËÉΩ‰πüÂæÄÂæÄ‰∏çÂêåÈ™åËØÅÈõÜ(validation set): Ê®°ÂûãËØÑ‰º∞ÂíåÈÄâÊã©‰∏≠Áî®‰∫é‰º∞ËÆ°ÊµãËØïÁöÑÊï∞ÊçÆÈõÜÁß∞‰∏∫ÁöÑÊï∞ÊçÆÈõÜÂæÄÂæÄÂ∞ÜËÆ≠ÁªÉÈõÜÂàíÂàÜ‰∏∫ËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜÔºåÂü∫‰∫éÈ™åËØÅÈõÜ‰∏äÁöÑÊÄßËÉΩÊù•ËøõË°åÊ®°ÂûãÈÄâÊã©ÂíåË∞ÉÂèÇ ÊÄßËÉΩÂ∫¶Èáè(performance measure)ÂÅáËÆæÊ£ÄÈ™åÔºàÂÖ∂ÂÆûÊàë‰∏ÄÁõ¥ÈÉΩÂπ∂‰∏çÊòØÁâπÂà´‰∫ÜËß£Ôºâ ÂÅáËÆæÊ£ÄÈ™åÁöÑÂü∫Êú¨ÂéüÁêÜÊòØÈáçË¶ÅÁöÑÁªüËÆ°Êé®Êñ≠ÈóÆÈ¢ò‰πã‰∏ÄÔºåÊ†πÊçÆÊ†∑Êú¨Êèê‰æõÁöÑ‰ø°ÊÅØÔºåÊ£ÄÈ™åÂÖ≥‰∫éÊÄª‰ΩìÊüê‰∏™ÂÅáËÆæÊòØÂê¶Ê≠£Á°Æ„ÄÇÂåÖÊã¨ÂèÇÊï∞ÁöÑÂÅáËÆæÊ£ÄÈ™åÔºàÂùáÂÄº„ÄÅÊñπÂ∑ÆÁ≠âÔºâÂíåÈùûÂèÇÊï∞ÔºàÂàÜÂ∏ÉÂïäÔºâÁöÑÂÅáËÆæÊ£ÄÈ™å„ÄÇ ÂèÇÊï∞Ê£ÄÈ™åÔºö ÊèêÂá∫ÂÅáËÆæH‚Äî-&gt;Âú®ÊûÑÈÄ†ÁªüËÆ°ÈáèÔºåÁ°ÆÂÆöÁªüËÆ°ÈáèÁöÑÂàÜÂ∏É‚Äî-&gt; Á°ÆÂÆöÊãíÁªùÂüüÂíåÊé•ÂèóÂüüÁöÑÂàÜÁïåÁ∫ø‚Äî-&gt; Âú®Ê†πÊçÆÊ†∑Êú¨ËÆ°ÁÆóÁªüËÆ°ÈáèÁöÑÂÄºu ‚Äî-&gt; Êé®Êñ≠ ÂàÜÂ∏ÉÊãüÂêàÊ£ÄÈ™å ÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÈÄöËøáÊ¶ÇÁéáËÆ∫ÂàÜÊûêÂØπÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÊúüÊúõÊ≥õÂåñÈîôËØØÁéáËøõË°åÊãÜËß£$x$: ÊµãËØïÊ†∑Êú¨$y_D$Ôºö $x$Âú®Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÊ†áËÆ∞$y$: $x$ÁöÑÁúüÂÆûÊ†áËÆ∞$f(x:D)$: Âú®ËÆ≠ÁªÉÈõÜ‰∏äÂ≠¶ÂæóÁöÑÊ®°Âûã$f$Âú®$x$‰∏äÈ¢ÑÊµãËæìÂá∫‰ª•ÂõûÂΩí‰ªªÂä°‰∏∫‰æãÂ≠êÔºöÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÊúüÊúõÈ¢ÑÊµã‰∏∫Ôºö \hat{f}(x) = E_D[f(x;D)]ÊñπÂ∑ÆÔºöÂ∫¶ÈáèÂêåÊ†∑ÁöÑÊ†∑Êú¨Â§ßÂ∞èÁöÑËÆ≠ÁªÉÈõÜÁöÑÂèòÂä®ÊâÄÂØºËá¥ÁöÑÂ≠¶‰π†ÊÄßËÉΩÁöÑÂèòÂåñÔºåÂç≥ÂàªÁîªÊï∞ÊçÆÊâ∞Âä®ÊâÄÈÄ†ÊàêÁöÑÂΩ±Âìç var(x)= E_D[(f(x;D)-\hat{f}(x))^2]Âô™Â£∞Ôºö Ë°®Ëææ‰∫ÜÂΩìÂâç‰ªªÂä°‰∏ä‰ªªÂä°Â≠¶‰π†ÁÆóÊ≥ïÊâÄËÉΩËææÂà∞ÁöÑÊúüÊúõÊ≥õÂåñËØØÂ∑ÆÁöÑ‰∏ãÁïåÔºåÂç≥ÂàªÁîª‰∫ÜÂ≠¶‰π†ÈóÆÈ¢òÊú¨Ë∫´ÁöÑÈöæÂ∫¶„ÄÇ \epsilon^2=E_D[(y_D-y)^2]ÊúüÊúõËæìÂá∫ÂíåÁúüÂÆûÊ†áËÆ∞ÁöÑÂ∑ÆÂà´Áß∞‰∏∫ÂÅèÂ∑Æ(bias): Â∫¶Èáè‰∫ÜÂ≠¶‰π†ÁÆóÊ≥ïÁöÑÊúüÊúõÈ¢ÑÊµã‰∏éÁúüÂÆûÁªìÊûúÁöÑÂÅèÁ¶ªÁ®ãÂ∫¶ÔºåÂç≥ÂàªÁîª‰∫ÜÂ≠¶‰π†ÁÆóÊ≥ïÊú¨Ë∫´ÁöÑÊãüÂêàËÉΩÂäõ bias^2(x)=(f(x)-y)^2Ëã•ÂÅáËÆæÂô™Â£∞ÊúüÊúõ‰∏∫Èõ∂ÔºåÈÇ£‰πàÁÆóÊ≥ïÁöÑÊúüÊúõÊ≥õÂåñËØØÂ∑ÆÔºö E(f;D)=E_D[(f(x;D)-y)^2]\\ =....=E_D[(f(x;D)-\hat{f}(x))^2]+(\hat{f}(x)-y)^2+E_D[(y_D-y)^2]E(f;D)=bias^2(x)+var(x)+\epsilon^2Áî±‰∏äÂºèÂèØÁü•ÔºåÊ≥õÂåñËÉΩÂäõÁî±Â≠¶‰π†ÁÆóÊ≥ïÁöÑËÉΩÂäõ„ÄÅÊï∞ÊçÆÁöÑÂÖÖÂàÜÊÄß„ÄÅÂ≠¶‰π†‰ªªÂä°Êú¨Ë∫´ÁöÑÈöæÂ∫¶ÂÖ±ÂêåÂÜ≥ÂÆöÁöÑ„ÄÇunderfitting: ÂÅèÂ∑Æ‰∏ªÂØºÊ≥õÂåñËØØÂ∑Æover fittingÔºö ËÆ≠ÁªÉÊï∞ÊçÆÂèëÁîüÁöÑÊâ∞Âä®Ê∏êÊ∏êË¢´Â≠¶‰π†Âà∞ÔºåÊñπÂ∑Æ‰∏ªÂØº‰∫ÜÊ≥õÂåñËØØÂ∑Æ Á¨¨‰∏âÁ´† Á∫øÊÄßÊ®°ÂûãÊàëËá™Â∑±ÂÖ∂ÂÆûÊòØ‰∏ÄÁõ¥ÂÅúÁïôÂú®Á∫øÊÄßÊ®°ÂûãÂ≠¶‰π†ËøáÁ®ãÔºåÂõ†‰∏∫ÊØèÊ¨°ÂºÄÂ§¥ÈÉΩÊòØËøô‰∏ÄÂº†ÔºåÊâÄ‰ª•ÊàëÂ∞±Â≠¶‰π†‰∫ÜÂæàÂ§öÊ¨°„ÄÇËøôÊ¨°‰∏çÂáÜÂ§áÂÜçÁªÜÁúã‰∫Ü„ÄÇ Á∫øÊÄßÂà§Âà´ÂàÜÊûê Linear Discriminant Analysis (LDA)Âü∫Êú¨ÊÄùÊÉ≥Ôºö Âú®ËÆ≠ÁªÉÊ†∑‰æãÈõÜ‰∏äÔºåËÆæÊ≥ïÂ∞ÜÊ†∑Êú¨‰æãÂ≠êÊäïÂΩ±Âà∞‰∏ÄÊù°Áõ¥Á∫ø‰∏ä‰ΩøÂæóÂêåÁ±ªÊ†∑‰æãÁöÑÊäïÂΩ±Â∞ΩÂèØËÉΩÊé•Ëøë„ÄÅÂºÇÁ±ªÊäïÂΩ±ÁÇπÂ∞ΩÂèØËÉΩËøúÁ¶ª„ÄÇÊï∞Â≠¶Ë°®ËææÔºö$D={(x_i,y_i)}_{i=1}^{m}$: data set$X_i$: Á¨¨$i$Á±ªÈõÜÂêà$u_i$: Á¨¨$i$Á±ªÈõÜÂêàÂùáÂÄºÂêëÈáè$\sum{i}$: Á¨¨$i$Á±ªÈõÜÂêàÂçèÊñπÂ∑ÆÁü©Èòµ$ w^Tu_i$Ôºö Á¨¨$i$Á±ªÈõÜÂêàÂú®Áõ¥Á∫ø‰∏äÁöÑÊäïÂΩ±$ w^T\sum_{i}w$: Ê†∑Êú¨ÁÇπÁöÑÂú®Áõ¥Á∫ø‰∏äÁöÑÊäïÂΩ±Â≠¶‰π†ÁÆóÊ≥ïÔºöÂêåÁ±ªÊõ¥ËøëÔºö$\min \sum_{i=1}^{n}(w^T\sum_{i}w)$Á±ª‰∏≠ÂøÉË∂äÂ§ßÔºö$\max ||w^{T}u_1-(\sum_{i=2}(w^{T}u_i))||_2^2$Âõ†Ê≠§ÔºåÊÉ≥ÊúÄÂ§ßÂåñÁöÑÁõÆÊ†áËÄÉËôë$i = 2$ÁöÑÊÉÖÂÜµ J = \frac{||w^Tu_0-w^Tu_1||_2^2}{w^T\sum_{i=1}w+w^T\sum_{i=2}w} =\frac{w^T(u_0-u_1)(u_0-u_1)^Tw}{w^T(\sum_1+\sum_2)w} Â∫îÁî®Á©∫Èó¥Âá†‰ΩïÂíåÁü©ÈòµÁöÑÂÖ≥Á≥ªÊèèËø∞ Á±ªÂÜÖÊï£Â∫¶Áü©Èòµ($S_W$)\sum_1+\sum_2 Á±ªÈó¥Êï£Â∫¶Áü©ÈòµÔºö(u_0-u_1)(u_0-u_1)^T ÊâÄ‰ª•ÔºåÊàë‰ª¨ÊÉ≥‰ºòÂåñÁõÆÊ†áÂ¶Ç‰∏ãÔºöJ = \frac{w^T_Sbw}{w^TS_ww}Â¶Ç‰ΩïÁ°ÆÂÆö$w$Âë¢ÔºüÊ≥®ÊÑèÂà∞ÂàÜÂ≠êÂàÜÊØçÈÉΩÊòØÂÖ≥‰∫é$w$ÁöÑ‰∫åÊ¨°ÂûãÔºåÂõ†Ê≠§Ëß£ËøôÂíåwÁöÑÊñπÂêëÊúâÂÖ≥Á≥ªÔºåÂõ†Ê≠§ÔºåÂèØ‰ª§ $w^TS_ww=1$,‰ºòÂåñÈóÆÈ¢òÂèØÊòØÂ¶Ç‰∏ãÔºö \min -w^TS_bw \\ s.t. w^TS_ww = 1ÊûÑÈÄ†lagrange ÂáΩÊï∞ L = -w^TS_bw+r(w^TS_ww-1)ÂØπ$w$Ê±ÇÂØºÂèØÂæóÔºö S_bw =rS_ww$S_b w$Âíå$ u_0 - u_1 $ ÊñπÂêëÊòØ$u_0-u_1$,‰∏çÂ¶®ËÆæ S_nw=r(u_0-u_1)so,w = s_w^{-1}(u_0-u_1)ËøôÈáåËÄÉËôëÂà∞Êï∞ÂÄºËß£ÁöÑÁ®≥ÂÆöÊÄßÔºåÂõ†Ê≠§ÂæÄÂæÄÊää$S_w$ËøõË°åÂ•áÂºÇÂÄºÂàÜËß£ Á¨¨ÂõõÁ´† ÂÜ≥Á≠ñÊ†ëÂÜ≥Á≠ñÊ†ëÊòØ‰∏ÄÁßçÁâπÂà´ÊôÆÈÄöÁöÑÁ¨¶ÂêàÁîüÊ¥ªÂÅöÂÜ≥Á≠ñÁöÑËøáÁ®ã„ÄÇ Á¨¨‰∫îÁ´† Á•ûÁªèÁΩëÁªúÁ•ûÁªèÁΩëÁªúÊúÄÂºÄÂßãÂá∫Áé∞ÊòØÊ†πÊçÆÁîüÁâ©Á•ûÁªèÁΩëÁªúÊù•ÁöÑ„ÄÇ ÊúÄÁÆÄÂçïÁöÑÁ•ûÁªèÁΩëÁªúÔºöÁ•ûÁªèÂÖÉÊ®°Âûã(neuron|unit)McCulloch and PittsÊäΩË±°Âá∫‚ÄúM-PÁ•ûÁªèÂÖÉÊ®°Âûã‚Äù ÊÑüÁü•Âô®ÔºàPerceptron)ËæìÂÖ•Â±ÇÂíåËæìÂá∫Â±ÇÔºåËæìÂá∫Â±ÇÔºöM-PÁ•ûÁªèÂÖÉÊÑüÁü•Âô®ÁöÑÂ≠¶‰π†ËøáÁ®ã‰∏ÄÂÆöÊòØÊî∂ÊïõÁöÑ Â§öÂ±ÇÂâçÈ¶àÁ•ûÁªèÁΩëÁªú Ôºàmulti-layer feddforward neural networks)ÂâçÈ¶àÔºöÁΩëÁªúÁöÑÊãìÊâëÁªìÊûÑ‰∏çÂ≠òÂú®ÁéØÊàñËÄÖÂõûË∑ØÁ•ûÁªèÂÖÉÁöÑÂ≠¶‰π†ËøáÁ®ãÔºöÂ∞±ÊòØÊ†πÊçÆËÆ≠ÁªÉÊï∞ÊçÆÊù•Ë∞ÉÊï¥Á•ûÁªèÂÖÉ‰πãÈó¥ÁöÑ‚ÄùËøûÊé•ÊùÉ‚Äù(connection weight),‰ª•ÂèäÊØè‰∏™ÂäüËÉΩÁ•ûÁªèÂÖÉÁöÑÈòôÂÄº ËØØÂ∑ÆÈÄÜ‰º†Êí≠ÁÆóÊ≥ïÔºö error BackPropagation (BP)ÂÖ®Â±ÄÊúÄÂ∞èÂíåÂ±ÄÈÉ®ÊúÄÂ∞èÁ•ûÁªèÁΩëÁªúÁöÑËÆ≠ÁªÉËøáÁ®ãÂÖ∂ÂÆû‰πüÂ∞±ÊòØÂèÇÊï∞ÂØª‰ºòÁöÑËøáÁ®ãÔºåÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑÊêúÁ¥†ÊòØ‰ΩøÁî®ÊúÄ‰∏∫ÂπøÊ≥õÁöÑÂèÇÊï∞ÂØª‰ºòÊñπÊ≥ïÔºå‰ΩÜÊòØÂ¶ÇÊûúËØØÂ∑ÆÂáΩÊï∞Âú®ÂΩìÂâçÁÇπÁöÑÊ¢ØÂ∫¶‰∏∫Èõ∂ÔºåÂàôÂæàÊúâÂèØËÉΩËææÂà∞Â±ÄÈÉ®ÊûÅÂ∞è„ÄÇ Á¨¨ÂÖ≠Á´† ÊîØÊåÅÂêëÈáèÊú∫ÊîØÊåÅÂêëÈáèÊú∫ÁöÑÂ≠¶‰π†ÂéüÁêÜÂæàÁÆÄÂçï‰πüÂæàÊúâË∂£Ôºå‰ªéÂàÜÁ±ªÈóÆÈ¢òÔºåÊÄé‰πà‰∏ÄÊ≠•‰∏ÄÊ≠•Âª∫Á´ãÁöÑ‰ºòÂåñÈóÆÈ¢òÔºå‰∏ÄÊ≠•‰∏ÄÊ≠•ÁöÑÂÆåÂñÑ‰ºòÂåñÈóÆÈ¢ò‰ª•ÂèäÊ±ÇËß£Ôºå‰ªéÁ°¨Èó¥ÈöîÂà∞ËΩØÈó¥ÈöîÔºåÂàÜÁ±ªÈóÆÈ¢òÊòØËÄÉËôëÂàÜÂØπÔºåËÄåÂõûÂΩíÈóÆÈ¢òÂ∏åÊúõÈ¢ÑÊµãÂÄºÂíåÂéüÂßãÂÄºÂ∞ΩÂèØËÉΩÁöÑÊé•ËøëÔºåËøôÊ†∑Â∞±ÈÄ†Êàê‰∫ÜÁ∫¶ÊùüÊù°‰ª∂ÔºåÁõÆÊ†áÊÄßÁöÑ‰∏çÂêå„ÄÇ ÊúÄÈáçË¶ÅÁöÑÊòØÂºïÂÖ•‰∫ÜÊ†∏ÊñπÊ≥ïÔºå‰ΩéÁª¥Á©∫Èó¥ÁöÑÈùûÁ∫øÊÄßÂÖ≥Á≥ªÊò†Â∞ÑÊàê‰∫ÜÈ´òÁª¥Á©∫Èó¥Á∫øÊÄßÂÖ≥Á≥ªÔºåËøôÊòØÁâπÂà´ÈáçË¶ÅÁöÑÊÄùÊÉ≥ Á¨¨ÂÖ´Á´† ÈõÜÊàêÂ≠¶‰π†Âü∫Êú¨ÊÄùÊÉ≥ÊûÑÂª∫‰∏ÄÁªÑÂü∫Â≠¶‰π†Âô®Ôºàbase learner)ÔºåÂú®ÁªìÂêà a. Â¶ÇÊûúÈõÜÊàê‰∏≠ÊòØÁõ∏ÂêåÁ±ªÂûãÁöÑ‰∏™‰ΩìÂ≠¶‰π†Âô®ÔºåÂ¶ÇÂÜ≥Á≠ñÊ†ëÔºåÂÖ®ÊòØÁ•ûÁªèÁΩëÁªúÁöÑÈõÜÊàê‚ÄúÂêåË¥®‚ÄùÔºàhomogeneous),‰∏™‰ΩìÂ≠¶‰π†Âô®Âè´Âü∫Â≠¶‰π†Âô® b. ‰∏çÂêåÁöÑÂ≠¶‰π†Âô®ÔºåÂºÇË¥®Ôºàheterogeneous)Ôºå‰∏™‰ΩìÂ≠¶‰π†Âô®Âè´ÁªÑ‰ª∂Â≠¶‰π†Âô® ‰∏∫‰ªÄ‰πàÊúâÊïà Â§öÊ†∑ÊÄßÁöÑÂü∫Â≠¶‰π†Âô® ‰∏çÂêåÁöÑÊ®°ÂûãÂèñÈïøË°•Áü≠ ÊØè‰∏™Âü∫Â≠¶‰π†Âô®ÈÉΩÁäØÈîôËØØÔºåÁªºÂêàËµ∑Êù•ÂèØËÉΩÊÄß‰∏çÂ§ß ‰∏æ‰∏™Ê†óÂ≠ê ‰πüËÆ∏‰∏Ä‰∏™Á∫øÊÄßÊ®°Âûã‰∏çËÉΩÁÆÄÂçïÂàÜÁ±ªÔºå‰ΩÜÊòØÂ§ö‰∏™Á∫øÊÄßÊ®°ÂûãÁªºÂêàÔºåÂèØÂ∞ÜÊï∞ÊçÆÈõÜÊàêÂäüÂàÜÁ±ª ÊûÑÂª∫‰∏çÂêåÁöÑÊú∫Âô®Â≠¶‰π†Q 1: Â¶Ç‰ΩïÂª∫Á´ãÂü∫Â≠¶‰π†Âô® Â∞ΩÈáèÊª°Ë∂≥Â§öÊ†∑ÊÄß M1: ‰∏çÂêåÁöÑÂ≠¶‰π†ÁÆóÊ≥ï M2: Áõ∏ÂêåÂ≠¶‰π†ÁÆóÊ≥ï„ÄÅ‰∏çÂêåÁöÑÂèÇÊï∞ M3: ‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÔºà‰∏çÂêåÁöÑÊ†∑Êú¨Â≠êÈõÜ„ÄÅÊï∞ÊçÆÈõÜ‰∏ä‰∏çÂêåÁöÑÁâπÂæÅÔºâ homogenous ensemble ÈááÁî®Áõ∏ÂêåÁöÑÂ≠¶‰π†ÁÆóÊ≥ï„ÄÅ‰∏çÂêåÁöÑËÆ≠ÁªÉÈõÜ Bagging Boosting Áõ∏ÂêåÁÆóÊ≥ïÔºå‰∏çÂêåÁöÑÂèÇÊï∞ËÆæÁΩÆ Áõ∏ÂêåÁöÑËÆ≠ÁªÉÈõÜÔºå‰∏çÂêåÁöÑÂ≠¶‰π†ÁÆóÊ≥ï Q2: Â¶Ç‰ΩïÁªºÂêàÂë¢Ôºü tÊäïÁ•®Ê≥ïÔºömajority voting weighted voting ËÆ≠ÁªÉ‰∏Ä‰∏™Êñ∞Ê®°ÂûãÁ°ÆÂÆöÂ¶Ç‰ΩïÁªºÂêà Stacking ÂÅèÂ•ΩÁöÑÁÆÄÂçïÊ®°Âûã ÁªºÂêàBagging = Boostrap AGGregatINGÊúâÊîæÂõûÈááÊ†∑ÔºåÂêåË¥®Â≠¶‰π†Âô® ÁÆóÊ≥ï1234567891011Input : ËÆ≠ÁªÉÈõÜ D=&#123;(x1,y1)&#125; Âü∫Â≠¶‰π†ÁÆóÊ≥ïA ËÆ≠ÁªÉËΩÆÊï∞ TËøáÁ®ã for t = 1,2,...,T do h_t= A(D,Dt) // DtÁ¨¨tÊ¨°ÈááÊ†∑ÁöÑÂàÜÂ∏É end forËæìÂá∫ ÂõûÂΩíÔºöAverage ÂàÜÁ±ªÔºöÊäïÁ•®Ê≥ï ‰ºòÁÇπÊ≤°ÊúâÁî®‰∫éÂª∫Ê®°ÁöÑÊ†∑Êú¨ÔºåÂèØ‰ª•Áî®‰ΩúÈ™åËØÅÈõÜÊù•ÂØπÊ≥õÂåñËÉΩÂäõËøõË°åÂåÖÂ§ñ‰º∞ËÆ°ÔºåÂèØ‰ª•ÂæóÂá∫BaggingÊ≥õÂåñËØØÂ∑ÆÁöÑÂåÖÂ§ñ‰º∞ËÆ° random forestÔºàRF)ËæìÂÖ•‰∏∫Ê†∑Êú¨ÈõÜ$D={(x,y1),(x2,y2),‚Ä¶(xm,ym)}$ÔºåÂº±ÂàÜÁ±ªÂô®Ëø≠‰ª£Ê¨°Êï∞T„ÄÇ ËæìÂá∫‰∏∫ÊúÄÁªàÁöÑÂº∫ÂàÜÁ±ªÂô®f(x)f(x) 1ÔºâÂØπ‰∫ét=1,2‚Ä¶,T: a)ÂØπËÆ≠ÁªÉÈõÜËøõË°åÁ¨¨tÊ¨°ÈöèÊú∫ÈááÊ†∑ÔºåÂÖ±ÈááÈõÜmÊ¨°ÔºåÂæóÂà∞ÂåÖÂê´m‰∏™Ê†∑Êú¨ÁöÑÈááÊ†∑ÈõÜ$Dt$ b)Áî®ÈááÊ†∑ÈõÜ$Dt$ËÆ≠ÁªÉÁ¨¨t‰∏™ÂÜ≥Á≠ñÊ†ëÊ®°Âûã$Gt(x)$ÔºåÂú®ËÆ≠ÁªÉÂÜ≥Á≠ñÊ†ëÊ®°ÂûãÁöÑËäÇÁÇπÁöÑÊó∂ÂÄôÔºå Âú®ËäÇÁÇπ‰∏äÊâÄÊúâÁöÑÊ†∑Êú¨ÁâπÂæÅ‰∏≠ÈÄâÊã©‰∏ÄÈÉ®ÂàÜÊ†∑Êú¨ÁâπÂæÅÔºå Âú®Ëøô‰∫õÈöèÊú∫ÈÄâÊã©ÁöÑÈÉ®ÂàÜÊ†∑Êú¨ÁâπÂæÅ‰∏≠ÈÄâÊã©‰∏Ä‰∏™ÊúÄ‰ºòÁöÑÁâπÂæÅÊù•ÂÅöÂÜ≥Á≠ñÊ†ëÁöÑÂ∑¶Âè≥Â≠êÊ†ëÂàíÂàÜ 2) Â¶ÇÊûúÊòØÂàÜÁ±ªÁÆóÊ≥ïÈ¢ÑÊµãÔºåÂàôT‰∏™Âº±Â≠¶‰π†Âô®ÊäïÂá∫ÊúÄÂ§öÁ•®Êï∞ÁöÑÁ±ªÂà´ÊàñËÄÖÁ±ªÂà´‰πã‰∏Ä‰∏∫ÊúÄÁªàÁ±ªÂà´„ÄÇÂ¶ÇÊûúÊòØÂõûÂΩíÁÆóÊ≥ïÔºåT‰∏™Âº±Â≠¶‰π†Âô®ÂæóÂà∞ÁöÑÂõûÂΩíÁªìÊûúËøõË°åÁÆóÊúØÂπ≥ÂùáÂæóÂà∞ÁöÑÂÄº‰∏∫ÊúÄÁªàÁöÑÊ®°ÂûãËæìÂá∫„ÄÇ ÂèÇÊï∞ËÆæÁΩÆ Âà©Áî®00BÊ†∑Êú¨ËØÑ‰º∞ÂèòÈáèÁöÑÈáçË¶ÅÊÄß Boosting ÊèêÈ´òÈ°∫Ê¨°Âª∫Á´ãÂ≠¶‰π†Âô®ÔºåÂ∞±ÊòØÂÖà‰ªéËÆ≠ÁªÉÈõÜ‰∏äËÆ≠ÁªÉ‰∏Ä‰∏™Âü∫Â≠¶‰π†Âô®ÔºåÂÜçÊ†πÊçÆÂ≠¶‰π†Âô®ÁöÑË°®Áé∞ÂØπËÆ≠ÁªÉÈõÜÂàÜÂ∏ÉËøõË°åË∞ÉÊï¥ÔºåËÆ©ÂÖàÂ≠¶‰π†Âô®ÈîôËØØËÆ≠ÁªÉÁöÑÊ†∑Êú¨Âú®ÂêéÁª≠Êî∂Âà∞Êõ¥Â§öÁöÑÂÖ≥Ê≥®ÔºåÁÑ∂ÂêéÂü∫‰∫éË∞ÉÊï¥ÁöÑÂàÜÂ∏ÉËÆ≠ÁªÉ‰∏ã‰∏Ä‰∏™Â≠¶‰π†Âô®ÔºåÊúÄÂêéÔºåÂú®Â∞ÜËøôT‰∏™Â≠¶‰π†Âô®ËøõË°åÂä†ÊùÉÁªìÂêà Âü∫Â≠¶‰π†Âô®ÁöÑÁ∫øÊÄßÁªÑÂêà H_N(x;P)=\sum_{t=1}^{N}\alpha_th_t(x;a_t)$a_t$ÊòØÁ¨¨$i$‰∏™Âº±Â≠¶‰π†Âô®ÁöÑÊúÄ‰ºòÂèÇÊï∞Ôºå$\alpha_t$ÊòØÂú®Âº∫ÂàÜÁ±ªÂô®‰∏≠ÁöÑÊØîÈáçÔºå$P$ÊòØ$a_t$Âíå$\alpha_t$ÁöÑÁªÑÂêà ÊúÄÂ∞èÂåñÊåáÊï∞ÊçüÂ§±ÂáΩÊï∞ l_{exp}(H|D)=E_{x~D}[e^{-f(x)H(x)}] H_n(x)=H_{n-1}(x)+\alpha_{n}h_{n}(x,a_n)l(h_i(x,a_t)|D)=E_{x~D}(exp(-f(x)h_i(x)))\\=p(f(x)=1)exp(-h_i(x))+p(f(x)=-1)exp(h_i(x))\frac{\partial l(h_i(x,a_t)|D)}{\partial h_i(x,a_t)}=\\ -p(f(x)=1)exp(-h_i(x))+p(f(x)=-1)exp(h_i(x))=0h(x)=\frac{1}{2}ln\frac{P(f(x)=1)}{P(f(x)=-1)}ÈááÂèñ‰∏çÂêåÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÂæóÂà∞‰∏çÂêåÁöÑÁ±ªÂûã https://blog.csdn.net/luanpeng825485697/article/details/79383492 GBDTStacking ‰∏çÂêåÂ≠¶‰π†Âô®ÔºåÁõ∏ÂêåÊï∞ÊçÆÈõÜ Á¨¨‰∏ÄÂ±Ç Á¨¨‰∫åÂ±ÇÔºö‰∏çÁî®Á¨¨‰∏ÄÂ±ÇÁöÑÊï∞ÊçÆ ÂèØÁî®‰∫§ÂèâÈ™åËØÅ Ê≥®ÊÑè‰∫ãÈ°πÔºö ËøáÊãüÂêàÈóÆÈ¢òÔºöÁ¨¨‰∫åÂ±ÇÁ∫øÊÄßÂõûÂΩí Á¨¨‰∏ÄÂ±ÇÂ∞ΩÂèØËÉΩÁöÑÂ§öÊ†∑ÊÄßÔºö ÁªºÂêàÂ•ΩÁöÑÊ®°Âûã Èò≤Ê≠¢ËøáÊãüÂêà 1. ÈöèÊú∫ÊÄß 2. Bagging Boosting Stacking ÊûÅÂ§ß‰ººÁÑ∂‰º∞ËÆ°‰ººÁÑ∂Ôºö Áõ∏‰ººÁöÑÊ†∑Â≠ê ÂØπ‰∫é‰∏ÄÁªÑÊï∞ÊçÆÔºåÂÅáËÆæÁ¨¶ÂêàÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÂ∏åÊúõÂ∑≤Áü•ÁÇπÂú®Ëøô‰∏™Ê≠£ÊÄÅÂàÜÂ∏ÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÊâÄÊúâÁÇπÂØπ‰∫éÁöÑÊ¶ÇÁéá‰πãÂíåÊàñËÄÖÁßØÊúÄÂ§ßÔºå ÔºåËìùËâ≤Ë°®Á§∫Êï∞ÊçÆÔºåÁ∫¢Ëâ≤Â∞±ÊòØÂÅöÂæóÊ≠£ÊÄÅÂàÜÂ∏É Á¨¨ÂçÅÁ´† ÈôçÁª¥‰∏éÂ∫¶ÈáèÂ≠¶‰π†kËøëÈÇªÂ≠¶‰π†k-Nearest Neighbor ÂéüÁêÜÔºö Âü∫‰∫éÊüêÁßçË∑ùÁ¶ªÂ∫¶ÈáèÊâæÂá∫ËÆ≠ÁªÉÈõÜ‰∏≠‰∏éÂÖ∂ÊúÄÈù†ËøëÁöÑk‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºåÊ†πÊçÆk‰∏™ÈÇªÂ±ÖÁöÑ‰ø°ÊÅØËøõË°åÈ¢ÑÊµã„ÄÇ ÁªôÂÆöÊµãËØïÊ†∑Êú¨$x$,Â¶ÇÊûúÊúÄÈÇªËøëÊ†∑Êú¨$z$,ÊúÄÈÇªËøëÂàÜÁ±ªÂô®Âá∫ÈîôÁöÑÊ¶ÇÁéáÂ∞±ÊòØ$x$‰∏é$z$‰∏çÂÜçÂêå‰∏ÄÁ±ª p(err) = 1-\sum_{c \in y}p(c|x)P(c|z)‰ΩéÁª¥ÂµåÂÖ•ÁºìËß£Áª¥Êï∞ÁÅæÈöæÁöÑÈáçË¶ÅÈÄîÁªè‰πã‰∏ÄÊòØÈôçÁª¥Ôºàdimension reductionÔºâËøôÊ†∑‰ΩøÂæóÂ≠êÁ©∫Èó¥‰∏≠Ê†∑Êú¨ÂØÜÂ∫¶Â§ßÂπÖÂ∫¶ÊèêÈ´òÔºåË∑ùÁ¶ªËÆ°ÁÆóÂèòÂæóÊõ¥ÂÆπÊòìÔºå Â§öÁª¥Áº©ÊîæÔºàMultiple Dimensional,ScalingÔºâMDS ÂÅáÂÆöm‰∏™Ê†∑Êú¨Âú®ÂéüÂßãÁ©∫Èó¥ÁöÑË∑ùÁ¶ªÁü©Èòµ$D$,Âú®‰ΩéÁª¥Á©∫Èó¥‰∏≠Ôºå‰∏§‰∏™Ê†∑Êú¨Ê¨ßÂºèË∑ùÁ¶ªÁ≠â‰∫éÂéüÁ©∫Èó¥ÁöÑË∑ùÁ¶ªÔºå$||z_i-z_j|| = dist_{ij}$, ‰ª§$B=Z^TZ$‰∏∫ÈôçÁª¥ÂêéÊ†∑Êú¨ÁöÑÂÜÖÁßØÁü©Èòµ, dist_{ij}^2=||z_i||^2+||z_j||^2-2z_iz_j=b_{ii}+b_{jj}-2b_{ij}ÂØπÈôçÁª¥ÂêéÊï∞ÊçÆ‰∏≠ÂøÉÂåñÔºåÂùáÂÄº‰∏∫0,$\sum_{i=1}^{m}z_i$,‰∫éÊòØ‰πéÂ∞±Êúâ$\sum_{i=1}^{M}b_{ij}=z_j(z_1+z_2+‚Ä¶+z_m)=0=\sum_{j=1}^{m}x_{ij}$ ,ÂèØÂæó \sum_{i=1}^{m}dist_{ij}^2=\sum_{i=1}^{m}(b_{ii}+b_{jj}-2b_{ij})=tr(B)_mb_{jj}\\ \sum_{i=1}^{m}\sum_{j=1}^{m}dist_{ij}^2 = 2m tr(B)\\ tr(B)=\sum_{i=1}^{m}||z_i||^2ÂèØÂæó b_{ij}=-\frac{1}{2}(dist_{ij}^2-dist_{i.}^2-dist_{.j}^2+dist{..}^2)ÂØπÁü©ÈòµBÂÅöÁâπÂæÅÂÄºÂàÜËß£(eigenvalue decomposition)Ôºå$B = V \land V$,Âàô Z = \land_{*}^{1/2}V_{*}Ê¨≤Ëé∑Âæó‰ΩéÁª¥Â≠êÁ©∫Èó¥ÔºåÊúÄÁÆÄÂçïÊòØÂØπÂéüÂßãÈ´òÁª¥Á©∫Èó¥ËøõË°åÁ∫øÊÄßÂèòÊç¢Ôºå$Z = W^TX$,ÁâπÂà´ÁöÑÔºå$W$ÂèñÊ≠£‰∫§ÂèòÊç¢Ôºå$W={w_1,w_2,‚Ä¶,w_{d‚Äô}}$WÊòØd‚Äô‰∏™dÁª¥Âü∫ÂêëÈáèÔºå ‰∏ªÊàêÂàÜÂàÜÊûêPrincipal Component Analysis ÔºöPCA Âú®Ê≠£‰∫§Á©∫Èó¥ÈáåÈù¢ÁöÑÊ†∑Êú¨ÔºåÁî®‰∏Ä‰∏™Ë∂ÖÂπ≥Èù¢ÂØπÊ†∑Êú¨ËøõË°åÊÅ∞ÂΩìÁöÑË°®ËææÔºåËá≥Â∞ëËøô‰∏™Ê†∑Êú¨ÁÇπÊª°Ë∂≥ ÊúÄËøëÈáçÊûÑÊÄßÔºö Ê†∑Êú¨ÁÇπÂà∞Ëøô‰∏™Ë∂ÖÂπ≥Èù¢ÁöÑË∑ùÁ¶ªË∂≥Â§üËøë ÊúÄÂ§ßÂèØÂàÜÊÄßÔºö Ê†∑Êú¨ÁÇπÂú®ËøôË∂ÖÂπ≥Èù¢‰∏äÁöÑÊäïÂΩ±Â∞ΩÂèØËÉΩÂàÜÂºÄ ÂØπ‰∫éÊúÄËøëÈáçÊûÑÊÄßÔºö ÂÅáËÆæÊ†∑Êú¨Âéª‰∏≠ÂøÉÂåñÔºåÂÜçÂÅáËÆæÊäïÂΩ±ÂèòÊç¢ÂêéÂæóÂà∞Ê¨£ÁöÑÊ≠£‰∫§ÂùêÊ†áÁ≥ª${w_1,w_2,‚Ä¶,w_d}$,dÁª¥Á©∫Èó¥ÈáåÈù¢ÁöÑ‰∏ÄÁªÑÂçï‰ΩçÊ≠£‰∫§Âü∫Ôºå$||w_i||_2=0$,$||w_i^Tw_j||=0$,Â¶ÇÊûúÂÜçÊñ∞ÂùêÊ†áÁ≥ª‰∏≠‰∏¢Êéâ‰∏ÄÈÉ®ÂàÜÂùêÊ†áÔºåÊ†∑Êú¨ÁÇπÂú®Êñ∞ÂùêÊ†áÁöÑÊäïÂΩ±ÊòØ$z_i={w_1^Tx_{i1}},..,w_{d‚Äô}^Tx_{i}$,‰∫éÊòØÂèà$z_{ij} =w_{j}^Tx_i$,$\hat{x_i}=\sum_{j}^{d‚Äô}w_jx_i$ \sum_{i=1}^{m}||\sum_{j=1}^{d'}z_{ij}w_j-x_i||_2^2=\sum_{i=1}^{m}z_i^Tz_i-2\sum_{i=1}^{m}z_i^TW^Tx_i+x_i^Tx_i\\ =\sum_{i=1}^{m}x_i^TWW^Tx_i-2\sum_{i=1}^{m}x_i^TWW^Tx_i+x_i^Tx_i\\ min -\sum_{i=1}^{m}z_i^Tz_i=-tr(Z^TZ)\\ min -tr(\sum_{i=1}^{m}W^Tx_ix_i^TW)=-tr(W^T(\sum_{i=1}^{m}x_i^Tx_i)WÔºâ=-tr(W^TXX^TW)\\ s.t W^TW = IÂØπ‰∫éÊúÄÂ§ßÂèØÂàÜÊÄß$(W^T\hat{X}=0)$ max tr(W^TXX^TW)\\s.t W^TW = IÊ†πÊçÆlagrange L(W,\lambda)=-tr(W^TXX^TW)-\lambda(W^TW-I)\\ \frac{\partial L}{\partial w_i}=-2w_iXX^T-2\lambda_i w_i=0\\ XX^Tw_i = \lambda w_i$XX^T$ÊòØÂçèÊñπÂ∑ÆÁü©Èòµ,$\lambda$ÊòØÁâπÂæÅÂÄºÔºå$w_i$ÊòØÁâπÂæÅÂêëÈáè ÁâπÂà´ÊèêÁ§∫Ôºå$x$ÈúÄË¶Å‰∏≠ÂøÉÂåñ ÂØπ‰∫éÁ∫øÊÄßPCAÈôçÁª¥ÊñπÊ≥ïÊòØ‰ªéÈ´òÁª¥Á©∫Èó¥Êò†Â∞ÑÂà∞‰ΩéÁª¥Á©∫Èó¥Ôºå$Z= W^TX$,ÁÑ∂ËÄå‰∏çÂ∞ëÊÉÖÂÜµÔºåÂàôÈúÄË¶ÅÈùûÁ∫øÊÄßÊò†Â∞ÑÊâçËÉΩÊâæÂà∞ÊÅ∞ÂΩìÁöÑ‰ΩéÁª¥ÂµåÂÖ•Ôºå $\phi(x)$ \max tr(\phi(X)\phi(X)^T)=tr( W^T\varphi(x)\varphi(x)^TW)\\ W^TW = I‰∫éÊòØÊúâ \varphi(x)^T\varphi(x)w_i=\lambda_iw_i\\ w_i=\frac{tr(\varphi(x)^T\varphi(x))}{\lambda_iw_i} z_j = \frac{\sum_{i=1}^{m}\varphi(x)^T\varphi(x)}{\lambda_iw_i}\varphi(x_i)\ =\frac{\sum_{i=1}^{m}\varphi(x_i)K(x_i,x)}{\lambda_iw_i}ÊµÅÂΩ¢Â≠¶‰π†ÔºàË°®Á§∫Â≠¶‰π†ÊúâÁÇπÂõ∞Èöæ)Á¨¨ÂçÅ‰∏ÄÁ´† ÁâπÂæÅÈÄâÊã©‰∏éÁ®ÄÁñèÂ≠¶‰π†ÂØπ‰∫é‰∏Ä‰∏™Â≠¶‰π†‰ªªÂä°ÔºåÂØπ‰ªªÂä°ÊúâÁî®ÁöÑÁâπÂæÅ,Áß∞‰∏∫‚Äùrelevant feature‚ÄùÔºåÂØπ‰∫éÊ≤°ÊúâÁî®ÁöÑÂ±ûÊÄß‚Äùirrelevant feature‚Äù,Âõ†Ê≠§‰ªéÁªôÂÆöÁâπÂæÅÈõÜÈÄâÊã©Âá∫Áõ∏ÂÖ≥ÁâπÂæÅÂ≠êÈõÜÁöÑËøáÁ®ãÔºåÁâπÂæÅÈÄâÊã©Ôºàfeature selection),ÂéüÂõ†‰∏ÄÔºåÈôçÁª¥ÔºõÂéüÂõ†‰∫åÔºöÈôç‰ΩéÂ≠¶‰π†ÁöÑ‰ªªÂä°„ÄÇ Êó†ÂÖ≥ÁâπÂæÅÔºåÂåÖÊã¨‰∏ÄÁ±ªÂÜó‰ΩôÁâπÂæÅÔºàredundant featureÔºâÔºåËÉΩÂ§ü‰ªéÂÖ∂‰ªñÁâπÂæÅÈáåÈù¢Êé®ÊºîÂá∫Êù•„ÄÇ ÁâπÂæÅÊêúÁ¥¢ÂâçÂêëÔºàforward)ÊêúÁ¥¢ÂØπ‰∫éÁâπÂæÅÈõÜÂêà$\{a_1,a_2,‚Ä¶,a_d \}$,ÊØè‰∏™ÁâπÂæÅÁúã‰Ωú‰∏Ä‰∏™ÂÄôÈÄâÈõÜÔºåÂØπËøô$d$ÂÄôÈÄâÁöÑÂçïÁâπÂæÅÂ≠êÈõÜËøõË°åËØÑ‰ª∑ÔºåÂèØÈÄâÂá∫ÊúÄ‰ºòÂ≠êÈõÜÔºåÁÑ∂ÂêéÔºåÂÜç‰∏ã‰∏ÄËΩÆÂ≠êÈõÜ‰∏≠ÔºåÊûÑÊàê‰∫Ü‰∏§‰∏™ÁâπÂæÅÂÄôÈÄâÁöÑÂ≠êÈõÜÔºå ÂêéÂêë (backward) ÊêúÁ¥¢ÊØèÊ¨°Â∞ùËØïÂéªÊéâ‰∏Ä‰∏™Êó†ÂÖ≥ÁâπÂæÅ ÂèåÂêë(bidirectional)ÊêúÁ¥¢‰∏äËø∞Êìç‰ΩúÂè™ÊòØË¥™ÂøÉÁ≠ñÁï•Ôºå‰ªÖ‰ªÖËÄÉËôë‰∫ÜÊú¨ËΩÆÈÄâÂÆöÈõÜÂêàÊúÄ‰ºò ‚Äã Â≠êÈõÜËØÑ‰ª∑Ôºàsubset evaluation)Â∑≤Áü•‰∏Ä‰∏™Êï∞ÊçÆÈõÜ$D$,ÂÅáÂÆöÁ¨¨$i$Á±ªÊ†∑Êú¨ÊâÄÂç†ÊØî‰æã$p_i$,ÂØπ‰∫éÂ±ûÊÄßÂ≠êÈõÜ$A$,ÂÅáËÆæÊ†πÊçÆÂèñÂÄºDÂàÜÊàêV‰∏™Â≠êÈõÜ$\{D^1,D^2,‚Ä¶,D^V\}$,ÂàôÂ≠êÈõÜAÁöÑ‰ø°ÂøÉ Â¢ûÁõä Gain(A) = Ent(D)-\sum_{i=1}^V\frac{|D^i|}{|D|}Ent(D^i)\\ Ent(D)=\sum_{i=1}^{|y|}p_ilog^{-p_i}‚Äã ‰ø°ÊÅØÂ¢ûÁõäGain(A)Ë∂äÂ§ßÔºåËØ¥ÊòéÁâπÂæÅÂ≠êÈõÜAÂåÖÂê´ÁöÑÊúâÂä©‰∫éÂàÜÁ±ªÁöÑ‰ø°ÊÅØË∂äÂ§öÔºåÁâπÂæÅÂ≠êÈõÜAÊòØÂØπÊï∞ÊçÆÈõÜDÁöÑ‰∏Ä‰∏™ÂàíÂàÜÔºåÊ†∑Êú¨DÁöÑÊ†áËÆ∞‰ø°ÊÅØYÂàôÂØπÂ∫îÁùÄDÁöÑÁúüÂÆûÂàíÂàÜÔºåÂ∞±ËÉΩÂØπAËøõË°åËØÑ‰ª∑ÔºåÂØπYÂØπÂ∫îÁöÑÂàíÂàÜÁöÑÂ∑ÆÂºÇË∂äÂ∞èÔºåÂàôËØ¥ÊòéAË∂äÂ•ΩÔºå ËøáÊª§ÂºèÈÄâÊã©Relief ÔºàRelevant FeatureÔºâ ËÆæËÆ°‰∏Ä‰∏™‚ÄúÁõ∏ÂÖ≥ÁªüËÆ°Èáè‚ÄùÊù•ÊèèËø∞Â∫¶ÈáèÁâπÂæÅÁöÑÈáçË¶ÅÊÄßÔºåËØ•ÁªüËÆ°ÈáèÊòØ‰∏Ä‰∏™ÂêëÈáèÔºåÊØè‰∏™ÂàÜÈáèÂØπÂ∫î‰∏Ä‰∏™ÂàùÂºèÁâπÂæÅÔºåËÄåÁâπÂæÅÂ≠êÈõÜÁöÑÈáçË¶ÅÊÄßÂàôÊòØÊØè‰∏™ÁâπÂæÅÂØπÂ∫îÁªüËÆ°ÈáèÂàÜÈáè‰πãÂíåÊù•ÂÜ≥ÂÆöÔºåÊúÄÁªàÂè™ÈúÄÊåáÂÆö‰∏Ä‰∏™ÈòôÂÄºÔºåÊ†πÊçÆÈòôÂÄºÈÄâÊã©ÁªüËÆ°ÈáèÂàÜÈáèÂØπÂ∫îÁöÑÁâπÂæÅÂç≥ÂèØ Â¶Ç‰ΩïÁ°ÆÂÆöÁõ∏ÂÖ≥ÁªüËÆ°Èáè ÁªôÂÆöËÆ≠ÁªÉÈõÜ$(x_i,y_i)$,ÂØπ‰∫éÂÆû‰æã$x_i$,Âú®ÂÖ∂ÂêåÁ±ªÊ†∑Êú¨‰∏≠ÊâæÊúÄËøëÈÇªÔºànear-hit),Âú®‰ªéÂºÇÁ±ªÊ†∑Êú¨‰∏≠ÂØªÊâæÂÖ∂ÊúÄËøëÈÇª$x_{x,nm}$Áß∞‰∏∫‚ÄúÁåúÈîôËøëÈÇª‚ÄùÔºå \delta^j =\sum_i-diff(x_i^j,x_{i.nh}^j)^2+diff(x_i^j,x_{i,nm}^j)^2ÂàÜÂÄºË∂äÂ§ßÔºåËØ¥ÊòéÂØπÂ∫îÂ±ûÊÄßÁöÑÂàÜÁ±ªËÉΩÂäõË∂äÂº∫ ÂØπ‰∫éÂ§öÂàÜÁ±ªÈóÆÈ¢ò \delta^j = \sum_i-diff(x_i^j,x_{i,nh}^j)^2+\sum_{l \neq k}p_l\ diff(x_i^j,x_{i,l,nm}^j)ËøôÁßçÊñπÊ≥ïÁúã‰∏Ä‰∏™Â±ûÊÄßÔºàÁâπÂæÅÔºâÈáç‰∏çÈáçË¶ÅÔºåÂÖàËÆ°ÁÆóÂá∫ÊØè‰∏™Â±ûÊÄßÁöÑÁªüËÆ°ÂàÜÈáèÔºåÊåâÁÖßÂÖ¨ÂºèÔºåÂ≠êÈõÜÁöÑËØÑ‰ª∑Â∞±ÊòØÂØπ‰∫éÂàÜÈáèÁöÑÂíå ÂåÖË£πÂºèÈÄâÊã©Áõ¥Êé•ÊääÊúÄÁªàÂ∞ÜË¶Å‰ΩøÁî®ÁöÑÂ≠¶‰π†Âô®ÁöÑÊÄßËÉΩ‰Ωú‰∏∫ÁâπÂæÅÂ≠êÈõÜÁöÑËØÑ‰ª∑ÂáÜÂàôÔºåÁâπÂæÅÈÄâÊã©ÁöÑÁõÆÁöÑÂ∞±ÊòØ‰∏∫ÁªôÂÆöÂ≠¶‰π†ÊúüÈÄâÊã©ÊúâÂà©ÂÖ∂ÊÄßËÉΩÁöÑÁâπÂæÅÂ≠êÈõÜ„ÄÇ LVWÔºàLas Vegas WrapperÔºâÊòØÂÖ∏ÂûãÁöÑÂåÖË£πÂºèÁâπÂæÅÈÄâÊã©ÊñπÊ≥ïÔºåÊãâÊñØÁª¥Âä†ÊñØÊñπÊ≥ïÔºàLas Vegas methodÔºâÊ°ÜÊû∂‰∏ã‰ΩøÁî®ÈöèÊú∫Á≠ñÁï•Êù•ËøõË°åÂ≠êÈõÜÊêúÁ¥¢ÔºåÂπ∂‰ª•ÊúÄÁªàÂàÜÁ±ªÂô®ÁöÑËØØÂ∑Æ‰∏∫ÁâπÂæÅÂ≠êÈõÜËØÑ‰ª∑ÂáÜÂàô ÁÆóÊ≥ï ÂµåÂÖ•ÂºèÈÄâÊã©Â≠¶‰π†Âô®Ëá™Âä®Âú∞ËøõË°åÁâπÂæÅÈÄâÊã© L-PËåÉÊï∞ L_P = ||X||_P = p\sqrt{\sum_{i=1}^{n}x_i^p} L0ËåÉÊï∞ ||X||_0=ÂêëÈáè‰∏≠ÈùûÈõ∂ÂÖÉÁ¥†ÁöÑ‰∏™Êï∞L1ËåÉÊï∞ ||x||_1 = \sum|x_i|L2ËåÉÊï∞ÔºåÊúÄÂ∏∏Áî® ||X||_2=\sqrt{x_i^2}Êó†Á©∑ËåÉÊï∞ ||x||=max|x_i|ÂØπ‰∫éÁ∫øÊÄßÂõûÂΩíÊ®°ÂûãÔºåÈò≤Ê≠¢ËøáÊãüÂêàÔºåÂ¶ÇÊûú‰ΩøÁî®L2,Áß∞‰∏∫Â≤≠ÂõûÂΩí(ridge regression),Â¶ÇÊûúÈááÂèñL1ËåÉÊï∞ÔºåÂàôÊúâÁß∞‰∏∫LASSOÔºåL1ÊØîL2Êõ¥Êòì‰∫éÁ®ÄÁñèËß£ÔºåÂèØ‰ª•ÁúãÂæóÂá∫L1ËåÉÊï∞Ê≠£ÂàôÂåñÁöÑËøáÁ®ãÂæóÂà∞‰∫Ü‰ªÖÈááÁî®‰∏ÄÈÉ®ÂàÜÂàùÂßãÂåñÁâπÂæÅÁöÑÊ®°Âûã„ÄÇ L1Ê≠£ÂàôÂåñÊ±ÇËß£ÂèØ‰ΩøÁî®ËøëÁ´ØÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï(Proximal Gradient Descent)PGD L-LipschitzÊù°‰ª∂ ËÆæÂáΩÊï∞$Œ¶(x)$Âú®ÊúâÈôê Âå∫Èó¥$[a,b]$‰∏äÊª°Ë∂≥Â¶Ç‰∏ãÊù°‰ª∂Ôºö (1) ÂΩì$x‚àà[a,b]$Êó∂Ôºå$Œ¶(x)‚àà[a,b]$ÔºåÂç≥$a‚â§Œ¶(x)‚â§b$. (2) ÂØπ‰ªªÊÑèÁöÑ$x1Ôºåx2‚àà[a,b]$Ôºå ÊÅíÊàêÁ´ãÔºö$|Œ¶(x1)-Œ¶(x2)|‚â§L|x1-x2|$. Â¶ÇÊûú$f(x)$ÂèØÂØºÔºåÂπ∂‰∏î$\nabla f$Êª°Ë∂≥L-LipschitzÊù°‰ª∂Ôºå ||\nabla f(x')-\nabla f(x)||_2^2]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Ë•øÁìú‰π¶</tag>
      </tags>
  </entry>
</search>
